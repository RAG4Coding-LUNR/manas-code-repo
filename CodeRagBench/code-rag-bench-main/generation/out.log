nohup: ignoring input
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Selected Tasks: ['odex-en']
Loading model in bf16
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 84.07it/s]
Loading dataset ..
Loading dataset ..
number of problems for this task is 100
  0%|          | 0/100 [00:00<?, ?it/s]
def f_3283984():
    """decode a hex string '4a4b4c' to UTF-8.
    """
    return  
 --------------------

def f_3844801(myList):
    """check if all elements in list `myList` are identical
    """
    return  
 --------------------

def f_4302166():
    """format number of spaces between strings `Python`, `:` and `Very Good` to be `20`
    """
    return  
 --------------------

def f_7555335(d):
    """convert a string `d` from CP-1251 to UTF-8
    """
    return  
 --------------------

def f_2544710(kwargs):
    """get rid of None values in dictionary `kwargs`
    """
    return  
 --------------------

def f_2544710(kwargs):
    """get rid of None values in dictionary `kwargs`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
pwd.getpwall()  
Return a list of all available password database entries, in arbitrary order.
subprocess.getoutput(cmd)  
Return output (stdout and stderr) of executing cmd in a shell. Like getstatusoutput(), except the exit code is ignored and the return value is a string containing the command’s output. Example: >>> subprocess.getoutput('ls /bin/ls')
'/bin/ls'
 Availability: POSIX & Windows.  Changed in version 3.3.4: Windows support added
test.support.script_helper.kill_python(p)  
Run the given subprocess.Popen process until completion and return stdout.
classmatplotlib.dviread.PsFont(texname, psname, effects, encoding, filename)[source]
 
Bases: tuple Create new instance of PsFont(texname, psname, effects, encoding, filename)   effects
 
Alias for field number 2 
   encoding
 
Alias for field number 3 
   filename
 
Alias for field number 4 
   psname
 
Alias for field number 1 
   texname
 
Alias for field number 0
numpy.distutils.exec_command.filepath_from_subprocess_output   distutils.exec_command.filepath_from_subprocess_output(output)[source]
 
Convert bytes in the encoding used by a subprocess into a filesystem-appropriate str. Inherited from exec_command, and possibly incorrect.
def f_14971373():
    """capture final output of a chain of system commands `ps -ef | grep something | wc -l`
    """
    return  
 --------------------

def f_6726636():
    """concatenate a list of strings `['a', 'b', 'c']`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
skimage.segmentation.join_segmentations(s1, s2) [source]
 
Return the join of the two input segmentations. The join J of S1 and S2 is defined as the segmentation in which two voxels are in the same segment if and only if they are in the same segment in both S1 and S2.  Parameters 
 
s1, s2numpy arrays 

s1 and s2 are label fields of the same shape.    Returns 
 
jnumpy array 

The join segmentation of s1 and s2.     Examples >>> from skimage.segmentation import join_segmentations
>>> s1 = np.array([[0, 0, 1, 1],
...                [0, 2, 1, 1],
...                [2, 2, 2, 1]])
>>> s2 = np.array([[0, 1, 1, 0],
...                [0, 1, 1, 0],
...                [0, 1, 1, 1]])
>>> join_segmentations(s1, s2)
array([[0, 1, 3, 2],
       [0, 5, 3, 2],
       [4, 5, 5, 3]])
numpy.intersect1d   numpy.intersect1d(ar1, ar2, assume_unique=False, return_indices=False)[source]
 
Find the intersection of two arrays. Return the sorted, unique values that are in both of the input arrays.  Parameters 
 
ar1, ar2array_like


Input arrays. Will be flattened if not already 1D.  
assume_uniquebool


If True, the input arrays are both assumed to be unique, which can speed up the calculation. If True but ar1 or ar2 are not unique, incorrect results and out-of-bounds indices could result. Default is False.  
return_indicesbool


If True, the indices which correspond to the intersection of the two arrays are returned. The first instance of a value is used if there are multiple. Default is False.  New in version 1.15.0.     Returns 
 
intersect1dndarray


Sorted 1D array of common and unique elements.  
comm1ndarray


The indices of the first occurrences of the common values in ar1. Only provided if return_indices is True.  
comm2ndarray


The indices of the first occurrences of the common values in ar2. Only provided if return_indices is True.      See also  numpy.lib.arraysetops

Module with a number of other functions for performing set operations on arrays.    Examples >>> np.intersect1d([1, 3, 4, 3], [3, 1, 2, 1])
array([1, 3])
 To intersect more than two arrays, use functools.reduce: >>> from functools import reduce
>>> reduce(np.intersect1d, ([1, 3, 4, 3], [3, 1, 2, 1], [6, 3, 4, 2]))
array([3])
 To return the indices of the values common to the input arrays along with the intersected values: >>> x = np.array([1, 1, 2, 3, 4])
>>> y = np.array([2, 1, 4, 6])
>>> xy, x_ind, y_ind = np.intersect1d(x, y, return_indices=True)
>>> x_ind, y_ind
(array([0, 2, 4]), array([1, 0, 2]))
>>> xy, x[x_ind], y[y_ind]
(array([1, 2, 4]), array([1, 2, 4]), array([1, 2, 4]))
pandas.Series.combine_first   Series.combine_first(other)[source]
 
Update null elements with value in the same location in ‘other’. Combine two Series objects by filling null values in one Series with non-null values from the other Series. Result index will be the union of the two indexes.  Parameters 
 
other:Series


The value(s) to be used for filling null values.    Returns 
 Series

The result of combining the provided Series with the other object.      See also  Series.combine

Perform element-wise operation on two Series using a given function.    Examples 
>>> s1 = pd.Series([1, np.nan])
>>> s2 = pd.Series([3, 4, 5])
>>> s1.combine_first(s2)
0    1.0
1    4.0
2    5.0
dtype: float64
  Null values still persist if the location of that null value does not exist in other 
>>> s1 = pd.Series({'falcon': np.nan, 'eagle': 160.0})
>>> s2 = pd.Series({'eagle': 200.0, 'duck': 30.0})
>>> s1.combine_first(s2)
duck       30.0
eagle     160.0
falcon      NaN
dtype: float64
pandas.Series.combine   Series.combine(other, func, fill_value=None)[source]
 
Combine the Series with a Series or scalar according to func. Combine the Series and other using func to perform elementwise selection for combined Series. fill_value is assumed when value is missing at some index from one of the two objects being combined.  Parameters 
 
other:Series or scalar


The value(s) to be combined with the Series.  
func:function


Function that takes two scalars as inputs and returns an element.  
fill_value:scalar, optional


The value to assume when an index is missing from one Series or the other. The default specifies to use the appropriate NaN value for the underlying dtype of the Series.    Returns 
 Series

The result of combining the Series with the other object.      See also  Series.combine_first

Combine Series values, choosing the calling Series’ values first.    Examples Consider 2 Datasets s1 and s2 containing highest clocked speeds of different birds. 
>>> s1 = pd.Series({'falcon': 330.0, 'eagle': 160.0})
>>> s1
falcon    330.0
eagle     160.0
dtype: float64
>>> s2 = pd.Series({'falcon': 345.0, 'eagle': 200.0, 'duck': 30.0})
>>> s2
falcon    345.0
eagle     200.0
duck       30.0
dtype: float64
  Now, to combine the two datasets and view the highest speeds of the birds across the two datasets 
>>> s1.combine(s2, max)
duck        NaN
eagle     200.0
falcon    345.0
dtype: float64
  In the previous example, the resulting value for duck is missing, because the maximum of a NaN and a float is a NaN. So, in the example, we set fill_value=0, so the maximum value returned will be the value from some dataset. 
>>> s1.combine(s2, max, fill_value=0)
duck       30.0
eagle     200.0
falcon    345.0
dtype: float64
numpy.in1d   numpy.in1d(ar1, ar2, assume_unique=False, invert=False)[source]
 
Test whether each element of a 1-D array is also present in a second array. Returns a boolean array the same length as ar1 that is True where an element of ar1 is in ar2 and False otherwise. We recommend using isin instead of in1d for new code.  Parameters 
 
ar1(M,) array_like


Input array.  
ar2array_like


The values against which to test each value of ar1.  
assume_uniquebool, optional


If True, the input arrays are both assumed to be unique, which can speed up the calculation. Default is False.  
invertbool, optional


If True, the values in the returned array are inverted (that is, False where an element of ar1 is in ar2 and True otherwise). Default is False. np.in1d(a, b, invert=True) is equivalent to (but is faster than) np.invert(in1d(a, b)).  New in version 1.8.0.     Returns 
 
in1d(M,) ndarray, bool


The values ar1[in1d] are in ar2.      See also  isin

Version of this function that preserves the shape of ar1.  numpy.lib.arraysetops

Module with a number of other functions for performing set operations on arrays.    Notes in1d can be considered as an element-wise function version of the python keyword in, for 1-D sequences. in1d(a, b) is roughly equivalent to np.array([item in b for item in a]). However, this idea fails if ar2 is a set, or similar (non-sequence) container: As ar2 is converted to an array, in those cases asarray(ar2) is an object array rather than the expected array of contained values.  New in version 1.4.0.  Examples >>> test = np.array([0, 1, 2, 5, 0])
>>> states = [0, 2]
>>> mask = np.in1d(test, states)
>>> mask
array([ True, False,  True, False,  True])
>>> test[mask]
array([0, 2, 0])
>>> mask = np.in1d(test, states, invert=True)
>>> mask
array([False,  True, False,  True, False])
>>> test[mask]
array([1, 5])
def f_18079563(s1, s2):
    """find intersection data between series `s1` and series `s2`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
flush_headers()  
Finally send the headers to the output stream and flush the internal headers buffer.  New in version 3.3.
FileResponse.set_headers(open_file)  
This method is automatically called during the response initialization and set various headers (Content-Length, Content-Type, and Content-Disposition) depending on open_file.
end_headers()  
Adds a blank line (indicating the end of the HTTP headers in the response) to the headers buffer and calls flush_headers().  Changed in version 3.2: The buffered headers are written to the output stream.
HTTPConnection.endheaders(message_body=None, *, encode_chunked=False)  
Send a blank line to the server, signalling the end of the headers. The optional message_body argument can be used to pass a message body associated with the request. If encode_chunked is True, the result of each iteration of message_body will be chunk-encoded as specified in RFC 7230, Section 3.3.1. How the data is encoded is dependent on the type of message_body. If message_body implements the buffer interface the encoding will result in a single chunk. If message_body is a collections.abc.Iterable, each iteration of message_body will result in a chunk. If message_body is a file object, each call to .read() will result in a chunk. The method automatically signals the end of the chunk-encoded data immediately after message_body.  Note Due to the chunked encoding specification, empty chunks yielded by an iterator body will be ignored by the chunk-encoder. This is to avoid premature termination of the read of the request by the target server due to malformed encoding.   New in version 3.6: Chunked encoding support. The encode_chunked parameter was added.
do_HEAD()  
This method serves the 'HEAD' request type: it sends the headers it would send for the equivalent GET request. See the do_GET() method for a more complete explanation of the possible headers.
def f_8315209(client):
    """sending http headers to `client`
    """
     
 --------------------
Please refer to the following documentation to generate the code:
class When(condition=None, then=None, **lookups)
tty.setraw(fd, when=termios.TCSAFLUSH)  
Change the mode of the file descriptor fd to raw. If when is omitted, it defaults to termios.TCSAFLUSH, and is passed to termios.tcsetattr().
write_sys_ex() 
 writes a timestamped system-exclusive midi message. write_sys_ex(when, msg) -> None  Writes a timestamped system-exclusive midi message.     
Parameters:

 
msg (list[int] or str) -- midi message 
when -- timestamp in milliseconds      Example: midi_output.write_sys_ex(0, '\xF0\x7D\x10\x11\x12\x13\xF7')

# is equivalent to

midi_output.write_sys_ex(pygame.midi.time(),
                         [0xF0, 0x7D, 0x10, 0x11, 0x12, 0x13, 0xF7])
pandas.DataFrame.asof   DataFrame.asof(where, subset=None)[source]
 
Return the last row(s) without any NaNs before where. The last row (for each element in where, if list) without any NaN is taken. In case of a DataFrame, the last row without NaN considering only the subset of columns (if not None) If there is no good value, NaN is returned for a Series or a Series of NaN values for a DataFrame  Parameters 
 
where:date or array-like of dates


Date(s) before which the last row(s) are returned.  
subset:str or array-like of str, default None


For DataFrame, if not None, only use these columns to check for NaNs.    Returns 
 scalar, Series, or DataFrame

The return can be:  scalar : when self is a Series and where is a scalar Series: when self is a Series and where is an array-like, or when self is a DataFrame and where is a scalar DataFrame : when self is a DataFrame and where is an array-like  Return scalar, Series, or DataFrame.      See also  merge_asof

Perform an asof merge. Similar to left join.    Notes Dates are assumed to be sorted. Raises if this is not the case. Examples A Series and a scalar where. 
>>> s = pd.Series([1, 2, np.nan, 4], index=[10, 20, 30, 40])
>>> s
10    1.0
20    2.0
30    NaN
40    4.0
dtype: float64
  
>>> s.asof(20)
2.0
  For a sequence where, a Series is returned. The first value is NaN, because the first element of where is before the first index value. 
>>> s.asof([5, 20])
5     NaN
20    2.0
dtype: float64
  Missing values are not considered. The following is 2.0, not NaN, even though NaN is at the index location for 30. 
>>> s.asof(30)
2.0
  Take all columns into consideration 
>>> df = pd.DataFrame({'a': [10, 20, 30, 40, 50],
...                    'b': [None, None, None, None, 500]},
...                   index=pd.DatetimeIndex(['2018-02-27 09:01:00',
...                                           '2018-02-27 09:02:00',
...                                           '2018-02-27 09:03:00',
...                                           '2018-02-27 09:04:00',
...                                           '2018-02-27 09:05:00']))
>>> df.asof(pd.DatetimeIndex(['2018-02-27 09:03:30',
...                           '2018-02-27 09:04:30']))
                      a   b
2018-02-27 09:03:30 NaN NaN
2018-02-27 09:04:30 NaN NaN
  Take a single column into consideration 
>>> df.asof(pd.DatetimeIndex(['2018-02-27 09:03:30',
...                           '2018-02-27 09:04:30']),
...         subset=['a'])
                         a   b
2018-02-27 09:03:30   30.0 NaN
2018-02-27 09:04:30   40.0 NaN
pandas.Series.asof   Series.asof(where, subset=None)[source]
 
Return the last row(s) without any NaNs before where. The last row (for each element in where, if list) without any NaN is taken. In case of a DataFrame, the last row without NaN considering only the subset of columns (if not None) If there is no good value, NaN is returned for a Series or a Series of NaN values for a DataFrame  Parameters 
 
where:date or array-like of dates


Date(s) before which the last row(s) are returned.  
subset:str or array-like of str, default None


For DataFrame, if not None, only use these columns to check for NaNs.    Returns 
 scalar, Series, or DataFrame

The return can be:  scalar : when self is a Series and where is a scalar Series: when self is a Series and where is an array-like, or when self is a DataFrame and where is a scalar DataFrame : when self is a DataFrame and where is an array-like  Return scalar, Series, or DataFrame.      See also  merge_asof

Perform an asof merge. Similar to left join.    Notes Dates are assumed to be sorted. Raises if this is not the case. Examples A Series and a scalar where. 
>>> s = pd.Series([1, 2, np.nan, 4], index=[10, 20, 30, 40])
>>> s
10    1.0
20    2.0
30    NaN
40    4.0
dtype: float64
  
>>> s.asof(20)
2.0
  For a sequence where, a Series is returned. The first value is NaN, because the first element of where is before the first index value. 
>>> s.asof([5, 20])
5     NaN
20    2.0
dtype: float64
  Missing values are not considered. The following is 2.0, not NaN, even though NaN is at the index location for 30. 
>>> s.asof(30)
2.0
  Take all columns into consideration 
>>> df = pd.DataFrame({'a': [10, 20, 30, 40, 50],
...                    'b': [None, None, None, None, 500]},
...                   index=pd.DatetimeIndex(['2018-02-27 09:01:00',
...                                           '2018-02-27 09:02:00',
...                                           '2018-02-27 09:03:00',
...                                           '2018-02-27 09:04:00',
...                                           '2018-02-27 09:05:00']))
>>> df.asof(pd.DatetimeIndex(['2018-02-27 09:03:30',
...                           '2018-02-27 09:04:30']))
                      a   b
2018-02-27 09:03:30 NaN NaN
2018-02-27 09:04:30 NaN NaN
  Take a single column into consideration 
>>> df.asof(pd.DatetimeIndex(['2018-02-27 09:03:30',
...                           '2018-02-27 09:04:30']),
...         subset=['a'])
                         a   b
2018-02-27 09:03:30   30.0 NaN
2018-02-27 09:04:30   40.0 NaN
def f_26153795(when):
    """Format a datetime string `when` to extract date only
    """
    return  
 --------------------

def f_172439(inputString):
    """split a multi-line string `inputString` into separate strings
    """
    return  
 --------------------

def f_172439():
    """Split a multi-line string ` a \n b \r\n c ` by new line character `\n`
    """
    return  
 --------------------

def f_13954222(b):
    """concatenate elements of list `b` by a colon ":"
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
numpy.matrix.sum method   matrix.sum(axis=None, dtype=None, out=None)[source]
 
Returns the sum of the matrix elements, along the given axis. Refer to numpy.sum for full documentation.  See also  numpy.sum
  Notes This is the same as ndarray.sum, except that where an ndarray would be returned, a matrix object is returned instead. Examples >>> x = np.matrix([[1, 2], [4, 3]])
>>> x.sum()
10
>>> x.sum(axis=1)
matrix([[3],
        [7]])
>>> x.sum(axis=1, dtype='float')
matrix([[3.],
        [7.]])
>>> out = np.zeros((2, 1), dtype='float')
>>> x.sum(axis=1, dtype='float', out=np.asmatrix(out))
matrix([[3.],
        [7.]])
numpy.ndarray.sum method   ndarray.sum(axis=None, dtype=None, out=None, keepdims=False, initial=0, where=True)
 
Return the sum of the array elements over the given axis. Refer to numpy.sum for full documentation.  See also  numpy.sum

equivalent function
numpy.trace   numpy.trace(a, offset=0, axis1=0, axis2=1, dtype=None, out=None)[source]
 
Return the sum along diagonals of the array. If a is 2-D, the sum along its diagonal with the given offset is returned, i.e., the sum of elements a[i,i+offset] for all i. If a has more than two dimensions, then the axes specified by axis1 and axis2 are used to determine the 2-D sub-arrays whose traces are returned. The shape of the resulting array is the same as that of a with axis1 and axis2 removed.  Parameters 
 
aarray_like


Input array, from which the diagonals are taken.  
offsetint, optional


Offset of the diagonal from the main diagonal. Can be both positive and negative. Defaults to 0.  
axis1, axis2int, optional


Axes to be used as the first and second axis of the 2-D sub-arrays from which the diagonals should be taken. Defaults are the first two axes of a.  
dtypedtype, optional


Determines the data-type of the returned array and of the accumulator where the elements are summed. If dtype has the value None and a is of integer type of precision less than the default integer precision, then the default integer precision is used. Otherwise, the precision is the same as that of a.  
outndarray, optional


Array into which the output is placed. Its type is preserved and it must be of the right shape to hold the output.    Returns 
 
sum_along_diagonalsndarray


If a is 2-D, the sum along the diagonal is returned. If a has larger dimensions, then an array of sums along diagonals is returned.      See also  
diag, diagonal, diagflat

  Examples >>> np.trace(np.eye(3))
3.0
>>> a = np.arange(8).reshape((2,2,2))
>>> np.trace(a)
array([6, 8])
 >>> a = np.arange(24).reshape((2,2,2,3))
>>> np.trace(a).shape
(2, 3)
numpy.recarray.sum method   recarray.sum(axis=None, dtype=None, out=None, keepdims=False, initial=0, where=True)
 
Return the sum of the array elements over the given axis. Refer to numpy.sum for full documentation.  See also  numpy.sum

equivalent function
numpy.ma.sum   ma.sum(self, axis=None, dtype=None, out=None, keepdims=<no value>) = <numpy.ma.core._frommethod object>
 
Return the sum of the array elements over the given axis. Masked elements are set to 0 internally. Refer to numpy.sum for full documentation.  See also  numpy.ndarray.sum

corresponding function for ndarrays  numpy.sum

equivalent function    Examples >>> x = np.ma.array([[1,2,3],[4,5,6],[7,8,9]], mask=[0] + [1,0]*4)
>>> x
masked_array(
  data=[[1, --, 3],
        [--, 5, --],
        [7, --, 9]],
  mask=[[False,  True, False],
        [ True, False,  True],
        [False,  True, False]],
  fill_value=999999)
>>> x.sum()
25
>>> x.sum(axis=1)
masked_array(data=[4, 5, 16],
             mask=[False, False, False],
       fill_value=999999)
>>> x.sum(axis=0)
masked_array(data=[8, 5, 12],
             mask=[False, False, False],
       fill_value=999999)
>>> print(type(x.sum(axis=0, dtype=np.int64)[0]))
<class 'numpy.int64'>
def f_13567345(a):
    """Calculate sum over all rows of 2D numpy array `a`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
warnings.resetwarnings()  
Reset the warnings filter. This discards the effect of all previous calls to filterwarnings(), including that of the -W command line options and calls to simplefilter().
logging.captureWarnings(capture)  
This function is used to turn the capture of warnings by logging on and off. If capture is True, warnings issued by the warnings module will be redirected to the logging system. Specifically, a warning will be formatted using warnings.formatwarning() and the resulting string logged to a logger named 'py.warnings' with a severity of WARNING. If capture is False, the redirection of warnings to the logging system will stop, and warnings will be redirected to their original destinations (i.e. those in effect before captureWarnings(True) was called).
numpy.testing.suppress_warnings   class numpy.testing.suppress_warnings(forwarding_rule='always')[source]
 
Context manager and decorator doing much the same as warnings.catch_warnings. However, it also provides a filter mechanism to work around https://bugs.python.org/issue4180. This bug causes Python before 3.4 to not reliably show warnings again after they have been ignored once (even within catch_warnings). It means that no “ignore” filter can be used easily, since following tests might need to see the warning. Additionally it allows easier specificity for testing warnings and can be nested.  Parameters 
 
forwarding_rulestr, optional


One of “always”, “once”, “module”, or “location”. Analogous to the usual warnings module filter mode, it is useful to reduce noise mostly on the outmost level. Unsuppressed and unrecorded warnings will be forwarded based on this rule. Defaults to “always”. “location” is equivalent to the warnings “default”, match by exact location the warning warning originated from.     Notes Filters added inside the context manager will be discarded again when leaving it. Upon entering all filters defined outside a context will be applied automatically. When a recording filter is added, matching warnings are stored in the log attribute as well as in the list returned by record. If filters are added and the module keyword is given, the warning registry of this module will additionally be cleared when applying it, entering the context, or exiting it. This could cause warnings to appear a second time after leaving the context if they were configured to be printed once (default) and were already printed before the context was entered. Nesting this context manager will work as expected when the forwarding rule is “always” (default). Unfiltered and unrecorded warnings will be passed out and be matched by the outer level. On the outmost level they will be printed (or caught by another warnings context). The forwarding rule argument can modify this behaviour. Like catch_warnings this context manager is not threadsafe. Examples With a context manager: with np.testing.suppress_warnings() as sup:
    sup.filter(DeprecationWarning, "Some text")
    sup.filter(module=np.ma.core)
    log = sup.record(FutureWarning, "Does this occur?")
    command_giving_warnings()
    # The FutureWarning was given once, the filtered warnings were
    # ignored. All other warnings abide outside settings (may be
    # printed/error)
    assert_(len(log) == 1)
    assert_(len(sup.log) == 1)  # also stored in log attribute
 Or as a decorator: sup = np.testing.suppress_warnings()
sup.filter(module=np.ma.core)  # module must match exactly
@sup
def some_function():
    # do something which causes a warning in np.ma.core
    pass
 Methods  
__call__(func) Function decorator to apply certain suppressions to a whole function.  
filter([category, message, module]) Add a new suppressing filter or apply it if the state is entered.  
record([category, message, module]) Append a new recording filter or apply it if the state is entered.
sys.warnoptions  
This is an implementation detail of the warnings framework; do not modify this value. Refer to the warnings module for more information on the warnings framework.
class warnings.catch_warnings(*, record=False, module=None)  
A context manager that copies and, upon exit, restores the warnings filter and the showwarning() function. If the record argument is False (the default) the context manager returns None on entry. If record is True, a list is returned that is progressively populated with objects as seen by a custom showwarning() function (which also suppresses output to sys.stdout). Each object in the list has attributes with the same names as the arguments to showwarning(). The module argument takes a module that will be used instead of the module returned when you import warnings whose filter will be protected. This argument exists primarily for testing the warnings module itself.  Note The catch_warnings manager works by replacing and then later restoring the module’s showwarning() function and internal list of filter specifications. This means the context manager is modifying global state and therefore is not thread-safe.
def f_29784889():
    """enable warnings using action 'always'
    """
     
 --------------------

def f_13550423(l):
    """concatenate items of list `l` with a space ' '
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
email.utils.parsedate(date)  
Attempts to parse a date according to the rules in RFC 2822. however, some mailers don’t follow that format as specified, so parsedate() tries to guess correctly in such cases. date is a string containing an RFC 2822 date, such as "Mon, 20 Nov 1995 19:12:08 -0500". If it succeeds in parsing the date, parsedate() returns a 9-tuple that can be passed directly to time.mktime(); otherwise None will be returned. Note that indexes 6, 7, and 8 of the result tuple are not usable.
imaplib.Internaldate2tuple(datestr)  
Parse an IMAP4 INTERNALDATE string and return corresponding local time. The return value is a time.struct_time tuple or None if the string has wrong format.
classmethod time.fromisoformat(time_string)  
Return a time corresponding to a time_string in one of the formats emitted by time.isoformat(). Specifically, this function supports strings in the format: HH[:MM[:SS[.fff[fff]]]][+HH:MM[:SS[.ffffff]]]
  Caution This does not support parsing arbitrary ISO 8601 strings. It is only intended as the inverse operation of time.isoformat().  Examples: >>> from datetime import time
>>> time.fromisoformat('04:23:01')
datetime.time(4, 23, 1)
>>> time.fromisoformat('04:23:01.000384')
datetime.time(4, 23, 1, 384)
>>> time.fromisoformat('04:23:01+04:00')
datetime.time(4, 23, 1, tzinfo=datetime.timezone(datetime.timedelta(seconds=14400)))
  New in version 3.7.
email.utils.parsedate_tz(date)  
Performs the same function as parsedate(), but returns either None or a 10-tuple; the first 9 elements make up a tuple that can be passed directly to time.mktime(), and the tenth is the offset of the date’s timezone from UTC (which is the official term for Greenwich Mean Time) 1. If the input string has no timezone, the last element of the tuple returned is 0, which represents UTC. Note that indexes 6, 7, and 8 of the result tuple are not usable.
parse_time(value)  
Parses a string and returns a datetime.time. UTC offsets aren’t supported; if value describes one, the result is None.
def f_698223():
    """parse a time string '30/03/09 16:31:32.123' containing milliseconds in it
    """
    return  
 --------------------

def f_6633523(my_string):
    """convert a string `my_string` with dot and comma into a float number `my_float`
    """
     
 --------------------

def f_6633523():
    """convert a string `123,456.908` with dot and comma into a floating number
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
sys.path  
A list of strings that specifies the search path for modules. Initialized from the environment variable PYTHONPATH, plus an installation-dependent default. As initialized upon program startup, the first item of this list, path[0], is the directory containing the script that was used to invoke the Python interpreter. If the script directory is not available (e.g. if the interpreter is invoked interactively or if the script is read from standard input), path[0] is the empty string, which directs Python to search modules in the current directory first. Notice that the script directory is inserted before the entries inserted as a result of PYTHONPATH. A program is free to modify this list for its own purposes. Only strings and bytes should be added to sys.path; all other data types are ignored during import.  See also Module site This describes how to use .pth files to extend sys.path.
multiprocessing.set_executable()  
Sets the path of the Python interpreter to use when starting a child process. (By default sys.executable is used). Embedders will probably need to do some thing like set_executable(os.path.join(sys.exec_prefix, 'pythonw.exe'))
 before they can create child processes.  Changed in version 3.4: Now supported on Unix when the 'spawn' start method is used.
modulefinder.AddPackagePath(pkg_name, path)  
Record that the package named pkg_name can be found in the specified path.
site.main()  
Adds all the standard site-specific directories to the module search path. This function is called automatically when this module is imported, unless the Python interpreter was started with the -S flag.  Changed in version 3.3: This function used to be called unconditionally.
pkgutil.extend_path(path, name)  
Extend the search path for the modules which comprise a package. Intended use is to place the following code in a package’s __init__.py: from pkgutil import extend_path
__path__ = extend_path(__path__, __name__)
 This will add to the package’s __path__ all subdirectories of directories on sys.path named after the package. This is useful if one wants to distribute different parts of a single logical package as multiple directories. It also looks for *.pkg files beginning where * matches the name argument. This feature is similar to *.pth files (see the site module for more information), except that it doesn’t special-case lines starting with import. A *.pkg file is trusted at face value: apart from checking for duplicates, all entries found in a *.pkg file are added to the path, regardless of whether they exist on the filesystem. (This is a feature.) If the input path is not a list (as is the case for frozen packages) it is returned unchanged. The input path is not modified; an extended copy is returned. Items are only appended to the copy at the end. It is assumed that sys.path is a sequence. Items of sys.path that are not strings referring to existing directories are ignored. Unicode items on sys.path that cause errors when used as filenames may cause this function to raise an exception (in line with os.path.isdir() behavior).
def f_3108285():
    """set python path '/path/to/whatever' in python script
    """
     
 --------------------
Please refer to the following documentation to generate the code:
Pattern.split(string, maxsplit=0)  
Identical to the split() function, using the compiled pattern.
str.split(sep=None, maxsplit=-1)  
Return a list of the words in the string, using sep as the delimiter string. If maxsplit is given, at most maxsplit splits are done (thus, the list will have at most maxsplit+1 elements). If maxsplit is not specified or -1, then there is no limit on the number of splits (all possible splits are made). If sep is given, consecutive delimiters are not grouped together and are deemed to delimit empty strings (for example, '1,,2'.split(',') returns ['1', '', '2']). The sep argument may consist of multiple characters (for example, '1<>2<>3'.split('<>') returns ['1', '2', '3']). Splitting an empty string with a specified separator returns ['']. For example: >>> '1,2,3'.split(',')
['1', '2', '3']
>>> '1,2,3'.split(',', maxsplit=1)
['1', '2,3']
>>> '1,2,,3,'.split(',')
['1', '2', '', '3', '']
 If sep is not specified or is None, a different splitting algorithm is applied: runs of consecutive whitespace are regarded as a single separator, and the result will contain no empty strings at the start or end if the string has leading or trailing whitespace. Consequently, splitting an empty string or a string consisting of just whitespace with a None separator returns []. For example: >>> '1 2 3'.split()
['1', '2', '3']
>>> '1 2 3'.split(maxsplit=1)
['1', '2 3']
>>> '   1   2   3   '.split()
['1', '2', '3']
str.rsplit(sep=None, maxsplit=-1)  
Return a list of the words in the string, using sep as the delimiter string. If maxsplit is given, at most maxsplit splits are done, the rightmost ones. If sep is not specified or None, any whitespace string is a separator. Except for splitting from the right, rsplit() behaves like split() which is described in detail below.
pandas.DataFrame.clip   DataFrame.clip(lower=None, upper=None, axis=None, inplace=False, *args, **kwargs)[source]
 
Trim values at input threshold(s). Assigns values outside boundary to boundary values. Thresholds can be singular values or array like, and in the latter case the clipping is performed element-wise in the specified axis.  Parameters 
 
lower:float or array-like, default None


Minimum threshold value. All values below this threshold will be set to it. A missing threshold (e.g NA) will not clip the value.  
upper:float or array-like, default None


Maximum threshold value. All values above this threshold will be set to it. A missing threshold (e.g NA) will not clip the value.  
axis:int or str axis name, optional


Align object with lower and upper along the given axis.  
inplace:bool, default False


Whether to perform the operation in place on the data.  *args, **kwargs

Additional keywords have no effect but might be accepted for compatibility with numpy.    Returns 
 Series or DataFrame or None

Same type as calling object with the values outside the clip boundaries replaced or None if inplace=True.      See also  Series.clip

Trim values at input threshold in series.  DataFrame.clip

Trim values at input threshold in dataframe.  numpy.clip

Clip (limit) the values in an array.    Examples 
>>> data = {'col_0': [9, -3, 0, -1, 5], 'col_1': [-2, -7, 6, 8, -5]}
>>> df = pd.DataFrame(data)
>>> df
   col_0  col_1
0      9     -2
1     -3     -7
2      0      6
3     -1      8
4      5     -5
  Clips per column using lower and upper thresholds: 
>>> df.clip(-4, 6)
   col_0  col_1
0      6     -2
1     -3     -4
2      0      6
3     -1      6
4      5     -4
  Clips using specific lower and upper thresholds per column element: 
>>> t = pd.Series([2, -4, -1, 6, 3])
>>> t
0    2
1   -4
2   -1
3    6
4    3
dtype: int64
  
>>> df.clip(t, t + 4, axis=0)
   col_0  col_1
0      6      2
1     -3     -4
2      0      3
3      6      8
4      5      3
  Clips using specific lower threshold per column element, with missing values: 
>>> t = pd.Series([2, -4, np.NaN, 6, 3])
>>> t
0    2.0
1   -4.0
2    NaN
3    6.0
4    3.0
dtype: float64
  
>>> df.clip(t, axis=0)
col_0  col_1
0      9      2
1     -3     -4
2      0      6
3      6      8
4      5      3
pandas.Series.clip   Series.clip(lower=None, upper=None, axis=None, inplace=False, *args, **kwargs)[source]
 
Trim values at input threshold(s). Assigns values outside boundary to boundary values. Thresholds can be singular values or array like, and in the latter case the clipping is performed element-wise in the specified axis.  Parameters 
 
lower:float or array-like, default None


Minimum threshold value. All values below this threshold will be set to it. A missing threshold (e.g NA) will not clip the value.  
upper:float or array-like, default None


Maximum threshold value. All values above this threshold will be set to it. A missing threshold (e.g NA) will not clip the value.  
axis:int or str axis name, optional


Align object with lower and upper along the given axis.  
inplace:bool, default False


Whether to perform the operation in place on the data.  *args, **kwargs

Additional keywords have no effect but might be accepted for compatibility with numpy.    Returns 
 Series or DataFrame or None

Same type as calling object with the values outside the clip boundaries replaced or None if inplace=True.      See also  Series.clip

Trim values at input threshold in series.  DataFrame.clip

Trim values at input threshold in dataframe.  numpy.clip

Clip (limit) the values in an array.    Examples 
>>> data = {'col_0': [9, -3, 0, -1, 5], 'col_1': [-2, -7, 6, 8, -5]}
>>> df = pd.DataFrame(data)
>>> df
   col_0  col_1
0      9     -2
1     -3     -7
2      0      6
3     -1      8
4      5     -5
  Clips per column using lower and upper thresholds: 
>>> df.clip(-4, 6)
   col_0  col_1
0      6     -2
1     -3     -4
2      0      6
3     -1      6
4      5     -4
  Clips using specific lower and upper thresholds per column element: 
>>> t = pd.Series([2, -4, -1, 6, 3])
>>> t
0    2
1   -4
2   -1
3    6
4    3
dtype: int64
  
>>> df.clip(t, t + 4, axis=0)
   col_0  col_1
0      6      2
1     -3     -4
2      0      3
3      6      8
4      5      3
  Clips using specific lower threshold per column element, with missing values: 
>>> t = pd.Series([2, -4, np.NaN, 6, 3])
>>> t
0    2.0
1   -4.0
2    NaN
3    6.0
4    3.0
dtype: float64
  
>>> df.clip(t, axis=0)
col_0  col_1
0      9      2
1     -3     -4
2      0      6
3      6      8
4      5      3
def f_2195340():
    """split string 'Words, words, words.' using a regex '(\\W+)'
    """
    return  
 --------------------

def f_17977584():
    """open a file `Output.txt` in append mode
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
mpl_toolkits.mplot3d The mplot3d toolkit adds simple 3D plotting capabilities (scatter, surface, line, mesh, etc.) to Matplotlib by supplying an Axes object that can create a 2D projection of a 3D scene. The resulting graph will have the same look and feel as regular 2D plots. Not the fastest or most feature complete 3D library out there, but it ships with Matplotlib and thus may be a lighter weight solution for some use cases. See the mplot3d tutorial for more information.  The interactive backends also provide the ability to rotate and zoom the 3D scene. One can rotate the 3D scene by simply clicking-and-dragging the scene. Zooming is done by right-clicking the scene and dragging the mouse up and down (unlike 2D plots, the toolbar zoom button is not used).  
mplot3d FAQ How is mplot3d different from Mayavi? My 3D plot doesn't look right at certain viewing angles I don't like how the 3D plot is laid out, how do I change that?     Note pyplot cannot be used to add content to 3D plots, because its function signatures are strictly 2D and cannot handle the additional information needed for 3D. Instead, use the explicit API by calling the respective methods on the Axes3D object.   axes3d  Note 3D plotting in Matplotlib is still not as mature as the 2D case. Please report any functions that do not behave as expected as a bug. In addition, help and patches would be greatly appreciated!   
axes3d.Axes3D(fig[, rect, azim, elev, ...]) 3D axes object.     axis3d  Note See mpl_toolkits.mplot3d.axis3d._axinfo for a dictionary containing constants that may be modified for controlling the look and feel of mplot3d axes (e.g., label spacing, font colors and panel colors). Historically, axis3d has suffered from having hard-coded constants that precluded user adjustments, and this dictionary was implemented in version 1.1 as a stop-gap measure.   
axis3d.Axis(adir, v_intervalx, d_intervalx, ...) An Axis class for the 3D plots.     art3d  
art3d.Line3D(xs, ys, zs, *args, **kwargs) 3D line object.  
art3d.Line3DCollection(segments, *args[, zorder]) A collection of 3D lines.  
art3d.Patch3D(*args[, zs, zdir]) 3D patch object.  
art3d.Patch3DCollection(*args[, zs, zdir, ...]) A collection of 3D patches.  
art3d.Path3DCollection(*args[, zs, zdir, ...]) A collection of 3D paths.  
art3d.PathPatch3D(path, *[, zs, zdir]) 3D PathPatch object.  
art3d.Poly3DCollection(verts, *args[, zsort]) A collection of 3D polygons.  
art3d.Text3D([x, y, z, text, zdir]) Text object with 3D position and direction.  
art3d.get_dir_vector(zdir) Return a direction vector.  
art3d.juggle_axes(xs, ys, zs, zdir) Reorder coordinates so that 2D xs, ys can be plotted in the plane orthogonal to zdir.  
art3d.line_2d_to_3d(line[, zs, zdir]) Convert a 2D line to 3D.  
art3d.line_collection_2d_to_3d(col[, zs, zdir]) Convert a LineCollection to a Line3DCollection object.  
art3d.patch_2d_to_3d(patch[, z, zdir]) Convert a Patch to a Patch3D object.  
art3d.patch_collection_2d_to_3d(col[, zs, ...]) Convert a PatchCollection into a Patch3DCollection object (or a PathCollection into a Path3DCollection object).  
art3d.pathpatch_2d_to_3d(pathpatch[, z, zdir]) Convert a PathPatch to a PathPatch3D object.  
art3d.poly_collection_2d_to_3d(col[, zs, zdir]) Convert a PolyCollection to a Poly3DCollection object.  
art3d.rotate_axes(xs, ys, zs, zdir) Reorder coordinates so that the axes are rotated with zdir along the original z axis.  
art3d.text_2d_to_3d(obj[, z, zdir]) Convert a Text to a Text3D object.     proj3d  
proj3d.inv_transform(xs, ys, zs, M)   
proj3d.persp_transformation(zfront, zback)   
proj3d.proj_points(points, M)   
proj3d.proj_trans_points(points, M)   
proj3d.proj_transform(xs, ys, zs, M) Transform the points by the projection matrix  
proj3d.proj_transform_clip(xs, ys, zs, M) Transform the points by the projection matrix and return the clipping result returns txs, tys, tzs, tis  
proj3d.rot_x(V, alpha)   
proj3d.transform(xs, ys, zs, M) Transform the points by the projection matrix  
proj3d.view_transformation(E, R, V)   
proj3d.world_transformation(xmin, xmax, ...) Produce a matrix that scales homogeneous coords in the specified ranges to [0, 1], or [0, pb_aspect[i]] if the plotbox aspect ratio is specified.
stringprep.map_table_b3(code)  
Return the mapped value for code according to tableB.3 (Mapping for case-folding used with no normalization).
tf.raw_ops.BatchFFT3D  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.raw_ops.BatchFFT3D  
tf.raw_ops.BatchFFT3D(
    input, name=None
)

 


 Args
  input   A Tensor of type complex64.  
  name   A name for the operation (optional).   
 


 Returns   A Tensor of type complex64.
handler403
HTTPRedirectHandler.http_error_303(req, fp, code, msg, hdrs)  
The same as http_error_301(), but called for the ‘see other’ response.
def f_22676():
    """download a file "http://www.example.com/songs/mp3.mp3" over HTTP and save to "mp3.mp3"
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
http.client — HTTP protocol client Source code: Lib/http/client.py This module defines classes which implement the client side of the HTTP and HTTPS protocols. It is normally not used directly — the module urllib.request uses it to handle URLs that use HTTP and HTTPS.  See also The Requests package is recommended for a higher-level HTTP client interface.   Note HTTPS support is only available if Python was compiled with SSL support (through the ssl module).  The module provides the following classes:  
class http.client.HTTPConnection(host, port=None, [timeout, ]source_address=None, blocksize=8192)  
An HTTPConnection instance represents one transaction with an HTTP server. It should be instantiated passing it a host and optional port number. If no port number is passed, the port is extracted from the host string if it has the form host:port, else the default HTTP port (80) is used. If the optional timeout parameter is given, blocking operations (like connection attempts) will timeout after that many seconds (if it is not given, the global default timeout setting is used). The optional source_address parameter may be a tuple of a (host, port) to use as the source address the HTTP connection is made from. The optional blocksize parameter sets the buffer size in bytes for sending a file-like message body. For example, the following calls all create instances that connect to the server at the same host and port: >>> h1 = http.client.HTTPConnection('www.python.org')
>>> h2 = http.client.HTTPConnection('www.python.org:80')
>>> h3 = http.client.HTTPConnection('www.python.org', 80)
>>> h4 = http.client.HTTPConnection('www.python.org', 80, timeout=10)
  Changed in version 3.2: source_address was added.   Changed in version 3.4: The strict parameter was removed. HTTP 0.9-style “Simple Responses” are not longer supported.   Changed in version 3.7: blocksize parameter was added.  
  
class http.client.HTTPSConnection(host, port=None, key_file=None, cert_file=None, [timeout, ]source_address=None, *, context=None, check_hostname=None, blocksize=8192)  
A subclass of HTTPConnection that uses SSL for communication with secure servers. Default port is 443. If context is specified, it must be a ssl.SSLContext instance describing the various SSL options. Please read Security considerations for more information on best practices.  Changed in version 3.2: source_address, context and check_hostname were added.   Changed in version 3.2: This class now supports HTTPS virtual hosts if possible (that is, if ssl.HAS_SNI is true).   Changed in version 3.4: The strict parameter was removed. HTTP 0.9-style “Simple Responses” are no longer supported.   Changed in version 3.4.3: This class now performs all the necessary certificate and hostname checks by default. To revert to the previous, unverified, behavior ssl._create_unverified_context() can be passed to the context parameter.   Changed in version 3.8: This class now enables TLS 1.3 ssl.SSLContext.post_handshake_auth for the default context or when cert_file is passed with a custom context.   Deprecated since version 3.6: key_file and cert_file are deprecated in favor of context. Please use ssl.SSLContext.load_cert_chain() instead, or let ssl.create_default_context() select the system’s trusted CA certificates for you. The check_hostname parameter is also deprecated; the ssl.SSLContext.check_hostname attribute of context should be used instead.  
  
class http.client.HTTPResponse(sock, debuglevel=0, method=None, url=None)  
Class whose instances are returned upon successful connection. Not instantiated directly by user.  Changed in version 3.4: The strict parameter was removed. HTTP 0.9 style “Simple Responses” are no longer supported.  
 This module provides the following function:  
http.client.parse_headers(fp)  
Parse the headers from a file pointer fp representing a HTTP request/response. The file has to be a BufferedIOBase reader (i.e. not text) and must provide a valid RFC 2822 style header. This function returns an instance of http.client.HTTPMessage that holds the header fields, but no payload (the same as HTTPResponse.msg and http.server.BaseHTTPRequestHandler.headers). After returning, the file pointer fp is ready to read the HTTP body.  Note parse_headers() does not parse the start-line of a HTTP message; it only parses the Name: value lines. The file has to be ready to read these field lines, so the first line should already be consumed before calling the function.  
 The following exceptions are raised as appropriate:  
exception http.client.HTTPException  
The base class of the other exceptions in this module. It is a subclass of Exception. 
  
exception http.client.NotConnected  
A subclass of HTTPException. 
  
exception http.client.InvalidURL  
A subclass of HTTPException, raised if a port is given and is either non-numeric or empty. 
  
exception http.client.UnknownProtocol  
A subclass of HTTPException. 
  
exception http.client.UnknownTransferEncoding  
A subclass of HTTPException. 
  
exception http.client.UnimplementedFileMode  
A subclass of HTTPException. 
  
exception http.client.IncompleteRead  
A subclass of HTTPException. 
  
exception http.client.ImproperConnectionState  
A subclass of HTTPException. 
  
exception http.client.CannotSendRequest  
A subclass of ImproperConnectionState. 
  
exception http.client.CannotSendHeader  
A subclass of ImproperConnectionState. 
  
exception http.client.ResponseNotReady  
A subclass of ImproperConnectionState. 
  
exception http.client.BadStatusLine  
A subclass of HTTPException. Raised if a server responds with a HTTP status code that we don’t understand. 
  
exception http.client.LineTooLong  
A subclass of HTTPException. Raised if an excessively long line is received in the HTTP protocol from the server. 
  
exception http.client.RemoteDisconnected  
A subclass of ConnectionResetError and BadStatusLine. Raised by HTTPConnection.getresponse() when the attempt to read the response results in no data read from the connection, indicating that the remote end has closed the connection.  New in version 3.5: Previously, BadStatusLine('') was raised.  
 The constants defined in this module are:  
http.client.HTTP_PORT  
The default port for the HTTP protocol (always 80). 
  
http.client.HTTPS_PORT  
The default port for the HTTPS protocol (always 443). 
  
http.client.responses  
This dictionary maps the HTTP 1.1 status codes to the W3C names. Example: http.client.responses[http.client.NOT_FOUND] is 'Not Found'. 
 See HTTP status codes for a list of HTTP status codes that are available in this module as constants. HTTPConnection Objects HTTPConnection instances have the following methods:  
HTTPConnection.request(method, url, body=None, headers={}, *, encode_chunked=False)  
This will send a request to the server using the HTTP request method method and the selector url. If body is specified, the specified data is sent after the headers are finished. It may be a str, a bytes-like object, an open file object, or an iterable of bytes. If body is a string, it is encoded as ISO-8859-1, the default for HTTP. If it is a bytes-like object, the bytes are sent as is. If it is a file object, the contents of the file is sent; this file object should support at least the read() method. If the file object is an instance of io.TextIOBase, the data returned by the read() method will be encoded as ISO-8859-1, otherwise the data returned by read() is sent as is. If body is an iterable, the elements of the iterable are sent as is until the iterable is exhausted. The headers argument should be a mapping of extra HTTP headers to send with the request. If headers contains neither Content-Length nor Transfer-Encoding, but there is a request body, one of those header fields will be added automatically. If body is None, the Content-Length header is set to 0 for methods that expect a body (PUT, POST, and PATCH). If body is a string or a bytes-like object that is not also a file, the Content-Length header is set to its length. Any other type of body (files and iterables in general) will be chunk-encoded, and the Transfer-Encoding header will automatically be set instead of Content-Length. The encode_chunked argument is only relevant if Transfer-Encoding is specified in headers. If encode_chunked is False, the HTTPConnection object assumes that all encoding is handled by the calling code. If it is True, the body will be chunk-encoded.  Note Chunked transfer encoding has been added to the HTTP protocol version 1.1. Unless the HTTP server is known to handle HTTP 1.1, the caller must either specify the Content-Length, or must pass a str or bytes-like object that is not also a file as the body representation.   New in version 3.2: body can now be an iterable.   Changed in version 3.6: If neither Content-Length nor Transfer-Encoding are set in headers, file and iterable body objects are now chunk-encoded. The encode_chunked argument was added. No attempt is made to determine the Content-Length for file objects.  
  
HTTPConnection.getresponse()  
Should be called after a request is sent to get the response from the server. Returns an HTTPResponse instance.  Note Note that you must have read the whole response before you can send a new request to the server.   Changed in version 3.5: If a ConnectionError or subclass is raised, the HTTPConnection object will be ready to reconnect when a new request is sent.  
  
HTTPConnection.set_debuglevel(level)  
Set the debugging level. The default debug level is 0, meaning no debugging output is printed. Any value greater than 0 will cause all currently defined debug output to be printed to stdout. The debuglevel is passed to any new HTTPResponse objects that are created.  New in version 3.1.  
  
HTTPConnection.set_tunnel(host, port=None, headers=None)  
Set the host and the port for HTTP Connect Tunnelling. This allows running the connection through a proxy server. The host and port arguments specify the endpoint of the tunneled connection (i.e. the address included in the CONNECT request, not the address of the proxy server). The headers argument should be a mapping of extra HTTP headers to send with the CONNECT request. For example, to tunnel through a HTTPS proxy server running locally on port 8080, we would pass the address of the proxy to the HTTPSConnection constructor, and the address of the host that we eventually want to reach to the set_tunnel() method: >>> import http.client
>>> conn = http.client.HTTPSConnection("localhost", 8080)
>>> conn.set_tunnel("www.python.org")
>>> conn.request("HEAD","/index.html")
  New in version 3.2.  
  
HTTPConnection.connect()  
Connect to the server specified when the object was created. By default, this is called automatically when making a request if the client does not already have a connection. 
  
HTTPConnection.close()  
Close the connection to the server. 
  
HTTPConnection.blocksize  
Buffer size in bytes for sending a file-like message body.  New in version 3.7.  
 As an alternative to using the request() method described above, you can also send your request step by step, by using the four functions below.  
HTTPConnection.putrequest(method, url, skip_host=False, skip_accept_encoding=False)  
This should be the first call after the connection to the server has been made. It sends a line to the server consisting of the method string, the url string, and the HTTP version (HTTP/1.1). To disable automatic sending of Host: or Accept-Encoding: headers (for example to accept additional content encodings), specify skip_host or skip_accept_encoding with non-False values. 
  
HTTPConnection.putheader(header, argument[, ...])  
Send an RFC 822-style header to the server. It sends a line to the server consisting of the header, a colon and a space, and the first argument. If more arguments are given, continuation lines are sent, each consisting of a tab and an argument. 
  
HTTPConnection.endheaders(message_body=None, *, encode_chunked=False)  
Send a blank line to the server, signalling the end of the headers. The optional message_body argument can be used to pass a message body associated with the request. If encode_chunked is True, the result of each iteration of message_body will be chunk-encoded as specified in RFC 7230, Section 3.3.1. How the data is encoded is dependent on the type of message_body. If message_body implements the buffer interface the encoding will result in a single chunk. If message_body is a collections.abc.Iterable, each iteration of message_body will result in a chunk. If message_body is a file object, each call to .read() will result in a chunk. The method automatically signals the end of the chunk-encoded data immediately after message_body.  Note Due to the chunked encoding specification, empty chunks yielded by an iterator body will be ignored by the chunk-encoder. This is to avoid premature termination of the read of the request by the target server due to malformed encoding.   New in version 3.6: Chunked encoding support. The encode_chunked parameter was added.  
  
HTTPConnection.send(data)  
Send data to the server. This should be used directly only after the endheaders() method has been called and before getresponse() is called. 
 HTTPResponse Objects An HTTPResponse instance wraps the HTTP response from the server. It provides access to the request headers and the entity body. The response is an iterable object and can be used in a with statement.  Changed in version 3.5: The io.BufferedIOBase interface is now implemented and all of its reader operations are supported.   
HTTPResponse.read([amt])  
Reads and returns the response body, or up to the next amt bytes. 
  
HTTPResponse.readinto(b)  
Reads up to the next len(b) bytes of the response body into the buffer b. Returns the number of bytes read.  New in version 3.3.  
  
HTTPResponse.getheader(name, default=None)  
Return the value of the header name, or default if there is no header matching name. If there is more than one header with the name name, return all of the values joined by ‘, ‘. If ‘default’ is any iterable other than a single string, its elements are similarly returned joined by commas. 
  
HTTPResponse.getheaders()  
Return a list of (header, value) tuples. 
  
HTTPResponse.fileno()  
Return the fileno of the underlying socket. 
  
HTTPResponse.msg  
A http.client.HTTPMessage instance containing the response headers. http.client.HTTPMessage is a subclass of email.message.Message. 
  
HTTPResponse.version  
HTTP protocol version used by server. 10 for HTTP/1.0, 11 for HTTP/1.1. 
  
HTTPResponse.url  
URL of the resource retrieved, commonly used to determine if a redirect was followed. 
  
HTTPResponse.headers  
Headers of the response in the form of an email.message.EmailMessage instance. 
  
HTTPResponse.status  
Status code returned by server. 
  
HTTPResponse.reason  
Reason phrase returned by server. 
  
HTTPResponse.debuglevel  
A debugging hook. If debuglevel is greater than zero, messages will be printed to stdout as the response is read and parsed. 
  
HTTPResponse.closed  
Is True if the stream is closed. 
  
HTTPResponse.geturl()  
 Deprecated since version 3.9: Deprecated in favor of url.  
  
HTTPResponse.info()  
 Deprecated since version 3.9: Deprecated in favor of headers.  
  
HTTPResponse.getstatus()  
 Deprecated since version 3.9: Deprecated in favor of status.  
 Examples Here is an example session that uses the GET method: >>> import http.client
>>> conn = http.client.HTTPSConnection("www.python.org")
>>> conn.request("GET", "/")
>>> r1 = conn.getresponse()
>>> print(r1.status, r1.reason)
200 OK
>>> data1 = r1.read()  # This will return entire content.
>>> # The following example demonstrates reading data in chunks.
>>> conn.request("GET", "/")
>>> r1 = conn.getresponse()
>>> while chunk := r1.read(200):
...     print(repr(chunk))
b'<!doctype html>\n<!--[if"...
...
>>> # Example of an invalid request
>>> conn = http.client.HTTPSConnection("docs.python.org")
>>> conn.request("GET", "/parrot.spam")
>>> r2 = conn.getresponse()
>>> print(r2.status, r2.reason)
404 Not Found
>>> data2 = r2.read()
>>> conn.close()
 Here is an example session that uses the HEAD method. Note that the HEAD method never returns any data. >>> import http.client
>>> conn = http.client.HTTPSConnection("www.python.org")
>>> conn.request("HEAD", "/")
>>> res = conn.getresponse()
>>> print(res.status, res.reason)
200 OK
>>> data = res.read()
>>> print(len(data))
0
>>> data == b''
True
 Here is an example session that shows how to POST requests: >>> import http.client, urllib.parse
>>> params = urllib.parse.urlencode({'@number': 12524, '@type': 'issue', '@action': 'show'})
>>> headers = {"Content-type": "application/x-www-form-urlencoded",
...            "Accept": "text/plain"}
>>> conn = http.client.HTTPConnection("bugs.python.org")
>>> conn.request("POST", "", params, headers)
>>> response = conn.getresponse()
>>> print(response.status, response.reason)
302 Found
>>> data = response.read()
>>> data
b'Redirecting to <a href="http://bugs.python.org/issue12524">http://bugs.python.org/issue12524</a>'
>>> conn.close()
 Client side HTTP PUT requests are very similar to POST requests. The difference lies only the server side where HTTP server will allow resources to be created via PUT request. It should be noted that custom HTTP methods are also handled in urllib.request.Request by setting the appropriate method attribute. Here is an example session that shows how to send a PUT request using http.client: >>> # This creates an HTTP message
>>> # with the content of BODY as the enclosed representation
>>> # for the resource http://localhost:8080/file
...
>>> import http.client
>>> BODY = "***filecontents***"
>>> conn = http.client.HTTPConnection("localhost", 8080)
>>> conn.request("PUT", "/file", BODY)
>>> response = conn.getresponse()
>>> print(response.status, response.reason)
200, OK
 HTTPMessage Objects An http.client.HTTPMessage instance holds the headers from an HTTP response. It is implemented using the email.message.Message class.
class urllib.request.FileHandler  
Open local files.
cgi — Common Gateway Interface support Source code: Lib/cgi.py Support module for Common Gateway Interface (CGI) scripts. This module defines a number of utilities for use by CGI scripts written in Python. Introduction A CGI script is invoked by an HTTP server, usually to process user input submitted through an HTML <FORM> or <ISINDEX> element. Most often, CGI scripts live in the server’s special cgi-bin directory. The HTTP server places all sorts of information about the request (such as the client’s hostname, the requested URL, the query string, and lots of other goodies) in the script’s shell environment, executes the script, and sends the script’s output back to the client. The script’s input is connected to the client too, and sometimes the form data is read this way; at other times the form data is passed via the “query string” part of the URL. This module is intended to take care of the different cases and provide a simpler interface to the Python script. It also provides a number of utilities that help in debugging scripts, and the latest addition is support for file uploads from a form (if your browser supports it). The output of a CGI script should consist of two sections, separated by a blank line. The first section contains a number of headers, telling the client what kind of data is following. Python code to generate a minimal header section looks like this: print("Content-Type: text/html")    # HTML is following
print()                             # blank line, end of headers
 The second section is usually HTML, which allows the client software to display nicely formatted text with header, in-line images, etc. Here’s Python code that prints a simple piece of HTML: print("<TITLE>CGI script output</TITLE>")
print("<H1>This is my first CGI script</H1>")
print("Hello, world!")
 Using the cgi module Begin by writing import cgi. When you write a new script, consider adding these lines: import cgitb
cgitb.enable()
 This activates a special exception handler that will display detailed reports in the Web browser if any errors occur. If you’d rather not show the guts of your program to users of your script, you can have the reports saved to files instead, with code like this: import cgitb
cgitb.enable(display=0, logdir="/path/to/logdir")
 It’s very helpful to use this feature during script development. The reports produced by cgitb provide information that can save you a lot of time in tracking down bugs. You can always remove the cgitb line later when you have tested your script and are confident that it works correctly. To get at submitted form data, use the FieldStorage class. If the form contains non-ASCII characters, use the encoding keyword parameter set to the value of the encoding defined for the document. It is usually contained in the META tag in the HEAD section of the HTML document or by the Content-Type header). This reads the form contents from the standard input or the environment (depending on the value of various environment variables set according to the CGI standard). Since it may consume standard input, it should be instantiated only once. The FieldStorage instance can be indexed like a Python dictionary. It allows membership testing with the in operator, and also supports the standard dictionary method keys() and the built-in function len(). Form fields containing empty strings are ignored and do not appear in the dictionary; to keep such values, provide a true value for the optional keep_blank_values keyword parameter when creating the FieldStorage instance. For instance, the following code (which assumes that the Content-Type header and blank line have already been printed) checks that the fields name and addr are both set to a non-empty string: form = cgi.FieldStorage()
if "name" not in form or "addr" not in form:
    print("<H1>Error</H1>")
    print("Please fill in the name and addr fields.")
    return
print("<p>name:", form["name"].value)
print("<p>addr:", form["addr"].value)
...further form processing here...
 Here the fields, accessed through form[key], are themselves instances of FieldStorage (or MiniFieldStorage, depending on the form encoding). The value attribute of the instance yields the string value of the field. The getvalue() method returns this string value directly; it also accepts an optional second argument as a default to return if the requested key is not present. If the submitted form data contains more than one field with the same name, the object retrieved by form[key] is not a FieldStorage or MiniFieldStorage instance but a list of such instances. Similarly, in this situation, form.getvalue(key) would return a list of strings. If you expect this possibility (when your HTML form contains multiple fields with the same name), use the getlist() method, which always returns a list of values (so that you do not need to special-case the single item case). For example, this code concatenates any number of username fields, separated by commas: value = form.getlist("username")
usernames = ",".join(value)
 If a field represents an uploaded file, accessing the value via the value attribute or the getvalue() method reads the entire file in memory as bytes. This may not be what you want. You can test for an uploaded file by testing either the filename attribute or the file attribute. You can then read the data from the file attribute before it is automatically closed as part of the garbage collection of the FieldStorage instance (the read() and readline() methods will return bytes): fileitem = form["userfile"]
if fileitem.file:
    # It's an uploaded file; count lines
    linecount = 0
    while True:
        line = fileitem.file.readline()
        if not line: break
        linecount = linecount + 1
 FieldStorage objects also support being used in a with statement, which will automatically close them when done. If an error is encountered when obtaining the contents of an uploaded file (for example, when the user interrupts the form submission by clicking on a Back or Cancel button) the done attribute of the object for the field will be set to the value -1. The file upload draft standard entertains the possibility of uploading multiple files from one field (using a recursive multipart/* encoding). When this occurs, the item will be a dictionary-like FieldStorage item. This can be determined by testing its type attribute, which should be multipart/form-data (or perhaps another MIME type matching multipart/*). In this case, it can be iterated over recursively just like the top-level form object. When a form is submitted in the “old” format (as the query string or as a single data part of type application/x-www-form-urlencoded), the items will actually be instances of the class MiniFieldStorage. In this case, the list, file, and filename attributes are always None. A form submitted via POST that also has a query string will contain both FieldStorage and MiniFieldStorage items.  Changed in version 3.4: The file attribute is automatically closed upon the garbage collection of the creating FieldStorage instance.   Changed in version 3.5: Added support for the context management protocol to the FieldStorage class.  Higher Level Interface The previous section explains how to read CGI form data using the FieldStorage class. This section describes a higher level interface which was added to this class to allow one to do it in a more readable and intuitive way. The interface doesn’t make the techniques described in previous sections obsolete — they are still useful to process file uploads efficiently, for example. The interface consists of two simple methods. Using the methods you can process form data in a generic way, without the need to worry whether only one or more values were posted under one name. In the previous section, you learned to write following code anytime you expected a user to post more than one value under one name: item = form.getvalue("item")
if isinstance(item, list):
    # The user is requesting more than one item.
else:
    # The user is requesting only one item.
 This situation is common for example when a form contains a group of multiple checkboxes with the same name: <input type="checkbox" name="item" value="1" />
<input type="checkbox" name="item" value="2" />
 In most situations, however, there’s only one form control with a particular name in a form and then you expect and need only one value associated with this name. So you write a script containing for example this code: user = form.getvalue("user").upper()
 The problem with the code is that you should never expect that a client will provide valid input to your scripts. For example, if a curious user appends another user=foo pair to the query string, then the script would crash, because in this situation the getvalue("user") method call returns a list instead of a string. Calling the upper() method on a list is not valid (since lists do not have a method of this name) and results in an AttributeError exception. Therefore, the appropriate way to read form data values was to always use the code which checks whether the obtained value is a single value or a list of values. That’s annoying and leads to less readable scripts. A more convenient approach is to use the methods getfirst() and getlist() provided by this higher level interface.  
FieldStorage.getfirst(name, default=None)  
This method always returns only one value associated with form field name. The method returns only the first value in case that more values were posted under such name. Please note that the order in which the values are received may vary from browser to browser and should not be counted on. 1 If no such form field or value exists then the method returns the value specified by the optional parameter default. This parameter defaults to None if not specified. 
  
FieldStorage.getlist(name)  
This method always returns a list of values associated with form field name. The method returns an empty list if no such form field or value exists for name. It returns a list consisting of one item if only one such value exists. 
 Using these methods you can write nice compact code: import cgi
form = cgi.FieldStorage()
user = form.getfirst("user", "").upper()    # This way it's safe.
for item in form.getlist("item"):
    do_something(item)
 Functions These are useful if you want more control, or if you want to employ some of the algorithms implemented in this module in other circumstances.  
cgi.parse(fp=None, environ=os.environ, keep_blank_values=False, strict_parsing=False, separator="&")  
Parse a query in the environment or from a file (the file defaults to sys.stdin). The keep_blank_values, strict_parsing and separator parameters are passed to urllib.parse.parse_qs() unchanged. 
  
cgi.parse_multipart(fp, pdict, encoding="utf-8", errors="replace", separator="&")  
Parse input of type multipart/form-data (for file uploads). Arguments are fp for the input file, pdict for a dictionary containing other parameters in the Content-Type header, and encoding, the request encoding. Returns a dictionary just like urllib.parse.parse_qs(): keys are the field names, each value is a list of values for that field. For non-file fields, the value is a list of strings. This is easy to use but not much good if you are expecting megabytes to be uploaded — in that case, use the FieldStorage class instead which is much more flexible.  Changed in version 3.7: Added the encoding and errors parameters. For non-file fields, the value is now a list of strings, not bytes.   Changed in version 3.9.2: Added the separator parameter.  
  
cgi.parse_header(string)  
Parse a MIME header (such as Content-Type) into a main value and a dictionary of parameters. 
  
cgi.test()  
Robust test CGI script, usable as main program. Writes minimal HTTP headers and formats all information provided to the script in HTML form. 
  
cgi.print_environ()  
Format the shell environment in HTML. 
  
cgi.print_form(form)  
Format a form in HTML. 
  
cgi.print_directory()  
Format the current directory in HTML. 
  
cgi.print_environ_usage()  
Print a list of useful (used by CGI) environment variables in HTML. 
 Caring about security There’s one important rule: if you invoke an external program (via the os.system() or os.popen() functions. or others with similar functionality), make very sure you don’t pass arbitrary strings received from the client to the shell. This is a well-known security hole whereby clever hackers anywhere on the Web can exploit a gullible CGI script to invoke arbitrary shell commands. Even parts of the URL or field names cannot be trusted, since the request doesn’t have to come from your form! To be on the safe side, if you must pass a string gotten from a form to a shell command, you should make sure the string contains only alphanumeric characters, dashes, underscores, and periods. Installing your CGI script on a Unix system Read the documentation for your HTTP server and check with your local system administrator to find the directory where CGI scripts should be installed; usually this is in a directory cgi-bin in the server tree. Make sure that your script is readable and executable by “others”; the Unix file mode should be 0o755 octal (use chmod 0755 filename). Make sure that the first line of the script contains #! starting in column 1 followed by the pathname of the Python interpreter, for instance: #!/usr/local/bin/python
 Make sure the Python interpreter exists and is executable by “others”. Make sure that any files your script needs to read or write are readable or writable, respectively, by “others” — their mode should be 0o644 for readable and 0o666 for writable. This is because, for security reasons, the HTTP server executes your script as user “nobody”, without any special privileges. It can only read (write, execute) files that everybody can read (write, execute). The current directory at execution time is also different (it is usually the server’s cgi-bin directory) and the set of environment variables is also different from what you get when you log in. In particular, don’t count on the shell’s search path for executables (PATH) or the Python module search path (PYTHONPATH) to be set to anything interesting. If you need to load modules from a directory which is not on Python’s default module search path, you can change the path in your script, before importing other modules. For example: import sys
sys.path.insert(0, "/usr/home/joe/lib/python")
sys.path.insert(0, "/usr/local/lib/python")
 (This way, the directory inserted last will be searched first!) Instructions for non-Unix systems will vary; check your HTTP server’s documentation (it will usually have a section on CGI scripts). Testing your CGI script Unfortunately, a CGI script will generally not run when you try it from the command line, and a script that works perfectly from the command line may fail mysteriously when run from the server. There’s one reason why you should still test your script from the command line: if it contains a syntax error, the Python interpreter won’t execute it at all, and the HTTP server will most likely send a cryptic error to the client. Assuming your script has no syntax errors, yet it does not work, you have no choice but to read the next section. Debugging CGI scripts First of all, check for trivial installation errors — reading the section above on installing your CGI script carefully can save you a lot of time. If you wonder whether you have understood the installation procedure correctly, try installing a copy of this module file (cgi.py) as a CGI script. When invoked as a script, the file will dump its environment and the contents of the form in HTML form. Give it the right mode etc, and send it a request. If it’s installed in the standard cgi-bin directory, it should be possible to send it a request by entering a URL into your browser of the form: http://yourhostname/cgi-bin/cgi.py?name=Joe+Blow&addr=At+Home
 If this gives an error of type 404, the server cannot find the script – perhaps you need to install it in a different directory. If it gives another error, there’s an installation problem that you should fix before trying to go any further. If you get a nicely formatted listing of the environment and form content (in this example, the fields should be listed as “addr” with value “At Home” and “name” with value “Joe Blow”), the cgi.py script has been installed correctly. If you follow the same procedure for your own script, you should now be able to debug it. The next step could be to call the cgi module’s test() function from your script: replace its main code with the single statement cgi.test()
 This should produce the same results as those gotten from installing the cgi.py file itself. When an ordinary Python script raises an unhandled exception (for whatever reason: of a typo in a module name, a file that can’t be opened, etc.), the Python interpreter prints a nice traceback and exits. While the Python interpreter will still do this when your CGI script raises an exception, most likely the traceback will end up in one of the HTTP server’s log files, or be discarded altogether. Fortunately, once you have managed to get your script to execute some code, you can easily send tracebacks to the Web browser using the cgitb module. If you haven’t done so already, just add the lines: import cgitb
cgitb.enable()
 to the top of your script. Then try running it again; when a problem occurs, you should see a detailed report that will likely make apparent the cause of the crash. If you suspect that there may be a problem in importing the cgitb module, you can use an even more robust approach (which only uses built-in modules): import sys
sys.stderr = sys.stdout
print("Content-Type: text/plain")
print()
...your code here...
 This relies on the Python interpreter to print the traceback. The content type of the output is set to plain text, which disables all HTML processing. If your script works, the raw HTML will be displayed by your client. If it raises an exception, most likely after the first two lines have been printed, a traceback will be displayed. Because no HTML interpretation is going on, the traceback will be readable. Common problems and solutions  Most HTTP servers buffer the output from CGI scripts until the script is completed. This means that it is not possible to display a progress report on the client’s display while the script is running. Check the installation instructions above. Check the HTTP server’s log files. (tail -f logfile in a separate window may be useful!) Always check a script for syntax errors first, by doing something like python script.py. If your script does not have any syntax errors, try adding import cgitb;
cgitb.enable() to the top of the script. When invoking external programs, make sure they can be found. Usually, this means using absolute path names — PATH is usually not set to a very useful value in a CGI script. When reading or writing external files, make sure they can be read or written by the userid under which your CGI script will be running: this is typically the userid under which the web server is running, or some explicitly specified userid for a web server’s suexec feature. Don’t try to give a CGI script a set-uid mode. This doesn’t work on most systems, and is a security liability as well.  Footnotes  
1  
Note that some recent versions of the HTML specification do state what order the field values should be supplied in, but knowing whether a request was received from a conforming browser, or even from a browser at all, is tedious and error-prone.
http.client.HTTP_PORT  
The default port for the HTTP protocol (always 80).
urllib — URL handling modules Source code: Lib/urllib/ urllib is a package that collects several modules for working with URLs:  
urllib.request for opening and reading URLs 
urllib.error containing the exceptions raised by urllib.request
 
urllib.parse for parsing URLs 
urllib.robotparser for parsing robots.txt files
def f_22676(url):
    """download a file 'http://www.example.com/' over HTTP
    """
     
 --------------------

def f_22676(url):
    """download a file `url` over HTTP
    """
    return  
 --------------------

def f_22676(url):
    """download a file `url` over HTTP and save to "10MB"
    """
     
 --------------------
Please refer to the following documentation to generate the code:
make_svn_version_py(delete=True)[source]
 
Appends a data function to the data_files list that will generate __svn_version__.py file to the current package directory. Generate package __svn_version__.py file from SVN revision number, it will be removed after python exits but will be available when sdist, etc commands are executed. Notes If __svn_version__.py existed before, nothing is done. This is intended for working with source directories that are in an SVN repository.
sys.version_info  
A tuple containing the five components of the version number: major, minor, micro, releaselevel, and serial. All values except releaselevel are integers; the release level is 'alpha', 'beta', 'candidate', or 'final'. The version_info value corresponding to the Python version 2.0 is (2, 0, 0, 'final', 0). The components can also be accessed by name, so sys.version_info[0] is equivalent to sys.version_info.major and so on.  Changed in version 3.1: Added named component attributes.
platform.python_version()  
Returns the Python version as string 'major.minor.patchlevel'. Note that unlike the Python sys.version, the returned value will always include the patchlevel (it defaults to 0).
ArgumentParser.exit(status=0, message=None)  
This method terminates the program, exiting with the specified status and, if given, it prints a message before that. The user can override this method to handle these steps differently: class ErrorCatchingArgumentParser(argparse.ArgumentParser):
    def exit(self, status=0, message=None):
        if status:
            raise Exception(f'Exiting because of an error: {message}')
        exit(status)
sys.version  
A string containing the version number of the Python interpreter plus additional information on the build number and compiler used. This string is displayed when the interactive interpreter is started. Do not extract version information out of it, rather, use version_info and the functions provided by the platform module.
def f_15405636(parser):
    """argparse add argument with flag '--version' and version action of '%(prog)s 2.0' to parser `parser`
    """
    return  
 --------------------

def f_17665809(d):
    """remove key 'c' from dictionary `d`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
tf.raw_ops.BoostedTreesSparseCalculateBestFeatureSplit Calculates gains for each feature and returns the best possible split information for the feature.  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.raw_ops.BoostedTreesSparseCalculateBestFeatureSplit  
tf.raw_ops.BoostedTreesSparseCalculateBestFeatureSplit(
    node_id_range, stats_summary_indices, stats_summary_values, stats_summary_shape,
    l1, l2, tree_complexity, min_node_weight, logits_dimension,
    split_type='inequality', name=None
)
 The split information is the best threshold (bucket id), gains and left/right node contributions per node for each feature. It is possible that not all nodes can be split on each feature. Hence, the list of possible nodes can differ between the features. Therefore, we return node_ids_list for each feature, containing the list of nodes that this feature can be used to split. In this manner, the output is the best split per features and per node, so that it needs to be combined later to produce the best split for each node (among all possible features). The output shapes are compatible in a way that the first dimension of all tensors are the same and equal to the number of possible split nodes for each feature.
 


 Args
  node_id_range   A Tensor of type int32. A Rank 1 tensor (shape=[2]) to specify the range [first, last) of node ids to process within stats_summary_list. The nodes are iterated between the two nodes specified by the tensor, as like for node_id in range(node_id_range[0], node_id_range[1]) (Note that the last index node_id_range[1] is exclusive).  
  stats_summary_indices   A Tensor of type int32. A Rank 2 int64 tensor of dense shape N, 4 for accumulated stats summary (gradient/hessian) per node per bucket for each feature. The second dimension contains node id, feature dimension, bucket id, and stats dim. stats dim is the sum of logits dimension and hessian dimension, hessian dimension can either be logits dimension if diagonal hessian is used, or logits dimension^2 if full hessian is used.  
  stats_summary_values   A Tensor of type float32. A Rank 1 float tensor of dense shape N, which supplies the values for each element in summary_indices.  
  stats_summary_shape   A Tensor of type int32. A Rank 1 float tensor of dense shape [4], which specifies the dense shape of the sparse tensor, which is [num tree nodes, feature dimensions, num buckets, stats dim].  
  l1   A Tensor of type float32. l1 regularization factor on leaf weights, per instance based.  
  l2   A Tensor of type float32. l2 regularization factor on leaf weights, per instance based.  
  tree_complexity   A Tensor of type float32. adjustment to the gain, per leaf based.  
  min_node_weight   A Tensor of type float32. minimum avg of hessians in a node before required for the node to be considered for splitting.  
  logits_dimension   An int that is >= 1. The dimension of logit, i.e., number of classes.  
  split_type   An optional string from: "inequality". Defaults to "inequality". A string indicating if this Op should perform inequality split or equality split.  
  name   A name for the operation (optional).   
 


 Returns   A tuple of Tensor objects (node_ids, gains, feature_dimensions, thresholds, left_node_contribs, right_node_contribs, split_with_default_directions).     node_ids   A Tensor of type int32.  
  gains   A Tensor of type float32.  
  feature_dimensions   A Tensor of type int32.  
  thresholds   A Tensor of type int32.  
  left_node_contribs   A Tensor of type float32.  
  right_node_contribs   A Tensor of type float32.  
  split_with_default_directions   A Tensor of type string.
tf.raw_ops.BoostedTreesCalculateBestFeatureSplit Calculates gains for each feature and returns the best possible split information for the feature.  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.raw_ops.BoostedTreesCalculateBestFeatureSplit  
tf.raw_ops.BoostedTreesCalculateBestFeatureSplit(
    node_id_range, stats_summary, l1, l2, tree_complexity, min_node_weight,
    logits_dimension, split_type='inequality', name=None
)
 The split information is the best threshold (bucket id), gains and left/right node contributions per node for each feature. It is possible that not all nodes can be split on each feature. Hence, the list of possible nodes can differ between the features. Therefore, we return node_ids_list for each feature, containing the list of nodes that this feature can be used to split. In this manner, the output is the best split per features and per node, so that it needs to be combined later to produce the best split for each node (among all possible features). The output shapes are compatible in a way that the first dimension of all tensors are the same and equal to the number of possible split nodes for each feature.
 


 Args
  node_id_range   A Tensor of type int32. A Rank 1 tensor (shape=[2]) to specify the range [first, last) of node ids to process within stats_summary_list. The nodes are iterated between the two nodes specified by the tensor, as like for node_id in range(node_id_range[0], node_id_range[1]) (Note that the last index node_id_range[1] is exclusive).  
  stats_summary   A Tensor of type float32. A Rank 4 tensor (#shape=[max_splits, feature_dims, bucket, stats_dims]) for accumulated stats summary (gradient/hessian) per node, per dimension, per buckets for each feature. The first dimension of the tensor is the maximum number of splits, and thus not all elements of it will be used, but only the indexes specified by node_ids will be used.  
  l1   A Tensor of type float32. l1 regularization factor on leaf weights, per instance based.  
  l2   A Tensor of type float32. l2 regularization factor on leaf weights, per instance based.  
  tree_complexity   A Tensor of type float32. adjustment to the gain, per leaf based.  
  min_node_weight   A Tensor of type float32. minimum avg of hessians in a node before required for the node to be considered for splitting.  
  logits_dimension   An int that is >= 1. The dimension of logit, i.e., number of classes.  
  split_type   An optional string from: "inequality", "equality". Defaults to "inequality". A string indicating if this Op should perform inequality split or equality split.  
  name   A name for the operation (optional).   
 


 Returns   A tuple of Tensor objects (node_ids, gains, feature_dimensions, thresholds, left_node_contribs, right_node_contribs, split_with_default_directions).     node_ids   A Tensor of type int32.  
  gains   A Tensor of type float32.  
  feature_dimensions   A Tensor of type int32.  
  thresholds   A Tensor of type int32.  
  left_node_contribs   A Tensor of type float32.  
  right_node_contribs   A Tensor of type float32.  
  split_with_default_directions   A Tensor of type string.
tf.raw_ops.BoostedTreesCalculateBestFeatureSplitV2 Calculates gains for each feature and returns the best possible split information for each node. However, if no split is found, then no split information is returned for that node.  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.raw_ops.BoostedTreesCalculateBestFeatureSplitV2  
tf.raw_ops.BoostedTreesCalculateBestFeatureSplitV2(
    node_id_range, stats_summaries_list, split_types, candidate_feature_ids, l1, l2,
    tree_complexity, min_node_weight, logits_dimension, name=None
)
 The split information is the best threshold (bucket id), gains and left/right node contributions per node for each feature. It is possible that not all nodes can be split on each feature. Hence, the list of possible nodes can differ between the features. Therefore, we return node_ids_list for each feature, containing the list of nodes that this feature can be used to split. In this manner, the output is the best split per features and per node, so that it needs to be combined later to produce the best split for each node (among all possible features). The output shapes are compatible in a way that the first dimension of all tensors are the same and equal to the number of possible split nodes for each feature.
 


 Args
  node_id_range   A Tensor of type int32. A Rank 1 tensor (shape=[2]) to specify the range [first, last) of node ids to process within stats_summary_list. The nodes are iterated between the two nodes specified by the tensor, as like for node_id in range(node_id_range[0], node_id_range[1]) (Note that the last index node_id_range[1] is exclusive).  
  stats_summaries_list   A list of at least 1 Tensor objects with type float32. A list of Rank 4 tensor (#shape=[max_splits, feature_dims, bucket, stats_dims]) for accumulated stats summary (gradient/hessian) per node, per dimension, per buckets for each feature. The first dimension of the tensor is the maximum number of splits, and thus not all elements of it will be used, but only the indexes specified by node_ids will be used.  
  split_types   A Tensor of type string. A Rank 1 tensor indicating if this Op should perform inequality split or equality split per feature.  
  candidate_feature_ids   A Tensor of type int32. Rank 1 tensor with ids for each feature. This is the real id of the feature.  
  l1   A Tensor of type float32. l1 regularization factor on leaf weights, per instance based.  
  l2   A Tensor of type float32. l2 regularization factor on leaf weights, per instance based.  
  tree_complexity   A Tensor of type float32. adjustment to the gain, per leaf based.  
  min_node_weight   A Tensor of type float32. minimum avg of hessians in a node before required for the node to be considered for splitting.  
  logits_dimension   An int that is >= 1. The dimension of logit, i.e., number of classes.  
  name   A name for the operation (optional).   
 


 Returns   A tuple of Tensor objects (node_ids, gains, feature_ids, feature_dimensions, thresholds, left_node_contribs, right_node_contribs, split_with_default_directions).     node_ids   A Tensor of type int32.  
  gains   A Tensor of type float32.  
  feature_ids   A Tensor of type int32.  
  feature_dimensions   A Tensor of type int32.  
  thresholds   A Tensor of type int32.  
  left_node_contribs   A Tensor of type float32.  
  right_node_contribs   A Tensor of type float32.  
  split_with_default_directions   A Tensor of type string.
pandas.read_csv   pandas.read_csv(filepath_or_buffer, sep=NoDefault.no_default, delimiter=None, header='infer', names=NoDefault.no_default, index_col=None, usecols=None, squeeze=None, prefix=NoDefault.no_default, mangle_dupe_cols=True, dtype=None, engine=None, converters=None, true_values=None, false_values=None, skipinitialspace=False, skiprows=None, skipfooter=0, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=False, skip_blank_lines=True, parse_dates=None, infer_datetime_format=False, keep_date_col=False, date_parser=None, dayfirst=False, cache_dates=True, iterator=False, chunksize=None, compression='infer', thousands=None, decimal='.', lineterminator=None, quotechar='"', quoting=0, doublequote=True, escapechar=None, comment=None, encoding=None, encoding_errors='strict', dialect=None, error_bad_lines=None, warn_bad_lines=None, on_bad_lines=None, delim_whitespace=False, low_memory=True, memory_map=False, float_precision=None, storage_options=None)[source]
 
Read a comma-separated values (csv) file into DataFrame. Also supports optionally iterating or breaking of the file into chunks. Additional help can be found in the online docs for IO Tools.  Parameters 
 
filepath_or_buffer:str, path object or file-like object


Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv. If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handle (e.g. via builtin open function) or StringIO.  
sep:str, default ‘,’


Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python’s builtin sniffer tool, csv.Sniffer. In addition, separators longer than 1 character and different from '\s+' will be interpreted as regular expressions and will also force the use of the Python parsing engine. Note that regex delimiters are prone to ignoring quoted data. Regex example: '\r\t'.  
delimiter:str, default None


Alias for sep.  
header:int, list of int, None, default ‘infer’


Row number(s) to use as the column names, and the start of the data. Default behavior is to infer the column names: if no names are passed the behavior is identical to header=0 and column names are inferred from the first line of the file, if column names are passed explicitly then the behavior is identical to header=None. Explicitly pass header=0 to be able to replace existing names. The header can be a list of integers that specify row locations for a multi-index on the columns e.g. [0,1,3]. Intervening rows that are not specified will be skipped (e.g. 2 in this example is skipped). Note that this parameter ignores commented lines and empty lines if skip_blank_lines=True, so header=0 denotes the first line of data rather than the first line of the file.  
names:array-like, optional


List of column names to use. If the file contains a header row, then you should explicitly pass header=0 to override the column names. Duplicates in this list are not allowed.  
index_col:int, str, sequence of int / str, or False, optional, default None


Column(s) to use as the row labels of the DataFrame, either given as string name or column index. If a sequence of int / str is given, a MultiIndex is used. Note: index_col=False can be used to force pandas to not use the first column as the index, e.g. when you have a malformed file with delimiters at the end of each line.  
usecols:list-like or callable, optional


Return a subset of the columns. If list-like, all elements must either be positional (i.e. integer indices into the document columns) or strings that correspond to column names provided either by the user in names or inferred from the document header row(s). If names are given, the document header row(s) are not taken into account. For example, a valid list-like usecols parameter would be [0, 1, 2] or ['foo', 'bar', 'baz']. Element order is ignored, so usecols=[0, 1] is the same as [1, 0]. To instantiate a DataFrame from data with element order preserved use pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']] for columns in ['foo', 'bar'] order or pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']] for ['bar', 'foo'] order. If callable, the callable function will be evaluated against the column names, returning names where the callable function evaluates to True. An example of a valid callable argument would be lambda x: x.upper() in
['AAA', 'BBB', 'DDD']. Using this parameter results in much faster parsing time and lower memory usage.  
squeeze:bool, default False


If the parsed data only contains one column then return a Series.  Deprecated since version 1.4.0: Append .squeeze("columns") to the call to read_csv to squeeze the data.   
prefix:str, optional


Prefix to add to column numbers when no header, e.g. ‘X’ for X0, X1, …  Deprecated since version 1.4.0: Use a list comprehension on the DataFrame’s columns after calling read_csv.   
mangle_dupe_cols:bool, default True


Duplicate columns will be specified as ‘X’, ‘X.1’, …’X.N’, rather than ‘X’…’X’. Passing in False will cause data to be overwritten if there are duplicate names in the columns.  
dtype:Type name or dict of column -> type, optional


Data type for data or columns. E.g. {‘a’: np.float64, ‘b’: np.int32, ‘c’: ‘Int64’} Use str or object together with suitable na_values settings to preserve and not interpret dtype. If converters are specified, they will be applied INSTEAD of dtype conversion.  
engine:{‘c’, ‘python’, ‘pyarrow’}, optional


Parser engine to use. The C and pyarrow engines are faster, while the python engine is currently more feature-complete. Multithreading is currently only supported by the pyarrow engine.  New in version 1.4.0: The “pyarrow” engine was added as an experimental engine, and some features are unsupported, or may not work correctly, with this engine.   
converters:dict, optional


Dict of functions for converting values in certain columns. Keys can either be integers or column labels.  
true_values:list, optional


Values to consider as True.  
false_values:list, optional


Values to consider as False.  
skipinitialspace:bool, default False


Skip spaces after delimiter.  
skiprows:list-like, int or callable, optional


Line numbers to skip (0-indexed) or number of lines to skip (int) at the start of the file. If callable, the callable function will be evaluated against the row indices, returning True if the row should be skipped and False otherwise. An example of a valid callable argument would be lambda x: x in [0, 2].  
skipfooter:int, default 0


Number of lines at bottom of file to skip (Unsupported with engine=’c’).  
nrows:int, optional


Number of rows of file to read. Useful for reading pieces of large files.  
na_values:scalar, str, list-like, or dict, optional


Additional strings to recognize as NA/NaN. If dict passed, specific per-column NA values. By default the following values are interpreted as NaN: ‘’, ‘#N/A’, ‘#N/A N/A’, ‘#NA’, ‘-1.#IND’, ‘-1.#QNAN’, ‘-NaN’, ‘-nan’, ‘1.#IND’, ‘1.#QNAN’, ‘<NA>’, ‘N/A’, ‘NA’, ‘NULL’, ‘NaN’, ‘n/a’, ‘nan’, ‘null’.  
keep_default_na:bool, default True


Whether or not to include the default NaN values when parsing the data. Depending on whether na_values is passed in, the behavior is as follows:  If keep_default_na is True, and na_values are specified, na_values is appended to the default NaN values used for parsing. If keep_default_na is True, and na_values are not specified, only the default NaN values are used for parsing. If keep_default_na is False, and na_values are specified, only the NaN values specified na_values are used for parsing. If keep_default_na is False, and na_values are not specified, no strings will be parsed as NaN.  Note that if na_filter is passed in as False, the keep_default_na and na_values parameters will be ignored.  
na_filter:bool, default True


Detect missing value markers (empty strings and the value of na_values). In data without any NAs, passing na_filter=False can improve the performance of reading a large file.  
verbose:bool, default False


Indicate number of NA values placed in non-numeric columns.  
skip_blank_lines:bool, default True


If True, skip over blank lines rather than interpreting as NaN values.  
parse_dates:bool or list of int or names or list of lists or dict, default False


The behavior is as follows:  boolean. If True -> try parsing the index. list of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3 each as a separate date column. list of lists. e.g. If [[1, 3]] -> combine columns 1 and 3 and parse as a single date column. dict, e.g. {‘foo’ : [1, 3]} -> parse columns 1, 3 as date and call result ‘foo’  If a column or index cannot be represented as an array of datetimes, say because of an unparsable value or a mixture of timezones, the column or index will be returned unaltered as an object data type. For non-standard datetime parsing, use pd.to_datetime after pd.read_csv. To parse an index or column with a mixture of timezones, specify date_parser to be a partially-applied pandas.to_datetime() with utc=True. See Parsing a CSV with mixed timezones for more. Note: A fast-path exists for iso8601-formatted dates.  
infer_datetime_format:bool, default False


If True and parse_dates is enabled, pandas will attempt to infer the format of the datetime strings in the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the parsing speed by 5-10x.  
keep_date_col:bool, default False


If True and parse_dates specifies combining multiple columns then keep the original columns.  
date_parser:function, optional


Function to use for converting a sequence of string columns to an array of datetime instances. The default uses dateutil.parser.parser to do the conversion. Pandas will try to call date_parser in three different ways, advancing to the next if an exception occurs: 1) Pass one or more arrays (as defined by parse_dates) as arguments; 2) concatenate (row-wise) the string values from the columns defined by parse_dates into a single array and pass that; and 3) call date_parser once for each row using one or more strings (corresponding to the columns defined by parse_dates) as arguments.  
dayfirst:bool, default False


DD/MM format dates, international and European format.  
cache_dates:bool, default True


If True, use a cache of unique, converted dates to apply the datetime conversion. May produce significant speed-up when parsing duplicate date strings, especially ones with timezone offsets.  New in version 0.25.0.   
iterator:bool, default False


Return TextFileReader object for iteration or getting chunks with get_chunk().  Changed in version 1.2: TextFileReader is a context manager.   
chunksize:int, optional


Return TextFileReader object for iteration. See the IO Tools docs for more information on iterator and chunksize.  Changed in version 1.2: TextFileReader is a context manager.   
compression:str or dict, default ‘infer’


For on-the-fly decompression of on-disk data. If ‘infer’ and ‘%s’ is path-like, then detect compression from the following extensions: ‘.gz’, ‘.bz2’, ‘.zip’, ‘.xz’, or ‘.zst’ (otherwise no compression). If using ‘zip’, the ZIP file must contain only one data file to be read in. Set to None for no decompression. Can also be a dict with key 'method' set to one of {'zip', 'gzip', 'bz2', 'zstd'} and other key-value pairs are forwarded to zipfile.ZipFile, gzip.GzipFile, bz2.BZ2File, or zstandard.ZstdDecompressor, respectively. As an example, the following could be passed for Zstandard decompression using a custom compression dictionary: compression={'method': 'zstd', 'dict_data': my_compression_dict}.  Changed in version 1.4.0: Zstandard support.   
thousands:str, optional


Thousands separator.  
decimal:str, default ‘.’


Character to recognize as decimal point (e.g. use ‘,’ for European data).  
lineterminator:str (length 1), optional


Character to break file into lines. Only valid with C parser.  
quotechar:str (length 1), optional


The character used to denote the start and end of a quoted item. Quoted items can include the delimiter and it will be ignored.  
quoting:int or csv.QUOTE_* instance, default 0


Control field quoting behavior per csv.QUOTE_* constants. Use one of QUOTE_MINIMAL (0), QUOTE_ALL (1), QUOTE_NONNUMERIC (2) or QUOTE_NONE (3).  
doublequote:bool, default True


When quotechar is specified and quoting is not QUOTE_NONE, indicate whether or not to interpret two consecutive quotechar elements INSIDE a field as a single quotechar element.  
escapechar:str (length 1), optional


One-character string used to escape other characters.  
comment:str, optional


Indicates remainder of line should not be parsed. If found at the beginning of a line, the line will be ignored altogether. This parameter must be a single character. Like empty lines (as long as skip_blank_lines=True), fully commented lines are ignored by the parameter header but not by skiprows. For example, if comment='#', parsing #empty\na,b,c\n1,2,3 with header=0 will result in ‘a,b,c’ being treated as the header.  
encoding:str, optional


Encoding to use for UTF when reading/writing (ex. ‘utf-8’). List of Python standard encodings .  Changed in version 1.2: When encoding is None, errors="replace" is passed to open(). Otherwise, errors="strict" is passed to open(). This behavior was previously only the case for engine="python".   Changed in version 1.3.0: encoding_errors is a new argument. encoding has no longer an influence on how encoding errors are handled.   
encoding_errors:str, optional, default “strict”


How encoding errors are treated. List of possible values .  New in version 1.3.0.   
dialect:str or csv.Dialect, optional


If provided, this parameter will override values (default or not) for the following parameters: delimiter, doublequote, escapechar, skipinitialspace, quotechar, and quoting. If it is necessary to override values, a ParserWarning will be issued. See csv.Dialect documentation for more details.  
error_bad_lines:bool, optional, default None


Lines with too many fields (e.g. a csv line with too many commas) will by default cause an exception to be raised, and no DataFrame will be returned. If False, then these “bad lines” will be dropped from the DataFrame that is returned.  Deprecated since version 1.3.0: The on_bad_lines parameter should be used instead to specify behavior upon encountering a bad line instead.   
warn_bad_lines:bool, optional, default None


If error_bad_lines is False, and warn_bad_lines is True, a warning for each “bad line” will be output.  Deprecated since version 1.3.0: The on_bad_lines parameter should be used instead to specify behavior upon encountering a bad line instead.   
on_bad_lines:{‘error’, ‘warn’, ‘skip’} or callable, default ‘error’


Specifies what to do upon encountering a bad line (a line with too many fields). Allowed values are :  
 ‘error’, raise an Exception when a bad line is encountered. ‘warn’, raise a warning when a bad line is encountered and skip that line. ‘skip’, skip bad lines without raising or warning when they are encountered.  
  New in version 1.3.0:   callable, function with signature (bad_line: list[str]) -> list[str] | None that will process a single bad line. bad_line is a list of strings split by the sep. If the function returns None`, the bad line will be ignored.
If the function returns a new list of strings with more elements than
expected, a ``ParserWarning will be emitted while dropping extra elements. Only supported when engine="python"    New in version 1.4.0.   
delim_whitespace:bool, default False


Specifies whether or not whitespace (e.g. ' ' or '    ') will be used as the sep. Equivalent to setting sep='\s+'. If this option is set to True, nothing should be passed in for the delimiter parameter.  
low_memory:bool, default True


Internally process the file in chunks, resulting in lower memory use while parsing, but possibly mixed type inference. To ensure no mixed types either set False, or specify the type with the dtype parameter. Note that the entire file is read into a single DataFrame regardless, use the chunksize or iterator parameter to return the data in chunks. (Only valid with C parser).  
memory_map:bool, default False


If a filepath is provided for filepath_or_buffer, map the file object directly onto memory and access the data directly from there. Using this option can improve performance because there is no longer any I/O overhead.  
float_precision:str, optional


Specifies which converter the C engine should use for floating-point values. The options are None or ‘high’ for the ordinary converter, ‘legacy’ for the original lower precision pandas converter, and ‘round_trip’ for the round-trip converter.  Changed in version 1.2.   
storage_options:dict, optional


Extra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib as header options. For other URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are forwarded to fsspec. Please see fsspec and urllib for more details.  New in version 1.2.     Returns 
 DataFrame or TextParser

A comma-separated values (csv) file is returned as two-dimensional data structure with labeled axes.      See also  DataFrame.to_csv

Write DataFrame to a comma-separated values (csv) file.  read_csv

Read a comma-separated values (csv) file into DataFrame.  read_fwf

Read a table of fixed-width formatted lines into DataFrame.    Examples 
>>> pd.read_csv('data.csv')
pandas.merge_asof   pandas.merge_asof(left, right, on=None, left_on=None, right_on=None, left_index=False, right_index=False, by=None, left_by=None, right_by=None, suffixes=('_x', '_y'), tolerance=None, allow_exact_matches=True, direction='backward')[source]
 
Perform a merge by key distance. This is similar to a left-join except that we match on nearest key rather than equal keys. Both DataFrames must be sorted by the key. For each row in the left DataFrame:  
 A “backward” search selects the last row in the right DataFrame whose ‘on’ key is less than or equal to the left’s key. A “forward” search selects the first row in the right DataFrame whose ‘on’ key is greater than or equal to the left’s key. A “nearest” search selects the row in the right DataFrame whose ‘on’ key is closest in absolute distance to the left’s key.  
 The default is “backward” and is compatible in versions below 0.20.0. The direction parameter was added in version 0.20.0 and introduces “forward” and “nearest”. Optionally match on equivalent keys with ‘by’ before searching with ‘on’.  Parameters 
 
left:DataFrame or named Series


right:DataFrame or named Series


on:label


Field name to join on. Must be found in both DataFrames. The data MUST be ordered. Furthermore this must be a numeric column, such as datetimelike, integer, or float. On or left_on/right_on must be given.  
left_on:label


Field name to join on in left DataFrame.  
right_on:label


Field name to join on in right DataFrame.  
left_index:bool


Use the index of the left DataFrame as the join key.  
right_index:bool


Use the index of the right DataFrame as the join key.  
by:column name or list of column names


Match on these columns before performing merge operation.  
left_by:column name


Field names to match on in the left DataFrame.  
right_by:column name


Field names to match on in the right DataFrame.  
suffixes:2-length sequence (tuple, list, …)


Suffix to apply to overlapping column names in the left and right side, respectively.  
tolerance:int or Timedelta, optional, default None


Select asof tolerance within this range; must be compatible with the merge index.  
allow_exact_matches:bool, default True


 If True, allow matching with the same ‘on’ value (i.e. less-than-or-equal-to / greater-than-or-equal-to) If False, don’t match the same ‘on’ value (i.e., strictly less-than / strictly greater-than).   
direction:‘backward’ (default), ‘forward’, or ‘nearest’


Whether to search for prior, subsequent, or closest matches.    Returns 
 
merged:DataFrame

    See also  merge

Merge with a database-style join.  merge_ordered

Merge with optional filling/interpolation.    Examples 
>>> left = pd.DataFrame({"a": [1, 5, 10], "left_val": ["a", "b", "c"]})
>>> left
    a left_val
0   1        a
1   5        b
2  10        c
  
>>> right = pd.DataFrame({"a": [1, 2, 3, 6, 7], "right_val": [1, 2, 3, 6, 7]})
>>> right
   a  right_val
0  1          1
1  2          2
2  3          3
3  6          6
4  7          7
  
>>> pd.merge_asof(left, right, on="a")
    a left_val  right_val
0   1        a          1
1   5        b          3
2  10        c          7
  
>>> pd.merge_asof(left, right, on="a", allow_exact_matches=False)
    a left_val  right_val
0   1        a        NaN
1   5        b        3.0
2  10        c        7.0
  
>>> pd.merge_asof(left, right, on="a", direction="forward")
    a left_val  right_val
0   1        a        1.0
1   5        b        6.0
2  10        c        NaN
  
>>> pd.merge_asof(left, right, on="a", direction="nearest")
    a left_val  right_val
0   1        a          1
1   5        b          6
2  10        c          7
  We can use indexed DataFrames as well. 
>>> left = pd.DataFrame({"left_val": ["a", "b", "c"]}, index=[1, 5, 10])
>>> left
   left_val
1         a
5         b
10        c
  
>>> right = pd.DataFrame({"right_val": [1, 2, 3, 6, 7]}, index=[1, 2, 3, 6, 7])
>>> right
   right_val
1          1
2          2
3          3
6          6
7          7
  
>>> pd.merge_asof(left, right, left_index=True, right_index=True)
   left_val  right_val
1         a          1
5         b          3
10        c          7
  Here is a real-world times-series example 
>>> quotes = pd.DataFrame(
...     {
...         "time": [
...             pd.Timestamp("2016-05-25 13:30:00.023"),
...             pd.Timestamp("2016-05-25 13:30:00.023"),
...             pd.Timestamp("2016-05-25 13:30:00.030"),
...             pd.Timestamp("2016-05-25 13:30:00.041"),
...             pd.Timestamp("2016-05-25 13:30:00.048"),
...             pd.Timestamp("2016-05-25 13:30:00.049"),
...             pd.Timestamp("2016-05-25 13:30:00.072"),
...             pd.Timestamp("2016-05-25 13:30:00.075")
...         ],
...         "ticker": [
...                "GOOG",
...                "MSFT",
...                "MSFT",
...                "MSFT",
...                "GOOG",
...                "AAPL",
...                "GOOG",
...                "MSFT"
...            ],
...            "bid": [720.50, 51.95, 51.97, 51.99, 720.50, 97.99, 720.50, 52.01],
...            "ask": [720.93, 51.96, 51.98, 52.00, 720.93, 98.01, 720.88, 52.03]
...     }
... )
>>> quotes
                     time ticker     bid     ask
0 2016-05-25 13:30:00.023   GOOG  720.50  720.93
1 2016-05-25 13:30:00.023   MSFT   51.95   51.96
2 2016-05-25 13:30:00.030   MSFT   51.97   51.98
3 2016-05-25 13:30:00.041   MSFT   51.99   52.00
4 2016-05-25 13:30:00.048   GOOG  720.50  720.93
5 2016-05-25 13:30:00.049   AAPL   97.99   98.01
6 2016-05-25 13:30:00.072   GOOG  720.50  720.88
7 2016-05-25 13:30:00.075   MSFT   52.01   52.03
  
>>> trades = pd.DataFrame(
...        {
...            "time": [
...                pd.Timestamp("2016-05-25 13:30:00.023"),
...                pd.Timestamp("2016-05-25 13:30:00.038"),
...                pd.Timestamp("2016-05-25 13:30:00.048"),
...                pd.Timestamp("2016-05-25 13:30:00.048"),
...                pd.Timestamp("2016-05-25 13:30:00.048")
...            ],
...            "ticker": ["MSFT", "MSFT", "GOOG", "GOOG", "AAPL"],
...            "price": [51.95, 51.95, 720.77, 720.92, 98.0],
...            "quantity": [75, 155, 100, 100, 100]
...        }
...    )
>>> trades
                     time ticker   price  quantity
0 2016-05-25 13:30:00.023   MSFT   51.95        75
1 2016-05-25 13:30:00.038   MSFT   51.95       155
2 2016-05-25 13:30:00.048   GOOG  720.77       100
3 2016-05-25 13:30:00.048   GOOG  720.92       100
4 2016-05-25 13:30:00.048   AAPL   98.00       100
  By default we are taking the asof of the quotes 
>>> pd.merge_asof(trades, quotes, on="time", by="ticker")
                     time ticker   price  quantity     bid     ask
0 2016-05-25 13:30:00.023   MSFT   51.95        75   51.95   51.96
1 2016-05-25 13:30:00.038   MSFT   51.95       155   51.97   51.98
2 2016-05-25 13:30:00.048   GOOG  720.77       100  720.50  720.93
3 2016-05-25 13:30:00.048   GOOG  720.92       100  720.50  720.93
4 2016-05-25 13:30:00.048   AAPL   98.00       100     NaN     NaN
  We only asof within 2ms between the quote time and the trade time 
>>> pd.merge_asof(
...     trades, quotes, on="time", by="ticker", tolerance=pd.Timedelta("2ms")
... )
                     time ticker   price  quantity     bid     ask
0 2016-05-25 13:30:00.023   MSFT   51.95        75   51.95   51.96
1 2016-05-25 13:30:00.038   MSFT   51.95       155     NaN     NaN
2 2016-05-25 13:30:00.048   GOOG  720.77       100  720.50  720.93
3 2016-05-25 13:30:00.048   GOOG  720.92       100  720.50  720.93
4 2016-05-25 13:30:00.048   AAPL   98.00       100     NaN     NaN
  We only asof within 10ms between the quote time and the trade time and we exclude exact matches on time. However prior data will propagate forward 
>>> pd.merge_asof(
...     trades,
...     quotes,
...     on="time",
...     by="ticker",
...     tolerance=pd.Timedelta("10ms"),
...     allow_exact_matches=False
... )
                     time ticker   price  quantity     bid     ask
0 2016-05-25 13:30:00.023   MSFT   51.95        75     NaN     NaN
1 2016-05-25 13:30:00.038   MSFT   51.95       155   51.97   51.98
2 2016-05-25 13:30:00.048   GOOG  720.77       100     NaN     NaN
3 2016-05-25 13:30:00.048   GOOG  720.92       100     NaN     NaN
4 2016-05-25 13:30:00.048   AAPL   98.00       100     NaN     NaN
def f_41861705(split_df, csv_df):
    """Create new DataFrame object by merging columns "key" of  dataframes `split_df` and `csv_df` and rename the columns from dataframes `split_df` and `csv_df` with suffix `_left` and `_right` respectively
    """
    return  
 --------------------

def f_10697757(s):
    """Split a string `s` by space with `4` splits
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
DEBUG  
Whether debug mode is enabled. When using flask run to start the development server, an interactive debugger will be shown for unhandled exceptions, and the server will be reloaded when code changes. The debug attribute maps to this config key. This is enabled when ENV is 'development' and is overridden by the FLASK_DEBUG environment variable. It may not behave as expected if set in code. Do not enable debug mode when deploying in production. Default: True if ENV is 'development', or False otherwise.
class werkzeug.debug.DebuggedApplication(app, evalex=False, request_key='werkzeug.request', console_path='/console', console_init_func=None, show_hidden_frames=False, pin_security=True, pin_logging=True)  
Enables debugging support for a given application: from werkzeug.debug import DebuggedApplication
from myapp import app
app = DebuggedApplication(app, evalex=True)
 The evalex keyword argument allows evaluating expressions in a traceback’s frame context.  Parameters 
 
app (WSGIApplication) – the WSGI application to run debugged. 
evalex (bool) – enable exception evaluation feature (interactive debugging). This requires a non-forking server. 
request_key (str) – The key that points to the request object in ths environment. This parameter is ignored in current versions. 
console_path (str) – the URL for a general purpose console. 
console_init_func (Optional[Callable[[], Dict[str, Any]]]) – the function that is executed before starting the general purpose console. The return value is used as initial namespace. 
show_hidden_frames (bool) – by default hidden traceback frames are skipped. You can show them by setting this parameter to True. 
pin_security (bool) – can be used to disable the pin based security system. 
pin_logging (bool) – enables the logging of the pin system.   Return type 
None
debug()
loop.set_debug(enabled: bool)  
Set the debug mode of the event loop.  Changed in version 3.7: The new Python Development Mode can now also be used to enable the debug mode.
class werkzeug.middleware.lint.LintMiddleware(app)  
Warns about common errors in the WSGI and HTTP behavior of the server and wrapped application. Some of the issues it checks are:  invalid status codes non-bytes sent to the WSGI server strings returned from the WSGI application non-empty conditional responses unquoted etags relative URLs in the Location header unsafe calls to wsgi.input unclosed iterators  Error information is emitted using the warnings module.  Parameters 
app (WSGIApplication) – The WSGI application to wrap.  Return type 
None   from werkzeug.middleware.lint import LintMiddleware
app = LintMiddleware(app)
def f_16344756(app):
    """enable debug mode on Flask application `app`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
numpy.savetxt   numpy.savetxt(fname, X, fmt='%.18e', delimiter=' ', newline='\n', header='', footer='', comments='# ', encoding=None)[source]
 
Save an array to a text file.  Parameters 
 
fnamefilename or file handle


If the filename ends in .gz, the file is automatically saved in compressed gzip format. loadtxt understands gzipped files transparently.  
X1D or 2D array_like


Data to be saved to a text file.  
fmtstr or sequence of strs, optional


A single format (%10.5f), a sequence of formats, or a multi-format string, e.g. ‘Iteration %d – %10.5f’, in which case delimiter is ignored. For complex X, the legal options for fmt are:  a single specifier, fmt=’%.4e’, resulting in numbers formatted like ‘ (%s+%sj)’ % (fmt, fmt)
 a full string specifying every real and imaginary part, e.g. ‘ %.4e %+.4ej %.4e %+.4ej %.4e %+.4ej’ for 3 columns a list of specifiers, one per column - in this case, the real and imaginary part must have separate specifiers, e.g. [‘%.3e + %.3ej’, ‘(%.15e%+.15ej)’] for 2 columns   
delimiterstr, optional


String or character separating columns.  
newlinestr, optional


String or character separating lines.  New in version 1.5.0.   
headerstr, optional


String that will be written at the beginning of the file.  New in version 1.7.0.   
footerstr, optional


String that will be written at the end of the file.  New in version 1.7.0.   
commentsstr, optional


String that will be prepended to the header and footer strings, to mark them as comments. Default: ‘# ‘, as expected by e.g. numpy.loadtxt.  New in version 1.7.0.   
encoding{None, str}, optional


Encoding used to encode the outputfile. Does not apply to output streams. If the encoding is something other than ‘bytes’ or ‘latin1’ you will not be able to load the file in NumPy versions < 1.14. Default is ‘latin1’.  New in version 1.14.0.       See also  save

Save an array to a binary file in NumPy .npy format  savez

Save several arrays into an uncompressed .npz archive  savez_compressed

Save several arrays into a compressed .npz archive    Notes Further explanation of the fmt parameter (%[flag]width[.precision]specifier):  flags:

- : left justify + : Forces to precede result with + or -. 0 : Left pad the number with zeros instead of space (see width).  width:

Minimum number of characters to be printed. The value is not truncated if it has more characters.  precision:

 For integer specifiers (eg. d,i,o,x), the minimum number of digits. For e, E and f specifiers, the number of digits to print after the decimal point. For g and G, the maximum number of significant digits. For s, the maximum number of characters.   specifiers:

c : character d or i : signed decimal integer e or E : scientific notation with e or E. f : decimal floating point g,G : use the shorter of e,E or f o : signed octal s : string of characters u : unsigned decimal integer x,X : unsigned hexadecimal integer   This explanation of fmt is not complete, for an exhaustive specification see [1]. References  1 
Format Specification Mini-Language, Python Documentation.   Examples >>> x = y = z = np.arange(0.0,5.0,1.0)
>>> np.savetxt('test.out', x, delimiter=',')   # X is an array
>>> np.savetxt('test.out', (x,y,z))   # x,y,z equal sized 1D arrays
>>> np.savetxt('test.out', x, fmt='%1.4e')   # use exponential notation
handleError(record)  
This method should be called from handlers when an exception is encountered during an emit() call. If the module-level attribute raiseExceptions is False, exceptions get silently ignored. This is what is mostly wanted for a logging system - most users will not care about errors in the logging system, they are more interested in application errors. You could, however, replace this with a custom handler if you wish. The specified record is the one which was being processed when the exception occurred. (The default value of raiseExceptions is True, as that is more useful during development).
predict(X) [source]
 
Predict the labels for the data samples in X using trained model.  Parameters 
 
Xarray-like of shape (n_samples, n_features) 

List of n_features-dimensional data points. Each row corresponds to a single data point.    Returns 
 
labelsarray, shape (n_samples,) 

Component labels.
curses.doupdate()  
Update the physical screen. The curses library keeps two data structures, one representing the current physical screen contents and a virtual screen representing the desired next state. The doupdate() ground updates the physical screen to match the virtual screen. The virtual screen may be updated by a noutrefresh() call after write operations such as addstr() have been performed on a window. The normal refresh() call is simply noutrefresh() followed by doupdate(); if you have to update multiple windows, you can speed performance and perhaps reduce screen flicker by issuing noutrefresh() calls on all windows, followed by a single doupdate().
fit(X, y) [source]
 
Run score function on (X, y) and get the appropriate features.  Parameters 
 
Xarray-like of shape (n_samples, n_features) 

The training input samples.  
yarray-like of shape (n_samples,) 

The target values (class labels in classification, real numbers in regression).    Returns 
 
selfobject
def f_40133826(mylist):
    """python save list `mylist` to file object 'save.txt'
    """
     
 --------------------

def f_4490961(P, T):
    """Multiply a matrix `P` with a 3d tensor `T` in scipy
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
tf.experimental.numpy.zeros TensorFlow variant of NumPy's zeros. 
tf.experimental.numpy.zeros(
    shape, dtype=float
)
 See the NumPy documentation for numpy.zeros.
tf.zeros     View source on GitHub    Creates a tensor with all elements set to zero.  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.zeros  
tf.zeros(
    shape, dtype=tf.dtypes.float32, name=None
)
 See also tf.zeros_like, tf.ones, tf.fill, tf.eye. This operation returns a tensor of type dtype with shape shape and all elements set to zero. 
tf.zeros([3, 4], tf.int32)
<tf.Tensor: shape=(3, 4), dtype=int32, numpy=
array([[0, 0, 0, 0],
       [0, 0, 0, 0],
       [0, 0, 0, 0]], dtype=int32)>

 


 Args
  shape   A list of integers, a tuple of integers, or a 1-D Tensor of type int32.  
  dtype   The DType of an element in the resulting Tensor.  
  name   Optional string. A name for the operation.   
 


 Returns   A Tensor with all elements set to zero.
numpy.ma.zeros   ma.zeros(shape, dtype=float, order='C', *, like=None) = <numpy.ma.core._convert2ma object>
 
Return a new array of given shape and type, filled with zeros.  Parameters 
 
shapeint or tuple of ints


Shape of the new array, e.g., (2, 3) or 2.  
dtypedata-type, optional


The desired data-type for the array, e.g., numpy.int8. Default is numpy.float64.  
order{‘C’, ‘F’}, optional, default: ‘C’


Whether to store multi-dimensional data in row-major (C-style) or column-major (Fortran-style) order in memory.  
likearray_like


Reference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as like supports the __array_function__ protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.  New in version 1.20.0.     Returns 
 
outMaskedArray


Array of zeros with the given shape, dtype, and order.      See also  zeros_like

Return an array of zeros with shape and type of input.  empty

Return a new uninitialized array.  ones

Return a new array setting values to one.  full

Return a new array of given shape filled with value.    Examples >>> np.zeros(5)
array([ 0.,  0.,  0.,  0.,  0.])
 >>> np.zeros((5,), dtype=int)
array([0, 0, 0, 0, 0])
 >>> np.zeros((2, 1))
array([[ 0.],
       [ 0.]])
 >>> s = (2,2)
>>> np.zeros(s)
array([[ 0.,  0.],
       [ 0.,  0.]])
 >>> np.zeros((2,), dtype=[('x', 'i4'), ('y', 'i4')]) # custom dtype
array([(0, 0), (0, 0)],
      dtype=[('x', '<i4'), ('y', '<i4')])
numpy.zeros   numpy.zeros(shape, dtype=float, order='C', *, like=None)
 
Return a new array of given shape and type, filled with zeros.  Parameters 
 
shapeint or tuple of ints


Shape of the new array, e.g., (2, 3) or 2.  
dtypedata-type, optional


The desired data-type for the array, e.g., numpy.int8. Default is numpy.float64.  
order{‘C’, ‘F’}, optional, default: ‘C’


Whether to store multi-dimensional data in row-major (C-style) or column-major (Fortran-style) order in memory.  
likearray_like


Reference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as like supports the __array_function__ protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.  New in version 1.20.0.     Returns 
 
outndarray


Array of zeros with the given shape, dtype, and order.      See also  zeros_like

Return an array of zeros with shape and type of input.  empty

Return a new uninitialized array.  ones

Return a new array setting values to one.  full

Return a new array of given shape filled with value.    Examples >>> np.zeros(5)
array([ 0.,  0.,  0.,  0.,  0.])
 >>> np.zeros((5,), dtype=int)
array([0, 0, 0, 0, 0])
 >>> np.zeros((2, 1))
array([[ 0.],
       [ 0.]])
 >>> s = (2,2)
>>> np.zeros(s)
array([[ 0.,  0.],
       [ 0.,  0.]])
 >>> np.zeros((2,), dtype=[('x', 'i4'), ('y', 'i4')]) # custom dtype
array([(0, 0), (0, 0)],
      dtype=[('x', '<i4'), ('y', '<i4')])
numpy.zeros_like   numpy.zeros_like(a, dtype=None, order='K', subok=True, shape=None)[source]
 
Return an array of zeros with the same shape and type as a given array.  Parameters 
 
aarray_like


The shape and data-type of a define these same attributes of the returned array.  
dtypedata-type, optional


Overrides the data type of the result.  New in version 1.6.0.   
order{‘C’, ‘F’, ‘A’, or ‘K’}, optional


Overrides the memory layout of the result. ‘C’ means C-order, ‘F’ means F-order, ‘A’ means ‘F’ if a is Fortran contiguous, ‘C’ otherwise. ‘K’ means match the layout of a as closely as possible.  New in version 1.6.0.   
subokbool, optional.


If True, then the newly created array will use the sub-class type of a, otherwise it will be a base-class array. Defaults to True.  
shapeint or sequence of ints, optional.


Overrides the shape of the result. If order=’K’ and the number of dimensions is unchanged, will try to keep order, otherwise, order=’C’ is implied.  New in version 1.17.0.     Returns 
 
outndarray


Array of zeros with the same shape and type as a.      See also  empty_like

Return an empty array with shape and type of input.  ones_like

Return an array of ones with shape and type of input.  full_like

Return a new array with shape of input filled with value.  zeros

Return a new array setting values to zero.    Examples >>> x = np.arange(6)
>>> x = x.reshape((2, 3))
>>> x
array([[0, 1, 2],
       [3, 4, 5]])
>>> np.zeros_like(x)
array([[0, 0, 0],
       [0, 0, 0]])
 >>> y = np.arange(3, dtype=float)
>>> y
array([0., 1., 2.])
>>> np.zeros_like(y)
array([0.,  0.,  0.])
def f_2173087():
    """Create 3d array of zeroes of size `(3,3,3)`
    """
    return  
 --------------------

def f_6266727(content):
    """cut off the last word of a sentence `content`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
numpy.float32[source]
 
alias of numpy.single
tf.experimental.numpy.asarray TensorFlow variant of NumPy's asarray. 
tf.experimental.numpy.asarray(
    a, dtype=None
)
 Unsupported arguments: order. See the NumPy documentation for numpy.asarray.
tf.experimental.numpy.array TensorFlow variant of NumPy's array. 
tf.experimental.numpy.array(
    val, dtype=None, copy=True, ndmin=0
)
 Since Tensors are immutable, a copy is made only if val is placed on a different device than the current one. Even if copy is False, a new Tensor may need to be built to satisfy dtype and ndim. This is used only if val is an ndarray or a Tensor. See the NumPy documentation for numpy.array.
numpy.float64[source]
 
alias of numpy.double
sklearn.utils.as_float_array  
sklearn.utils.as_float_array(X, *, copy=True, force_all_finite=True) [source]
 
Converts an array-like to an array of floats. The new dtype will be np.float32 or np.float64, depending on the original type. The function can create a copy or modify the argument depending on the argument copy.  Parameters 
 
X{array-like, sparse matrix} 

copybool, default=True 

If True, a copy of X will be created. If False, a copy may still be returned if X’s dtype is not a floating point type.  
force_all_finitebool or ‘allow-nan’, default=True 

Whether to raise an error on np.inf, np.nan, pd.NA in X. The possibilities are:  True: Force all values of X to be finite. False: accepts np.inf, np.nan, pd.NA in X. ‘allow-nan’: accepts only np.nan and pd.NA values in X. Values cannot be infinite.   New in version 0.20: force_all_finite accepts the string 'allow-nan'.   Changed in version 0.23: Accepts pd.NA and converts it into np.nan     Returns 
 
XT{ndarray, sparse matrix} 

An array of type float.
def f_30385151(x):
    """convert scalar `x` to array
    """
     
 --------------------

def f_15856127(L):
    """sum all elements of nested list `L`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
classmethod float.fromhex(s)  
Class method to return the float represented by a hexadecimal string s. The string s may have leading and trailing whitespace.
class float([x])  
Return a floating point number constructed from a number or string x. If the argument is a string, it should contain a decimal number, optionally preceded by a sign, and optionally embedded in whitespace. The optional sign may be '+' or '-'; a '+' sign has no effect on the value produced. The argument may also be a string representing a NaN (not-a-number), or a positive or negative infinity. More precisely, the input must conform to the following grammar after leading and trailing whitespace characters are removed: 
sign           ::=  "+" | "-"
infinity       ::=  "Infinity" | "inf"
nan            ::=  "nan"
numeric_value  ::=  floatnumber | infinity | nan
numeric_string ::=  [sign] numeric_value
 Here floatnumber is the form of a Python floating-point literal, described in Floating point literals. Case is not significant, so, for example, “inf”, “Inf”, “INFINITY” and “iNfINity” are all acceptable spellings for positive infinity. Otherwise, if the argument is an integer or a floating point number, a floating point number with the same value (within Python’s floating point precision) is returned. If the argument is outside the range of a Python float, an OverflowError will be raised. For a general Python object x, float(x) delegates to x.__float__(). If __float__() is not defined then it falls back to __index__(). If no argument is given, 0.0 is returned. Examples: >>> float('+1.23')
1.23
>>> float('   -12345\n')
-12345.0
>>> float('1e-003')
0.001
>>> float('+1E6')
1000000.0
>>> float('-Infinity')
-inf
 The float type is described in Numeric Types — int, float, complex.  Changed in version 3.6: Grouping digits with underscores as in code literals is allowed.   Changed in version 3.7: x is now a positional-only parameter.   Changed in version 3.8: Falls back to __index__() if __float__() is not defined.
float.hex()  
Return a representation of a floating-point number as a hexadecimal string. For finite floating-point numbers, this representation will always include a leading 0x and a trailing p and exponent.
locale.atof(string)  
Converts a string to a floating point number, following the LC_NUMERIC settings.
classmethod fromhex(string)  
This bytes class method returns a bytes object, decoding the given string object. The string must contain two hexadecimal digits per byte, with ASCII whitespace being ignored. >>> bytes.fromhex('2Ef0 F1f2  ')
b'.\xf0\xf1\xf2'
  Changed in version 3.7: bytes.fromhex() now skips all ASCII whitespace in the string, not just spaces.
def f_1592158():
    """convert hex string '470FC614' to a float number
    """
    return  
 --------------------

def f_5010536(my_dict):
    """Multiple each value by `2` for all keys in a dictionary `my_dict`
    """
     
 --------------------
Please refer to the following documentation to generate the code:
curses.napms(ms)  
Sleep for ms milliseconds.
curses.def_shell_mode()  
Save the current terminal mode as the “shell” mode, the mode when the running program is not using curses. (Its counterpart is the “program” mode, when the program is using curses capabilities.) Subsequent calls to reset_shell_mode() will restore this mode.
time.sleep(secs)  
Suspend execution of the calling thread for the given number of seconds. The argument may be a floating point number to indicate a more precise sleep time. The actual suspension time may be less than that requested because any caught signal will terminate the sleep() following execution of that signal’s catching routine. Also, the suspension time may be longer than requested by an arbitrary amount because of the scheduling of other activity in the system.  Changed in version 3.5: The function now sleeps at least secs even if the sleep is interrupted by a signal, except if the signal handler raises an exception (see PEP 475 for the rationale).
math.sin(x)  
Return the sine of x radians.
IDLE Source code: Lib/idlelib/ IDLE is Python’s Integrated Development and Learning Environment. IDLE has the following features:  coded in 100% pure Python, using the tkinter GUI toolkit cross-platform: works mostly the same on Windows, Unix, and macOS Python shell window (interactive interpreter) with colorizing of code input, output, and error messages multi-window text editor with multiple undo, Python colorizing, smart indent, call tips, auto completion, and other features search within any window, replace within editor windows, and search through multiple files (grep) debugger with persistent breakpoints, stepping, and viewing of global and local namespaces configuration, browsers, and other dialogs  Menus IDLE has two main window types, the Shell window and the Editor window. It is possible to have multiple editor windows simultaneously. On Windows and Linux, each has its own top menu. Each menu documented below indicates which window type it is associated with. Output windows, such as used for Edit => Find in Files, are a subtype of editor window. They currently have the same top menu but a different default title and context menu. On macOS, there is one application menu. It dynamically changes according to the window currently selected. It has an IDLE menu, and some entries described below are moved around to conform to Apple guidelines. File menu (Shell and Editor)  New File

Create a new file editing window.  Open…

Open an existing file with an Open dialog.  Recent Files

Open a list of recent files. Click one to open it.  Open Module…

Open an existing module (searches sys.path).    Class Browser

Show functions, classes, and methods in the current Editor file in a tree structure. In the shell, open a module first.  Path Browser

Show sys.path directories, modules, functions, classes and methods in a tree structure.  Save

Save the current window to the associated file, if there is one. Windows that have been changed since being opened or last saved have a * before and after the window title. If there is no associated file, do Save As instead.  Save As…

Save the current window with a Save As dialog. The file saved becomes the new associated file for the window.  Save Copy As…

Save the current window to different file without changing the associated file.  Print Window

Print the current window to the default printer.  Close

Close the current window (ask to save if unsaved).  Exit

Close all windows and quit IDLE (ask to save unsaved windows).   Edit menu (Shell and Editor)  Undo

Undo the last change to the current window. A maximum of 1000 changes may be undone.  Redo

Redo the last undone change to the current window.  Cut

Copy selection into the system-wide clipboard; then delete the selection.  Copy

Copy selection into the system-wide clipboard.  Paste

Insert contents of the system-wide clipboard into the current window.   The clipboard functions are also available in context menus.  Select All

Select the entire contents of the current window.  Find…

Open a search dialog with many options  Find Again

Repeat the last search, if there is one.  Find Selection

Search for the currently selected string, if there is one.  Find in Files…

Open a file search dialog. Put results in a new output window.  Replace…

Open a search-and-replace dialog.  Go to Line

Move the cursor to the beginning of the line requested and make that line visible. A request past the end of the file goes to the end. Clear any selection and update the line and column status.  Show Completions

Open a scrollable list allowing selection of existing names. See Completions in the Editing and navigation section below.  Expand Word

Expand a prefix you have typed to match a full word in the same window; repeat to get a different expansion.  Show call tip

After an unclosed parenthesis for a function, open a small window with function parameter hints. See Calltips in the Editing and navigation section below.  Show surrounding parens

Highlight the surrounding parenthesis.   Format menu (Editor window only)  Indent Region

Shift selected lines right by the indent width (default 4 spaces).  Dedent Region

Shift selected lines left by the indent width (default 4 spaces).  Comment Out Region

Insert ## in front of selected lines.  Uncomment Region

Remove leading # or ## from selected lines.  Tabify Region

Turn leading stretches of spaces into tabs. (Note: We recommend using 4 space blocks to indent Python code.)  Untabify Region

Turn all tabs into the correct number of spaces.  Toggle Tabs

Open a dialog to switch between indenting with spaces and tabs.  New Indent Width

Open a dialog to change indent width. The accepted default by the Python community is 4 spaces.  Format Paragraph

Reformat the current blank-line-delimited paragraph in comment block or multiline string or selected line in a string. All lines in the paragraph will be formatted to less than N columns, where N defaults to 72.  Strip trailing whitespace

Remove trailing space and other whitespace characters after the last non-whitespace character of a line by applying str.rstrip to each line, including lines within multiline strings. Except for Shell windows, remove extra newlines at the end of the file.   Run menu (Editor window only)  Run Module

Do Check Module. If no error, restart the shell to clean the environment, then execute the module. Output is displayed in the Shell window. Note that output requires use of print or write. When execution is complete, the Shell retains focus and displays a prompt. At this point, one may interactively explore the result of execution. This is similar to executing a file with python -i file at a command line.    Run… Customized

Same as Run Module, but run the module with customized settings. Command Line Arguments extend sys.argv as if passed on a command line. The module can be run in the Shell without restarting.    Check Module

Check the syntax of the module currently open in the Editor window. If the module has not been saved IDLE will either prompt the user to save or autosave, as selected in the General tab of the Idle Settings dialog. If there is a syntax error, the approximate location is indicated in the Editor window.    Python Shell

Open or wake up the Python Shell window.   Shell menu (Shell window only)  View Last Restart

Scroll the shell window to the last Shell restart.  Restart Shell

Restart the shell to clean the environment and reset display and exception handling.  Previous History

Cycle through earlier commands in history which match the current entry.  Next History

Cycle through later commands in history which match the current entry.  Interrupt Execution

Stop a running program.   Debug menu (Shell window only)  Go to File/Line

Look on the current line. with the cursor, and the line above for a filename and line number. If found, open the file if not already open, and show the line. Use this to view source lines referenced in an exception traceback and lines found by Find in Files. Also available in the context menu of the Shell window and Output windows.    Debugger (toggle)

When activated, code entered in the Shell or run from an Editor will run under the debugger. In the Editor, breakpoints can be set with the context menu. This feature is still incomplete and somewhat experimental.  Stack Viewer

Show the stack traceback of the last exception in a tree widget, with access to locals and globals.  Auto-open Stack Viewer

Toggle automatically opening the stack viewer on an unhandled exception.   Options menu (Shell and Editor)  Configure IDLE

Open a configuration dialog and change preferences for the following: fonts, indentation, keybindings, text color themes, startup windows and size, additional help sources, and extensions. On macOS, open the configuration dialog by selecting Preferences in the application menu. For more details, see Setting preferences under Help and preferences.   Most configuration options apply to all windows or all future windows. The option items below only apply to the active window.  Show/Hide Code Context (Editor Window only)

Open a pane at the top of the edit window which shows the block context of the code which has scrolled above the top of the window. See Code Context in the Editing and Navigation section below.  Show/Hide Line Numbers (Editor Window only)

Open a column to the left of the edit window which shows the number of each line of text. The default is off, which may be changed in the preferences (see Setting preferences).  Zoom/Restore Height

Toggles the window between normal size and maximum height. The initial size defaults to 40 lines by 80 chars unless changed on the General tab of the Configure IDLE dialog. The maximum height for a screen is determined by momentarily maximizing a window the first time one is zoomed on the screen. Changing screen settings may invalidate the saved height. This toggle has no effect when a window is maximized.   Window menu (Shell and Editor) Lists the names of all open windows; select one to bring it to the foreground (deiconifying it if necessary). Help menu (Shell and Editor)  About IDLE

Display version, copyright, license, credits, and more.  IDLE Help

Display this IDLE document, detailing the menu options, basic editing and navigation, and other tips.  Python Docs

Access local Python documentation, if installed, or start a web browser and open docs.python.org showing the latest Python documentation.  Turtle Demo

Run the turtledemo module with example Python code and turtle drawings.   Additional help sources may be added here with the Configure IDLE dialog under the General tab. See the Help sources subsection below for more on Help menu choices. Context Menus Open a context menu by right-clicking in a window (Control-click on macOS). Context menus have the standard clipboard functions also on the Edit menu.  Cut

Copy selection into the system-wide clipboard; then delete the selection.  Copy

Copy selection into the system-wide clipboard.  Paste

Insert contents of the system-wide clipboard into the current window.   Editor windows also have breakpoint functions. Lines with a breakpoint set are specially marked. Breakpoints only have an effect when running under the debugger. Breakpoints for a file are saved in the user’s .idlerc directory.  Set Breakpoint

Set a breakpoint on the current line.  Clear Breakpoint

Clear the breakpoint on that line.   Shell and Output windows also have the following.  Go to file/line

Same as in Debug menu.   The Shell window also has an output squeezing facility explained in the Python Shell window subsection below.  Squeeze

If the cursor is over an output line, squeeze all the output between the code above and the prompt below down to a ‘Squeezed text’ label.   Editing and navigation Editor windows IDLE may open editor windows when it starts, depending on settings and how you start IDLE. Thereafter, use the File menu. There can be only one open editor window for a given file. The title bar contains the name of the file, the full path, and the version of Python and IDLE running the window. The status bar contains the line number (‘Ln’) and column number (‘Col’). Line numbers start with 1; column numbers with 0. IDLE assumes that files with a known .py* extension contain Python code and that other files do not. Run Python code with the Run menu. Key bindings In this section, ‘C’ refers to the Control key on Windows and Unix and the Command key on macOS.  
Backspace deletes to the left; Del deletes to the right 
C-Backspace delete word left; C-Del delete word to the right Arrow keys and Page Up/Page Down to move around 
C-LeftArrow and C-RightArrow moves by words 
Home/End go to begin/end of line 
C-Home/C-End go to begin/end of file 
Some useful Emacs bindings are inherited from Tcl/Tk:  
C-a beginning of line 
C-e end of line 
C-k kill line (but doesn’t put it in clipboard) 
C-l center window around the insertion point 
C-b go backward one character without deleting (usually you can also use the cursor key for this) 
C-f go forward one character without deleting (usually you can also use the cursor key for this) 
C-p go up one line (usually you can also use the cursor key for this) 
C-d delete next character    Standard keybindings (like C-c to copy and C-v to paste) may work. Keybindings are selected in the Configure IDLE dialog. Automatic indentation After a block-opening statement, the next line is indented by 4 spaces (in the Python Shell window by one tab). After certain keywords (break, return etc.) the next line is dedented. In leading indentation, Backspace deletes up to 4 spaces if they are there. Tab inserts spaces (in the Python Shell window one tab), number depends on Indent width. Currently, tabs are restricted to four spaces due to Tcl/Tk limitations. See also the indent/dedent region commands on the Format menu. Completions Completions are supplied, when requested and available, for module names, attributes of classes or functions, or filenames. Each request method displays a completion box with existing names. (See tab completions below for an exception.) For any box, change the name being completed and the item highlighted in the box by typing and deleting characters; by hitting Up, Down, PageUp, PageDown, Home, and End keys; and by a single click within the box. Close the box with Escape, Enter, and double Tab keys or clicks outside the box. A double click within the box selects and closes. One way to open a box is to type a key character and wait for a predefined interval. This defaults to 2 seconds; customize it in the settings dialog. (To prevent auto popups, set the delay to a large number of milliseconds, such as 100000000.) For imported module names or class or function attributes, type ‘.’. For filenames in the root directory, type os.sep or os.altsep immediately after an opening quote. (On Windows, one can specify a drive first.) Move into subdirectories by typing a directory name and a separator. Instead of waiting, or after a box is closed, open a completion box immediately with Show Completions on the Edit menu. The default hot key is C-space. If one types a prefix for the desired name before opening the box, the first match or near miss is made visible. The result is the same as if one enters a prefix after the box is displayed. Show Completions after a quote completes filenames in the current directory instead of a root directory. Hitting Tab after a prefix usually has the same effect as Show Completions. (With no prefix, it indents.) However, if there is only one match to the prefix, that match is immediately added to the editor text without opening a box. Invoking ‘Show Completions’, or hitting Tab after a prefix, outside of a string and without a preceding ‘.’ opens a box with keywords, builtin names, and available module-level names. When editing code in an editor (as oppose to Shell), increase the available module-level names by running your code and not restarting the Shell thereafter. This is especially useful after adding imports at the top of a file. This also increases possible attribute completions. Completion boxes intially exclude names beginning with ‘_’ or, for modules, not included in ‘__all__’. The hidden names can be accessed by typing ‘_’ after ‘.’, either before or after the box is opened. Calltips A calltip is shown automatically when one types ( after the name of an accessible function. A function name expression may include dots and subscripts. A calltip remains until it is clicked, the cursor is moved out of the argument area, or ) is typed. Whenever the cursor is in the argument part of a definition, select Edit and “Show Call Tip” on the menu or enter its shortcut to display a calltip. The calltip consists of the function’s signature and docstring up to the latter’s first blank line or the fifth non-blank line. (Some builtin functions lack an accessible signature.) A ‘/’ or ‘*’ in the signature indicates that the preceding or following arguments are passed by position or name (keyword) only. Details are subject to change. In Shell, the accessible functions depends on what modules have been imported into the user process, including those imported by Idle itself, and which definitions have been run, all since the last restart. For example, restart the Shell and enter itertools.count(. A calltip appears because Idle imports itertools into the user process for its own use. (This could change.) Enter turtle.write( and nothing appears. Idle does not itself import turtle. The menu entry and shortcut also do nothing. Enter import turtle. Thereafter, turtle.write( will display a calltip. In an editor, import statements have no effect until one runs the file. One might want to run a file after writing import statements, after adding function definitions, or after opening an existing file. Code Context Within an editor window containing Python code, code context can be toggled in order to show or hide a pane at the top of the window. When shown, this pane freezes the opening lines for block code, such as those beginning with class, def, or if keywords, that would have otherwise scrolled out of view. The size of the pane will be expanded and contracted as needed to show the all current levels of context, up to the maximum number of lines defined in the Configure IDLE dialog (which defaults to 15). If there are no current context lines and the feature is toggled on, a single blank line will display. Clicking on a line in the context pane will move that line to the top of the editor. The text and background colors for the context pane can be configured under the Highlights tab in the Configure IDLE dialog. Python Shell window With IDLE’s Shell, one enters, edits, and recalls complete statements. Most consoles and terminals only work with a single physical line at a time. When one pastes code into Shell, it is not compiled and possibly executed until one hits Return. One may edit pasted code first. If one pastes more that one statement into Shell, the result will be a SyntaxError when multiple statements are compiled as if they were one. The editing features described in previous subsections work when entering code interactively. IDLE’s Shell window also responds to the following keys.  
C-c interrupts executing command 
C-d sends end-of-file; closes window if typed at a >>> prompt 
Alt-/ (Expand word) is also useful to reduce typing Command history  
Alt-p retrieves previous command matching what you have typed. On macOS use C-p. 
Alt-n retrieves next. On macOS use C-n. 
Return while on any previous command retrieves that command    Text colors Idle defaults to black on white text, but colors text with special meanings. For the shell, these are shell output, shell error, user output, and user error. For Python code, at the shell prompt or in an editor, these are keywords, builtin class and function names, names following class and def, strings, and comments. For any text window, these are the cursor (when present), found text (when possible), and selected text. Text coloring is done in the background, so uncolorized text is occasionally visible. To change the color scheme, use the Configure IDLE dialog Highlighting tab. The marking of debugger breakpoint lines in the editor and text in popups and dialogs is not user-configurable. Startup and code execution Upon startup with the -s option, IDLE will execute the file referenced by the environment variables IDLESTARTUP or PYTHONSTARTUP. IDLE first checks for IDLESTARTUP; if IDLESTARTUP is present the file referenced is run. If IDLESTARTUP is not present, IDLE checks for PYTHONSTARTUP. Files referenced by these environment variables are convenient places to store functions that are used frequently from the IDLE shell, or for executing import statements to import common modules. In addition, Tk also loads a startup file if it is present. Note that the Tk file is loaded unconditionally. This additional file is .Idle.py and is looked for in the user’s home directory. Statements in this file will be executed in the Tk namespace, so this file is not useful for importing functions to be used from IDLE’s Python shell. Command line usage idle.py [-c command] [-d] [-e] [-h] [-i] [-r file] [-s] [-t title] [-] [arg] ...

-c command  run command in the shell window
-d          enable debugger and open shell window
-e          open editor window
-h          print help message with legal combinations and exit
-i          open shell window
-r file     run file in shell window
-s          run $IDLESTARTUP or $PYTHONSTARTUP first, in shell window
-t title    set title of shell window
-           run stdin in shell (- must be last option before args)
 If there are arguments:  If -, -c, or r is used, all arguments are placed in sys.argv[1:...] and sys.argv[0] is set to '', '-c', or '-r'. No editor window is opened, even if that is the default set in the Options dialog. Otherwise, arguments are files opened for editing and sys.argv reflects the arguments passed to IDLE itself.  Startup failure IDLE uses a socket to communicate between the IDLE GUI process and the user code execution process. A connection must be established whenever the Shell starts or restarts. (The latter is indicated by a divider line that says ‘RESTART’). If the user process fails to connect to the GUI process, it usually displays a Tk error box with a ‘cannot connect’ message that directs the user here. It then exits. One specific connection failure on Unix systems results from misconfigured masquerading rules somewhere in a system’s network setup. When IDLE is started from a terminal, one will see a message starting with ** Invalid host:. The valid value is 127.0.0.1 (idlelib.rpc.LOCALHOST). One can diagnose with tcpconnect -irv 127.0.0.1 6543 in one terminal window and tcplisten <same args> in another. A common cause of failure is a user-written file with the same name as a standard library module, such as random.py and tkinter.py. When such a file is located in the same directory as a file that is about to be run, IDLE cannot import the stdlib file. The current fix is to rename the user file. Though less common than in the past, an antivirus or firewall program may stop the connection. If the program cannot be taught to allow the connection, then it must be turned off for IDLE to work. It is safe to allow this internal connection because no data is visible on external ports. A similar problem is a network mis-configuration that blocks connections. Python installation issues occasionally stop IDLE: multiple versions can clash, or a single installation might need admin access. If one undo the clash, or cannot or does not want to run as admin, it might be easiest to completely remove Python and start over. A zombie pythonw.exe process could be a problem. On Windows, use Task Manager to check for one and stop it if there is. Sometimes a restart initiated by a program crash or Keyboard Interrupt (control-C) may fail to connect. Dismissing the error box or using Restart Shell on the Shell menu may fix a temporary problem. When IDLE first starts, it attempts to read user configuration files in ~/.idlerc/ (~ is one’s home directory). If there is a problem, an error message should be displayed. Leaving aside random disk glitches, this can be prevented by never editing the files by hand. Instead, use the configuration dialog, under Options. Once there is an error in a user configuration file, the best solution may be to delete it and start over with the settings dialog. If IDLE quits with no message, and it was not started from a console, try starting it from a console or terminal (python -m idlelib) and see if this results in an error message. On Unix-based systems with tcl/tk older than 8.6.11 (see About IDLE) certain characters of certain fonts can cause a tk failure with a message to the terminal. This can happen either if one starts IDLE to edit a file with such a character or later when entering such a character. If one cannot upgrade tcl/tk, then re-configure IDLE to use a font that works better. Running user code With rare exceptions, the result of executing Python code with IDLE is intended to be the same as executing the same code by the default method, directly with Python in a text-mode system console or terminal window. However, the different interface and operation occasionally affect visible results. For instance, sys.modules starts with more entries, and threading.active_count() returns 2 instead of 1. By default, IDLE runs user code in a separate OS process rather than in the user interface process that runs the shell and editor. In the execution process, it replaces sys.stdin, sys.stdout, and sys.stderr with objects that get input from and send output to the Shell window. The original values stored in sys.__stdin__, sys.__stdout__, and sys.__stderr__ are not touched, but may be None. Sending print output from one process to a text widget in another is slower than printing to a system terminal in the same process. This has the most effect when printing multiple arguments, as the string for each argument, each separator, the newline are sent separately. For development, this is usually not a problem, but if one wants to print faster in IDLE, format and join together everything one wants displayed together and then print a single string. Both format strings and str.join() can help combine fields and lines. IDLE’s standard stream replacements are not inherited by subprocesses created in the execution process, whether directly by user code or by modules such as multiprocessing. If such subprocess use input from sys.stdin or print or write to sys.stdout or sys.stderr, IDLE should be started in a command line window. The secondary subprocess will then be attached to that window for input and output. If sys is reset by user code, such as with importlib.reload(sys), IDLE’s changes are lost and input from the keyboard and output to the screen will not work correctly. When Shell has the focus, it controls the keyboard and screen. This is normally transparent, but functions that directly access the keyboard and screen will not work. These include system-specific functions that determine whether a key has been pressed and if so, which. The IDLE code running in the execution process adds frames to the call stack that would not be there otherwise. IDLE wraps sys.getrecursionlimit and sys.setrecursionlimit to reduce the effect of the additional stack frames. When user code raises SystemExit either directly or by calling sys.exit, IDLE returns to a Shell prompt instead of exiting. User output in Shell When a program outputs text, the result is determined by the corresponding output device. When IDLE executes user code, sys.stdout and sys.stderr are connected to the display area of IDLE’s Shell. Some of its features are inherited from the underlying Tk Text widget. Others are programmed additions. Where it matters, Shell is designed for development rather than production runs. For instance, Shell never throws away output. A program that sends unlimited output to Shell will eventually fill memory, resulting in a memory error. In contrast, some system text windows only keep the last n lines of output. A Windows console, for instance, keeps a user-settable 1 to 9999 lines, with 300 the default. A Tk Text widget, and hence IDLE’s Shell, displays characters (codepoints) in the BMP (Basic Multilingual Plane) subset of Unicode. Which characters are displayed with a proper glyph and which with a replacement box depends on the operating system and installed fonts. Tab characters cause the following text to begin after the next tab stop. (They occur every 8 ‘characters’). Newline characters cause following text to appear on a new line. Other control characters are ignored or displayed as a space, box, or something else, depending on the operating system and font. (Moving the text cursor through such output with arrow keys may exhibit some surprising spacing behavior.) >>> s = 'a\tb\a<\x02><\r>\bc\nd'  # Enter 22 chars.
>>> len(s)
14
>>> s  # Display repr(s)
'a\tb\x07<\x02><\r>\x08c\nd'
>>> print(s, end='')  # Display s as is.
# Result varies by OS and font.  Try it.
 The repr function is used for interactive echo of expression values. It returns an altered version of the input string in which control codes, some BMP codepoints, and all non-BMP codepoints are replaced with escape codes. As demonstrated above, it allows one to identify the characters in a string, regardless of how they are displayed. Normal and error output are generally kept separate (on separate lines) from code input and each other. They each get different highlight colors. For SyntaxError tracebacks, the normal ‘^’ marking where the error was detected is replaced by coloring the text with an error highlight. When code run from a file causes other exceptions, one may right click on a traceback line to jump to the corresponding line in an IDLE editor. The file will be opened if necessary. Shell has a special facility for squeezing output lines down to a ‘Squeezed text’ label. This is done automatically for output over N lines (N = 50 by default). N can be changed in the PyShell section of the General page of the Settings dialog. Output with fewer lines can be squeezed by right clicking on the output. This can be useful lines long enough to slow down scrolling. Squeezed output is expanded in place by double-clicking the label. It can also be sent to the clipboard or a separate view window by right-clicking the label. Developing tkinter applications IDLE is intentionally different from standard Python in order to facilitate development of tkinter programs. Enter import tkinter as tk;
root = tk.Tk() in standard Python and nothing appears. Enter the same in IDLE and a tk window appears. In standard Python, one must also enter root.update() to see the window. IDLE does the equivalent in the background, about 20 times a second, which is about every 50 milliseconds. Next enter b = tk.Button(root, text='button'); b.pack(). Again, nothing visibly changes in standard Python until one enters root.update(). Most tkinter programs run root.mainloop(), which usually does not return until the tk app is destroyed. If the program is run with python -i or from an IDLE editor, a >>> shell prompt does not appear until mainloop() returns, at which time there is nothing left to interact with. When running a tkinter program from an IDLE editor, one can comment out the mainloop call. One then gets a shell prompt immediately and can interact with the live application. One just has to remember to re-enable the mainloop call when running in standard Python. Running without a subprocess By default, IDLE executes user code in a separate subprocess via a socket, which uses the internal loopback interface. This connection is not externally visible and no data is sent to or received from the Internet. If firewall software complains anyway, you can ignore it. If the attempt to make the socket connection fails, Idle will notify you. Such failures are sometimes transient, but if persistent, the problem may be either a firewall blocking the connection or misconfiguration of a particular system. Until the problem is fixed, one can run Idle with the -n command line switch. If IDLE is started with the -n command line switch it will run in a single process and will not create the subprocess which runs the RPC Python execution server. This can be useful if Python cannot create the subprocess or the RPC socket interface on your platform. However, in this mode user code is not isolated from IDLE itself. Also, the environment is not restarted when Run/Run Module (F5) is selected. If your code has been modified, you must reload() the affected modules and re-import any specific items (e.g. from foo import baz) if the changes are to take effect. For these reasons, it is preferable to run IDLE with the default subprocess if at all possible.  Deprecated since version 3.4.  Help and preferences Help sources Help menu entry “IDLE Help” displays a formatted html version of the IDLE chapter of the Library Reference. The result, in a read-only tkinter text window, is close to what one sees in a web browser. Navigate through the text with a mousewheel, the scrollbar, or up and down arrow keys held down. Or click the TOC (Table of Contents) button and select a section header in the opened box. Help menu entry “Python Docs” opens the extensive sources of help, including tutorials, available at docs.python.org/x.y, where ‘x.y’ is the currently running Python version. If your system has an off-line copy of the docs (this may be an installation option), that will be opened instead. Selected URLs can be added or removed from the help menu at any time using the General tab of the Configure IDLE dialog. Setting preferences The font preferences, highlighting, keys, and general preferences can be changed via Configure IDLE on the Option menu. Non-default user settings are saved in a .idlerc directory in the user’s home directory. Problems caused by bad user configuration files are solved by editing or deleting one or more of the files in .idlerc. On the Font tab, see the text sample for the effect of font face and size on multiple characters in multiple languages. Edit the sample to add other characters of personal interest. Use the sample to select monospaced fonts. If particular characters have problems in Shell or an editor, add them to the top of the sample and try changing first size and then font. On the Highlights and Keys tab, select a built-in or custom color theme and key set. To use a newer built-in color theme or key set with older IDLEs, save it as a new custom theme or key set and it well be accessible to older IDLEs. IDLE on macOS Under System Preferences: Dock, one can set “Prefer tabs when opening documents” to “Always”. This setting is not compatible with the tk/tkinter GUI framework used by IDLE, and it breaks a few IDLE features. Extensions IDLE contains an extension facility. Preferences for extensions can be changed with the Extensions tab of the preferences dialog. See the beginning of config-extensions.def in the idlelib directory for further information. The only current default extension is zzdummy, an example also used for testing.
def f_13745648():
    """running bash script 'sleep.sh'
    """
    return  
 --------------------

def f_44778(l):
    """Join elements of list `l` with a comma `,`
    """
    return  
 --------------------

def f_44778(myList):
    """make a comma-separated string from a list `myList`
    """
     
 --------------------

def f_7286365():
    """reverse the list that contains 1 to 10
    """
    return  
 --------------------

def f_18454570():
    """remove substring 'bag,' from a string 'lamp, bag, mirror'
    """
    return  
 --------------------

def f_4357787(s):
    """Reverse the order of words, delimited by `.`, in string `s`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
time.ctime([secs])  
Convert a time expressed in seconds since the epoch to a string of a form: 'Sun Jun 20 23:21:05 1993' representing local time. The day field is two characters long and is space padded if the day is a single digit, e.g.: 'Wed Jun  9 04:26:40 1993'. If secs is not provided or None, the current time as returned by time() is used. ctime(secs) is equivalent to asctime(localtime(secs)). Locale information is not used by ctime().
http_date(epoch_seconds=None) [source]
 
Formats the time to match the RFC 1123#section-5.2.14 date format as specified by HTTP RFC 7231#section-7.1.1.1. Accepts a floating point number expressed in seconds since the epoch in UTC–such as that outputted by time.time(). If set to None, defaults to the current time. Outputs a string in the format Wdy, DD Mon YYYY HH:MM:SS GMT.
pandas.Timedelta.isoformat   Timedelta.isoformat()
 
Format Timedelta as ISO 8601 Duration like P[n]Y[n]M[n]DT[n]H[n]M[n]S, where the [n] s are replaced by the values. See https://en.wikipedia.org/wiki/ISO_8601#Durations.  Returns 
 str
    See also  Timestamp.isoformat

Function is used to convert the given Timestamp object into the ISO format.    Notes The longest component is days, whose value may be larger than 365. Every component is always included, even if its value is 0. Pandas uses nanosecond precision, so up to 9 decimal places may be included in the seconds component. Trailing 0’s are removed from the seconds component after the decimal. We do not 0 pad components, so it’s …T5H…, not …T05H… Examples 
>>> td = pd.Timedelta(days=6, minutes=50, seconds=3,
...                   milliseconds=10, microseconds=10, nanoseconds=12)
  
>>> td.isoformat()
'P6DT0H50M3.010010012S'
>>> pd.Timedelta(hours=1, seconds=10).isoformat()
'P0DT1H0M10S'
>>> pd.Timedelta(days=500.5).isoformat()
'P500DT12H0M0S'
shlex.quote(s)  
Return a shell-escaped version of the string s. The returned value is a string that can safely be used as one token in a shell command line, for cases where you cannot use a list. This idiom would be unsafe: >>> filename = 'somefile; rm -rf ~'
>>> command = 'ls -l {}'.format(filename)
>>> print(command)  # executed by a shell: boom!
ls -l somefile; rm -rf ~
 quote() lets you plug the security hole: >>> from shlex import quote
>>> command = 'ls -l {}'.format(quote(filename))
>>> print(command)
ls -l 'somefile; rm -rf ~'
>>> remote_command = 'ssh home {}'.format(quote(command))
>>> print(remote_command)
ssh home 'ls -l '"'"'somefile; rm -rf ~'"'"''
 The quoting is compatible with UNIX shells and with split(): >>> from shlex import split
>>> remote_command = split(remote_command)
>>> remote_command
['ssh', 'home', "ls -l 'somefile; rm -rf ~'"]
>>> command = split(remote_command[-1])
>>> command
['ls', '-l', 'somefile; rm -rf ~']
  New in version 3.3.
flask.escape()  
Replace the characters &, <, >, ', and " in the string with HTML-safe sequences. Use this if you need to display text that might contain such characters in HTML. If the object has an __html__ method, it is called and the return value is assumed to already be safe for HTML.  Parameters 
s – An object to be converted to a string and escaped.  Returns 
A Markup string with the escaped text.
def f_21787496(s):
    """convert epoch time represented as milliseconds `s` to string using format '%Y-%m-%d %H:%M:%S.%f'
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
http_date(epoch_seconds=None) [source]
 
Formats the time to match the RFC 1123#section-5.2.14 date format as specified by HTTP RFC 7231#section-7.1.1.1. Accepts a floating point number expressed in seconds since the epoch in UTC–such as that outputted by time.time(). If set to None, defaults to the current time. Outputs a string in the format Wdy, DD Mon YYYY HH:MM:SS GMT.
time.asctime([t])  
Convert a tuple or struct_time representing a time as returned by gmtime() or localtime() to a string of the following form: 'Sun Jun 20 23:21:05 1993'. The day field is two characters long and is space padded if the day is a single digit, e.g.: 'Wed Jun  9 04:26:40 1993'. If t is not provided, the current time as returned by localtime() is used. Locale information is not used by asctime().  Note Unlike the C function of the same name, asctime() does not add a trailing newline.
time.ctime([secs])  
Convert a time expressed in seconds since the epoch to a string of a form: 'Sun Jun 20 23:21:05 1993' representing local time. The day field is two characters long and is space padded if the day is a single digit, e.g.: 'Wed Jun  9 04:26:40 1993'. If secs is not provided or None, the current time as returned by time() is used. ctime(secs) is equivalent to asctime(localtime(secs)). Locale information is not used by ctime().
skimage.color.separate_stains(rgb, conv_matrix) [source]
 
RGB to stain color space conversion.  Parameters 
 
rgb(…, 3) array_like 

The image in RGB format. Final dimension denotes channels.  conv_matrix: ndarray

The stain separation matrix as described by G. Landini [1].    Returns 
 
out(…, 3) ndarray 

The image in stain color space. Same dimensions as input.    Raises 
 ValueError

If rgb is not at least 2-D with shape (…, 3).     Notes Stain separation matrices available in the color module and their respective colorspace:  
hed_from_rgb: Hematoxylin + Eosin + DAB 
hdx_from_rgb: Hematoxylin + DAB 
fgx_from_rgb: Feulgen + Light Green 
bex_from_rgb: Giemsa stain : Methyl Blue + Eosin 
rbd_from_rgb: FastRed + FastBlue + DAB 
gdx_from_rgb: Methyl Green + DAB 
hax_from_rgb: Hematoxylin + AEC 
bro_from_rgb: Blue matrix Anilline Blue + Red matrix Azocarmine + Orange matrix Orange-G 
bpx_from_rgb: Methyl Blue + Ponceau Fuchsin 
ahx_from_rgb: Alcian Blue + Hematoxylin 
hpx_from_rgb: Hematoxylin + PAS  This implementation borrows some ideas from DIPlib [2], e.g. the compensation using a small value to avoid log artifacts when calculating the Beer-Lambert law. References  
1  
https://web.archive.org/web/20160624145052/http://www.mecourse.com/landinig/software/cdeconv/cdeconv.html  
2  
https://github.com/DIPlib/diplib/  
3  
A. C. Ruifrok and D. A. Johnston, “Quantification of histochemical staining by color deconvolution,” Anal. Quant. Cytol. Histol., vol. 23, no. 4, pp. 291–299, Aug. 2001.   Examples >>> from skimage import data
>>> from skimage.color import separate_stains, hdx_from_rgb
>>> ihc = data.immunohistochemistry()
>>> ihc_hdx = separate_stains(ihc, hdx_from_rgb)
stat.IO_REPARSE_TAG_SYMLINK  
stat.IO_REPARSE_TAG_MOUNT_POINT  
stat.IO_REPARSE_TAG_APPEXECLINK  
 New in version 3.8.
def f_21787496():
    """parse milliseconds epoch time '1236472051807' to format '%Y-%m-%d %H:%M:%S'
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
get_prev_week(date)  
Returns a date object containing the first day of the week before the date provided. This function can also return None or raise an Http404 exception, depending on the values of allow_empty and allow_future.
get_next_week(date)  
Returns a date object containing the first day of the week after the date provided. This function can also return None or raise an Http404 exception, depending on the values of allow_empty and allow_future.
get_previous_day(date)  
Returns a date object containing the previous valid day. This function can also return None or raise an Http404 exception, depending on the values of allow_empty and allow_future.
str.removeprefix(prefix, /)  
If the string starts with the prefix string, return string[len(prefix):]. Otherwise, return a copy of the original string: >>> 'TestHook'.removeprefix('Test')
'Hook'
>>> 'BaseTestCase'.removeprefix('Test')
'BaseTestCase'
  New in version 3.9.
pandas.DataFrame.plot   DataFrame.plot(*args, **kwargs)[source]
 
Make plots of Series or DataFrame. Uses the backend specified by the option plotting.backend. By default, matplotlib is used.  Parameters 
 
data:Series or DataFrame


The object for which the method is called.  
x:label or position, default None


Only used if data is a DataFrame.  
y:label, position or list of label, positions, default None


Allows plotting of one column versus another. Only used if data is a DataFrame.  
kind:str


The kind of plot to produce:  ‘line’ : line plot (default) ‘bar’ : vertical bar plot ‘barh’ : horizontal bar plot ‘hist’ : histogram ‘box’ : boxplot ‘kde’ : Kernel Density Estimation plot ‘density’ : same as ‘kde’ ‘area’ : area plot ‘pie’ : pie plot ‘scatter’ : scatter plot (DataFrame only) ‘hexbin’ : hexbin plot (DataFrame only)   
ax:matplotlib axes object, default None


An axes of the current figure.  
subplots:bool, default False


Make separate subplots for each column.  
sharex:bool, default True if ax is None else False


In case subplots=True, share x axis and set some x axis labels to invisible; defaults to True if ax is None otherwise False if an ax is passed in; Be aware, that passing in both an ax and sharex=True will alter all x axis labels for all axis in a figure.  
sharey:bool, default False


In case subplots=True, share y axis and set some y axis labels to invisible.  
layout:tuple, optional


(rows, columns) for the layout of subplots.  
figsize:a tuple (width, height) in inches


Size of a figure object.  
use_index:bool, default True


Use index as ticks for x axis.  
title:str or list


Title to use for the plot. If a string is passed, print the string at the top of the figure. If a list is passed and subplots is True, print each item in the list above the corresponding subplot.  
grid:bool, default None (matlab style default)


Axis grid lines.  
legend:bool or {‘reverse’}


Place legend on axis subplots.  
style:list or dict


The matplotlib line style per column.  
logx:bool or ‘sym’, default False


Use log scaling or symlog scaling on x axis. .. versionchanged:: 0.25.0  
logy:bool or ‘sym’ default False


Use log scaling or symlog scaling on y axis. .. versionchanged:: 0.25.0  
loglog:bool or ‘sym’, default False


Use log scaling or symlog scaling on both x and y axes. .. versionchanged:: 0.25.0  
xticks:sequence


Values to use for the xticks.  
yticks:sequence


Values to use for the yticks.  
xlim:2-tuple/list


Set the x limits of the current axes.  
ylim:2-tuple/list


Set the y limits of the current axes.  
xlabel:label, optional


Name to use for the xlabel on x-axis. Default uses index name as xlabel, or the x-column name for planar plots.  New in version 1.1.0.   Changed in version 1.2.0: Now applicable to planar plots (scatter, hexbin).   
ylabel:label, optional


Name to use for the ylabel on y-axis. Default will show no ylabel, or the y-column name for planar plots.  New in version 1.1.0.   Changed in version 1.2.0: Now applicable to planar plots (scatter, hexbin).   
rot:int, default None


Rotation for ticks (xticks for vertical, yticks for horizontal plots).  
fontsize:int, default None


Font size for xticks and yticks.  
colormap:str or matplotlib colormap object, default None


Colormap to select colors from. If string, load colormap with that name from matplotlib.  
colorbar:bool, optional


If True, plot colorbar (only relevant for ‘scatter’ and ‘hexbin’ plots).  
position:float


Specify relative alignments for bar plot layout. From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5 (center).  
table:bool, Series or DataFrame, default False


If True, draw a table using the data in the DataFrame and the data will be transposed to meet matplotlib’s default layout. If a Series or DataFrame is passed, use passed data to draw a table.  
yerr:DataFrame, Series, array-like, dict and str


See Plotting with Error Bars for detail.  
xerr:DataFrame, Series, array-like, dict and str


Equivalent to yerr.  
stacked:bool, default False in line and bar plots, and True in area plot


If True, create stacked plot.  
sort_columns:bool, default False


Sort column names to determine plot ordering.  
secondary_y:bool or sequence, default False


Whether to plot on the secondary y-axis if a list/tuple, which columns to plot on secondary y-axis.  
mark_right:bool, default True


When using a secondary_y axis, automatically mark the column labels with “(right)” in the legend.  
include_bool:bool, default is False


If True, boolean values can be plotted.  
backend:str, default None


Backend to use instead of the backend specified in the option plotting.backend. For instance, ‘matplotlib’. Alternatively, to specify the plotting.backend for the whole session, set pd.options.plotting.backend.  New in version 1.0.0.   **kwargs

Options to pass to matplotlib plotting method.    Returns 
 
matplotlib.axes.Axes or numpy.ndarray of them

If the backend is not the default matplotlib one, the return value will be the object returned by the backend.     Notes  See matplotlib documentation online for more on this subject If kind = ‘bar’ or ‘barh’, you can specify relative alignments for bar plot layout by position keyword. From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5 (center)
def f_20573459():
    """get the date 7 days before the current date
    """
    return  
 --------------------

def f_15352457(column, data):
    """sum elements at index `column` of each list in list `data`
    """
    return  
 --------------------

def f_15352457(array):
    """sum columns of a list `array`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
base64.standard_b64encode(s)  
Encode bytes-like object s using the standard Base64 alphabet and return the encoded bytes.
binascii.a2b_base64(string)  
Convert a block of base64 data back to binary and return the binary data. More than one line may be passed at a time.
base64.b16encode(s)  
Encode the bytes-like object s using Base16 and return the encoded bytes.
tf.raw_ops.EncodeBase64 Encode strings into web-safe base64 format.  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.raw_ops.EncodeBase64  
tf.raw_ops.EncodeBase64(
    input, pad=False, name=None
)
 Refer to the following article for more information on base64 format: en.wikipedia.org/wiki/Base64. Base64 strings may have padding with '=' at the end so that the encoded has length multiple of 4. See Padding section of the link above. Web-safe means that the encoder uses - and _ instead of + and /.
 


 Args
  input   A Tensor of type string. Strings to be encoded.  
  pad   An optional bool. Defaults to False. Bool whether padding is applied at the ends.  
  name   A name for the operation (optional).   
 


 Returns   A Tensor of type string.
tf.io.encode_base64 Encode strings into web-safe base64 format.  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.encode_base64, tf.compat.v1.io.encode_base64  
tf.io.encode_base64(
    input, pad=False, name=None
)
 Refer to the following article for more information on base64 format: en.wikipedia.org/wiki/Base64. Base64 strings may have padding with '=' at the end so that the encoded has length multiple of 4. See Padding section of the link above. Web-safe means that the encoder uses - and _ instead of + and /.
 


 Args
  input   A Tensor of type string. Strings to be encoded.  
  pad   An optional bool. Defaults to False. Bool whether padding is applied at the ends.  
  name   A name for the operation (optional).   
 


 Returns   A Tensor of type string.
def f_23164058():
    """encode binary string 'your string' to base64 code
    """
    return  
 --------------------

def f_11533274(dicts):
    """combine list of dictionaries `dicts` with the same keys in each list to a single dictionary
    """
    return  
 --------------------

def f_11533274(dicts):
    """Merge a nested dictionary `dicts` into a flat dictionary by concatenating nested values with the same key `k`
    """
    return  
 --------------------

def f_14026704(request):
    """get the url parameter 'myParam' in a Flask view
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
IMAP4.myrights(mailbox)  
Show my ACLs for a mailbox (i.e. the rights that I have on mailbox).
update_viewlim()[source]
 
[Deprecated] Notes  Deprecated since version 3.4:
update_viewlim()[source]
 
[Deprecated] Notes  Deprecated since version 3.4:
array.fromlist(list)  
Append items from the list. This is equivalent to for x in list:
a.append(x) except that if there is a type error, the array is unchanged.
FieldStorage.getlist(name)  
This method always returns a list of values associated with form field name. The method returns an empty list if no such form field or value exists for name. It returns a list consisting of one item if only one such value exists.
def f_11236006(mylist):
    """identify duplicate values in list `mylist`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
SET_NULL  
Set the ForeignKey null; this is only possible if null is True.
none()
empty_result_set_value  
 New in Django 4.0.  Override empty_result_set_value to None since most aggregate functions result in NULL when applied to an empty result set.
empty_result_set_value  
 New in Django 4.0.  Tells Django which value should be returned when the expression is used to apply a function over an empty result set. Defaults to NotImplemented which forces the expression to be computed on the database.
None  
The sole value of the type NoneType. None is frequently used to represent the absence of a value, as when default arguments are not passed to a function. Assignments to None are illegal and raise a SyntaxError.
def f_20211942(db):
    """Insert a 'None' value into a SQLite3 table.
    """
    return  
 --------------------

def f_406121(list_of_menuitems):
    """flatten list `list_of_menuitems`
    """
    return  
 --------------------

def f_4741537(a, b):
    """append elements of a set `b` to a list `a`
    """
     
 --------------------

def f_15851568(x):
    """Split a string `x` by last occurrence of character `-`
    """
    return  
 --------------------

def f_15851568(x):
    """get the last part of a string before the character '-'
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
class urllib.request.FTPHandler  
Open FTP URLs.
FTP.storbinary(cmd, fp, blocksize=8192, callback=None, rest=None)  
Store a file in binary transfer mode. cmd should be an appropriate STOR command: "STOR filename". fp is a file object (opened in binary mode) which is read until EOF using its read() method in blocks of size blocksize to provide the data to be stored. The blocksize argument defaults to 8192. callback is an optional single parameter callable that is called on each block of data after it is sent. rest means the same thing as in the transfercmd() method.  Changed in version 3.2: rest parameter added.
FTP.storlines(cmd, fp, callback=None)  
Store a file in line mode. cmd should be an appropriate STOR command (see storbinary()). Lines are read until EOF from the file object fp (opened in binary mode) using its readline() method to provide the data to be stored. callback is an optional single parameter callable that is called on each line after it is sent.
FTP.login(user='anonymous', passwd='', acct='')  
Log in as the given user. The passwd and acct parameters are optional and default to the empty string. If no user is specified, it defaults to 'anonymous'. If user is 'anonymous', the default passwd is 'anonymous@'. This function should be called only once for each instance, after a connection has been established; it should not be called at all if a host and user were given when the instance was created. Most FTP commands are only allowed after the client has logged in. The acct parameter supplies “accounting information”; few systems implement this.
FTP.voidcmd(cmd)  
Send a simple command string to the server and handle the response. Return nothing if a response code corresponding to success (codes in the range 200–299) is received. Raise error_reply otherwise. Raises an auditing event ftplib.sendcmd with arguments self, cmd.
def f_17438096(filename, ftp):
    """upload file using FTP
    """
     
 --------------------
Please refer to the following documentation to generate the code:
maximum(other) → Tensor  
See torch.maximum()
numpy.matrix.max method   matrix.max(axis=None, out=None)[source]
 
Return the maximum value along an axis.  Parameters 
 See `amax` for complete descriptions
    See also  
amax, ndarray.max

  Notes This is the same as ndarray.max, but returns a matrix object where ndarray.max would return an ndarray. Examples >>> x = np.matrix(np.arange(12).reshape((3,4))); x
matrix([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11]])
>>> x.max()
11
>>> x.max(0)
matrix([[ 8,  9, 10, 11]])
>>> x.max(1)
matrix([[ 3],
        [ 7],
        [11]])
torch.maximum(input, other, *, out=None) → Tensor  
Computes the element-wise maximum of input and other.  Note If one of the elements being compared is a NaN, then that element is returned. maximum() is not supported for tensors with complex dtypes.   Parameters 
 
input (Tensor) – the input tensor. 
other (Tensor) – the second input tensor   Keyword Arguments 
out (Tensor, optional) – the output tensor.   Example: >>> a = torch.tensor((1, 2, -1))
>>> b = torch.tensor((3, 0, 4))
>>> torch.maximum(a, b)
tensor([3, 2, 4])
max(dim=None, keepdim=False) -> Tensor or (Tensor, Tensor)  
See torch.max()
torch.fmax(input, other, *, out=None) → Tensor  
Computes the element-wise maximum of input and other. This is like torch.maximum() except it handles NaNs differently: if exactly one of the two elements being compared is a NaN then the non-NaN element is taken as the maximum. Only if both elements are NaN is NaN propagated. This function is a wrapper around C++’s std::fmax and is similar to NumPy’s fmax function. Supports broadcasting to a common shape, type promotion, and integer and floating-point inputs.  Parameters 
 
input (Tensor) – the input tensor. 
other (Tensor) – the second input tensor   Keyword Arguments 
out (Tensor, optional) – the output tensor.   Example: >>> a = torch.tensor([9.7, float('nan'), 3.1, float('nan')])
>>> b = torch.tensor([-2.2, 0.5, float('nan'), float('nan')])
>>> torch.fmax(a, b)
tensor([9.7000, 0.5000, 3.1000,    nan])
def f_28742436():
    """create array containing the maximum value of respective elements of array `[2, 3, 4]` and array `[1, 5, 2]`
    """
    return  
 --------------------

def f_34280147(l):
    """print a list `l` and move first 3 elements to the end of the list
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
tf.config.LogicalDevice Abstraction for a logical device initialized by the runtime.  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.config.LogicalDevice  
tf.config.LogicalDevice(
    name, device_type
)
 A tf.config.LogicalDevice corresponds to an initialized logical device on a tf.config.PhysicalDevice or a remote device visible to the cluster. Tensors and operations can be placed on a specific logical device by calling tf.device with a specified tf.config.LogicalDevice. Fields:  
name: The fully qualified name of the device. Can be used for Op or function placement. 
device_type: String declaring the type of device such as "CPU" or "GPU". 
 


 Attributes
  name  
 
  device_type
matplotlib.axes.Axes.get_aspect   Axes.get_aspect()[source]
 
Return the aspect ratio of the axes scaling. This is either "auto" or a float giving the ratio of y/x-scale.
buffer_rgba()[source]
 
Get the image as a memoryview to the renderer's buffer. draw must be called at least once before this function will work and to update the renderer for any subsequent changes to the Figure.
remove_callback(oid)[source]
 
Remove a callback based on its observer id.  See also  add_callback
numpy.record.tolist method   record.tolist()
 
Scalar method identical to the corresponding array attribute. Please see ndarray.tolist.
def f_4172131():
    """create a random list of integers
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
time.microsecond  
In range(1000000).
datetime.microsecond  
In range(1000000).
pandas.Timestamp.microsecond   Timestamp.microsecond
pandas.Series.dt.microseconds   Series.dt.microseconds
 
Number of microseconds (>= 0 and less than 1 second) for each element.
pandas.DatetimeIndex.microsecond   propertyDatetimeIndex.microsecond
 
The microseconds of the datetime. Examples 
>>> datetime_series = pd.Series(
...     pd.date_range("2000-01-01", periods=3, freq="us")
... )
>>> datetime_series
0   2000-01-01 00:00:00.000000
1   2000-01-01 00:00:00.000001
2   2000-01-01 00:00:00.000002
dtype: datetime64[ns]
>>> datetime_series.dt.microsecond
0       0
1       1
2       2
dtype: int64
def f_6677332():
    """Using %f with strftime() in Python to get microseconds
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
pandas.DataFrame.filter   DataFrame.filter(items=None, like=None, regex=None, axis=None)[source]
 
Subset the dataframe rows or columns according to the specified index labels. Note that this routine does not filter a dataframe on its contents. The filter is applied to the labels of the index.  Parameters 
 
items:list-like


Keep labels from axis which are in items.  
like:str


Keep labels from axis for which “like in label == True”.  
regex:str (regular expression)


Keep labels from axis for which re.search(regex, label) == True.  
axis:{0 or ‘index’, 1 or ‘columns’, None}, default None


The axis to filter on, expressed either as an index (int) or axis name (str). By default this is the info axis, ‘index’ for Series, ‘columns’ for DataFrame.    Returns 
 same type as input object
    See also  DataFrame.loc

Access a group of rows and columns by label(s) or a boolean array.    Notes The items, like, and regex parameters are enforced to be mutually exclusive. axis defaults to the info axis that is used when indexing with []. Examples 
>>> df = pd.DataFrame(np.array(([1, 2, 3], [4, 5, 6])),
...                   index=['mouse', 'rabbit'],
...                   columns=['one', 'two', 'three'])
>>> df
        one  two  three
mouse     1    2      3
rabbit    4    5      6
  
>>> # select columns by name
>>> df.filter(items=['one', 'three'])
         one  three
mouse     1      3
rabbit    4      6
  
>>> # select columns by regular expression
>>> df.filter(regex='e$', axis=1)
         one  three
mouse     1      3
rabbit    4      6
  
>>> # select rows containing 'bbi'
>>> df.filter(like='bbi', axis=0)
         one  two  three
rabbit    4    5      6
pandas.Series.filter   Series.filter(items=None, like=None, regex=None, axis=None)[source]
 
Subset the dataframe rows or columns according to the specified index labels. Note that this routine does not filter a dataframe on its contents. The filter is applied to the labels of the index.  Parameters 
 
items:list-like


Keep labels from axis which are in items.  
like:str


Keep labels from axis for which “like in label == True”.  
regex:str (regular expression)


Keep labels from axis for which re.search(regex, label) == True.  
axis:{0 or ‘index’, 1 or ‘columns’, None}, default None


The axis to filter on, expressed either as an index (int) or axis name (str). By default this is the info axis, ‘index’ for Series, ‘columns’ for DataFrame.    Returns 
 same type as input object
    See also  DataFrame.loc

Access a group of rows and columns by label(s) or a boolean array.    Notes The items, like, and regex parameters are enforced to be mutually exclusive. axis defaults to the info axis that is used when indexing with []. Examples 
>>> df = pd.DataFrame(np.array(([1, 2, 3], [4, 5, 6])),
...                   index=['mouse', 'rabbit'],
...                   columns=['one', 'two', 'three'])
>>> df
        one  two  three
mouse     1    2      3
rabbit    4    5      6
  
>>> # select columns by name
>>> df.filter(items=['one', 'three'])
         one  three
mouse     1      3
rabbit    4      6
  
>>> # select columns by regular expression
>>> df.filter(regex='e$', axis=1)
         one  three
mouse     1      3
rabbit    4      6
  
>>> # select rows containing 'bbi'
>>> df.filter(like='bbi', axis=0)
         one  two  three
rabbit    4    5      6
pandas.Series.str.extractall   Series.str.extractall(pat, flags=0)[source]
 
Extract capture groups in the regex pat as columns in DataFrame. For each subject string in the Series, extract groups from all matches of regular expression pat. When each subject string in the Series has exactly one match, extractall(pat).xs(0, level=’match’) is the same as extract(pat).  Parameters 
 
pat:str


Regular expression pattern with capturing groups.  
flags:int, default 0 (no flags)


A re module flag, for example re.IGNORECASE. These allow to modify regular expression matching for things like case, spaces, etc. Multiple flags can be combined with the bitwise OR operator, for example re.IGNORECASE | re.MULTILINE.    Returns 
 DataFrame

A DataFrame with one row for each match, and one column for each group. Its rows have a MultiIndex with first levels that come from the subject Series. The last level is named ‘match’ and indexes the matches in each item of the Series. Any capture group names in regular expression pat will be used for column names; otherwise capture group numbers will be used.      See also  extract

Returns first match only (not all matches).    Examples A pattern with one group will return a DataFrame with one column. Indices with no matches will not appear in the result. 
>>> s = pd.Series(["a1a2", "b1", "c1"], index=["A", "B", "C"])
>>> s.str.extractall(r"[ab](\d)")
        0
match
A 0      1
  1      2
B 0      1
  Capture group names are used for column names of the result. 
>>> s.str.extractall(r"[ab](?P<digit>\d)")
        digit
match
A 0         1
  1         2
B 0         1
  A pattern with two groups will return a DataFrame with two columns. 
>>> s.str.extractall(r"(?P<letter>[ab])(?P<digit>\d)")
        letter digit
match
A 0          a     1
  1          a     2
B 0          b     1
  Optional groups that do not match are NaN in the result. 
>>> s.str.extractall(r"(?P<letter>[ab])?(?P<digit>\d)")
        letter digit
match
A 0          a     1
  1          a     2
B 0          b     1
C 0        NaN     1
frame_type  
This attribute is set to 'ROWS'.
class RTrim(expression, **extra)
def f_15325182(df):
    """filter rows in pandas starting with alphabet 'f' using regular expression.
    """
    return  
 --------------------

def f_583557(tab):
    """print a 2 dimensional list `tab` as a table with delimiters
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
pandas.DataFrame.drop   DataFrame.drop(labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise')[source]
 
Drop specified labels from rows or columns. Remove rows or columns by specifying label names and corresponding axis, or by specifying directly index or column names. When using a multi-index, labels on different levels can be removed by specifying the level. See the user guide <advanced.shown_levels> for more information about the now unused levels.  Parameters 
 
labels:single label or list-like


Index or column labels to drop. A tuple will be used as a single label and not treated as a list-like.  
axis:{0 or ‘index’, 1 or ‘columns’}, default 0


Whether to drop labels from the index (0 or ‘index’) or columns (1 or ‘columns’).  
index:single label or list-like


Alternative to specifying axis (labels, axis=0 is equivalent to index=labels).  
columns:single label or list-like


Alternative to specifying axis (labels, axis=1 is equivalent to columns=labels).  
level:int or level name, optional


For MultiIndex, level from which the labels will be removed.  
inplace:bool, default False


If False, return a copy. Otherwise, do operation inplace and return None.  
errors:{‘ignore’, ‘raise’}, default ‘raise’


If ‘ignore’, suppress error and only existing labels are dropped.    Returns 
 DataFrame or None

DataFrame without the removed index or column labels or None if inplace=True.    Raises 
 KeyError

If any of the labels is not found in the selected axis.      See also  DataFrame.loc

Label-location based indexer for selection by label.  DataFrame.dropna

Return DataFrame with labels on given axis omitted where (all or any) data are missing.  DataFrame.drop_duplicates

Return DataFrame with duplicate rows removed, optionally only considering certain columns.  Series.drop

Return Series with specified index labels removed.    Examples 
>>> df = pd.DataFrame(np.arange(12).reshape(3, 4),
...                   columns=['A', 'B', 'C', 'D'])
>>> df
   A  B   C   D
0  0  1   2   3
1  4  5   6   7
2  8  9  10  11
  Drop columns 
>>> df.drop(['B', 'C'], axis=1)
   A   D
0  0   3
1  4   7
2  8  11
  
>>> df.drop(columns=['B', 'C'])
   A   D
0  0   3
1  4   7
2  8  11
  Drop a row by index 
>>> df.drop([0, 1])
   A  B   C   D
2  8  9  10  11
  Drop columns and/or rows of MultiIndex DataFrame 
>>> midx = pd.MultiIndex(levels=[['lama', 'cow', 'falcon'],
...                              ['speed', 'weight', 'length']],
...                      codes=[[0, 0, 0, 1, 1, 1, 2, 2, 2],
...                             [0, 1, 2, 0, 1, 2, 0, 1, 2]])
>>> df = pd.DataFrame(index=midx, columns=['big', 'small'],
...                   data=[[45, 30], [200, 100], [1.5, 1], [30, 20],
...                         [250, 150], [1.5, 0.8], [320, 250],
...                         [1, 0.8], [0.3, 0.2]])
>>> df
                big     small
lama    speed   45.0    30.0
        weight  200.0   100.0
        length  1.5     1.0
cow     speed   30.0    20.0
        weight  250.0   150.0
        length  1.5     0.8
falcon  speed   320.0   250.0
        weight  1.0     0.8
        length  0.3     0.2
  Drop a specific index combination from the MultiIndex DataFrame, i.e., drop the combination 'falcon' and 'weight', which deletes only the corresponding row 
>>> df.drop(index=('falcon', 'weight'))
                big     small
lama    speed   45.0    30.0
        weight  200.0   100.0
        length  1.5     1.0
cow     speed   30.0    20.0
        weight  250.0   150.0
        length  1.5     0.8
falcon  speed   320.0   250.0
        length  0.3     0.2
  
>>> df.drop(index='cow', columns='small')
                big
lama    speed   45.0
        weight  200.0
        length  1.5
falcon  speed   320.0
        weight  1.0
        length  0.3
  
>>> df.drop(index='length', level=1)
                big     small
lama    speed   45.0    30.0
        weight  200.0   100.0
cow     speed   30.0    20.0
        weight  250.0   150.0
falcon  speed   320.0   250.0
        weight  1.0     0.8
pandas.DataFrame.dropna   DataFrame.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)[source]
 
Remove missing values. See the User Guide for more on which values are considered missing, and how to work with missing data.  Parameters 
 
axis:{0 or ‘index’, 1 or ‘columns’}, default 0


Determine if rows or columns which contain missing values are removed.  0, or ‘index’ : Drop rows which contain missing values. 1, or ‘columns’ : Drop columns which contain missing value.   Changed in version 1.0.0: Pass tuple or list to drop on multiple axes. Only a single axis is allowed.   
how:{‘any’, ‘all’}, default ‘any’


Determine if row or column is removed from DataFrame, when we have at least one NA or all NA.  ‘any’ : If any NA values are present, drop that row or column. ‘all’ : If all values are NA, drop that row or column.   
thresh:int, optional


Require that many non-NA values.  
subset:column label or sequence of labels, optional


Labels along other axis to consider, e.g. if you are dropping rows these would be a list of columns to include.  
inplace:bool, default False


If True, do operation inplace and return None.    Returns 
 DataFrame or None

DataFrame with NA entries dropped from it or None if inplace=True.      See also  DataFrame.isna

Indicate missing values.  DataFrame.notna

Indicate existing (non-missing) values.  DataFrame.fillna

Replace missing values.  Series.dropna

Drop missing values.  Index.dropna

Drop missing indices.    Examples 
>>> df = pd.DataFrame({"name": ['Alfred', 'Batman', 'Catwoman'],
...                    "toy": [np.nan, 'Batmobile', 'Bullwhip'],
...                    "born": [pd.NaT, pd.Timestamp("1940-04-25"),
...                             pd.NaT]})
>>> df
       name        toy       born
0    Alfred        NaN        NaT
1    Batman  Batmobile 1940-04-25
2  Catwoman   Bullwhip        NaT
  Drop the rows where at least one element is missing. 
>>> df.dropna()
     name        toy       born
1  Batman  Batmobile 1940-04-25
  Drop the columns where at least one element is missing. 
>>> df.dropna(axis='columns')
       name
0    Alfred
1    Batman
2  Catwoman
  Drop the rows where all elements are missing. 
>>> df.dropna(how='all')
       name        toy       born
0    Alfred        NaN        NaT
1    Batman  Batmobile 1940-04-25
2  Catwoman   Bullwhip        NaT
  Keep only the rows with at least 2 non-NA values. 
>>> df.dropna(thresh=2)
       name        toy       born
1    Batman  Batmobile 1940-04-25
2  Catwoman   Bullwhip        NaT
  Define in which columns to look for missing values. 
>>> df.dropna(subset=['name', 'toy'])
       name        toy       born
1    Batman  Batmobile 1940-04-25
2  Catwoman   Bullwhip        NaT
  Keep the DataFrame with valid entries in the same variable. 
>>> df.dropna(inplace=True)
>>> df
     name        toy       born
1  Batman  Batmobile 1940-04-25
pandas.DataFrame.drop_duplicates   DataFrame.drop_duplicates(subset=None, keep='first', inplace=False, ignore_index=False)[source]
 
Return DataFrame with duplicate rows removed. Considering certain columns is optional. Indexes, including time indexes are ignored.  Parameters 
 
subset:column label or sequence of labels, optional


Only consider certain columns for identifying duplicates, by default use all of the columns.  
keep:{‘first’, ‘last’, False}, default ‘first’


Determines which duplicates (if any) to keep. - first : Drop duplicates except for the first occurrence. - last : Drop duplicates except for the last occurrence. - False : Drop all duplicates.  
inplace:bool, default False


Whether to drop duplicates in place or to return a copy.  
ignore_index:bool, default False


If True, the resulting axis will be labeled 0, 1, …, n - 1.  New in version 1.0.0.     Returns 
 DataFrame or None

DataFrame with duplicates removed or None if inplace=True.      See also  DataFrame.value_counts

Count unique combinations of columns.    Examples Consider dataset containing ramen rating. 
>>> df = pd.DataFrame({
...     'brand': ['Yum Yum', 'Yum Yum', 'Indomie', 'Indomie', 'Indomie'],
...     'style': ['cup', 'cup', 'cup', 'pack', 'pack'],
...     'rating': [4, 4, 3.5, 15, 5]
... })
>>> df
    brand style  rating
0  Yum Yum   cup     4.0
1  Yum Yum   cup     4.0
2  Indomie   cup     3.5
3  Indomie  pack    15.0
4  Indomie  pack     5.0
  By default, it removes duplicate rows based on all columns. 
>>> df.drop_duplicates()
    brand style  rating
0  Yum Yum   cup     4.0
2  Indomie   cup     3.5
3  Indomie  pack    15.0
4  Indomie  pack     5.0
  To remove duplicates on specific column(s), use subset. 
>>> df.drop_duplicates(subset=['brand'])
    brand style  rating
0  Yum Yum   cup     4.0
2  Indomie   cup     3.5
  To remove duplicates and keep last occurrences, use keep. 
>>> df.drop_duplicates(subset=['brand', 'style'], keep='last')
    brand style  rating
1  Yum Yum   cup     4.0
2  Indomie   cup     3.5
4  Indomie  pack     5.0
Cookbook This is a repository for short and sweet examples and links for useful pandas recipes. We encourage users to add to this documentation. Adding interesting links and/or inline examples to this section is a great First Pull Request. Simplified, condensed, new-user friendly, in-line examples have been inserted where possible to augment the Stack-Overflow and GitHub links. Many of the links contain expanded information, above what the in-line examples offer. pandas (pd) and NumPy (np) are the only two abbreviated imported modules. The rest are kept explicitly imported for newer users.  Idioms These are some neat pandas idioms if-then/if-then-else on one column, and assignment to another one or more columns: 
In [1]: df = pd.DataFrame(
   ...:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ...: )
   ...: 

In [2]: df
Out[2]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50
   if-then… An if-then on one column 
In [3]: df.loc[df.AAA >= 5, "BBB"] = -1

In [4]: df
Out[4]: 
   AAA  BBB  CCC
0    4   10  100
1    5   -1   50
2    6   -1  -30
3    7   -1  -50
  An if-then with assignment to 2 columns: 
In [5]: df.loc[df.AAA >= 5, ["BBB", "CCC"]] = 555

In [6]: df
Out[6]: 
   AAA  BBB  CCC
0    4   10  100
1    5  555  555
2    6  555  555
3    7  555  555
  Add another line with different logic, to do the -else 
In [7]: df.loc[df.AAA < 5, ["BBB", "CCC"]] = 2000

In [8]: df
Out[8]: 
   AAA   BBB   CCC
0    4  2000  2000
1    5   555   555
2    6   555   555
3    7   555   555
  Or use pandas where after you’ve set up a mask 
In [9]: df_mask = pd.DataFrame(
   ...:     {"AAA": [True] * 4, "BBB": [False] * 4, "CCC": [True, False] * 2}
   ...: )
   ...: 

In [10]: df.where(df_mask, -1000)
Out[10]: 
   AAA   BBB   CCC
0    4 -1000  2000
1    5 -1000 -1000
2    6 -1000   555
3    7 -1000 -1000
  if-then-else using NumPy’s where() 
In [11]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ....: )
   ....: 

In [12]: df
Out[12]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50

In [13]: df["logic"] = np.where(df["AAA"] > 5, "high", "low")

In [14]: df
Out[14]: 
   AAA  BBB  CCC logic
0    4   10  100   low
1    5   20   50   low
2    6   30  -30  high
3    7   40  -50  high
    Splitting Split a frame with a boolean criterion 
In [15]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ....: )
   ....: 

In [16]: df
Out[16]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50

In [17]: df[df.AAA <= 5]
Out[17]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50

In [18]: df[df.AAA > 5]
Out[18]: 
   AAA  BBB  CCC
2    6   30  -30
3    7   40  -50
    Building criteria Select with multi-column criteria 
In [19]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ....: )
   ....: 

In [20]: df
Out[20]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50
  …and (without assignment returns a Series) 
In [21]: df.loc[(df["BBB"] < 25) & (df["CCC"] >= -40), "AAA"]
Out[21]: 
0    4
1    5
Name: AAA, dtype: int64
  …or (without assignment returns a Series) 
In [22]: df.loc[(df["BBB"] > 25) | (df["CCC"] >= -40), "AAA"]
Out[22]: 
0    4
1    5
2    6
3    7
Name: AAA, dtype: int64
  …or (with assignment modifies the DataFrame.) 
In [23]: df.loc[(df["BBB"] > 25) | (df["CCC"] >= 75), "AAA"] = 0.1

In [24]: df
Out[24]: 
   AAA  BBB  CCC
0  0.1   10  100
1  5.0   20   50
2  0.1   30  -30
3  0.1   40  -50
  Select rows with data closest to certain value using argsort 
In [25]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ....: )
   ....: 

In [26]: df
Out[26]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50

In [27]: aValue = 43.0

In [28]: df.loc[(df.CCC - aValue).abs().argsort()]
Out[28]: 
   AAA  BBB  CCC
1    5   20   50
0    4   10  100
2    6   30  -30
3    7   40  -50
  Dynamically reduce a list of criteria using a binary operators 
In [29]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ....: )
   ....: 

In [30]: df
Out[30]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50

In [31]: Crit1 = df.AAA <= 5.5

In [32]: Crit2 = df.BBB == 10.0

In [33]: Crit3 = df.CCC > -40.0
  One could hard code: 
In [34]: AllCrit = Crit1 & Crit2 & Crit3
  …Or it can be done with a list of dynamically built criteria 
In [35]: import functools

In [36]: CritList = [Crit1, Crit2, Crit3]

In [37]: AllCrit = functools.reduce(lambda x, y: x & y, CritList)

In [38]: df[AllCrit]
Out[38]: 
   AAA  BBB  CCC
0    4   10  100
     Selection  Dataframes The indexing docs. Using both row labels and value conditionals 
In [39]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ....: )
   ....: 

In [40]: df
Out[40]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50

In [41]: df[(df.AAA <= 6) & (df.index.isin([0, 2, 4]))]
Out[41]: 
   AAA  BBB  CCC
0    4   10  100
2    6   30  -30
  Use loc for label-oriented slicing and iloc positional slicing GH2904 
In [42]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]},
   ....:     index=["foo", "bar", "boo", "kar"],
   ....: )
   ....: 
  There are 2 explicit slicing methods, with a third general case  Positional-oriented (Python slicing style : exclusive of end) Label-oriented (Non-Python slicing style : inclusive of end) General (Either slicing style : depends on if the slice contains labels or positions)  
In [43]: df.loc["bar":"kar"]  # Label
Out[43]: 
     AAA  BBB  CCC
bar    5   20   50
boo    6   30  -30
kar    7   40  -50

# Generic
In [44]: df[0:3]
Out[44]: 
     AAA  BBB  CCC
foo    4   10  100
bar    5   20   50
boo    6   30  -30

In [45]: df["bar":"kar"]
Out[45]: 
     AAA  BBB  CCC
bar    5   20   50
boo    6   30  -30
kar    7   40  -50
  Ambiguity arises when an index consists of integers with a non-zero start or non-unit increment. 
In [46]: data = {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}

In [47]: df2 = pd.DataFrame(data=data, index=[1, 2, 3, 4])  # Note index starts at 1.

In [48]: df2.iloc[1:3]  # Position-oriented
Out[48]: 
   AAA  BBB  CCC
2    5   20   50
3    6   30  -30

In [49]: df2.loc[1:3]  # Label-oriented
Out[49]: 
   AAA  BBB  CCC
1    4   10  100
2    5   20   50
3    6   30  -30
  Using inverse operator (~) to take the complement of a mask 
In [50]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ....: )
   ....: 

In [51]: df
Out[51]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50

In [52]: df[~((df.AAA <= 6) & (df.index.isin([0, 2, 4])))]
Out[52]: 
   AAA  BBB  CCC
1    5   20   50
3    7   40  -50
    New columns Efficiently and dynamically creating new columns using applymap 
In [53]: df = pd.DataFrame({"AAA": [1, 2, 1, 3], "BBB": [1, 1, 2, 2], "CCC": [2, 1, 3, 1]})

In [54]: df
Out[54]: 
   AAA  BBB  CCC
0    1    1    2
1    2    1    1
2    1    2    3
3    3    2    1

In [55]: source_cols = df.columns  # Or some subset would work too

In [56]: new_cols = [str(x) + "_cat" for x in source_cols]

In [57]: categories = {1: "Alpha", 2: "Beta", 3: "Charlie"}

In [58]: df[new_cols] = df[source_cols].applymap(categories.get)

In [59]: df
Out[59]: 
   AAA  BBB  CCC  AAA_cat BBB_cat  CCC_cat
0    1    1    2    Alpha   Alpha     Beta
1    2    1    1     Beta   Alpha    Alpha
2    1    2    3    Alpha    Beta  Charlie
3    3    2    1  Charlie    Beta    Alpha
  Keep other columns when using min() with groupby 
In [60]: df = pd.DataFrame(
   ....:     {"AAA": [1, 1, 1, 2, 2, 2, 3, 3], "BBB": [2, 1, 3, 4, 5, 1, 2, 3]}
   ....: )
   ....: 

In [61]: df
Out[61]: 
   AAA  BBB
0    1    2
1    1    1
2    1    3
3    2    4
4    2    5
5    2    1
6    3    2
7    3    3
  Method 1 : idxmin() to get the index of the minimums 
In [62]: df.loc[df.groupby("AAA")["BBB"].idxmin()]
Out[62]: 
   AAA  BBB
1    1    1
5    2    1
6    3    2
  Method 2 : sort then take first of each 
In [63]: df.sort_values(by="BBB").groupby("AAA", as_index=False).first()
Out[63]: 
   AAA  BBB
0    1    1
1    2    1
2    3    2
  Notice the same results, with the exception of the index.    Multiindexing The multindexing docs. Creating a MultiIndex from a labeled frame 
In [64]: df = pd.DataFrame(
   ....:     {
   ....:         "row": [0, 1, 2],
   ....:         "One_X": [1.1, 1.1, 1.1],
   ....:         "One_Y": [1.2, 1.2, 1.2],
   ....:         "Two_X": [1.11, 1.11, 1.11],
   ....:         "Two_Y": [1.22, 1.22, 1.22],
   ....:     }
   ....: )
   ....: 

In [65]: df
Out[65]: 
   row  One_X  One_Y  Two_X  Two_Y
0    0    1.1    1.2   1.11   1.22
1    1    1.1    1.2   1.11   1.22
2    2    1.1    1.2   1.11   1.22

# As Labelled Index
In [66]: df = df.set_index("row")

In [67]: df
Out[67]: 
     One_X  One_Y  Two_X  Two_Y
row                            
0      1.1    1.2   1.11   1.22
1      1.1    1.2   1.11   1.22
2      1.1    1.2   1.11   1.22

# With Hierarchical Columns
In [68]: df.columns = pd.MultiIndex.from_tuples([tuple(c.split("_")) for c in df.columns])

In [69]: df
Out[69]: 
     One        Two      
       X    Y     X     Y
row                      
0    1.1  1.2  1.11  1.22
1    1.1  1.2  1.11  1.22
2    1.1  1.2  1.11  1.22

# Now stack & Reset
In [70]: df = df.stack(0).reset_index(1)

In [71]: df
Out[71]: 
    level_1     X     Y
row                    
0       One  1.10  1.20
0       Two  1.11  1.22
1       One  1.10  1.20
1       Two  1.11  1.22
2       One  1.10  1.20
2       Two  1.11  1.22

# And fix the labels (Notice the label 'level_1' got added automatically)
In [72]: df.columns = ["Sample", "All_X", "All_Y"]

In [73]: df
Out[73]: 
    Sample  All_X  All_Y
row                     
0      One   1.10   1.20
0      Two   1.11   1.22
1      One   1.10   1.20
1      Two   1.11   1.22
2      One   1.10   1.20
2      Two   1.11   1.22
   Arithmetic Performing arithmetic with a MultiIndex that needs broadcasting 
In [74]: cols = pd.MultiIndex.from_tuples(
   ....:     [(x, y) for x in ["A", "B", "C"] for y in ["O", "I"]]
   ....: )
   ....: 

In [75]: df = pd.DataFrame(np.random.randn(2, 6), index=["n", "m"], columns=cols)

In [76]: df
Out[76]: 
          A                   B                   C          
          O         I         O         I         O         I
n  0.469112 -0.282863 -1.509059 -1.135632  1.212112 -0.173215
m  0.119209 -1.044236 -0.861849 -2.104569 -0.494929  1.071804

In [77]: df = df.div(df["C"], level=1)

In [78]: df
Out[78]: 
          A                   B              C     
          O         I         O         I    O    I
n  0.387021  1.633022 -1.244983  6.556214  1.0  1.0
m -0.240860 -0.974279  1.741358 -1.963577  1.0  1.0
    Slicing Slicing a MultiIndex with xs 
In [79]: coords = [("AA", "one"), ("AA", "six"), ("BB", "one"), ("BB", "two"), ("BB", "six")]

In [80]: index = pd.MultiIndex.from_tuples(coords)

In [81]: df = pd.DataFrame([11, 22, 33, 44, 55], index, ["MyData"])

In [82]: df
Out[82]: 
        MyData
AA one      11
   six      22
BB one      33
   two      44
   six      55
  To take the cross section of the 1st level and 1st axis the index: 
# Note : level and axis are optional, and default to zero
In [83]: df.xs("BB", level=0, axis=0)
Out[83]: 
     MyData
one      33
two      44
six      55
  …and now the 2nd level of the 1st axis. 
In [84]: df.xs("six", level=1, axis=0)
Out[84]: 
    MyData
AA      22
BB      55
  Slicing a MultiIndex with xs, method #2 
In [85]: import itertools

In [86]: index = list(itertools.product(["Ada", "Quinn", "Violet"], ["Comp", "Math", "Sci"]))

In [87]: headr = list(itertools.product(["Exams", "Labs"], ["I", "II"]))

In [88]: indx = pd.MultiIndex.from_tuples(index, names=["Student", "Course"])

In [89]: cols = pd.MultiIndex.from_tuples(headr)  # Notice these are un-named

In [90]: data = [[70 + x + y + (x * y) % 3 for x in range(4)] for y in range(9)]

In [91]: df = pd.DataFrame(data, indx, cols)

In [92]: df
Out[92]: 
               Exams     Labs    
                   I  II    I  II
Student Course                   
Ada     Comp      70  71   72  73
        Math      71  73   75  74
        Sci       72  75   75  75
Quinn   Comp      73  74   75  76
        Math      74  76   78  77
        Sci       75  78   78  78
Violet  Comp      76  77   78  79
        Math      77  79   81  80
        Sci       78  81   81  81

In [93]: All = slice(None)

In [94]: df.loc["Violet"]
Out[94]: 
       Exams     Labs    
           I  II    I  II
Course                   
Comp      76  77   78  79
Math      77  79   81  80
Sci       78  81   81  81

In [95]: df.loc[(All, "Math"), All]
Out[95]: 
               Exams     Labs    
                   I  II    I  II
Student Course                   
Ada     Math      71  73   75  74
Quinn   Math      74  76   78  77
Violet  Math      77  79   81  80

In [96]: df.loc[(slice("Ada", "Quinn"), "Math"), All]
Out[96]: 
               Exams     Labs    
                   I  II    I  II
Student Course                   
Ada     Math      71  73   75  74
Quinn   Math      74  76   78  77

In [97]: df.loc[(All, "Math"), ("Exams")]
Out[97]: 
                 I  II
Student Course        
Ada     Math    71  73
Quinn   Math    74  76
Violet  Math    77  79

In [98]: df.loc[(All, "Math"), (All, "II")]
Out[98]: 
               Exams Labs
                  II   II
Student Course           
Ada     Math      73   74
Quinn   Math      76   77
Violet  Math      79   80
  Setting portions of a MultiIndex with xs   Sorting Sort by specific column or an ordered list of columns, with a MultiIndex 
In [99]: df.sort_values(by=("Labs", "II"), ascending=False)
Out[99]: 
               Exams     Labs    
                   I  II    I  II
Student Course                   
Violet  Sci       78  81   81  81
        Math      77  79   81  80
        Comp      76  77   78  79
Quinn   Sci       75  78   78  78
        Math      74  76   78  77
        Comp      73  74   75  76
Ada     Sci       72  75   75  75
        Math      71  73   75  74
        Comp      70  71   72  73
  Partial selection, the need for sortedness GH2995   Levels Prepending a level to a multiindex Flatten Hierarchical columns    Missing data The missing data docs. Fill forward a reversed timeseries 
In [100]: df = pd.DataFrame(
   .....:     np.random.randn(6, 1),
   .....:     index=pd.date_range("2013-08-01", periods=6, freq="B"),
   .....:     columns=list("A"),
   .....: )
   .....: 

In [101]: df.loc[df.index[3], "A"] = np.nan

In [102]: df
Out[102]: 
                   A
2013-08-01  0.721555
2013-08-02 -0.706771
2013-08-05 -1.039575
2013-08-06       NaN
2013-08-07 -0.424972
2013-08-08  0.567020

In [103]: df.reindex(df.index[::-1]).ffill()
Out[103]: 
                   A
2013-08-08  0.567020
2013-08-07 -0.424972
2013-08-06 -0.424972
2013-08-05 -1.039575
2013-08-02 -0.706771
2013-08-01  0.721555
  cumsum reset at NaN values  Replace Using replace with backrefs    Grouping The grouping docs. Basic grouping with apply Unlike agg, apply’s callable is passed a sub-DataFrame which gives you access to all the columns 
In [104]: df = pd.DataFrame(
   .....:     {
   .....:         "animal": "cat dog cat fish dog cat cat".split(),
   .....:         "size": list("SSMMMLL"),
   .....:         "weight": [8, 10, 11, 1, 20, 12, 12],
   .....:         "adult": [False] * 5 + [True] * 2,
   .....:     }
   .....: )
   .....: 

In [105]: df
Out[105]: 
  animal size  weight  adult
0    cat    S       8  False
1    dog    S      10  False
2    cat    M      11  False
3   fish    M       1  False
4    dog    M      20  False
5    cat    L      12   True
6    cat    L      12   True

# List the size of the animals with the highest weight.
In [106]: df.groupby("animal").apply(lambda subf: subf["size"][subf["weight"].idxmax()])
Out[106]: 
animal
cat     L
dog     M
fish    M
dtype: object
  Using get_group 
In [107]: gb = df.groupby(["animal"])

In [108]: gb.get_group("cat")
Out[108]: 
  animal size  weight  adult
0    cat    S       8  False
2    cat    M      11  False
5    cat    L      12   True
6    cat    L      12   True
  Apply to different items in a group 
In [109]: def GrowUp(x):
   .....:     avg_weight = sum(x[x["size"] == "S"].weight * 1.5)
   .....:     avg_weight += sum(x[x["size"] == "M"].weight * 1.25)
   .....:     avg_weight += sum(x[x["size"] == "L"].weight)
   .....:     avg_weight /= len(x)
   .....:     return pd.Series(["L", avg_weight, True], index=["size", "weight", "adult"])
   .....: 

In [110]: expected_df = gb.apply(GrowUp)

In [111]: expected_df
Out[111]: 
       size   weight  adult
animal                     
cat       L  12.4375   True
dog       L  20.0000   True
fish      L   1.2500   True
  Expanding apply 
In [112]: S = pd.Series([i / 100.0 for i in range(1, 11)])

In [113]: def cum_ret(x, y):
   .....:     return x * (1 + y)
   .....: 

In [114]: def red(x):
   .....:     return functools.reduce(cum_ret, x, 1.0)
   .....: 

In [115]: S.expanding().apply(red, raw=True)
Out[115]: 
0    1.010000
1    1.030200
2    1.061106
3    1.103550
4    1.158728
5    1.228251
6    1.314229
7    1.419367
8    1.547110
9    1.701821
dtype: float64
  Replacing some values with mean of the rest of a group 
In [116]: df = pd.DataFrame({"A": [1, 1, 2, 2], "B": [1, -1, 1, 2]})

In [117]: gb = df.groupby("A")

In [118]: def replace(g):
   .....:     mask = g < 0
   .....:     return g.where(mask, g[~mask].mean())
   .....: 

In [119]: gb.transform(replace)
Out[119]: 
     B
0  1.0
1 -1.0
2  1.5
3  1.5
  Sort groups by aggregated data 
In [120]: df = pd.DataFrame(
   .....:     {
   .....:         "code": ["foo", "bar", "baz"] * 2,
   .....:         "data": [0.16, -0.21, 0.33, 0.45, -0.59, 0.62],
   .....:         "flag": [False, True] * 3,
   .....:     }
   .....: )
   .....: 

In [121]: code_groups = df.groupby("code")

In [122]: agg_n_sort_order = code_groups[["data"]].transform(sum).sort_values(by="data")

In [123]: sorted_df = df.loc[agg_n_sort_order.index]

In [124]: sorted_df
Out[124]: 
  code  data   flag
1  bar -0.21   True
4  bar -0.59  False
0  foo  0.16  False
3  foo  0.45   True
2  baz  0.33  False
5  baz  0.62   True
  Create multiple aggregated columns 
In [125]: rng = pd.date_range(start="2014-10-07", periods=10, freq="2min")

In [126]: ts = pd.Series(data=list(range(10)), index=rng)

In [127]: def MyCust(x):
   .....:     if len(x) > 2:
   .....:         return x[1] * 1.234
   .....:     return pd.NaT
   .....: 

In [128]: mhc = {"Mean": np.mean, "Max": np.max, "Custom": MyCust}

In [129]: ts.resample("5min").apply(mhc)
Out[129]: 
                     Mean  Max Custom
2014-10-07 00:00:00   1.0    2  1.234
2014-10-07 00:05:00   3.5    4    NaT
2014-10-07 00:10:00   6.0    7  7.404
2014-10-07 00:15:00   8.5    9    NaT

In [130]: ts
Out[130]: 
2014-10-07 00:00:00    0
2014-10-07 00:02:00    1
2014-10-07 00:04:00    2
2014-10-07 00:06:00    3
2014-10-07 00:08:00    4
2014-10-07 00:10:00    5
2014-10-07 00:12:00    6
2014-10-07 00:14:00    7
2014-10-07 00:16:00    8
2014-10-07 00:18:00    9
Freq: 2T, dtype: int64
  Create a value counts column and reassign back to the DataFrame 
In [131]: df = pd.DataFrame(
   .....:     {"Color": "Red Red Red Blue".split(), "Value": [100, 150, 50, 50]}
   .....: )
   .....: 

In [132]: df
Out[132]: 
  Color  Value
0   Red    100
1   Red    150
2   Red     50
3  Blue     50

In [133]: df["Counts"] = df.groupby(["Color"]).transform(len)

In [134]: df
Out[134]: 
  Color  Value  Counts
0   Red    100       3
1   Red    150       3
2   Red     50       3
3  Blue     50       1
  Shift groups of the values in a column based on the index 
In [135]: df = pd.DataFrame(
   .....:     {"line_race": [10, 10, 8, 10, 10, 8], "beyer": [99, 102, 103, 103, 88, 100]},
   .....:     index=[
   .....:         "Last Gunfighter",
   .....:         "Last Gunfighter",
   .....:         "Last Gunfighter",
   .....:         "Paynter",
   .....:         "Paynter",
   .....:         "Paynter",
   .....:     ],
   .....: )
   .....: 

In [136]: df
Out[136]: 
                 line_race  beyer
Last Gunfighter         10     99
Last Gunfighter         10    102
Last Gunfighter          8    103
Paynter                 10    103
Paynter                 10     88
Paynter                  8    100

In [137]: df["beyer_shifted"] = df.groupby(level=0)["beyer"].shift(1)

In [138]: df
Out[138]: 
                 line_race  beyer  beyer_shifted
Last Gunfighter         10     99            NaN
Last Gunfighter         10    102           99.0
Last Gunfighter          8    103          102.0
Paynter                 10    103            NaN
Paynter                 10     88          103.0
Paynter                  8    100           88.0
  Select row with maximum value from each group 
In [139]: df = pd.DataFrame(
   .....:     {
   .....:         "host": ["other", "other", "that", "this", "this"],
   .....:         "service": ["mail", "web", "mail", "mail", "web"],
   .....:         "no": [1, 2, 1, 2, 1],
   .....:     }
   .....: ).set_index(["host", "service"])
   .....: 

In [140]: mask = df.groupby(level=0).agg("idxmax")

In [141]: df_count = df.loc[mask["no"]].reset_index()

In [142]: df_count
Out[142]: 
    host service  no
0  other     web   2
1   that    mail   1
2   this    mail   2
  Grouping like Python’s itertools.groupby 
In [143]: df = pd.DataFrame([0, 1, 0, 1, 1, 1, 0, 1, 1], columns=["A"])

In [144]: df["A"].groupby((df["A"] != df["A"].shift()).cumsum()).groups
Out[144]: {1: [0], 2: [1], 3: [2], 4: [3, 4, 5], 5: [6], 6: [7, 8]}

In [145]: df["A"].groupby((df["A"] != df["A"].shift()).cumsum()).cumsum()
Out[145]: 
0    0
1    1
2    0
3    1
4    2
5    3
6    0
7    1
8    2
Name: A, dtype: int64
   Expanding data Alignment and to-date Rolling Computation window based on values instead of counts Rolling Mean by Time Interval   Splitting Splitting a frame Create a list of dataframes, split using a delineation based on logic included in rows. 
In [146]: df = pd.DataFrame(
   .....:     data={
   .....:         "Case": ["A", "A", "A", "B", "A", "A", "B", "A", "A"],
   .....:         "Data": np.random.randn(9),
   .....:     }
   .....: )
   .....: 

In [147]: dfs = list(
   .....:     zip(
   .....:         *df.groupby(
   .....:             (1 * (df["Case"] == "B"))
   .....:             .cumsum()
   .....:             .rolling(window=3, min_periods=1)
   .....:             .median()
   .....:         )
   .....:     )
   .....: )[-1]
   .....: 

In [148]: dfs[0]
Out[148]: 
  Case      Data
0    A  0.276232
1    A -1.087401
2    A -0.673690
3    B  0.113648

In [149]: dfs[1]
Out[149]: 
  Case      Data
4    A -1.478427
5    A  0.524988
6    B  0.404705

In [150]: dfs[2]
Out[150]: 
  Case      Data
7    A  0.577046
8    A -1.715002
    Pivot The Pivot docs. Partial sums and subtotals 
In [151]: df = pd.DataFrame(
   .....:     data={
   .....:         "Province": ["ON", "QC", "BC", "AL", "AL", "MN", "ON"],
   .....:         "City": [
   .....:             "Toronto",
   .....:             "Montreal",
   .....:             "Vancouver",
   .....:             "Calgary",
   .....:             "Edmonton",
   .....:             "Winnipeg",
   .....:             "Windsor",
   .....:         ],
   .....:         "Sales": [13, 6, 16, 8, 4, 3, 1],
   .....:     }
   .....: )
   .....: 

In [152]: table = pd.pivot_table(
   .....:     df,
   .....:     values=["Sales"],
   .....:     index=["Province"],
   .....:     columns=["City"],
   .....:     aggfunc=np.sum,
   .....:     margins=True,
   .....: )
   .....: 

In [153]: table.stack("City")
Out[153]: 
                    Sales
Province City            
AL       All         12.0
         Calgary      8.0
         Edmonton     4.0
BC       All         16.0
         Vancouver   16.0
...                   ...
All      Montreal     6.0
         Toronto     13.0
         Vancouver   16.0
         Windsor      1.0
         Winnipeg     3.0

[20 rows x 1 columns]
  Frequency table like plyr in R 
In [154]: grades = [48, 99, 75, 80, 42, 80, 72, 68, 36, 78]

In [155]: df = pd.DataFrame(
   .....:     {
   .....:         "ID": ["x%d" % r for r in range(10)],
   .....:         "Gender": ["F", "M", "F", "M", "F", "M", "F", "M", "M", "M"],
   .....:         "ExamYear": [
   .....:             "2007",
   .....:             "2007",
   .....:             "2007",
   .....:             "2008",
   .....:             "2008",
   .....:             "2008",
   .....:             "2008",
   .....:             "2009",
   .....:             "2009",
   .....:             "2009",
   .....:         ],
   .....:         "Class": [
   .....:             "algebra",
   .....:             "stats",
   .....:             "bio",
   .....:             "algebra",
   .....:             "algebra",
   .....:             "stats",
   .....:             "stats",
   .....:             "algebra",
   .....:             "bio",
   .....:             "bio",
   .....:         ],
   .....:         "Participated": [
   .....:             "yes",
   .....:             "yes",
   .....:             "yes",
   .....:             "yes",
   .....:             "no",
   .....:             "yes",
   .....:             "yes",
   .....:             "yes",
   .....:             "yes",
   .....:             "yes",
   .....:         ],
   .....:         "Passed": ["yes" if x > 50 else "no" for x in grades],
   .....:         "Employed": [
   .....:             True,
   .....:             True,
   .....:             True,
   .....:             False,
   .....:             False,
   .....:             False,
   .....:             False,
   .....:             True,
   .....:             True,
   .....:             False,
   .....:         ],
   .....:         "Grade": grades,
   .....:     }
   .....: )
   .....: 

In [156]: df.groupby("ExamYear").agg(
   .....:     {
   .....:         "Participated": lambda x: x.value_counts()["yes"],
   .....:         "Passed": lambda x: sum(x == "yes"),
   .....:         "Employed": lambda x: sum(x),
   .....:         "Grade": lambda x: sum(x) / len(x),
   .....:     }
   .....: )
   .....: 
Out[156]: 
          Participated  Passed  Employed      Grade
ExamYear                                           
2007                 3       2         3  74.000000
2008                 3       3         0  68.500000
2009                 3       2         2  60.666667
  Plot pandas DataFrame with year over year data To create year and month cross tabulation: 
In [157]: df = pd.DataFrame(
   .....:     {"value": np.random.randn(36)},
   .....:     index=pd.date_range("2011-01-01", freq="M", periods=36),
   .....: )
   .....: 

In [158]: pd.pivot_table(
   .....:     df, index=df.index.month, columns=df.index.year, values="value", aggfunc="sum"
   .....: )
   .....: 
Out[158]: 
        2011      2012      2013
1  -1.039268 -0.968914  2.565646
2  -0.370647 -1.294524  1.431256
3  -1.157892  0.413738  1.340309
4  -1.344312  0.276662 -1.170299
5   0.844885 -0.472035 -0.226169
6   1.075770 -0.013960  0.410835
7  -0.109050 -0.362543  0.813850
8   1.643563 -0.006154  0.132003
9  -1.469388 -0.923061 -0.827317
10  0.357021  0.895717 -0.076467
11 -0.674600  0.805244 -1.187678
12 -1.776904 -1.206412  1.130127
    Apply Rolling apply to organize - Turning embedded lists into a MultiIndex frame 
In [159]: df = pd.DataFrame(
   .....:     data={
   .....:         "A": [[2, 4, 8, 16], [100, 200], [10, 20, 30]],
   .....:         "B": [["a", "b", "c"], ["jj", "kk"], ["ccc"]],
   .....:     },
   .....:     index=["I", "II", "III"],
   .....: )
   .....: 

In [160]: def SeriesFromSubList(aList):
   .....:     return pd.Series(aList)
   .....: 

In [161]: df_orgz = pd.concat(
   .....:     {ind: row.apply(SeriesFromSubList) for ind, row in df.iterrows()}
   .....: )
   .....: 

In [162]: df_orgz
Out[162]: 
         0     1     2     3
I   A    2     4     8  16.0
    B    a     b     c   NaN
II  A  100   200   NaN   NaN
    B   jj    kk   NaN   NaN
III A   10  20.0  30.0   NaN
    B  ccc   NaN   NaN   NaN
  Rolling apply with a DataFrame returning a Series Rolling Apply to multiple columns where function calculates a Series before a Scalar from the Series is returned 
In [163]: df = pd.DataFrame(
   .....:     data=np.random.randn(2000, 2) / 10000,
   .....:     index=pd.date_range("2001-01-01", periods=2000),
   .....:     columns=["A", "B"],
   .....: )
   .....: 

In [164]: df
Out[164]: 
                   A         B
2001-01-01 -0.000144 -0.000141
2001-01-02  0.000161  0.000102
2001-01-03  0.000057  0.000088
2001-01-04 -0.000221  0.000097
2001-01-05 -0.000201 -0.000041
...              ...       ...
2006-06-19  0.000040 -0.000235
2006-06-20 -0.000123 -0.000021
2006-06-21 -0.000113  0.000114
2006-06-22  0.000136  0.000109
2006-06-23  0.000027  0.000030

[2000 rows x 2 columns]

In [165]: def gm(df, const):
   .....:     v = ((((df["A"] + df["B"]) + 1).cumprod()) - 1) * const
   .....:     return v.iloc[-1]
   .....: 

In [166]: s = pd.Series(
   .....:     {
   .....:         df.index[i]: gm(df.iloc[i: min(i + 51, len(df) - 1)], 5)
   .....:         for i in range(len(df) - 50)
   .....:     }
   .....: )
   .....: 

In [167]: s
Out[167]: 
2001-01-01    0.000930
2001-01-02    0.002615
2001-01-03    0.001281
2001-01-04    0.001117
2001-01-05    0.002772
                ...   
2006-04-30    0.003296
2006-05-01    0.002629
2006-05-02    0.002081
2006-05-03    0.004247
2006-05-04    0.003928
Length: 1950, dtype: float64
  Rolling apply with a DataFrame returning a Scalar Rolling Apply to multiple columns where function returns a Scalar (Volume Weighted Average Price) 
In [168]: rng = pd.date_range(start="2014-01-01", periods=100)

In [169]: df = pd.DataFrame(
   .....:     {
   .....:         "Open": np.random.randn(len(rng)),
   .....:         "Close": np.random.randn(len(rng)),
   .....:         "Volume": np.random.randint(100, 2000, len(rng)),
   .....:     },
   .....:     index=rng,
   .....: )
   .....: 

In [170]: df
Out[170]: 
                Open     Close  Volume
2014-01-01 -1.611353 -0.492885    1219
2014-01-02 -3.000951  0.445794    1054
2014-01-03 -0.138359 -0.076081    1381
2014-01-04  0.301568  1.198259    1253
2014-01-05  0.276381 -0.669831    1728
...              ...       ...     ...
2014-04-06 -0.040338  0.937843    1188
2014-04-07  0.359661 -0.285908    1864
2014-04-08  0.060978  1.714814     941
2014-04-09  1.759055 -0.455942    1065
2014-04-10  0.138185 -1.147008    1453

[100 rows x 3 columns]

In [171]: def vwap(bars):
   .....:     return (bars.Close * bars.Volume).sum() / bars.Volume.sum()
   .....: 

In [172]: window = 5

In [173]: s = pd.concat(
   .....:     [
   .....:         (pd.Series(vwap(df.iloc[i: i + window]), index=[df.index[i + window]]))
   .....:         for i in range(len(df) - window)
   .....:     ]
   .....: )
   .....: 

In [174]: s.round(2)
Out[174]: 
2014-01-06    0.02
2014-01-07    0.11
2014-01-08    0.10
2014-01-09    0.07
2014-01-10   -0.29
              ... 
2014-04-06   -0.63
2014-04-07   -0.02
2014-04-08   -0.03
2014-04-09    0.34
2014-04-10    0.29
Length: 95, dtype: float64
     Timeseries Between times Using indexer between time Constructing a datetime range that excludes weekends and includes only certain times Vectorized Lookup Aggregation and plotting time series Turn a matrix with hours in columns and days in rows into a continuous row sequence in the form of a time series. How to rearrange a Python pandas DataFrame? Dealing with duplicates when reindexing a timeseries to a specified frequency Calculate the first day of the month for each entry in a DatetimeIndex 
In [175]: dates = pd.date_range("2000-01-01", periods=5)

In [176]: dates.to_period(freq="M").to_timestamp()
Out[176]: 
DatetimeIndex(['2000-01-01', '2000-01-01', '2000-01-01', '2000-01-01',
               '2000-01-01'],
              dtype='datetime64[ns]', freq=None)
   Resampling The Resample docs. Using Grouper instead of TimeGrouper for time grouping of values Time grouping with some missing values Valid frequency arguments to Grouper Timeseries Grouping using a MultiIndex Using TimeGrouper and another grouping to create subgroups, then apply a custom function GH3791 Resampling with custom periods Resample intraday frame without adding new days Resample minute data Resample with groupby    Merge The Join docs. Concatenate two dataframes with overlapping index (emulate R rbind) 
In [177]: rng = pd.date_range("2000-01-01", periods=6)

In [178]: df1 = pd.DataFrame(np.random.randn(6, 3), index=rng, columns=["A", "B", "C"])

In [179]: df2 = df1.copy()
  Depending on df construction, ignore_index may be needed 
In [180]: df = pd.concat([df1, df2], ignore_index=True)

In [181]: df
Out[181]: 
           A         B         C
0  -0.870117 -0.479265 -0.790855
1   0.144817  1.726395 -0.464535
2  -0.821906  1.597605  0.187307
3  -0.128342 -1.511638 -0.289858
4   0.399194 -1.430030 -0.639760
5   1.115116 -2.012600  1.810662
6  -0.870117 -0.479265 -0.790855
7   0.144817  1.726395 -0.464535
8  -0.821906  1.597605  0.187307
9  -0.128342 -1.511638 -0.289858
10  0.399194 -1.430030 -0.639760
11  1.115116 -2.012600  1.810662
  Self Join of a DataFrame GH2996 
In [182]: df = pd.DataFrame(
   .....:     data={
   .....:         "Area": ["A"] * 5 + ["C"] * 2,
   .....:         "Bins": [110] * 2 + [160] * 3 + [40] * 2,
   .....:         "Test_0": [0, 1, 0, 1, 2, 0, 1],
   .....:         "Data": np.random.randn(7),
   .....:     }
   .....: )
   .....: 

In [183]: df
Out[183]: 
  Area  Bins  Test_0      Data
0    A   110       0 -0.433937
1    A   110       1 -0.160552
2    A   160       0  0.744434
3    A   160       1  1.754213
4    A   160       2  0.000850
5    C    40       0  0.342243
6    C    40       1  1.070599

In [184]: df["Test_1"] = df["Test_0"] - 1

In [185]: pd.merge(
   .....:     df,
   .....:     df,
   .....:     left_on=["Bins", "Area", "Test_0"],
   .....:     right_on=["Bins", "Area", "Test_1"],
   .....:     suffixes=("_L", "_R"),
   .....: )
   .....: 
Out[185]: 
  Area  Bins  Test_0_L    Data_L  Test_1_L  Test_0_R    Data_R  Test_1_R
0    A   110         0 -0.433937        -1         1 -0.160552         0
1    A   160         0  0.744434        -1         1  1.754213         0
2    A   160         1  1.754213         0         2  0.000850         1
3    C    40         0  0.342243        -1         1  1.070599         0
  How to set the index and join KDB like asof join Join with a criteria based on the values Using searchsorted to merge based on values inside a range   Plotting The Plotting docs. Make Matplotlib look like R Setting x-axis major and minor labels Plotting multiple charts in an IPython Jupyter notebook Creating a multi-line plot Plotting a heatmap Annotate a time-series plot Annotate a time-series plot #2 Generate Embedded plots in excel files using Pandas, Vincent and xlsxwriter Boxplot for each quartile of a stratifying variable 
In [186]: df = pd.DataFrame(
   .....:     {
   .....:         "stratifying_var": np.random.uniform(0, 100, 20),
   .....:         "price": np.random.normal(100, 5, 20),
   .....:     }
   .....: )
   .....: 

In [187]: df["quartiles"] = pd.qcut(
   .....:     df["stratifying_var"], 4, labels=["0-25%", "25-50%", "50-75%", "75-100%"]
   .....: )
   .....: 

In [188]: df.boxplot(column="price", by="quartiles")
Out[188]: <AxesSubplot:title={'center':'price'}, xlabel='quartiles'>
     Data in/out Performance comparison of SQL vs HDF5  CSV The CSV docs read_csv in action appending to a csv Reading a csv chunk-by-chunk Reading only certain rows of a csv chunk-by-chunk Reading the first few lines of a frame Reading a file that is compressed but not by gzip/bz2 (the native compressed formats which read_csv understands). This example shows a WinZipped file, but is a general application of opening the file within a context manager and using that handle to read. See here Inferring dtypes from a file Dealing with bad lines GH2886 Write a multi-row index CSV without writing duplicates  Reading multiple files to create a single DataFrame The best way to combine multiple files into a single DataFrame is to read the individual frames one by one, put all of the individual frames into a list, and then combine the frames in the list using pd.concat(): 
In [189]: for i in range(3):
   .....:     data = pd.DataFrame(np.random.randn(10, 4))
   .....:     data.to_csv("file_{}.csv".format(i))
   .....: 

In [190]: files = ["file_0.csv", "file_1.csv", "file_2.csv"]

In [191]: result = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)
  You can use the same approach to read all files matching a pattern. Here is an example using glob: 
In [192]: import glob

In [193]: import os

In [194]: files = glob.glob("file_*.csv")

In [195]: result = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)
  Finally, this strategy will work with the other pd.read_*(...) functions described in the io docs.   Parsing date components in multi-columns Parsing date components in multi-columns is faster with a format 
In [196]: i = pd.date_range("20000101", periods=10000)

In [197]: df = pd.DataFrame({"year": i.year, "month": i.month, "day": i.day})

In [198]: df.head()
Out[198]: 
   year  month  day
0  2000      1    1
1  2000      1    2
2  2000      1    3
3  2000      1    4
4  2000      1    5

In [199]: %timeit pd.to_datetime(df.year * 10000 + df.month * 100 + df.day, format='%Y%m%d')
   .....: ds = df.apply(lambda x: "%04d%02d%02d" % (x["year"], x["month"], x["day"]), axis=1)
   .....: ds.head()
   .....: %timeit pd.to_datetime(ds)
   .....: 
8.7 ms +- 765 us per loop (mean +- std. dev. of 7 runs, 100 loops each)
2.1 ms +- 419 us per loop (mean +- std. dev. of 7 runs, 100 loops each)
    Skip row between header and data 
In [200]: data = """;;;;
   .....:  ;;;;
   .....:  ;;;;
   .....:  ;;;;
   .....:  ;;;;
   .....:  ;;;;
   .....: ;;;;
   .....:  ;;;;
   .....:  ;;;;
   .....: ;;;;
   .....: date;Param1;Param2;Param4;Param5
   .....:     ;m²;°C;m²;m
   .....: ;;;;
   .....: 01.01.1990 00:00;1;1;2;3
   .....: 01.01.1990 01:00;5;3;4;5
   .....: 01.01.1990 02:00;9;5;6;7
   .....: 01.01.1990 03:00;13;7;8;9
   .....: 01.01.1990 04:00;17;9;10;11
   .....: 01.01.1990 05:00;21;11;12;13
   .....: """
   .....: 
   Option 1: pass rows explicitly to skip rows 
In [201]: from io import StringIO

In [202]: pd.read_csv(
   .....:     StringIO(data),
   .....:     sep=";",
   .....:     skiprows=[11, 12],
   .....:     index_col=0,
   .....:     parse_dates=True,
   .....:     header=10,
   .....: )
   .....: 
Out[202]: 
                     Param1  Param2  Param4  Param5
date                                               
1990-01-01 00:00:00       1       1       2       3
1990-01-01 01:00:00       5       3       4       5
1990-01-01 02:00:00       9       5       6       7
1990-01-01 03:00:00      13       7       8       9
1990-01-01 04:00:00      17       9      10      11
1990-01-01 05:00:00      21      11      12      13
    Option 2: read column names and then data 
In [203]: pd.read_csv(StringIO(data), sep=";", header=10, nrows=10).columns
Out[203]: Index(['date', 'Param1', 'Param2', 'Param4', 'Param5'], dtype='object')

In [204]: columns = pd.read_csv(StringIO(data), sep=";", header=10, nrows=10).columns

In [205]: pd.read_csv(
   .....:     StringIO(data), sep=";", index_col=0, header=12, parse_dates=True, names=columns
   .....: )
   .....: 
Out[205]: 
                     Param1  Param2  Param4  Param5
date                                               
1990-01-01 00:00:00       1       1       2       3
1990-01-01 01:00:00       5       3       4       5
1990-01-01 02:00:00       9       5       6       7
1990-01-01 03:00:00      13       7       8       9
1990-01-01 04:00:00      17       9      10      11
1990-01-01 05:00:00      21      11      12      13
      SQL The SQL docs Reading from databases with SQL   Excel The Excel docs Reading from a filelike handle Modifying formatting in XlsxWriter output Loading only visible sheets GH19842#issuecomment-892150745   HTML Reading HTML tables from a server that cannot handle the default request header   HDFStore The HDFStores docs Simple queries with a Timestamp Index Managing heterogeneous data using a linked multiple table hierarchy GH3032 Merging on-disk tables with millions of rows Avoiding inconsistencies when writing to a store from multiple processes/threads De-duplicating a large store by chunks, essentially a recursive reduction operation. Shows a function for taking in data from csv file and creating a store by chunks, with date parsing as well. See here Creating a store chunk-by-chunk from a csv file Appending to a store, while creating a unique index Large Data work flows Reading in a sequence of files, then providing a global unique index to a store while appending Groupby on a HDFStore with low group density Groupby on a HDFStore with high group density Hierarchical queries on a HDFStore Counting with a HDFStore Troubleshoot HDFStore exceptions Setting min_itemsize with strings Using ptrepack to create a completely-sorted-index on a store Storing Attributes to a group node 
In [206]: df = pd.DataFrame(np.random.randn(8, 3))

In [207]: store = pd.HDFStore("test.h5")

In [208]: store.put("df", df)

# you can store an arbitrary Python object via pickle
In [209]: store.get_storer("df").attrs.my_attribute = {"A": 10}

In [210]: store.get_storer("df").attrs.my_attribute
Out[210]: {'A': 10}
  You can create or load a HDFStore in-memory by passing the driver parameter to PyTables. Changes are only written to disk when the HDFStore is closed. 
In [211]: store = pd.HDFStore("test.h5", "w", driver="H5FD_CORE")

In [212]: df = pd.DataFrame(np.random.randn(8, 3))

In [213]: store["test"] = df

# only after closing the store, data is written to disk:
In [214]: store.close()
    Binary files pandas readily accepts NumPy record arrays, if you need to read in a binary file consisting of an array of C structs. For example, given this C program in a file called main.c compiled with gcc main.c -std=gnu99 on a 64-bit machine, 
#include <stdio.h>
#include <stdint.h>

typedef struct _Data
{
    int32_t count;
    double avg;
    float scale;
} Data;

int main(int argc, const char *argv[])
{
    size_t n = 10;
    Data d[n];

    for (int i = 0; i < n; ++i)
    {
        d[i].count = i;
        d[i].avg = i + 1.0;
        d[i].scale = (float) i + 2.0f;
    }

    FILE *file = fopen("binary.dat", "wb");
    fwrite(&d, sizeof(Data), n, file);
    fclose(file);

    return 0;
}
  the following Python code will read the binary file 'binary.dat' into a pandas DataFrame, where each element of the struct corresponds to a column in the frame: 
names = "count", "avg", "scale"

# note that the offsets are larger than the size of the type because of
# struct padding
offsets = 0, 8, 16
formats = "i4", "f8", "f4"
dt = np.dtype({"names": names, "offsets": offsets, "formats": formats}, align=True)
df = pd.DataFrame(np.fromfile("binary.dat", dt))
   Note The offsets of the structure elements may be different depending on the architecture of the machine on which the file was created. Using a raw binary file format like this for general data storage is not recommended, as it is not cross platform. We recommended either HDF5 or parquet, both of which are supported by pandas’ IO facilities.     Computation Numerical integration (sample-based) of a time series  Correlation Often it’s useful to obtain the lower (or upper) triangular form of a correlation matrix calculated from DataFrame.corr(). This can be achieved by passing a boolean mask to where as follows: 
In [215]: df = pd.DataFrame(np.random.random(size=(100, 5)))

In [216]: corr_mat = df.corr()

In [217]: mask = np.tril(np.ones_like(corr_mat, dtype=np.bool_), k=-1)

In [218]: corr_mat.where(mask)
Out[218]: 
          0         1         2        3   4
0       NaN       NaN       NaN      NaN NaN
1 -0.079861       NaN       NaN      NaN NaN
2 -0.236573  0.183801       NaN      NaN NaN
3 -0.013795 -0.051975  0.037235      NaN NaN
4 -0.031974  0.118342 -0.073499 -0.02063 NaN
  The method argument within DataFrame.corr can accept a callable in addition to the named correlation types. Here we compute the distance correlation matrix for a DataFrame object. 
In [219]: def distcorr(x, y):
   .....:     n = len(x)
   .....:     a = np.zeros(shape=(n, n))
   .....:     b = np.zeros(shape=(n, n))
   .....:     for i in range(n):
   .....:         for j in range(i + 1, n):
   .....:             a[i, j] = abs(x[i] - x[j])
   .....:             b[i, j] = abs(y[i] - y[j])
   .....:     a += a.T
   .....:     b += b.T
   .....:     a_bar = np.vstack([np.nanmean(a, axis=0)] * n)
   .....:     b_bar = np.vstack([np.nanmean(b, axis=0)] * n)
   .....:     A = a - a_bar - a_bar.T + np.full(shape=(n, n), fill_value=a_bar.mean())
   .....:     B = b - b_bar - b_bar.T + np.full(shape=(n, n), fill_value=b_bar.mean())
   .....:     cov_ab = np.sqrt(np.nansum(A * B)) / n
   .....:     std_a = np.sqrt(np.sqrt(np.nansum(A ** 2)) / n)
   .....:     std_b = np.sqrt(np.sqrt(np.nansum(B ** 2)) / n)
   .....:     return cov_ab / std_a / std_b
   .....: 

In [220]: df = pd.DataFrame(np.random.normal(size=(100, 3)))

In [221]: df.corr(method=distcorr)
Out[221]: 
          0         1         2
0  1.000000  0.197613  0.216328
1  0.197613  1.000000  0.208749
2  0.216328  0.208749  1.000000
     Timedeltas The Timedeltas docs. Using timedeltas 
In [222]: import datetime

In [223]: s = pd.Series(pd.date_range("2012-1-1", periods=3, freq="D"))

In [224]: s - s.max()
Out[224]: 
0   -2 days
1   -1 days
2    0 days
dtype: timedelta64[ns]

In [225]: s.max() - s
Out[225]: 
0   2 days
1   1 days
2   0 days
dtype: timedelta64[ns]

In [226]: s - datetime.datetime(2011, 1, 1, 3, 5)
Out[226]: 
0   364 days 20:55:00
1   365 days 20:55:00
2   366 days 20:55:00
dtype: timedelta64[ns]

In [227]: s + datetime.timedelta(minutes=5)
Out[227]: 
0   2012-01-01 00:05:00
1   2012-01-02 00:05:00
2   2012-01-03 00:05:00
dtype: datetime64[ns]

In [228]: datetime.datetime(2011, 1, 1, 3, 5) - s
Out[228]: 
0   -365 days +03:05:00
1   -366 days +03:05:00
2   -367 days +03:05:00
dtype: timedelta64[ns]

In [229]: datetime.timedelta(minutes=5) + s
Out[229]: 
0   2012-01-01 00:05:00
1   2012-01-02 00:05:00
2   2012-01-03 00:05:00
dtype: datetime64[ns]
  Adding and subtracting deltas and dates 
In [230]: deltas = pd.Series([datetime.timedelta(days=i) for i in range(3)])

In [231]: df = pd.DataFrame({"A": s, "B": deltas})

In [232]: df
Out[232]: 
           A      B
0 2012-01-01 0 days
1 2012-01-02 1 days
2 2012-01-03 2 days

In [233]: df["New Dates"] = df["A"] + df["B"]

In [234]: df["Delta"] = df["A"] - df["New Dates"]

In [235]: df
Out[235]: 
           A      B  New Dates   Delta
0 2012-01-01 0 days 2012-01-01  0 days
1 2012-01-02 1 days 2012-01-03 -1 days
2 2012-01-03 2 days 2012-01-05 -2 days

In [236]: df.dtypes
Out[236]: 
A             datetime64[ns]
B            timedelta64[ns]
New Dates     datetime64[ns]
Delta        timedelta64[ns]
dtype: object
  Another example Values can be set to NaT using np.nan, similar to datetime 
In [237]: y = s - s.shift()

In [238]: y
Out[238]: 
0      NaT
1   1 days
2   1 days
dtype: timedelta64[ns]

In [239]: y[1] = np.nan

In [240]: y
Out[240]: 
0      NaT
1      NaT
2   1 days
dtype: timedelta64[ns]
    Creating example data To create a dataframe from every combination of some given values, like R’s expand.grid() function, we can create a dict where the keys are column names and the values are lists of the data values: 
In [241]: def expand_grid(data_dict):
   .....:     rows = itertools.product(*data_dict.values())
   .....:     return pd.DataFrame.from_records(rows, columns=data_dict.keys())
   .....: 

In [242]: df = expand_grid(
   .....:     {"height": [60, 70], "weight": [100, 140, 180], "sex": ["Male", "Female"]}
   .....: )
   .....: 

In [243]: df
Out[243]: 
    height  weight     sex
0       60     100    Male
1       60     100  Female
2       60     140    Male
3       60     140  Female
4       60     180    Male
5       60     180  Female
6       70     100    Male
7       70     100  Female
8       70     140    Male
9       70     140  Female
10      70     180    Male
11      70     180  Female
pygame.event.Event() 
 create a new event object Event(type, dict) -> EventType instance Event(type, **attributes) -> EventType instance  Creates a new event with the given type and attributes. The attributes can come from a dictionary argument with string keys or from keyword arguments.
def f_38535931(df, tuples):
    """pandas: delete rows in dataframe `df` based on multiple columns values
    """
    return  
 --------------------

def f_13945749(goals, penalties):
    """format the variables `goals` and `penalties` using string formatting
    """
    return  
 --------------------

def f_13945749(goals, penalties):
    """format string "({} goals, ${})" with variables `goals` and `penalties`
    """
    return  
 --------------------

def f_18524642(L):
    """convert list of lists `L` to list of integers
    """
    return  
 --------------------

def f_18524642(L):
    """convert a list of lists `L` to list of integers
    """
     
 --------------------

def f_7138686(lines, myfile):
    """write the elements of list `lines` concatenated by special character '\n' to file `myfile`
    """
     
 --------------------
Please refer to the following documentation to generate the code:
comment(text)  
Creates a comment with the given text. If insert_comments is true, this will also add it to the tree.  New in version 3.8.
text  
The source code text involved in the error.
token_generator  
Instance of the class to check the password. This will default to default_token_generator, it’s an instance of django.contrib.auth.tokens.PasswordResetTokenGenerator.
token_generator  
Instance of the class to check the one time link. This will default to default_token_generator, it’s an instance of django.contrib.auth.tokens.PasswordResetTokenGenerator.
textwrap.dedent(text)  
Remove any common leading whitespace from every line in text. This can be used to make triple-quoted strings line up with the left edge of the display, while still presenting them in the source code in indented form. Note that tabs and spaces are both treated as whitespace, but they are not equal: the lines "  hello" and "\thello" are considered to have no common leading whitespace. Lines containing only whitespace are ignored in the input and normalized to a single newline character in the output. For example: def test():
    # end first line with \ to avoid the empty line!
    s = '''\
    hello
      world
    '''
    print(repr(s))          # prints '    hello\n      world\n    '
    print(repr(dedent(s)))  # prints 'hello\n  world\n'
def f_17238587(text):
    """Remove duplicate words from a string `text` using regex
    """
     
 --------------------
Please refer to the following documentation to generate the code:
pandas.DataFrame.count   DataFrame.count(axis=0, level=None, numeric_only=False)[source]
 
Count non-NA cells for each column or row. The values None, NaN, NaT, and optionally numpy.inf (depending on pandas.options.mode.use_inf_as_na) are considered NA.  Parameters 
 
axis:{0 or ‘index’, 1 or ‘columns’}, default 0


If 0 or ‘index’ counts are generated for each column. If 1 or ‘columns’ counts are generated for each row.  
level:int or str, optional


If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a DataFrame. A str specifies the level name.  
numeric_only:bool, default False


Include only float, int or boolean data.    Returns 
 Series or DataFrame

For each column/row the number of non-NA/null entries. If level is specified returns a DataFrame.      See also  Series.count

Number of non-NA elements in a Series.  DataFrame.value_counts

Count unique combinations of columns.  DataFrame.shape

Number of DataFrame rows and columns (including NA elements).  DataFrame.isna

Boolean same-sized DataFrame showing places of NA elements.    Examples Constructing DataFrame from a dictionary: 
>>> df = pd.DataFrame({"Person":
...                    ["John", "Myla", "Lewis", "John", "Myla"],
...                    "Age": [24., np.nan, 21., 33, 26],
...                    "Single": [False, True, True, True, False]})
>>> df
   Person   Age  Single
0    John  24.0   False
1    Myla   NaN    True
2   Lewis  21.0    True
3    John  33.0    True
4    Myla  26.0   False
  Notice the uncounted NA values: 
>>> df.count()
Person    5
Age       4
Single    5
dtype: int64
  Counts for each row: 
>>> df.count(axis='columns')
0    3
1    2
2    3
3    3
4    3
dtype: int64
pandas.DataFrame.value_counts   DataFrame.value_counts(subset=None, normalize=False, sort=True, ascending=False, dropna=True)[source]
 
Return a Series containing counts of unique rows in the DataFrame.  New in version 1.1.0.   Parameters 
 
subset:list-like, optional


Columns to use when counting unique combinations.  
normalize:bool, default False


Return proportions rather than frequencies.  
sort:bool, default True


Sort by frequencies.  
ascending:bool, default False


Sort in ascending order.  
dropna:bool, default True


Don’t include counts of rows that contain NA values.  New in version 1.3.0.     Returns 
 Series
    See also  Series.value_counts

Equivalent method on Series.    Notes The returned Series will have a MultiIndex with one level per input column. By default, rows that contain any NA values are omitted from the result. By default, the resulting Series will be in descending order so that the first element is the most frequently-occurring row. Examples 
>>> df = pd.DataFrame({'num_legs': [2, 4, 4, 6],
...                    'num_wings': [2, 0, 0, 0]},
...                   index=['falcon', 'dog', 'cat', 'ant'])
>>> df
        num_legs  num_wings
falcon         2          2
dog            4          0
cat            4          0
ant            6          0
  
>>> df.value_counts()
num_legs  num_wings
4         0            2
2         2            1
6         0            1
dtype: int64
  
>>> df.value_counts(sort=False)
num_legs  num_wings
2         2            1
4         0            2
6         0            1
dtype: int64
  
>>> df.value_counts(ascending=True)
num_legs  num_wings
2         2            1
6         0            1
4         0            2
dtype: int64
  
>>> df.value_counts(normalize=True)
num_legs  num_wings
4         0            0.50
2         2            0.25
6         0            0.25
dtype: float64
  With dropna set to False we can also count rows with NA values. 
>>> df = pd.DataFrame({'first_name': ['John', 'Anne', 'John', 'Beth'],
...                    'middle_name': ['Smith', pd.NA, pd.NA, 'Louise']})
>>> df
  first_name middle_name
0       John       Smith
1       Anne        <NA>
2       John        <NA>
3       Beth      Louise
  
>>> df.value_counts()
first_name  middle_name
Beth        Louise         1
John        Smith          1
dtype: int64
  
>>> df.value_counts(dropna=False)
first_name  middle_name
Anne        NaN            1
Beth        Louise         1
John        Smith          1
            NaN            1
dtype: int64
pandas.Series.count   Series.count(level=None)[source]
 
Return number of non-NA/null observations in the Series.  Parameters 
 
level:int or level name, default None


If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a smaller Series.    Returns 
 int or Series (if level specified)

Number of non-null values in the Series.      See also  DataFrame.count

Count non-NA cells for each column or row.    Examples 
>>> s = pd.Series([0.0, 1.0, np.nan])
>>> s.count()
2
pandas.Series.value_counts   Series.value_counts(normalize=False, sort=True, ascending=False, bins=None, dropna=True)[source]
 
Return a Series containing counts of unique values. The resulting object will be in descending order so that the first element is the most frequently-occurring element. Excludes NA values by default.  Parameters 
 
normalize:bool, default False


If True then the object returned will contain the relative frequencies of the unique values.  
sort:bool, default True


Sort by frequencies.  
ascending:bool, default False


Sort in ascending order.  
bins:int, optional


Rather than count values, group them into half-open bins, a convenience for pd.cut, only works with numeric data.  
dropna:bool, default True


Don’t include counts of NaN.    Returns 
 Series
    See also  Series.count

Number of non-NA elements in a Series.  DataFrame.count

Number of non-NA elements in a DataFrame.  DataFrame.value_counts

Equivalent method on DataFrames.    Examples 
>>> index = pd.Index([3, 1, 2, 3, 4, np.nan])
>>> index.value_counts()
3.0    2
1.0    1
2.0    1
4.0    1
dtype: int64
  With normalize set to True, returns the relative frequency by dividing all values by the sum of values. 
>>> s = pd.Series([3, 1, 2, 3, 4, np.nan])
>>> s.value_counts(normalize=True)
3.0    0.4
1.0    0.2
2.0    0.2
4.0    0.2
dtype: float64
  bins Bins can be useful for going from a continuous variable to a categorical variable; instead of counting unique apparitions of values, divide the index in the specified number of half-open bins. 
>>> s.value_counts(bins=3)
(0.996, 2.0]    2
(2.0, 3.0]      2
(3.0, 4.0]      1
dtype: int64
  dropna With dropna set to False we can also see NaN index values. 
>>> s.value_counts(dropna=False)
3.0    2
1.0    1
2.0    1
4.0    1
NaN    1
dtype: int64
pandas.core.groupby.DataFrameGroupBy.nunique   DataFrameGroupBy.nunique(dropna=True)[source]
 
Return DataFrame with counts of unique elements in each position.  Parameters 
 
dropna:bool, default True


Don’t include NaN in the counts.    Returns 
 nunique: DataFrame
   Examples 
>>> df = pd.DataFrame({'id': ['spam', 'egg', 'egg', 'spam',
...                           'ham', 'ham'],
...                    'value1': [1, 5, 5, 2, 5, 5],
...                    'value2': list('abbaxy')})
>>> df
     id  value1 value2
0  spam       1      a
1   egg       5      b
2   egg       5      b
3  spam       2      a
4   ham       5      x
5   ham       5      y
  
>>> df.groupby('id').nunique()
      value1  value2
id
egg        1       1
ham        1       2
spam       2       1
  Check for rows with the same id but conflicting values: 
>>> df.groupby('id').filter(lambda g: (g.nunique() > 1).any())
     id  value1 value2
0  spam       1      a
3  spam       2      a
4   ham       5      x
5   ham       5      y
def f_26053849(df):
    """count non zero values in each column in pandas data frame `df`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
winreg.ExpandEnvironmentStrings(str)  
Expands environment variable placeholders %NAME% in strings like REG_EXPAND_SZ: >>> ExpandEnvironmentStrings('%windir%')
'C:\\Windows'
 Raises an auditing event winreg.ExpandEnvironmentStrings with argument str.
glob.escape(pathname)  
Escape all special characters ('?', '*' and '['). This is useful if you want to match an arbitrary literal string that may have special characters in it. Special characters in drive/UNC sharepoints are not escaped, e.g. on Windows escape('//?/c:/Quo vadis?.txt') returns '//?/c:/Quo vadis[?].txt'.  New in version 3.4.
email.utils.quote(str)  
Return a new string with backslashes in str replaced by two backslashes, and double quotes replaced by backslash-double quote.
numpy.distutils.misc_util.cyg2win32(path: str) → str[source]
 
Convert a path from Cygwin-native to Windows-native. Uses the cygpath utility (part of the Base install) to do the actual conversion. Falls back to returning the original path if this fails. Handles the default /cygdrive mount prefix as well as the /proc/cygdrive portable prefix, custom cygdrive prefixes such as / or /mnt, and absolute paths such as /usr/src/ or /home/username  Parameters 
 
pathstr


The path to convert    Returns 
 
converted_pathstr


The converted path     Notes Documentation for cygpath utility: https://cygwin.com/cygwin-ug-net/cygpath.html Documentation for the C function it wraps: https://cygwin.com/cygwin-api/func-cygwin-conv-path.html
winreg.HKEY_DYN_DATA  
This key is not used in versions of Windows after 98.
def f_15534223():
    """search for string that matches regular expression pattern '(?<!Distillr)\\\\AcroTray\\.exe' in string 'C:\\SomeDir\\AcroTray.exe'
    """
    return  
 --------------------

def f_5453026():
    """split string 'QH QD JC KD JS' into a list on white spaces
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
xml.parsers.expat.errors.XML_ERROR_TAG_MISMATCH  
An end tag did not match the innermost open start tag.
HTMLParser.get_starttag_text()  
Return the text of the most recently opened start tag. This should not normally be needed for structured processing, but may be useful in dealing with HTML “as deployed” or for re-generating input with minimal changes (whitespace between attributes can be preserved, etc.).
SimpleTestCase.assertInHTML(needle, haystack, count=None, msg_prefix='')  
Asserts that the HTML fragment needle is contained in the haystack one. If the count integer argument is specified, then additionally the number of needle occurrences will be strictly verified. Whitespace in most cases is ignored, and attribute ordering is not significant. See assertHTMLEqual() for more details.
findtext(match, default=None, namespaces=None)  
Same as Element.findtext(), starting at the root of the tree.
iterfind(match, namespaces=None)  
Same as Element.iterfind(), starting at the root of the tree.  New in version 3.2.
def f_18168684(line):
    """search for occurrences of regex pattern '>.*<' in xml string `line`
    """
    return  
 --------------------

def f_4914277(filename):
    """erase all the contents of a file `filename`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
classmethod datetime.strptime(date_string, format)  
Return a datetime corresponding to date_string, parsed according to format. This is equivalent to: datetime(*(time.strptime(date_string, format)[0:6]))
 ValueError is raised if the date_string and format can’t be parsed by time.strptime() or if it returns a value which isn’t a time tuple. For a complete list of formatting directives, see strftime() and strptime() Behavior.
classmethod date.fromisoformat(date_string)  
Return a date corresponding to a date_string given in the format YYYY-MM-DD: >>> from datetime import date
>>> date.fromisoformat('2019-12-04')
datetime.date(2019, 12, 4)
 This is the inverse of date.isoformat(). It only supports the format YYYY-MM-DD.  New in version 3.7.
pandas.StringDtype   classpandas.StringDtype(storage=None)[source]
 
Extension dtype for string data.  New in version 1.0.0.   Warning StringDtype is considered experimental. The implementation and parts of the API may change without warning. In particular, StringDtype.na_value may change to no longer be numpy.nan.   Parameters 
 
storage:{“python”, “pyarrow”}, optional


If not given, the value of pd.options.mode.string_storage.     Examples 
>>> pd.StringDtype()
string[python]
  
>>> pd.StringDtype(storage="pyarrow")
string[pyarrow]
  Attributes       
None     Methods       
None
classmethod datetime.fromisoformat(date_string)  
Return a datetime corresponding to a date_string in one of the formats emitted by date.isoformat() and datetime.isoformat(). Specifically, this function supports strings in the format: YYYY-MM-DD[*HH[:MM[:SS[.fff[fff]]]][+HH:MM[:SS[.ffffff]]]]
 where * can match any single character.  Caution This does not support parsing arbitrary ISO 8601 strings - it is only intended as the inverse operation of datetime.isoformat(). A more full-featured ISO 8601 parser, dateutil.parser.isoparse is available in the third-party package dateutil.  Examples: >>> from datetime import datetime
>>> datetime.fromisoformat('2011-11-04')
datetime.datetime(2011, 11, 4, 0, 0)
>>> datetime.fromisoformat('2011-11-04T00:05:23')
datetime.datetime(2011, 11, 4, 0, 5, 23)
>>> datetime.fromisoformat('2011-11-04 00:05:23.283')
datetime.datetime(2011, 11, 4, 0, 5, 23, 283000)
>>> datetime.fromisoformat('2011-11-04 00:05:23.283+00:00')
datetime.datetime(2011, 11, 4, 0, 5, 23, 283000, tzinfo=datetime.timezone.utc)
>>> datetime.fromisoformat('2011-11-04T00:05:23+04:00')   
datetime.datetime(2011, 11, 4, 0, 5, 23,
    tzinfo=datetime.timezone(datetime.timedelta(seconds=14400)))
  New in version 3.7.
tf.keras.layers.Maximum     View source on GitHub    Layer that computes the maximum (element-wise) a list of inputs. Inherits From: Layer, Module  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.keras.layers.Maximum  
tf.keras.layers.Maximum(
    **kwargs
)
 It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape). 
tf.keras.layers.Maximum()([np.arange(5).reshape(5, 1),
                           np.arange(5, 10).reshape(5, 1)])
<tf.Tensor: shape=(5, 1), dtype=int64, numpy=
array([[5],
     [6],
     [7],
     [8],
     [9]])>
 
x1 = tf.keras.layers.Dense(8)(np.arange(10).reshape(5, 2))
x2 = tf.keras.layers.Dense(8)(np.arange(10, 20).reshape(5, 2))
maxed = tf.keras.layers.Maximum()([x1, x2])
maxed.shape
TensorShape([5, 8])

 


 Arguments
  **kwargs   standard layer keyword arguments.
def f_19068269(string_date):
    """convert a string `string_date` into datetime using the format '%Y-%m-%d %H:%M:%S.%f'
    """
    return  
 --------------------

def f_20683167(thelist):
    """find the index of a list with the first element equal to '332' within the list of lists `thelist`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
tf.strings.lower Converts all uppercase characters into their respective lowercase replacements.  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.strings.lower  
tf.strings.lower(
    input, encoding='', name=None
)
 Example: 
tf.strings.lower("CamelCase string and ALL CAPS")
<tf.Tensor: shape=(), dtype=string, numpy=b'camelcase string and all caps'>

 


 Args
  input   A Tensor of type string.  
  encoding   An optional string. Defaults to "".  
  name   A name for the operation (optional).   
 


 Returns   A Tensor of type string.
numpy.char.lower   char.lower(a)[source]
 
Return an array with the elements converted to lowercase. Call str.lower element-wise. For 8-bit strings, this method is locale-dependent.  Parameters 
 
aarray_like, {str, unicode}


Input array.    Returns 
 
outndarray, {str, unicode}


Output array of str or unicode, depending on input type      See also  str.lower
  Examples >>> c = np.array(['A1B C', '1BCA', 'BCA1']); c
array(['A1B C', '1BCA', 'BCA1'], dtype='<U5')
>>> np.char.lower(c)
array(['a1b c', '1bca', 'bca1'], dtype='<U5')
operator.ilshift(a, b)  
operator.__ilshift__(a, b)  
a = ilshift(a, b) is equivalent to a <<= b.
operator.ilshift(a, b)  
operator.__ilshift__(a, b)  
a = ilshift(a, b) is equivalent to a <<= b.
str.isalnum()  
Return True if all characters in the string are alphanumeric and there is at least one character, False otherwise. A character c is alphanumeric if one of the following returns True: c.isalpha(), c.isdecimal(), c.isdigit(), or c.isnumeric().
def f_30693804(text):
    """lower a string `text` and remove non-alphanumeric characters aside from space
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
operator.ilshift(a, b)  
operator.__ilshift__(a, b)  
a = ilshift(a, b) is equivalent to a <<= b.
operator.ilshift(a, b)  
operator.__ilshift__(a, b)  
a = ilshift(a, b) is equivalent to a <<= b.
str.isalnum()  
Return True if all characters in the string are alphanumeric and there is at least one character, False otherwise. A character c is alphanumeric if one of the following returns True: c.isalpha(), c.isdecimal(), c.isdigit(), or c.isnumeric().
numpy.genfromtxt   numpy.genfromtxt(fname, dtype=<class 'float'>, comments='#', delimiter=None, skip_header=0, skip_footer=0, converters=None, missing_values=None, filling_values=None, usecols=None, names=None, excludelist=None, deletechars=" !#$%&'()*+, -./:;<=>?@[\\]^{|}~", replace_space='_', autostrip=False, case_sensitive=True, defaultfmt='f%i', unpack=None, usemask=False, loose=True, invalid_raise=True, max_rows=None, encoding='bytes', *, like=None)[source]
 
Load data from a text file, with missing values handled as specified. Each line past the first skip_header lines is split at the delimiter character, and characters following the comments character are discarded.  Parameters 
 
fnamefile, str, pathlib.Path, list of str, generator


File, filename, list, or generator to read. If the filename extension is .gz or .bz2, the file is first decompressed. Note that generators must return bytes or strings. The strings in a list or produced by a generator are treated as lines.  
dtypedtype, optional


Data type of the resulting array. If None, the dtypes will be determined by the contents of each column, individually.  
commentsstr, optional


The character used to indicate the start of a comment. All the characters occurring on a line after a comment are discarded.  
delimiterstr, int, or sequence, optional


The string used to separate values. By default, any consecutive whitespaces act as delimiter. An integer or sequence of integers can also be provided as width(s) of each field.  
skiprowsint, optional


skiprows was removed in numpy 1.10. Please use skip_header instead.  
skip_headerint, optional


The number of lines to skip at the beginning of the file.  
skip_footerint, optional


The number of lines to skip at the end of the file.  
convertersvariable, optional


The set of functions that convert the data of a column to a value. The converters can also be used to provide a default value for missing data: converters = {3: lambda s: float(s or 0)}.  
missingvariable, optional


missing was removed in numpy 1.10. Please use missing_values instead.  
missing_valuesvariable, optional


The set of strings corresponding to missing data.  
filling_valuesvariable, optional


The set of values to be used as default when the data are missing.  
usecolssequence, optional


Which columns to read, with 0 being the first. For example, usecols = (1, 4, 5) will extract the 2nd, 5th and 6th columns.  
names{None, True, str, sequence}, optional


If names is True, the field names are read from the first line after the first skip_header lines. This line can optionally be preceded by a comment delimiter. If names is a sequence or a single-string of comma-separated names, the names will be used to define the field names in a structured dtype. If names is None, the names of the dtype fields will be used, if any.  
excludelistsequence, optional


A list of names to exclude. This list is appended to the default list [‘return’,’file’,’print’]. Excluded names are appended with an underscore: for example, file would become file_.  
deletecharsstr, optional


A string combining invalid characters that must be deleted from the names.  
defaultfmtstr, optional


A format used to define default field names, such as “f%i” or “f_%02i”.  
autostripbool, optional


Whether to automatically strip white spaces from the variables.  
replace_spacechar, optional


Character(s) used in replacement of white spaces in the variable names. By default, use a ‘_’.  
case_sensitive{True, False, ‘upper’, ‘lower’}, optional


If True, field names are case sensitive. If False or ‘upper’, field names are converted to upper case. If ‘lower’, field names are converted to lower case.  
unpackbool, optional


If True, the returned array is transposed, so that arguments may be unpacked using x, y, z = genfromtxt(...). When used with a structured data-type, arrays are returned for each field. Default is False.  
usemaskbool, optional


If True, return a masked array. If False, return a regular array.  
loosebool, optional


If True, do not raise errors for invalid values.  
invalid_raisebool, optional


If True, an exception is raised if an inconsistency is detected in the number of columns. If False, a warning is emitted and the offending lines are skipped.  
max_rowsint, optional


The maximum number of rows to read. Must not be used with skip_footer at the same time. If given, the value must be at least 1. Default is to read the entire file.  New in version 1.10.0.   
encodingstr, optional


Encoding used to decode the inputfile. Does not apply when fname is a file object. The special value ‘bytes’ enables backward compatibility workarounds that ensure that you receive byte arrays when possible and passes latin1 encoded strings to converters. Override this value to receive unicode arrays and pass strings as input to converters. If set to None the system default is used. The default value is ‘bytes’.  New in version 1.14.0.   
likearray_like


Reference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as like supports the __array_function__ protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.  New in version 1.20.0.     Returns 
 
outndarray


Data read from the text file. If usemask is True, this is a masked array.      See also  numpy.loadtxt

equivalent function when no data is missing.    Notes  When spaces are used as delimiters, or when no delimiter has been given as input, there should not be any missing data between two fields. When the variables are named (either by a flexible dtype or with names), there must not be any header in the file (else a ValueError exception is raised). Individual values are not stripped of spaces by default. When using a custom converter, make sure the function does remove spaces.  References  1 
NumPy User Guide, section I/O with NumPy.   Examples >>> from io import StringIO
>>> import numpy as np
 Comma delimited file with mixed dtype >>> s = StringIO(u"1,1.3,abcde")
>>> data = np.genfromtxt(s, dtype=[('myint','i8'),('myfloat','f8'),
... ('mystring','S5')], delimiter=",")
>>> data
array((1, 1.3, b'abcde'),
      dtype=[('myint', '<i8'), ('myfloat', '<f8'), ('mystring', 'S5')])
 Using dtype = None >>> _ = s.seek(0) # needed for StringIO example only
>>> data = np.genfromtxt(s, dtype=None,
... names = ['myint','myfloat','mystring'], delimiter=",")
>>> data
array((1, 1.3, b'abcde'),
      dtype=[('myint', '<i8'), ('myfloat', '<f8'), ('mystring', 'S5')])
 Specifying dtype and names >>> _ = s.seek(0)
>>> data = np.genfromtxt(s, dtype="i8,f8,S5",
... names=['myint','myfloat','mystring'], delimiter=",")
>>> data
array((1, 1.3, b'abcde'),
      dtype=[('myint', '<i8'), ('myfloat', '<f8'), ('mystring', 'S5')])
 An example with fixed-width columns >>> s = StringIO(u"11.3abcde")
>>> data = np.genfromtxt(s, dtype=None, names=['intvar','fltvar','strvar'],
...     delimiter=[1,3,5])
>>> data
array((1, 1.3, b'abcde'),
      dtype=[('intvar', '<i8'), ('fltvar', '<f8'), ('strvar', 'S5')])
 An example to show comments >>> f = StringIO('''
... text,# of chars
... hello world,11
... numpy,5''')
>>> np.genfromtxt(f, dtype='S12,S12', delimiter=',')
array([(b'text', b''), (b'hello world', b'11'), (b'numpy', b'5')],
  dtype=[('f0', 'S12'), ('f1', 'S12')])
numpy.char.lower   char.lower(a)[source]
 
Return an array with the elements converted to lowercase. Call str.lower element-wise. For 8-bit strings, this method is locale-dependent.  Parameters 
 
aarray_like, {str, unicode}


Input array.    Returns 
 
outndarray, {str, unicode}


Output array of str or unicode, depending on input type      See also  str.lower
  Examples >>> c = np.array(['A1B C', '1BCA', 'BCA1']); c
array(['A1B C', '1BCA', 'BCA1'], dtype='<U5')
>>> np.char.lower(c)
array(['a1b c', '1bca', 'bca1'], dtype='<U5')
def f_30693804(text):
    """remove all non-alphanumeric characters except space from a string `text` and lower it
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
fmt_ds='$%d.%s^\\mathrm{h}$'
deg_mark='^\\mathrm{h}'
fmt_d='$%d^\\mathrm{h}$'
matplotlib.pyplot.semilogx   matplotlib.pyplot.semilogx(*args, **kwargs)[source]
 
Make a plot with log scaling on the x axis. Call signatures: semilogx([x], y, [fmt], data=None, **kwargs)
semilogx([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)
 This is just a thin wrapper around plot which additionally changes the x-axis to log scaling. All of the concepts and parameters of plot can be used here as well. The additional parameters base, subs, and nonpositive control the x-axis properties. They are just forwarded to Axes.set_xscale.  Parameters 
 
basefloat, default: 10


Base of the x logarithm.  
subsarray-like, optional


The location of the minor xticks. If None, reasonable locations are automatically chosen depending on the number of decades in the plot. See Axes.set_xscale for details.  
nonpositive{'mask', 'clip'}, default: 'mask'


Non-positive values in x can be masked as invalid, or clipped to a very small positive number.  **kwargs

All parameters supported by plot.    Returns 
 list of Line2D


Objects representing the plotted data.
matplotlib.axes.Axes.semilogx   Axes.semilogx(*args, **kwargs)[source]
 
Make a plot with log scaling on the x axis. Call signatures: semilogx([x], y, [fmt], data=None, **kwargs)
semilogx([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)
 This is just a thin wrapper around plot which additionally changes the x-axis to log scaling. All of the concepts and parameters of plot can be used here as well. The additional parameters base, subs, and nonpositive control the x-axis properties. They are just forwarded to Axes.set_xscale.  Parameters 
 
basefloat, default: 10


Base of the x logarithm.  
subsarray-like, optional


The location of the minor xticks. If None, reasonable locations are automatically chosen depending on the number of decades in the plot. See Axes.set_xscale for details.  
nonpositive{'mask', 'clip'}, default: 'mask'


Non-positive values in x can be masked as invalid, or clipped to a very small positive number.  **kwargs

All parameters supported by plot.    Returns 
 list of Line2D


Objects representing the plotted data.     
  Examples using matplotlib.axes.Axes.semilogx
 
   Log Demo   

   Log Axis   

   Transformations Tutorial
def f_17138464(x, y):
    """subscript text 'H20' with '2' as subscripted in matplotlib labels for arrays 'x' and 'y'.
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
fmt_ds='$%d.%s^\\mathrm{h}$'
deg_mark='^\\mathrm{h}'
fmt_d='$%d^\\mathrm{h}$'
matplotlib.pyplot.semilogx   matplotlib.pyplot.semilogx(*args, **kwargs)[source]
 
Make a plot with log scaling on the x axis. Call signatures: semilogx([x], y, [fmt], data=None, **kwargs)
semilogx([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)
 This is just a thin wrapper around plot which additionally changes the x-axis to log scaling. All of the concepts and parameters of plot can be used here as well. The additional parameters base, subs, and nonpositive control the x-axis properties. They are just forwarded to Axes.set_xscale.  Parameters 
 
basefloat, default: 10


Base of the x logarithm.  
subsarray-like, optional


The location of the minor xticks. If None, reasonable locations are automatically chosen depending on the number of decades in the plot. See Axes.set_xscale for details.  
nonpositive{'mask', 'clip'}, default: 'mask'


Non-positive values in x can be masked as invalid, or clipped to a very small positive number.  **kwargs

All parameters supported by plot.    Returns 
 list of Line2D


Objects representing the plotted data.
matplotlib.axes.Axes.semilogx   Axes.semilogx(*args, **kwargs)[source]
 
Make a plot with log scaling on the x axis. Call signatures: semilogx([x], y, [fmt], data=None, **kwargs)
semilogx([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)
 This is just a thin wrapper around plot which additionally changes the x-axis to log scaling. All of the concepts and parameters of plot can be used here as well. The additional parameters base, subs, and nonpositive control the x-axis properties. They are just forwarded to Axes.set_xscale.  Parameters 
 
basefloat, default: 10


Base of the x logarithm.  
subsarray-like, optional


The location of the minor xticks. If None, reasonable locations are automatically chosen depending on the number of decades in the plot. See Axes.set_xscale for details.  
nonpositive{'mask', 'clip'}, default: 'mask'


Non-positive values in x can be masked as invalid, or clipped to a very small positive number.  **kwargs

All parameters supported by plot.    Returns 
 list of Line2D


Objects representing the plotted data.     
  Examples using matplotlib.axes.Axes.semilogx
 
   Log Demo   

   Log Axis   

   Transformations Tutorial
def f_17138464(x, y):
    """subscript text 'H20' with '2' as subscripted in matplotlib labels for arrays 'x' and 'y'.
    """
    return  
 --------------------

def f_9138112(mylist):
    """loop over a list `mylist` if sublists length equals 3
    """
    return  
 --------------------

def f_1807026():
    """initialize a list `lst` of 100 objects Object()
    """
     
 --------------------
Please refer to the following documentation to generate the code:
tf.sparse.sparse_dense_matmul     View source on GitHub    Multiply SparseTensor (or dense Matrix) (of rank 2) "A" by dense matrix  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.sparse.matmul, tf.compat.v1.sparse.sparse_dense_matmul, tf.compat.v1.sparse_tensor_dense_matmul  
tf.sparse.sparse_dense_matmul(
    sp_a, b, adjoint_a=False, adjoint_b=False, name=None
)
 (or SparseTensor) "B". Please note that one and only one of the inputs MUST be a SparseTensor and the other MUST be a dense matrix. No validity checking is performed on the indices of A. However, the following input format is recommended for optimal behavior:  If adjoint_a == false: A should be sorted in lexicographically increasing order. Use sparse.reorder if you're not sure. If adjoint_a == true: A should be sorted in order of increasing dimension 1 (i.e., "column major" order instead of "row major" order).  Using tf.nn.embedding_lookup_sparse for sparse multiplication: It's not obvious but you can consider embedding_lookup_sparse as another sparse and dense multiplication. In some situations, you may prefer to use embedding_lookup_sparse even though you're not dealing with embeddings. There are two questions to ask in the decision process: Do you need gradients computed as sparse too? Is your sparse data represented as two SparseTensors: ids and values? There is more explanation about data format below. If you answer any of these questions as yes, consider using tf.nn.embedding_lookup_sparse. Following explains differences between the expected SparseTensors: For example if dense form of your sparse data has shape [3, 5] and values: [[  a      ]
 [b       c]
 [    d    ]]
 SparseTensor format expected by sparse_tensor_dense_matmul: sp_a (indices, values): [0, 1]: a
[1, 0]: b
[1, 4]: c
[2, 2]: d
 SparseTensor format expected by embedding_lookup_sparse: sp_ids sp_weights [0, 0]: 1                [0, 0]: a
[1, 0]: 0                [1, 0]: b
[1, 1]: 4                [1, 1]: c
[2, 0]: 2                [2, 0]: d
 Deciding when to use sparse_tensor_dense_matmul vs. matmul(a_is_sparse=True): There are a number of questions to ask in the decision process, including:  Will the SparseTensor A fit in memory if densified? Is the column count of the product large (>> 1)? Is the density of A larger than approximately 15%?  If the answer to several of these questions is yes, consider converting the SparseTensor to a dense one and using tf.matmul with a_is_sparse=True. This operation tends to perform well when A is more sparse, if the column size of the product is small (e.g. matrix-vector multiplication), if sp_a.dense_shape takes on large values. Below is a rough speed comparison between sparse_tensor_dense_matmul, labeled 'sparse', and matmul(a_is_sparse=True), labeled 'dense'. For purposes of the comparison, the time spent converting from a SparseTensor to a dense Tensor is not included, so it is overly conservative with respect to the time ratio. Benchmark system: CPU: Intel Ivybridge with HyperThreading (6 cores) dL1:32KB dL2:256KB dL3:12MB GPU: NVidia Tesla k40c Compiled with: -c opt --config=cuda --copt=-mavx tensorflow/python/sparse_tensor_dense_matmul_op_test --benchmarks
A sparse [m, k] with % nonzero values between 1% and 80%
B dense [k, n]

% nnz  n   gpu   m     k     dt(dense)     dt(sparse)   dt(sparse)/dt(dense)
0.01   1   True  100   100   0.000221166   0.00010154   0.459112
0.01   1   True  100   1000  0.00033858    0.000109275  0.322745
0.01   1   True  1000  100   0.000310557   9.85661e-05  0.317385
0.01   1   True  1000  1000  0.0008721     0.000100875  0.115669
0.01   1   False 100   100   0.000208085   0.000107603  0.51711
0.01   1   False 100   1000  0.000327112   9.51118e-05  0.290762
0.01   1   False 1000  100   0.000308222   0.00010345   0.335635
0.01   1   False 1000  1000  0.000865721   0.000101397  0.117124
0.01   10  True  100   100   0.000218522   0.000105537  0.482958
0.01   10  True  100   1000  0.000340882   0.000111641  0.327506
0.01   10  True  1000  100   0.000315472   0.000117376  0.372064
0.01   10  True  1000  1000  0.000905493   0.000123263  0.136128
0.01   10  False 100   100   0.000221529   9.82571e-05  0.44354
0.01   10  False 100   1000  0.000330552   0.000112615  0.340687
0.01   10  False 1000  100   0.000341277   0.000114097  0.334324
0.01   10  False 1000  1000  0.000819944   0.000120982  0.147549
0.01   25  True  100   100   0.000207806   0.000105977  0.509981
0.01   25  True  100   1000  0.000322879   0.00012921   0.400181
0.01   25  True  1000  100   0.00038262    0.00014158   0.370035
0.01   25  True  1000  1000  0.000865438   0.000202083  0.233504
0.01   25  False 100   100   0.000209401   0.000104696  0.499979
0.01   25  False 100   1000  0.000321161   0.000130737  0.407076
0.01   25  False 1000  100   0.000377012   0.000136801  0.362856
0.01   25  False 1000  1000  0.000861125   0.00020272   0.235413
0.2    1   True  100   100   0.000206952   9.69219e-05  0.46833
0.2    1   True  100   1000  0.000348674   0.000147475  0.422959
0.2    1   True  1000  100   0.000336908   0.00010122   0.300439
0.2    1   True  1000  1000  0.001022      0.000203274  0.198898
0.2    1   False 100   100   0.000207532   9.5412e-05   0.459746
0.2    1   False 100   1000  0.000356127   0.000146824  0.41228
0.2    1   False 1000  100   0.000322664   0.000100918  0.312764
0.2    1   False 1000  1000  0.000998987   0.000203442  0.203648
0.2    10  True  100   100   0.000211692   0.000109903  0.519165
0.2    10  True  100   1000  0.000372819   0.000164321  0.440753
0.2    10  True  1000  100   0.000338651   0.000144806  0.427596
0.2    10  True  1000  1000  0.00108312    0.000758876  0.70064
0.2    10  False 100   100   0.000215727   0.000110502  0.512231
0.2    10  False 100   1000  0.000375419   0.0001613    0.429653
0.2    10  False 1000  100   0.000336999   0.000145628  0.432132
0.2    10  False 1000  1000  0.00110502    0.000762043  0.689618
0.2    25  True  100   100   0.000218705   0.000129913  0.594009
0.2    25  True  100   1000  0.000394794   0.00029428   0.745402
0.2    25  True  1000  100   0.000404483   0.0002693    0.665788
0.2    25  True  1000  1000  0.0012002     0.00194494   1.62052
0.2    25  False 100   100   0.000221494   0.0001306    0.589632
0.2    25  False 100   1000  0.000396436   0.000297204  0.74969
0.2    25  False 1000  100   0.000409346   0.000270068  0.659754
0.2    25  False 1000  1000  0.00121051    0.00193737   1.60046
0.5    1   True  100   100   0.000214981   9.82111e-05  0.456836
0.5    1   True  100   1000  0.000415328   0.000223073  0.537101
0.5    1   True  1000  100   0.000358324   0.00011269   0.314492
0.5    1   True  1000  1000  0.00137612    0.000437401  0.317851
0.5    1   False 100   100   0.000224196   0.000101423  0.452386
0.5    1   False 100   1000  0.000400987   0.000223286  0.556841
0.5    1   False 1000  100   0.000368825   0.00011224   0.304318
0.5    1   False 1000  1000  0.00136036    0.000429369  0.31563
0.5    10  True  100   100   0.000222125   0.000112308  0.505608
0.5    10  True  100   1000  0.000461088   0.00032357   0.701753
0.5    10  True  1000  100   0.000394624   0.000225497  0.571422
0.5    10  True  1000  1000  0.00158027    0.00190898   1.20801
0.5    10  False 100   100   0.000232083   0.000114978  0.495418
0.5    10  False 100   1000  0.000454574   0.000324632  0.714146
0.5    10  False 1000  100   0.000379097   0.000227768  0.600817
0.5    10  False 1000  1000  0.00160292    0.00190168   1.18638
0.5    25  True  100   100   0.00023429    0.000151703  0.647501
0.5    25  True  100   1000  0.000497462   0.000598873  1.20386
0.5    25  True  1000  100   0.000460778   0.000557038  1.20891
0.5    25  True  1000  1000  0.00170036    0.00467336   2.74845
0.5    25  False 100   100   0.000228981   0.000155334  0.678371
0.5    25  False 100   1000  0.000496139   0.000620789  1.25124
0.5    25  False 1000  100   0.00045473    0.000551528  1.21287
0.5    25  False 1000  1000  0.00171793    0.00467152   2.71927
0.8    1   True  100   100   0.000222037   0.000105301  0.47425
0.8    1   True  100   1000  0.000410804   0.000329327  0.801664
0.8    1   True  1000  100   0.000349735   0.000131225  0.375212
0.8    1   True  1000  1000  0.00139219    0.000677065  0.48633
0.8    1   False 100   100   0.000214079   0.000107486  0.502085
0.8    1   False 100   1000  0.000413746   0.000323244  0.781261
0.8    1   False 1000  100   0.000348983   0.000131983  0.378193
0.8    1   False 1000  1000  0.00136296    0.000685325  0.50282
0.8    10  True  100   100   0.000229159   0.00011825   0.516017
0.8    10  True  100   1000  0.000498845   0.000532618  1.0677
0.8    10  True  1000  100   0.000383126   0.00029935   0.781336
0.8    10  True  1000  1000  0.00162866    0.00307312   1.88689
0.8    10  False 100   100   0.000230783   0.000124958  0.541452
0.8    10  False 100   1000  0.000493393   0.000550654  1.11606
0.8    10  False 1000  100   0.000377167   0.000298581  0.791642
0.8    10  False 1000  1000  0.00165795    0.00305103   1.84024
0.8    25  True  100   100   0.000233496   0.000175241  0.75051
0.8    25  True  100   1000  0.00055654    0.00102658   1.84458
0.8    25  True  1000  100   0.000463814   0.000783267  1.68875
0.8    25  True  1000  1000  0.00186905    0.00755344   4.04132
0.8    25  False 100   100   0.000240243   0.000175047  0.728625
0.8    25  False 100   1000  0.000578102   0.00104499   1.80763
0.8    25  False 1000  100   0.000485113   0.000776849  1.60138
0.8    25  False 1000  1000  0.00211448    0.00752736   3.55992

 


 Args
  sp_a   SparseTensor (or dense Matrix) A, of rank 2.  
  b   dense Matrix (or SparseTensor) B, with the same dtype as sp_a.  
  adjoint_a   Use the adjoint of A in the matrix multiply. If A is complex, this is transpose(conj(A)). Otherwise it's transpose(A).  
  adjoint_b   Use the adjoint of B in the matrix multiply. If B is complex, this is transpose(conj(B)). Otherwise it's transpose(B).  
  name   A name prefix for the returned tensors (optional)   
 


 Returns   A dense matrix (pseudo-code in dense np.matrix notation): A = A.H if adjoint_a else A B = B.H if adjoint_b else B return A*B
classmethod path_hook(*loader_details)  
A class method which returns a closure for use on sys.path_hooks. An instance of FileFinder is returned by the closure using the path argument given to the closure directly and loader_details indirectly. If the argument to the closure is not an existing directory, ImportError is raised.
read([n])  
Return a bytes containing up to n bytes starting from the current file position. If the argument is omitted, None or negative, return all bytes from the current file position to the end of the mapping. The file position is updated to point after the bytes that were returned.  Changed in version 3.3: Argument can be omitted or None.
read1([size])  
In BytesIO, this is the same as read().  Changed in version 3.7: The size argument is now optional.
tf.compat.v1.reduce_sum Computes the sum of elements across dimensions of a tensor. (deprecated arguments)  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.math.reduce_sum  
tf.compat.v1.reduce_sum(
    input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None,
    keep_dims=None
)
 Warning: SOME ARGUMENTS ARE DEPRECATED: (keep_dims). They will be removed in a future version. Instructions for updating: keep_dims is deprecated, use keepdims instead Reduces input_tensor along the dimensions given in axis. Unless keepdims is true, the rank of the tensor is reduced by 1 for each of the entries in axis, which must be unique. If keepdims is true, the reduced dimensions are retained with length 1. If axis is None, all dimensions are reduced, and a tensor with a single element is returned. For example: x = tf.constant([[1, 1, 1], [1, 1, 1]])
tf.reduce_sum(x)  # 6
tf.reduce_sum(x, 0)  # [2, 2, 2]
tf.reduce_sum(x, 1)  # [3, 3]
tf.reduce_sum(x, 1, keepdims=True)  # [[3], [3]]
tf.reduce_sum(x, [0, 1])  # 6

 


 Args
  input_tensor   The tensor to reduce. Should have numeric type.  
  axis   The dimensions to reduce. If None (the default), reduces all dimensions. Must be in the range [-rank(input_tensor), rank(input_tensor)).  
  keepdims   If true, retains reduced dimensions with length 1.  
  name   A name for the operation (optional).  
  reduction_indices   The old (deprecated) name for axis.  
  keep_dims   Deprecated alias for keepdims.   
 


 Returns   The reduced tensor, of the same dtype as the input_tensor.  
 Numpy Compatibility Equivalent to np.sum apart the fact that numpy upcast uint8 and int32 to int64 while tensorflow returns the same dtype as the input.
def f_13793321(df1, df2):
    """joining data from dataframe `df1` with data from dataframe `df2` based on matching values of column 'Date_Time' in both dataframes
    """
    return  
 --------------------

def f_3367288(str1):
    """use `%s` operator to print variable values `str1` inside a string
    """
    return  
 --------------------

def f_3475251():
    """Split a string '2.MATCHES $$TEXT$$ STRING' by a delimiter '$$TEXT$$'
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
turtle.back(distance)  
turtle.bk(distance)  
turtle.backward(distance)  
 Parameters 
distance – a number   Move the turtle backward by distance, opposite to the direction the turtle is headed. Do not change the turtle’s heading. >>> turtle.position()
(0.00,0.00)
>>> turtle.backward(30)
>>> turtle.position()
(-30.00,0.00)
turtle.back(distance)  
turtle.bk(distance)  
turtle.backward(distance)  
 Parameters 
distance – a number   Move the turtle backward by distance, opposite to the direction the turtle is headed. Do not change the turtle’s heading. >>> turtle.position()
(0.00,0.00)
>>> turtle.backward(30)
>>> turtle.position()
(-30.00,0.00)
turtle.back(distance)  
turtle.bk(distance)  
turtle.backward(distance)  
 Parameters 
distance – a number   Move the turtle backward by distance, opposite to the direction the turtle is headed. Do not change the turtle’s heading. >>> turtle.position()
(0.00,0.00)
>>> turtle.backward(30)
>>> turtle.position()
(-30.00,0.00)
SSLSocket.version()  
Return the actual SSL protocol version negotiated by the connection as a string, or None is no secure connection is established. As of this writing, possible return values include "SSLv2", "SSLv3", "TLSv1", "TLSv1.1" and "TLSv1.2". Recent OpenSSL versions may define more return values.  New in version 3.5.
urllib.request.pathname2url(path)  
Convert the pathname path from the local syntax for a path to the form used in the path component of a URL. This does not produce a complete URL. The return value will already be quoted using the quote() function.
def f_273192(directory):
    """check if directory `directory ` exists and create it if necessary
    """
     
 --------------------
Please refer to the following documentation to generate the code:
tf.io.gfile.makedirs     View source on GitHub    Creates a directory and all parent/intermediate directories.  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.io.gfile.makedirs  
tf.io.gfile.makedirs(
    path
)
 It succeeds if path already exists and is writable.
 


 Args
  path   string, name of the directory to be created   
 


 Raises
  errors.OpError   If the operation fails.
tf.io.gfile.mkdir     View source on GitHub    Creates a directory with the name given by path.  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.io.gfile.mkdir  
tf.io.gfile.mkdir(
    path
)

 


 Args
  path   string, name of the directory to be created    Notes: The parent directories need to exist. Use tf.io.gfile.makedirs instead if there is the possibility that the parent dirs don't exist.
 


 Raises
  errors.OpError   If the operation fails.
class asyncio.DatagramTransport(BaseTransport)  
A transport for datagram (UDP) connections. Instances of the DatagramTransport class are returned from the loop.create_datagram_endpoint() event loop method.
os.path.normcase(path)  
Normalize the case of a pathname. On Windows, convert all characters in the pathname to lowercase, and also convert forward slashes to backward slashes. On other operating systems, return the path unchanged.  Changed in version 3.6: Accepts a path-like object.
curses.is_term_resized(nlines, ncols)  
Return True if resize_term() would modify the window structure, False otherwise.
def f_273192(path):
    """check if a directory `path` exists and create it if necessary
    """
     
 --------------------
Please refer to the following documentation to generate the code:
tf.io.gfile.makedirs     View source on GitHub    Creates a directory and all parent/intermediate directories.  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.io.gfile.makedirs  
tf.io.gfile.makedirs(
    path
)
 It succeeds if path already exists and is writable.
 


 Args
  path   string, name of the directory to be created   
 


 Raises
  errors.OpError   If the operation fails.
tf.io.gfile.mkdir     View source on GitHub    Creates a directory with the name given by path.  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.io.gfile.mkdir  
tf.io.gfile.mkdir(
    path
)

 


 Args
  path   string, name of the directory to be created    Notes: The parent directories need to exist. Use tf.io.gfile.makedirs instead if there is the possibility that the parent dirs don't exist.
 


 Raises
  errors.OpError   If the operation fails.
class asyncio.DatagramTransport(BaseTransport)  
A transport for datagram (UDP) connections. Instances of the DatagramTransport class are returned from the loop.create_datagram_endpoint() event loop method.
os.path.normcase(path)  
Normalize the case of a pathname. On Windows, convert all characters in the pathname to lowercase, and also convert forward slashes to backward slashes. On other operating systems, return the path unchanged.  Changed in version 3.6: Accepts a path-like object.
curses.is_term_resized(nlines, ncols)  
Return True if resize_term() would modify the window structure, False otherwise.
def f_273192(path):
    """check if a directory `path` exists and create it if necessary
    """
     
 --------------------
Please refer to the following documentation to generate the code:
stringprep.map_table_b3(code)  
Return the mapped value for code according to tableB.3 (Mapping for case-folding used with no normalization).
class email.policy.Compat32(**kw)  
This concrete Policy is the backward compatibility policy. It replicates the behavior of the email package in Python 3.2. The policy module also defines an instance of this class, compat32, that is used as the default policy. Thus the default behavior of the email package is to maintain compatibility with Python 3.2. The following attributes have values that are different from the Policy default:  
mangle_from_  
The default is True. 
 The class provides the following concrete implementations of the abstract methods of Policy:  
header_source_parse(sourcelines)  
The name is parsed as everything up to the ‘:’ and returned unmodified. The value is determined by stripping leading whitespace off the remainder of the first line, joining all subsequent lines together, and stripping any trailing carriage return or linefeed characters. 
  
header_store_parse(name, value)  
The name and value are returned unmodified. 
  
header_fetch_parse(name, value)  
If the value contains binary data, it is converted into a Header object using the unknown-8bit charset. Otherwise it is returned unmodified. 
  
fold(name, value)  
Headers are folded using the Header folding algorithm, which preserves existing line breaks in the value, and wraps each resulting line to the max_line_length. Non-ASCII binary data are CTE encoded using the unknown-8bit charset. 
  
fold_binary(name, value)  
Headers are folded using the Header folding algorithm, which preserves existing line breaks in the value, and wraps each resulting line to the max_line_length. If cte_type is 7bit, non-ascii binary data is CTE encoded using the unknown-8bit charset. Otherwise the original source header is used, with its existing line breaks and any (RFC invalid) binary data it may contain.
CookiePolicy.hide_cookie2  
Don’t add Cookie2 header to requests (the presence of this header indicates to the server that we understand RFC 2965 cookies).
mpl_toolkits.axes_grid1.axes_divider.HBoxDivider   classmpl_toolkits.axes_grid1.axes_divider.HBoxDivider(fig, *args, horizontal=None, vertical=None, aspect=None, anchor='C')[source]
 
Bases: mpl_toolkits.axes_grid1.axes_divider.SubplotDivider A SubplotDivider for laying out axes horizontally, while ensuring that they have equal heights. Examples (Source code, png, pdf)     Parameters 
 
figmatplotlib.figure.Figure


*argstuple (nrows, ncols, index) or int


The array of subplots in the figure has dimensions (nrows,
ncols), and index is the index of the subplot being created. index starts at 1 in the upper left corner and increases to the right. If nrows, ncols, and index are all single digit numbers, then args can be passed as a single 3-digit number (e.g. 234 for (2, 3, 4)).       locate(nx, ny, nx1=None, ny1=None, axes=None, renderer=None)[source]
 
 Parameters 
 
nx, nx1int


Integers specifying the column-position of the cell. When nx1 is None, a single nx-th column is specified. Otherwise location of columns spanning between nx to nx1 (but excluding nx1-th column) is specified.  
ny, ny1int


Same as nx and nx1, but for row positions.  axes
renderer
   
   new_locator(nx, nx1=None)[source]
 
Create a new AxesLocator for the specified cell.  Parameters 
 
nx, nx1int


Integers specifying the column-position of the cell. When nx1 is None, a single nx-th column is specified. Otherwise location of columns spanning between nx to nx1 (but excluding nx1-th column) is specified.     
 
  Examples using mpl_toolkits.axes_grid1.axes_divider.HBoxDivider
 
   HBoxDivider demo
email.policy.compat32  
An instance of Compat32, providing backward compatibility with the behavior of the email package in Python 3.2.
def f_18785032(text):
    """Replace a separate word 'H3' by 'H1' in a string 'text'
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
encodings.idna.ToASCII(label)  
Convert a label to ASCII, as specified in RFC 3490. UseSTD3ASCIIRules is assumed to be false.
string.ascii_letters  
The concatenation of the ascii_lowercase and ascii_uppercase constants described below. This value is not locale-dependent.
ascii(object)  
As repr(), return a string containing a printable representation of an object, but escape the non-ASCII characters in the string returned by repr() using \x, \u or \U escapes. This generates a string similar to that returned by repr() in Python 2.
re.A  
re.ASCII  
Make \w, \W, \b, \B, \d, \D, \s and \S perform ASCII-only matching instead of full Unicode matching. This is only meaningful for Unicode patterns, and is ignored for byte patterns. Corresponds to the inline flag (?a). Note that for backward compatibility, the re.U flag still exists (as well as its synonym re.UNICODE and its embedded counterpart (?u)), but these are redundant in Python 3 since matches are Unicode by default for strings (and Unicode matching isn’t allowed for bytes).
string.ascii_lowercase  
The lowercase letters 'abcdefghijklmnopqrstuvwxyz'. This value is not locale-dependent and will not change.
def f_1450897():
    """substitute ASCII letters in string 'aas30dsa20' with empty string ''
    """
    return  
 --------------------

def f_1450897():
    """get digits only from a string `aas30dsa20` using lambda function
    """
    return  
 --------------------

def f_14435268(soup):
    """access a tag called "name" in beautifulsoup `soup`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
concat_matrix=b'cm'[source]
pandas.api.extensions.ExtensionArray._concat_same_type   classmethodExtensionArray._concat_same_type(to_concat)[source]
 
Concatenate multiple array of this dtype.  Parameters 
 
to_concat:sequence of this type

  Returns 
 ExtensionArray
pandas.DataFrame.join   DataFrame.join(other, on=None, how='left', lsuffix='', rsuffix='', sort=False)[source]
 
Join columns of another DataFrame. Join columns with other DataFrame either on index or on a key column. Efficiently join multiple DataFrame objects by index at once by passing a list.  Parameters 
 
other:DataFrame, Series, or list of DataFrame


Index should be similar to one of the columns in this one. If a Series is passed, its name attribute must be set, and that will be used as the column name in the resulting joined DataFrame.  
on:str, list of str, or array-like, optional


Column or index level name(s) in the caller to join on the index in other, otherwise joins index-on-index. If multiple values given, the other DataFrame must have a MultiIndex. Can pass an array as the join key if it is not already contained in the calling DataFrame. Like an Excel VLOOKUP operation.  
how:{‘left’, ‘right’, ‘outer’, ‘inner’}, default ‘left’


How to handle the operation of the two objects.  left: use calling frame’s index (or column if on is specified) right: use other’s index. outer: form union of calling frame’s index (or column if on is specified) with other’s index, and sort it. lexicographically. inner: form intersection of calling frame’s index (or column if on is specified) with other’s index, preserving the order of the calling’s one. 
cross: creates the cartesian product from both frames, preserves the order of the left keys.  New in version 1.2.0.     
lsuffix:str, default ‘’


Suffix to use from left frame’s overlapping columns.  
rsuffix:str, default ‘’


Suffix to use from right frame’s overlapping columns.  
sort:bool, default False


Order result DataFrame lexicographically by the join key. If False, the order of the join key depends on the join type (how keyword).    Returns 
 DataFrame

A dataframe containing columns from both the caller and other.      See also  DataFrame.merge

For column(s)-on-column(s) operations.    Notes Parameters on, lsuffix, and rsuffix are not supported when passing a list of DataFrame objects. Support for specifying index levels as the on parameter was added in version 0.23.0. Examples 
>>> df = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3', 'K4', 'K5'],
...                    'A': ['A0', 'A1', 'A2', 'A3', 'A4', 'A5']})
  
>>> df
  key   A
0  K0  A0
1  K1  A1
2  K2  A2
3  K3  A3
4  K4  A4
5  K5  A5
  
>>> other = pd.DataFrame({'key': ['K0', 'K1', 'K2'],
...                       'B': ['B0', 'B1', 'B2']})
  
>>> other
  key   B
0  K0  B0
1  K1  B1
2  K2  B2
  Join DataFrames using their indexes. 
>>> df.join(other, lsuffix='_caller', rsuffix='_other')
  key_caller   A key_other    B
0         K0  A0        K0   B0
1         K1  A1        K1   B1
2         K2  A2        K2   B2
3         K3  A3       NaN  NaN
4         K4  A4       NaN  NaN
5         K5  A5       NaN  NaN
  If we want to join using the key columns, we need to set key to be the index in both df and other. The joined DataFrame will have key as its index. 
>>> df.set_index('key').join(other.set_index('key'))
      A    B
key
K0   A0   B0
K1   A1   B1
K2   A2   B2
K3   A3  NaN
K4   A4  NaN
K5   A5  NaN
  Another option to join using the key columns is to use the on parameter. DataFrame.join always uses other’s index but we can use any column in df. This method preserves the original DataFrame’s index in the result. 
>>> df.join(other.set_index('key'), on='key')
  key   A    B
0  K0  A0   B0
1  K1  A1   B1
2  K2  A2   B2
3  K3  A3  NaN
4  K4  A4  NaN
5  K5  A5  NaN
  Using non-unique key values shows how they are matched. 
>>> df = pd.DataFrame({'key': ['K0', 'K1', 'K1', 'K3', 'K0', 'K1'],
...                    'A': ['A0', 'A1', 'A2', 'A3', 'A4', 'A5']})
  
>>> df
  key   A
0  K0  A0
1  K1  A1
2  K1  A2
3  K3  A3
4  K0  A4
5  K1  A5
  
>>> df.join(other.set_index('key'), on='key')
  key   A    B
0  K0  A0   B0
1  K1  A1   B1
2  K1  A2   B1
3  K3  A3  NaN
4  K0  A4   B0
5  K1  A5   B1
operator.concat(a, b)  
operator.__concat__(a, b)  
Return a + b for a and b sequences.
classBarAB(widthA=1.0, angleA=0, widthB=1.0, angleB=0)[source]
 
Bases: matplotlib.patches.ArrowStyle._Curve An arrow with vertical bars | at both ends.  Parameters 
 
widthA, widthBfloat, default: 1.0


Width of the bracket.  
angleA, angleBfloat, default: 0 degrees


Orientation of the bracket, as a counterclockwise angle. 0 degrees means perpendicular to the line.       arrow='|-|'
def f_20180210(A, B):
    """Create new matrix object  by concatenating data from matrix A and matrix B
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
numpy.c_   numpy.c_ = <numpy.lib.index_tricks.CClass object>
 
Translates slice objects to concatenation along the second axis. This is short-hand for np.r_['-1,2,0', index expression], which is useful because of its common occurrence. In particular, arrays will be stacked along their last axis after being upgraded to at least 2-D with 1’s post-pended to the shape (column vectors made out of 1-D arrays).  See also  column_stack

Stack 1-D arrays as columns into a 2-D array.  r_

For more detailed documentation.    Examples >>> np.c_[np.array([1,2,3]), np.array([4,5,6])]
array([[1, 4],
       [2, 5],
       [3, 6]])
>>> np.c_[np.array([[1,2,3]]), 0, 0, np.array([[4,5,6]])]
array([[1, 2, 3, ..., 4, 5, 6]])
numpy.ma.append   ma.append(a, b, axis=None)[source]
 
Append values to the end of an array.  New in version 1.9.0.   Parameters 
 
aarray_like


Values are appended to a copy of this array.  
barray_like


These values are appended to a copy of a. It must be of the correct shape (the same shape as a, excluding axis). If axis is not specified, b can be any shape and will be flattened before use.  
axisint, optional


The axis along which v are appended. If axis is not given, both a and b are flattened before use.    Returns 
 
appendMaskedArray


A copy of a with b appended to axis. Note that append does not occur in-place: a new array is allocated and filled. If axis is None, the result is a flattened array.      See also  numpy.append

Equivalent function in the top-level NumPy module.    Examples >>> import numpy.ma as ma
>>> a = ma.masked_values([1, 2, 3], 2)
>>> b = ma.masked_values([[4, 5, 6], [7, 8, 9]], 7)
>>> ma.append(a, b)
masked_array(data=[1, --, 3, 4, 5, 6, --, 8, 9],
             mask=[False,  True, False, False, False, False,  True, False,
                   False],
       fill_value=999999)
numpy.concatenate   numpy.concatenate((a1, a2, ...), axis=0, out=None, dtype=None, casting="same_kind")
 
Join a sequence of arrays along an existing axis.  Parameters 
 
a1, a2, …sequence of array_like


The arrays must have the same shape, except in the dimension corresponding to axis (the first, by default).  
axisint, optional


The axis along which the arrays will be joined. If axis is None, arrays are flattened before use. Default is 0.  
outndarray, optional


If provided, the destination to place the result. The shape must be correct, matching that of what concatenate would have returned if no out argument were specified.  
dtypestr or dtype


If provided, the destination array will have this dtype. Cannot be provided together with out.  New in version 1.20.0.   
casting{‘no’, ‘equiv’, ‘safe’, ‘same_kind’, ‘unsafe’}, optional


Controls what kind of data casting may occur. Defaults to ‘same_kind’.  New in version 1.20.0.     Returns 
 
resndarray


The concatenated array.      See also  ma.concatenate

Concatenate function that preserves input masks.  array_split

Split an array into multiple sub-arrays of equal or near-equal size.  split

Split array into a list of multiple sub-arrays of equal size.  hsplit

Split array into multiple sub-arrays horizontally (column wise).  vsplit

Split array into multiple sub-arrays vertically (row wise).  dsplit

Split array into multiple sub-arrays along the 3rd axis (depth).  stack

Stack a sequence of arrays along a new axis.  block

Assemble arrays from blocks.  hstack

Stack arrays in sequence horizontally (column wise).  vstack

Stack arrays in sequence vertically (row wise).  dstack

Stack arrays in sequence depth wise (along third dimension).  column_stack

Stack 1-D arrays as columns into a 2-D array.    Notes When one or more of the arrays to be concatenated is a MaskedArray, this function will return a MaskedArray object instead of an ndarray, but the input masks are not preserved. In cases where a MaskedArray is expected as input, use the ma.concatenate function from the masked array module instead. Examples >>> a = np.array([[1, 2], [3, 4]])
>>> b = np.array([[5, 6]])
>>> np.concatenate((a, b), axis=0)
array([[1, 2],
       [3, 4],
       [5, 6]])
>>> np.concatenate((a, b.T), axis=1)
array([[1, 2, 5],
       [3, 4, 6]])
>>> np.concatenate((a, b), axis=None)
array([1, 2, 3, 4, 5, 6])
 This function will not preserve masking of MaskedArray inputs. >>> a = np.ma.arange(3)
>>> a[1] = np.ma.masked
>>> b = np.arange(2, 5)
>>> a
masked_array(data=[0, --, 2],
             mask=[False,  True, False],
       fill_value=999999)
>>> b
array([2, 3, 4])
>>> np.concatenate([a, b])
masked_array(data=[0, 1, 2, 2, 3, 4],
             mask=False,
       fill_value=999999)
>>> np.ma.concatenate([a, b])
masked_array(data=[0, --, 2, 2, 3, 4],
             mask=[False,  True, False, False, False, False],
       fill_value=999999)
numpy.ufunc.outer method   ufunc.outer(A, B, /, **kwargs)
 
Apply the ufunc op to all pairs (a, b) with a in A and b in B. Let M = A.ndim, N = B.ndim. Then the result, C, of op.outer(A, B) is an array of dimension M + N such that:  \[C[i_0, ..., i_{M-1}, j_0, ..., j_{N-1}] = op(A[i_0, ..., i_{M-1}], B[j_0, ..., j_{N-1}])\] For A and B one-dimensional, this is equivalent to: r = empty(len(A),len(B))
for i in range(len(A)):
    for j in range(len(B)):
        r[i,j] = op(A[i], B[j])  # op = ufunc in question
  Parameters 
 
Aarray_like


First array  
Barray_like


Second array  
kwargsany


Arguments to pass on to the ufunc. Typically dtype or out. See ufunc for a comprehensive overview of all available arguments.    Returns 
 
rndarray


Output array      See also  numpy.outer

A less powerful version of np.multiply.outer that ravels all inputs to 1D. This exists primarily for compatibility with old code.  tensordot

np.tensordot(a, b, axes=((), ())) and np.multiply.outer(a, b) behave same for all dimensions of a and b.    Examples >>> np.multiply.outer([1, 2, 3], [4, 5, 6])
array([[ 4,  5,  6],
       [ 8, 10, 12],
       [12, 15, 18]])
 A multi-dimensional example: >>> A = np.array([[1, 2, 3], [4, 5, 6]])
>>> A.shape
(2, 3)
>>> B = np.array([[1, 2, 3, 4]])
>>> B.shape
(1, 4)
>>> C = np.multiply.outer(A, B)
>>> C.shape; C
(2, 3, 1, 4)
array([[[[ 1,  2,  3,  4]],
        [[ 2,  4,  6,  8]],
        [[ 3,  6,  9, 12]]],
       [[[ 4,  8, 12, 16]],
        [[ 5, 10, 15, 20]],
        [[ 6, 12, 18, 24]]]])
numpy.ma.concatenate   ma.concatenate(arrays, axis=0)[source]
 
Concatenate a sequence of arrays along the given axis.  Parameters 
 
arrayssequence of array_like


The arrays must have the same shape, except in the dimension corresponding to axis (the first, by default).  
axisint, optional


The axis along which the arrays will be joined. Default is 0.    Returns 
 
resultMaskedArray


The concatenated array with any masked entries preserved.      See also  numpy.concatenate

Equivalent function in the top-level NumPy module.    Examples >>> import numpy.ma as ma
>>> a = ma.arange(3)
>>> a[1] = ma.masked
>>> b = ma.arange(2, 5)
>>> a
masked_array(data=[0, --, 2],
             mask=[False,  True, False],
       fill_value=999999)
>>> b
masked_array(data=[2, 3, 4],
             mask=False,
       fill_value=999999)
>>> ma.concatenate([a, b])
masked_array(data=[0, --, 2, 2, 3, 4],
             mask=[False,  True, False, False, False, False],
       fill_value=999999)
def f_20180210(A, B):
    """concat two matrices `A` and `B` in numpy
    """
    return  
 --------------------
  1%|          | 1/100 [00:01<02:05,  1.27s/it]  2%|▏         | 2/100 [00:02<01:34,  1.03it/s]  3%|▎         | 3/100 [00:02<01:07,  1.45it/s]  4%|▍         | 4/100 [00:03<01:15,  1.28it/s]  5%|▌         | 5/100 [00:04<01:20,  1.18it/s]  6%|▌         | 6/100 [00:05<01:27,  1.07it/s]  7%|▋         | 7/100 [00:06<01:29,  1.03it/s]  8%|▊         | 8/100 [00:06<01:11,  1.30it/s]  9%|▉         | 9/100 [00:08<01:23,  1.09it/s] 10%|█         | 10/100 [00:10<02:18,  1.53s/it] 11%|█         | 11/100 [00:18<04:53,  3.30s/it] 12%|█▏        | 12/100 [00:18<03:33,  2.42s/it] 13%|█▎        | 13/100 [00:19<02:38,  1.83s/it] 14%|█▍        | 14/100 [00:19<01:59,  1.38s/it] 15%|█▌        | 15/100 [04:25<1:46:37, 75.27s/it] 16%|█▌        | 16/100 [04:26<1:13:50, 52.75s/it] 17%|█▋        | 17/100 [04:26<51:06, 36.95s/it]   18%|█▊        | 18/100 [04:27<35:33, 26.02s/it] 19%|█▉        | 19/100 [04:28<25:15, 18.71s/it] 20%|██        | 20/100 [04:29<17:35, 13.19s/it] 21%|██        | 21/100 [04:29<12:26,  9.45s/it] 22%|██▏       | 22/100 [04:30<08:53,  6.84s/it] 24%|██▍       | 24/100 [04:31<04:45,  3.76s/it] 25%|██▌       | 25/100 [04:32<04:01,  3.22s/it] 27%|██▋       | 27/100 [04:34<02:51,  2.36s/it] 28%|██▊       | 28/100 [04:35<02:21,  1.96s/it] 29%|██▉       | 29/100 [04:35<01:48,  1.53s/it] 30%|███       | 30/100 [04:37<01:58,  1.69s/it] 31%|███       | 31/100 [04:38<01:29,  1.29s/it] 32%|███▏      | 32/100 [04:38<01:06,  1.03it/s] 33%|███▎      | 33/100 [04:41<01:52,  1.68s/it] 34%|███▍      | 34/100 [04:41<01:22,  1.26s/it] 35%|███▌      | 35/100 [04:43<01:28,  1.37s/it] 36%|███▌      | 36/100 [04:44<01:12,  1.13s/it] 37%|███▋      | 37/100 [04:44<00:58,  1.07it/s] 38%|███▊      | 38/100 [04:45<00:55,  1.11it/s] 39%|███▉      | 39/100 [04:45<00:42,  1.43it/s] 40%|████      | 40/100 [04:46<00:42,  1.41it/s] 41%|████      | 41/100 [04:47<00:41,  1.42it/s] 42%|████▏     | 42/100 [04:47<00:32,  1.80it/s] 43%|████▎     | 43/100 [04:47<00:27,  2.07it/s] 44%|████▍     | 44/100 [04:48<00:26,  2.12it/s] 45%|████▌     | 45/100 [04:48<00:21,  2.55it/s] 46%|████▌     | 46/100 [04:48<00:19,  2.83it/s] 47%|████▋     | 47/100 [04:49<00:27,  1.90it/s] 48%|████▊     | 48/100 [04:53<01:15,  1.45s/it] 49%|████▉     | 49/100 [04:53<00:59,  1.17s/it] 50%|█████     | 50/100 [04:53<00:45,  1.11it/s] 51%|█████     | 51/100 [04:54<00:35,  1.38it/s] 52%|█████▏    | 52/100 [04:54<00:30,  1.56it/s] 53%|█████▎    | 53/100 [04:54<00:24,  1.95it/s] 54%|█████▍    | 54/100 [04:54<00:19,  2.37it/s] 55%|█████▌    | 55/100 [04:57<00:42,  1.06it/s] 56%|█████▌    | 56/100 [04:57<00:35,  1.24it/s] 57%|█████▋    | 57/100 [04:57<00:26,  1.65it/s] 58%|█████▊    | 58/100 [04:57<00:20,  2.06it/s] 59%|█████▉    | 59/100 [04:59<00:36,  1.14it/s] 60%|██████    | 60/100 [04:59<00:27,  1.48it/s] 61%|██████    | 61/100 [05:00<00:20,  1.86it/s] 62%|██████▏   | 62/100 [05:00<00:19,  1.94it/s] 63%|██████▎   | 63/100 [06:45<19:36, 31.79s/it] 64%|██████▍   | 64/100 [06:45<13:23, 22.33s/it] 65%|██████▌   | 65/100 [06:46<09:12, 15.78s/it] 66%|██████▌   | 66/100 [06:46<06:16, 11.08s/it] 67%|██████▋   | 67/100 [06:46<04:21,  7.92s/it] 69%|██████▉   | 69/100 [06:48<02:22,  4.59s/it] 70%|███████   | 70/100 [06:48<01:46,  3.55s/it] 71%|███████   | 71/100 [06:48<01:17,  2.67s/it] 72%|███████▏  | 72/100 [06:49<00:57,  2.05s/it] 73%|███████▎  | 73/100 [06:49<00:44,  1.66s/it] 74%|███████▍  | 74/100 [06:50<00:35,  1.36s/it] 75%|███████▌  | 75/100 [06:51<00:28,  1.14s/it] 76%|███████▌  | 76/100 [06:51<00:23,  1.02it/s] 77%|███████▋  | 77/100 [06:51<00:16,  1.36it/s] 78%|███████▊  | 78/100 [06:52<00:12,  1.72it/s] 79%|███████▉  | 79/100 [06:52<00:09,  2.27it/s] 81%|████████  | 81/100 [06:52<00:08,  2.36it/s] 82%|████████▏ | 82/100 [06:53<00:08,  2.14it/s] 83%|████████▎ | 83/100 [06:54<00:08,  2.00it/s] 84%|████████▍ | 84/100 [06:55<00:09,  1.66it/s] 85%|████████▌ | 85/100 [06:57<00:15,  1.02s/it] 86%|████████▌ | 86/100 [06:57<00:10,  1.29it/s] 88%|████████▊ | 88/100 [06:57<00:07,  1.69it/s] 89%|████████▉ | 89/100 [06:59<00:08,  1.31it/s] 90%|█████████ | 90/100 [06:59<00:06,  1.47it/s] 91%|█████████ | 91/100 [07:00<00:05,  1.66it/s] 92%|█████████▏| 92/100 [07:00<00:05,  1.53it/s] 93%|█████████▎| 93/100 [07:01<00:04,  1.59it/s] 94%|█████████▍| 94/100 [07:02<00:03,  1.50it/s] 95%|█████████▌| 95/100 [09:11<03:10, 38.06s/it] 96%|█████████▌| 96/100 [09:11<01:48, 27.07s/it] 97%|█████████▋| 97/100 [09:12<00:57, 19.21s/it] 98%|█████████▊| 98/100 [09:12<00:27, 13.55s/it] 99%|█████████▉| 99/100 [09:12<00:09,  9.62s/it]100%|██████████| 100/100 [09:13<00:00,  6.94s/it]100%|██████████| 100/100 [09:13<00:00,  5.53s/it]
Evaluating generations...
Process Process-52:
Traceback (most recent call last):
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 58, in unsafe_execute
    with create_tempdir():
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 113, in create_tempdir
    with tempfile.TemporaryDirectory() as dirname:
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 878, in __exit__
    self.cleanup()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 882, in cleanup
    self._rmtree(self.name, ignore_errors=self._ignore_cleanup_errors)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 864, in _rmtree
    _shutil.rmtree(name, onerror=onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 725, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 679, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
TypeError: 'NoneType' object is not callable
Process Process-115:
Traceback (most recent call last):
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 58, in unsafe_execute
    with create_tempdir():
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 113, in create_tempdir
    with tempfile.TemporaryDirectory() as dirname:
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 878, in __exit__
    self.cleanup()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 882, in cleanup
    self._rmtree(self.name, ignore_errors=self._ignore_cleanup_errors)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 864, in _rmtree
    _shutil.rmtree(name, onerror=onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 725, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 679, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
TypeError: 'NoneType' object is not callable
Process Process-146:
Traceback (most recent call last):
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 58, in unsafe_execute
    with create_tempdir():
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 113, in create_tempdir
    with tempfile.TemporaryDirectory() as dirname:
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 878, in __exit__
    self.cleanup()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 882, in cleanup
    self._rmtree(self.name, ignore_errors=self._ignore_cleanup_errors)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 864, in _rmtree
    _shutil.rmtree(name, onerror=onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 725, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 679, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
TypeError: 'NoneType' object is not callable
Process Process-164:
Traceback (most recent call last):
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 58, in unsafe_execute
    with create_tempdir():
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 113, in create_tempdir
    with tempfile.TemporaryDirectory() as dirname:
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 878, in __exit__
    self.cleanup()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 882, in cleanup
    self._rmtree(self.name, ignore_errors=self._ignore_cleanup_errors)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 864, in _rmtree
    _shutil.rmtree(name, onerror=onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 725, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 679, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
TypeError: 'NoneType' object is not callable
{
  "odex-en": {
    "pass@1": 0.26
  },
  "config": {
    "prefix": "",
    "do_sample": true,
    "temperature": 0.2,
    "top_k": 0,
    "top_p": 0.95,
    "n_samples": 1,
    "eos": "<|endoftext|>",
    "ignore_eos": false,
    "seed": 0,
    "remove_linebreak": false,
    "model_backend": "hf",
    "model": "unsloth/Qwen2.5-Coder-7B-Instruct",
    "modeltype": "causal",
    "peft_model": null,
    "revision": null,
    "token": null,
    "trust_remote_code": false,
    "tasks": "odex-en",
    "instruction_tokens": null,
    "batch_size": 1,
    "max_length_input": 4096,
    "max_length_generation": 5120,
    "topk_docs": 5,
    "precision": "bf16",
    "load_in_8bit": false,
    "load_in_4bit": false,
    "left_padding": false,
    "limit": 100,
    "limit_start": 0,
    "save_every_k_tasks": -1,
    "postprocess": true,
    "allow_code_execution": true,
    "generation_only": false,
    "load_generations_path": null,
    "load_data_path": null,
    "metric_output_path": "evaluation_results.json",
    "save_generations": false,
    "load_generations_intermediate_paths": null,
    "save_generations_path": "generations.json",
    "save_references": false,
    "save_references_path": "references.json",
    "prompt": "prompt",
    "new_tokens_only": false,
    "max_memory_per_gpu": null,
    "check_references": false,
    "dataset_path": "json",
    "dataset_name": null,
    "data_files_test": "/home/avisingh/CodeRagBench/code-rag-bench-main/retrieval/odex_codesage_generation.json",
    "cache_dir": null,
    "model_cache_dir": null,
    "setup_repoeval": false,
    "repoeval_input_repo_dir": "../retrieval/output/repoeval/repositories/function_level",
    "repoeval_cache_dir": "scripts/repoeval",
    "data_files": {
      "test": "/home/avisingh/CodeRagBench/code-rag-bench-main/retrieval/odex_codesage_generation.json"
    },
    "tokenizer": "unsloth/Qwen2.5-Coder-7B-Instruct"
  }
}
