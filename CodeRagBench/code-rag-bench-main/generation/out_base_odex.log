nohup: ignoring input
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Selected Tasks: ['odex-en']
Loading model in bf16
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 67.47it/s]
Loading dataset ..
Loading dataset ..
number of problems for this task is 439
  0%|          | 0/439 [00:00<?, ?it/s]
def f_3283984():
    """decode a hex string '4a4b4c' to UTF-8.
    """
    return  
 --------------------

def f_3844801(myList):
    """check if all elements in list `myList` are identical
    """
    return  
 --------------------

def f_4302166():
    """format number of spaces between strings `Python`, `:` and `Very Good` to be `20`
    """
    return  
 --------------------

def f_7555335(d):
    """convert a string `d` from CP-1251 to UTF-8
    """
    return  
 --------------------

def f_2544710(kwargs):
    """get rid of None values in dictionary `kwargs`
    """
    return  
 --------------------

def f_2544710(kwargs):
    """get rid of None values in dictionary `kwargs`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
pwd.getpwall()  
Return a list of all available password database entries, in arbitrary order.
subprocess.getoutput(cmd)  
Return output (stdout and stderr) of executing cmd in a shell. Like getstatusoutput(), except the exit code is ignored and the return value is a string containing the command’s output. Example: >>> subprocess.getoutput('ls /bin/ls')
'/bin/ls'
 Availability: POSIX & Windows.  Changed in version 3.3.4: Windows support added
test.support.script_helper.kill_python(p)  
Run the given subprocess.Popen process until completion and return stdout.
classmatplotlib.dviread.PsFont(texname, psname, effects, encoding, filename)[source]
 
Bases: tuple Create new instance of PsFont(texname, psname, effects, encoding, filename)   effects
 
Alias for field number 2 
   encoding
 
Alias for field number 3 
   filename
 
Alias for field number 4 
   psname
 
Alias for field number 1 
   texname
 
Alias for field number 0
numpy.distutils.exec_command.filepath_from_subprocess_output   distutils.exec_command.filepath_from_subprocess_output(output)[source]
 
Convert bytes in the encoding used by a subprocess into a filesystem-appropriate str. Inherited from exec_command, and possibly incorrect.
def f_14971373():
    """capture final output of a chain of system commands `ps -ef | grep something | wc -l`
    """
    return  
 --------------------

def f_6726636():
    """concatenate a list of strings `['a', 'b', 'c']`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
skimage.segmentation.join_segmentations(s1, s2) [source]
 
Return the join of the two input segmentations. The join J of S1 and S2 is defined as the segmentation in which two voxels are in the same segment if and only if they are in the same segment in both S1 and S2.  Parameters 
 
s1, s2numpy arrays 

s1 and s2 are label fields of the same shape.    Returns 
 
jnumpy array 

The join segmentation of s1 and s2.     Examples >>> from skimage.segmentation import join_segmentations
>>> s1 = np.array([[0, 0, 1, 1],
...                [0, 2, 1, 1],
...                [2, 2, 2, 1]])
>>> s2 = np.array([[0, 1, 1, 0],
...                [0, 1, 1, 0],
...                [0, 1, 1, 1]])
>>> join_segmentations(s1, s2)
array([[0, 1, 3, 2],
       [0, 5, 3, 2],
       [4, 5, 5, 3]])
numpy.intersect1d   numpy.intersect1d(ar1, ar2, assume_unique=False, return_indices=False)[source]
 
Find the intersection of two arrays. Return the sorted, unique values that are in both of the input arrays.  Parameters 
 
ar1, ar2array_like


Input arrays. Will be flattened if not already 1D.  
assume_uniquebool


If True, the input arrays are both assumed to be unique, which can speed up the calculation. If True but ar1 or ar2 are not unique, incorrect results and out-of-bounds indices could result. Default is False.  
return_indicesbool


If True, the indices which correspond to the intersection of the two arrays are returned. The first instance of a value is used if there are multiple. Default is False.  New in version 1.15.0.     Returns 
 
intersect1dndarray


Sorted 1D array of common and unique elements.  
comm1ndarray


The indices of the first occurrences of the common values in ar1. Only provided if return_indices is True.  
comm2ndarray


The indices of the first occurrences of the common values in ar2. Only provided if return_indices is True.      See also  numpy.lib.arraysetops

Module with a number of other functions for performing set operations on arrays.    Examples >>> np.intersect1d([1, 3, 4, 3], [3, 1, 2, 1])
array([1, 3])
 To intersect more than two arrays, use functools.reduce: >>> from functools import reduce
>>> reduce(np.intersect1d, ([1, 3, 4, 3], [3, 1, 2, 1], [6, 3, 4, 2]))
array([3])
 To return the indices of the values common to the input arrays along with the intersected values: >>> x = np.array([1, 1, 2, 3, 4])
>>> y = np.array([2, 1, 4, 6])
>>> xy, x_ind, y_ind = np.intersect1d(x, y, return_indices=True)
>>> x_ind, y_ind
(array([0, 2, 4]), array([1, 0, 2]))
>>> xy, x[x_ind], y[y_ind]
(array([1, 2, 4]), array([1, 2, 4]), array([1, 2, 4]))
pandas.Series.combine_first   Series.combine_first(other)[source]
 
Update null elements with value in the same location in ‘other’. Combine two Series objects by filling null values in one Series with non-null values from the other Series. Result index will be the union of the two indexes.  Parameters 
 
other:Series


The value(s) to be used for filling null values.    Returns 
 Series

The result of combining the provided Series with the other object.      See also  Series.combine

Perform element-wise operation on two Series using a given function.    Examples 
>>> s1 = pd.Series([1, np.nan])
>>> s2 = pd.Series([3, 4, 5])
>>> s1.combine_first(s2)
0    1.0
1    4.0
2    5.0
dtype: float64
  Null values still persist if the location of that null value does not exist in other 
>>> s1 = pd.Series({'falcon': np.nan, 'eagle': 160.0})
>>> s2 = pd.Series({'eagle': 200.0, 'duck': 30.0})
>>> s1.combine_first(s2)
duck       30.0
eagle     160.0
falcon      NaN
dtype: float64
pandas.Series.combine   Series.combine(other, func, fill_value=None)[source]
 
Combine the Series with a Series or scalar according to func. Combine the Series and other using func to perform elementwise selection for combined Series. fill_value is assumed when value is missing at some index from one of the two objects being combined.  Parameters 
 
other:Series or scalar


The value(s) to be combined with the Series.  
func:function


Function that takes two scalars as inputs and returns an element.  
fill_value:scalar, optional


The value to assume when an index is missing from one Series or the other. The default specifies to use the appropriate NaN value for the underlying dtype of the Series.    Returns 
 Series

The result of combining the Series with the other object.      See also  Series.combine_first

Combine Series values, choosing the calling Series’ values first.    Examples Consider 2 Datasets s1 and s2 containing highest clocked speeds of different birds. 
>>> s1 = pd.Series({'falcon': 330.0, 'eagle': 160.0})
>>> s1
falcon    330.0
eagle     160.0
dtype: float64
>>> s2 = pd.Series({'falcon': 345.0, 'eagle': 200.0, 'duck': 30.0})
>>> s2
falcon    345.0
eagle     200.0
duck       30.0
dtype: float64
  Now, to combine the two datasets and view the highest speeds of the birds across the two datasets 
>>> s1.combine(s2, max)
duck        NaN
eagle     200.0
falcon    345.0
dtype: float64
  In the previous example, the resulting value for duck is missing, because the maximum of a NaN and a float is a NaN. So, in the example, we set fill_value=0, so the maximum value returned will be the value from some dataset. 
>>> s1.combine(s2, max, fill_value=0)
duck       30.0
eagle     200.0
falcon    345.0
dtype: float64
numpy.in1d   numpy.in1d(ar1, ar2, assume_unique=False, invert=False)[source]
 
Test whether each element of a 1-D array is also present in a second array. Returns a boolean array the same length as ar1 that is True where an element of ar1 is in ar2 and False otherwise. We recommend using isin instead of in1d for new code.  Parameters 
 
ar1(M,) array_like


Input array.  
ar2array_like


The values against which to test each value of ar1.  
assume_uniquebool, optional


If True, the input arrays are both assumed to be unique, which can speed up the calculation. Default is False.  
invertbool, optional


If True, the values in the returned array are inverted (that is, False where an element of ar1 is in ar2 and True otherwise). Default is False. np.in1d(a, b, invert=True) is equivalent to (but is faster than) np.invert(in1d(a, b)).  New in version 1.8.0.     Returns 
 
in1d(M,) ndarray, bool


The values ar1[in1d] are in ar2.      See also  isin

Version of this function that preserves the shape of ar1.  numpy.lib.arraysetops

Module with a number of other functions for performing set operations on arrays.    Notes in1d can be considered as an element-wise function version of the python keyword in, for 1-D sequences. in1d(a, b) is roughly equivalent to np.array([item in b for item in a]). However, this idea fails if ar2 is a set, or similar (non-sequence) container: As ar2 is converted to an array, in those cases asarray(ar2) is an object array rather than the expected array of contained values.  New in version 1.4.0.  Examples >>> test = np.array([0, 1, 2, 5, 0])
>>> states = [0, 2]
>>> mask = np.in1d(test, states)
>>> mask
array([ True, False,  True, False,  True])
>>> test[mask]
array([0, 2, 0])
>>> mask = np.in1d(test, states, invert=True)
>>> mask
array([False,  True, False,  True, False])
>>> test[mask]
array([1, 5])
def f_18079563(s1, s2):
    """find intersection data between series `s1` and series `s2`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
flush_headers()  
Finally send the headers to the output stream and flush the internal headers buffer.  New in version 3.3.
FileResponse.set_headers(open_file)  
This method is automatically called during the response initialization and set various headers (Content-Length, Content-Type, and Content-Disposition) depending on open_file.
end_headers()  
Adds a blank line (indicating the end of the HTTP headers in the response) to the headers buffer and calls flush_headers().  Changed in version 3.2: The buffered headers are written to the output stream.
HTTPConnection.endheaders(message_body=None, *, encode_chunked=False)  
Send a blank line to the server, signalling the end of the headers. The optional message_body argument can be used to pass a message body associated with the request. If encode_chunked is True, the result of each iteration of message_body will be chunk-encoded as specified in RFC 7230, Section 3.3.1. How the data is encoded is dependent on the type of message_body. If message_body implements the buffer interface the encoding will result in a single chunk. If message_body is a collections.abc.Iterable, each iteration of message_body will result in a chunk. If message_body is a file object, each call to .read() will result in a chunk. The method automatically signals the end of the chunk-encoded data immediately after message_body.  Note Due to the chunked encoding specification, empty chunks yielded by an iterator body will be ignored by the chunk-encoder. This is to avoid premature termination of the read of the request by the target server due to malformed encoding.   New in version 3.6: Chunked encoding support. The encode_chunked parameter was added.
do_HEAD()  
This method serves the 'HEAD' request type: it sends the headers it would send for the equivalent GET request. See the do_GET() method for a more complete explanation of the possible headers.
def f_8315209(client):
    """sending http headers to `client`
    """
     
 --------------------
Please refer to the following documentation to generate the code:
class When(condition=None, then=None, **lookups)
tty.setraw(fd, when=termios.TCSAFLUSH)  
Change the mode of the file descriptor fd to raw. If when is omitted, it defaults to termios.TCSAFLUSH, and is passed to termios.tcsetattr().
write_sys_ex() 
 writes a timestamped system-exclusive midi message. write_sys_ex(when, msg) -> None  Writes a timestamped system-exclusive midi message.     
Parameters:

 
msg (list[int] or str) -- midi message 
when -- timestamp in milliseconds      Example: midi_output.write_sys_ex(0, '\xF0\x7D\x10\x11\x12\x13\xF7')

# is equivalent to

midi_output.write_sys_ex(pygame.midi.time(),
                         [0xF0, 0x7D, 0x10, 0x11, 0x12, 0x13, 0xF7])
pandas.DataFrame.asof   DataFrame.asof(where, subset=None)[source]
 
Return the last row(s) without any NaNs before where. The last row (for each element in where, if list) without any NaN is taken. In case of a DataFrame, the last row without NaN considering only the subset of columns (if not None) If there is no good value, NaN is returned for a Series or a Series of NaN values for a DataFrame  Parameters 
 
where:date or array-like of dates


Date(s) before which the last row(s) are returned.  
subset:str or array-like of str, default None


For DataFrame, if not None, only use these columns to check for NaNs.    Returns 
 scalar, Series, or DataFrame

The return can be:  scalar : when self is a Series and where is a scalar Series: when self is a Series and where is an array-like, or when self is a DataFrame and where is a scalar DataFrame : when self is a DataFrame and where is an array-like  Return scalar, Series, or DataFrame.      See also  merge_asof

Perform an asof merge. Similar to left join.    Notes Dates are assumed to be sorted. Raises if this is not the case. Examples A Series and a scalar where. 
>>> s = pd.Series([1, 2, np.nan, 4], index=[10, 20, 30, 40])
>>> s
10    1.0
20    2.0
30    NaN
40    4.0
dtype: float64
  
>>> s.asof(20)
2.0
  For a sequence where, a Series is returned. The first value is NaN, because the first element of where is before the first index value. 
>>> s.asof([5, 20])
5     NaN
20    2.0
dtype: float64
  Missing values are not considered. The following is 2.0, not NaN, even though NaN is at the index location for 30. 
>>> s.asof(30)
2.0
  Take all columns into consideration 
>>> df = pd.DataFrame({'a': [10, 20, 30, 40, 50],
...                    'b': [None, None, None, None, 500]},
...                   index=pd.DatetimeIndex(['2018-02-27 09:01:00',
...                                           '2018-02-27 09:02:00',
...                                           '2018-02-27 09:03:00',
...                                           '2018-02-27 09:04:00',
...                                           '2018-02-27 09:05:00']))
>>> df.asof(pd.DatetimeIndex(['2018-02-27 09:03:30',
...                           '2018-02-27 09:04:30']))
                      a   b
2018-02-27 09:03:30 NaN NaN
2018-02-27 09:04:30 NaN NaN
  Take a single column into consideration 
>>> df.asof(pd.DatetimeIndex(['2018-02-27 09:03:30',
...                           '2018-02-27 09:04:30']),
...         subset=['a'])
                         a   b
2018-02-27 09:03:30   30.0 NaN
2018-02-27 09:04:30   40.0 NaN
pandas.Series.asof   Series.asof(where, subset=None)[source]
 
Return the last row(s) without any NaNs before where. The last row (for each element in where, if list) without any NaN is taken. In case of a DataFrame, the last row without NaN considering only the subset of columns (if not None) If there is no good value, NaN is returned for a Series or a Series of NaN values for a DataFrame  Parameters 
 
where:date or array-like of dates


Date(s) before which the last row(s) are returned.  
subset:str or array-like of str, default None


For DataFrame, if not None, only use these columns to check for NaNs.    Returns 
 scalar, Series, or DataFrame

The return can be:  scalar : when self is a Series and where is a scalar Series: when self is a Series and where is an array-like, or when self is a DataFrame and where is a scalar DataFrame : when self is a DataFrame and where is an array-like  Return scalar, Series, or DataFrame.      See also  merge_asof

Perform an asof merge. Similar to left join.    Notes Dates are assumed to be sorted. Raises if this is not the case. Examples A Series and a scalar where. 
>>> s = pd.Series([1, 2, np.nan, 4], index=[10, 20, 30, 40])
>>> s
10    1.0
20    2.0
30    NaN
40    4.0
dtype: float64
  
>>> s.asof(20)
2.0
  For a sequence where, a Series is returned. The first value is NaN, because the first element of where is before the first index value. 
>>> s.asof([5, 20])
5     NaN
20    2.0
dtype: float64
  Missing values are not considered. The following is 2.0, not NaN, even though NaN is at the index location for 30. 
>>> s.asof(30)
2.0
  Take all columns into consideration 
>>> df = pd.DataFrame({'a': [10, 20, 30, 40, 50],
...                    'b': [None, None, None, None, 500]},
...                   index=pd.DatetimeIndex(['2018-02-27 09:01:00',
...                                           '2018-02-27 09:02:00',
...                                           '2018-02-27 09:03:00',
...                                           '2018-02-27 09:04:00',
...                                           '2018-02-27 09:05:00']))
>>> df.asof(pd.DatetimeIndex(['2018-02-27 09:03:30',
...                           '2018-02-27 09:04:30']))
                      a   b
2018-02-27 09:03:30 NaN NaN
2018-02-27 09:04:30 NaN NaN
  Take a single column into consideration 
>>> df.asof(pd.DatetimeIndex(['2018-02-27 09:03:30',
...                           '2018-02-27 09:04:30']),
...         subset=['a'])
                         a   b
2018-02-27 09:03:30   30.0 NaN
2018-02-27 09:04:30   40.0 NaN
def f_26153795(when):
    """Format a datetime string `when` to extract date only
    """
    return  
 --------------------

def f_172439(inputString):
    """split a multi-line string `inputString` into separate strings
    """
    return  
 --------------------

def f_172439():
    """Split a multi-line string ` a \n b \r\n c ` by new line character `\n`
    """
    return  
 --------------------

def f_13954222(b):
    """concatenate elements of list `b` by a colon ":"
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
numpy.matrix.sum method   matrix.sum(axis=None, dtype=None, out=None)[source]
 
Returns the sum of the matrix elements, along the given axis. Refer to numpy.sum for full documentation.  See also  numpy.sum
  Notes This is the same as ndarray.sum, except that where an ndarray would be returned, a matrix object is returned instead. Examples >>> x = np.matrix([[1, 2], [4, 3]])
>>> x.sum()
10
>>> x.sum(axis=1)
matrix([[3],
        [7]])
>>> x.sum(axis=1, dtype='float')
matrix([[3.],
        [7.]])
>>> out = np.zeros((2, 1), dtype='float')
>>> x.sum(axis=1, dtype='float', out=np.asmatrix(out))
matrix([[3.],
        [7.]])
numpy.ndarray.sum method   ndarray.sum(axis=None, dtype=None, out=None, keepdims=False, initial=0, where=True)
 
Return the sum of the array elements over the given axis. Refer to numpy.sum for full documentation.  See also  numpy.sum

equivalent function
numpy.trace   numpy.trace(a, offset=0, axis1=0, axis2=1, dtype=None, out=None)[source]
 
Return the sum along diagonals of the array. If a is 2-D, the sum along its diagonal with the given offset is returned, i.e., the sum of elements a[i,i+offset] for all i. If a has more than two dimensions, then the axes specified by axis1 and axis2 are used to determine the 2-D sub-arrays whose traces are returned. The shape of the resulting array is the same as that of a with axis1 and axis2 removed.  Parameters 
 
aarray_like


Input array, from which the diagonals are taken.  
offsetint, optional


Offset of the diagonal from the main diagonal. Can be both positive and negative. Defaults to 0.  
axis1, axis2int, optional


Axes to be used as the first and second axis of the 2-D sub-arrays from which the diagonals should be taken. Defaults are the first two axes of a.  
dtypedtype, optional


Determines the data-type of the returned array and of the accumulator where the elements are summed. If dtype has the value None and a is of integer type of precision less than the default integer precision, then the default integer precision is used. Otherwise, the precision is the same as that of a.  
outndarray, optional


Array into which the output is placed. Its type is preserved and it must be of the right shape to hold the output.    Returns 
 
sum_along_diagonalsndarray


If a is 2-D, the sum along the diagonal is returned. If a has larger dimensions, then an array of sums along diagonals is returned.      See also  
diag, diagonal, diagflat

  Examples >>> np.trace(np.eye(3))
3.0
>>> a = np.arange(8).reshape((2,2,2))
>>> np.trace(a)
array([6, 8])
 >>> a = np.arange(24).reshape((2,2,2,3))
>>> np.trace(a).shape
(2, 3)
numpy.recarray.sum method   recarray.sum(axis=None, dtype=None, out=None, keepdims=False, initial=0, where=True)
 
Return the sum of the array elements over the given axis. Refer to numpy.sum for full documentation.  See also  numpy.sum

equivalent function
numpy.ma.sum   ma.sum(self, axis=None, dtype=None, out=None, keepdims=<no value>) = <numpy.ma.core._frommethod object>
 
Return the sum of the array elements over the given axis. Masked elements are set to 0 internally. Refer to numpy.sum for full documentation.  See also  numpy.ndarray.sum

corresponding function for ndarrays  numpy.sum

equivalent function    Examples >>> x = np.ma.array([[1,2,3],[4,5,6],[7,8,9]], mask=[0] + [1,0]*4)
>>> x
masked_array(
  data=[[1, --, 3],
        [--, 5, --],
        [7, --, 9]],
  mask=[[False,  True, False],
        [ True, False,  True],
        [False,  True, False]],
  fill_value=999999)
>>> x.sum()
25
>>> x.sum(axis=1)
masked_array(data=[4, 5, 16],
             mask=[False, False, False],
       fill_value=999999)
>>> x.sum(axis=0)
masked_array(data=[8, 5, 12],
             mask=[False, False, False],
       fill_value=999999)
>>> print(type(x.sum(axis=0, dtype=np.int64)[0]))
<class 'numpy.int64'>
def f_13567345(a):
    """Calculate sum over all rows of 2D numpy array `a`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
warnings.resetwarnings()  
Reset the warnings filter. This discards the effect of all previous calls to filterwarnings(), including that of the -W command line options and calls to simplefilter().
logging.captureWarnings(capture)  
This function is used to turn the capture of warnings by logging on and off. If capture is True, warnings issued by the warnings module will be redirected to the logging system. Specifically, a warning will be formatted using warnings.formatwarning() and the resulting string logged to a logger named 'py.warnings' with a severity of WARNING. If capture is False, the redirection of warnings to the logging system will stop, and warnings will be redirected to their original destinations (i.e. those in effect before captureWarnings(True) was called).
numpy.testing.suppress_warnings   class numpy.testing.suppress_warnings(forwarding_rule='always')[source]
 
Context manager and decorator doing much the same as warnings.catch_warnings. However, it also provides a filter mechanism to work around https://bugs.python.org/issue4180. This bug causes Python before 3.4 to not reliably show warnings again after they have been ignored once (even within catch_warnings). It means that no “ignore” filter can be used easily, since following tests might need to see the warning. Additionally it allows easier specificity for testing warnings and can be nested.  Parameters 
 
forwarding_rulestr, optional


One of “always”, “once”, “module”, or “location”. Analogous to the usual warnings module filter mode, it is useful to reduce noise mostly on the outmost level. Unsuppressed and unrecorded warnings will be forwarded based on this rule. Defaults to “always”. “location” is equivalent to the warnings “default”, match by exact location the warning warning originated from.     Notes Filters added inside the context manager will be discarded again when leaving it. Upon entering all filters defined outside a context will be applied automatically. When a recording filter is added, matching warnings are stored in the log attribute as well as in the list returned by record. If filters are added and the module keyword is given, the warning registry of this module will additionally be cleared when applying it, entering the context, or exiting it. This could cause warnings to appear a second time after leaving the context if they were configured to be printed once (default) and were already printed before the context was entered. Nesting this context manager will work as expected when the forwarding rule is “always” (default). Unfiltered and unrecorded warnings will be passed out and be matched by the outer level. On the outmost level they will be printed (or caught by another warnings context). The forwarding rule argument can modify this behaviour. Like catch_warnings this context manager is not threadsafe. Examples With a context manager: with np.testing.suppress_warnings() as sup:
    sup.filter(DeprecationWarning, "Some text")
    sup.filter(module=np.ma.core)
    log = sup.record(FutureWarning, "Does this occur?")
    command_giving_warnings()
    # The FutureWarning was given once, the filtered warnings were
    # ignored. All other warnings abide outside settings (may be
    # printed/error)
    assert_(len(log) == 1)
    assert_(len(sup.log) == 1)  # also stored in log attribute
 Or as a decorator: sup = np.testing.suppress_warnings()
sup.filter(module=np.ma.core)  # module must match exactly
@sup
def some_function():
    # do something which causes a warning in np.ma.core
    pass
 Methods  
__call__(func) Function decorator to apply certain suppressions to a whole function.  
filter([category, message, module]) Add a new suppressing filter or apply it if the state is entered.  
record([category, message, module]) Append a new recording filter or apply it if the state is entered.
sys.warnoptions  
This is an implementation detail of the warnings framework; do not modify this value. Refer to the warnings module for more information on the warnings framework.
class warnings.catch_warnings(*, record=False, module=None)  
A context manager that copies and, upon exit, restores the warnings filter and the showwarning() function. If the record argument is False (the default) the context manager returns None on entry. If record is True, a list is returned that is progressively populated with objects as seen by a custom showwarning() function (which also suppresses output to sys.stdout). Each object in the list has attributes with the same names as the arguments to showwarning(). The module argument takes a module that will be used instead of the module returned when you import warnings whose filter will be protected. This argument exists primarily for testing the warnings module itself.  Note The catch_warnings manager works by replacing and then later restoring the module’s showwarning() function and internal list of filter specifications. This means the context manager is modifying global state and therefore is not thread-safe.
def f_29784889():
    """enable warnings using action 'always'
    """
     
 --------------------

def f_13550423(l):
    """concatenate items of list `l` with a space ' '
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
email.utils.parsedate(date)  
Attempts to parse a date according to the rules in RFC 2822. however, some mailers don’t follow that format as specified, so parsedate() tries to guess correctly in such cases. date is a string containing an RFC 2822 date, such as "Mon, 20 Nov 1995 19:12:08 -0500". If it succeeds in parsing the date, parsedate() returns a 9-tuple that can be passed directly to time.mktime(); otherwise None will be returned. Note that indexes 6, 7, and 8 of the result tuple are not usable.
imaplib.Internaldate2tuple(datestr)  
Parse an IMAP4 INTERNALDATE string and return corresponding local time. The return value is a time.struct_time tuple or None if the string has wrong format.
classmethod time.fromisoformat(time_string)  
Return a time corresponding to a time_string in one of the formats emitted by time.isoformat(). Specifically, this function supports strings in the format: HH[:MM[:SS[.fff[fff]]]][+HH:MM[:SS[.ffffff]]]
  Caution This does not support parsing arbitrary ISO 8601 strings. It is only intended as the inverse operation of time.isoformat().  Examples: >>> from datetime import time
>>> time.fromisoformat('04:23:01')
datetime.time(4, 23, 1)
>>> time.fromisoformat('04:23:01.000384')
datetime.time(4, 23, 1, 384)
>>> time.fromisoformat('04:23:01+04:00')
datetime.time(4, 23, 1, tzinfo=datetime.timezone(datetime.timedelta(seconds=14400)))
  New in version 3.7.
email.utils.parsedate_tz(date)  
Performs the same function as parsedate(), but returns either None or a 10-tuple; the first 9 elements make up a tuple that can be passed directly to time.mktime(), and the tenth is the offset of the date’s timezone from UTC (which is the official term for Greenwich Mean Time) 1. If the input string has no timezone, the last element of the tuple returned is 0, which represents UTC. Note that indexes 6, 7, and 8 of the result tuple are not usable.
parse_time(value)  
Parses a string and returns a datetime.time. UTC offsets aren’t supported; if value describes one, the result is None.
def f_698223():
    """parse a time string '30/03/09 16:31:32.123' containing milliseconds in it
    """
    return  
 --------------------

def f_6633523(my_string):
    """convert a string `my_string` with dot and comma into a float number `my_float`
    """
     
 --------------------

def f_6633523():
    """convert a string `123,456.908` with dot and comma into a floating number
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
sys.path  
A list of strings that specifies the search path for modules. Initialized from the environment variable PYTHONPATH, plus an installation-dependent default. As initialized upon program startup, the first item of this list, path[0], is the directory containing the script that was used to invoke the Python interpreter. If the script directory is not available (e.g. if the interpreter is invoked interactively or if the script is read from standard input), path[0] is the empty string, which directs Python to search modules in the current directory first. Notice that the script directory is inserted before the entries inserted as a result of PYTHONPATH. A program is free to modify this list for its own purposes. Only strings and bytes should be added to sys.path; all other data types are ignored during import.  See also Module site This describes how to use .pth files to extend sys.path.
multiprocessing.set_executable()  
Sets the path of the Python interpreter to use when starting a child process. (By default sys.executable is used). Embedders will probably need to do some thing like set_executable(os.path.join(sys.exec_prefix, 'pythonw.exe'))
 before they can create child processes.  Changed in version 3.4: Now supported on Unix when the 'spawn' start method is used.
modulefinder.AddPackagePath(pkg_name, path)  
Record that the package named pkg_name can be found in the specified path.
site.main()  
Adds all the standard site-specific directories to the module search path. This function is called automatically when this module is imported, unless the Python interpreter was started with the -S flag.  Changed in version 3.3: This function used to be called unconditionally.
pkgutil.extend_path(path, name)  
Extend the search path for the modules which comprise a package. Intended use is to place the following code in a package’s __init__.py: from pkgutil import extend_path
__path__ = extend_path(__path__, __name__)
 This will add to the package’s __path__ all subdirectories of directories on sys.path named after the package. This is useful if one wants to distribute different parts of a single logical package as multiple directories. It also looks for *.pkg files beginning where * matches the name argument. This feature is similar to *.pth files (see the site module for more information), except that it doesn’t special-case lines starting with import. A *.pkg file is trusted at face value: apart from checking for duplicates, all entries found in a *.pkg file are added to the path, regardless of whether they exist on the filesystem. (This is a feature.) If the input path is not a list (as is the case for frozen packages) it is returned unchanged. The input path is not modified; an extended copy is returned. Items are only appended to the copy at the end. It is assumed that sys.path is a sequence. Items of sys.path that are not strings referring to existing directories are ignored. Unicode items on sys.path that cause errors when used as filenames may cause this function to raise an exception (in line with os.path.isdir() behavior).
def f_3108285():
    """set python path '/path/to/whatever' in python script
    """
     
 --------------------
Please refer to the following documentation to generate the code:
Pattern.split(string, maxsplit=0)  
Identical to the split() function, using the compiled pattern.
str.split(sep=None, maxsplit=-1)  
Return a list of the words in the string, using sep as the delimiter string. If maxsplit is given, at most maxsplit splits are done (thus, the list will have at most maxsplit+1 elements). If maxsplit is not specified or -1, then there is no limit on the number of splits (all possible splits are made). If sep is given, consecutive delimiters are not grouped together and are deemed to delimit empty strings (for example, '1,,2'.split(',') returns ['1', '', '2']). The sep argument may consist of multiple characters (for example, '1<>2<>3'.split('<>') returns ['1', '2', '3']). Splitting an empty string with a specified separator returns ['']. For example: >>> '1,2,3'.split(',')
['1', '2', '3']
>>> '1,2,3'.split(',', maxsplit=1)
['1', '2,3']
>>> '1,2,,3,'.split(',')
['1', '2', '', '3', '']
 If sep is not specified or is None, a different splitting algorithm is applied: runs of consecutive whitespace are regarded as a single separator, and the result will contain no empty strings at the start or end if the string has leading or trailing whitespace. Consequently, splitting an empty string or a string consisting of just whitespace with a None separator returns []. For example: >>> '1 2 3'.split()
['1', '2', '3']
>>> '1 2 3'.split(maxsplit=1)
['1', '2 3']
>>> '   1   2   3   '.split()
['1', '2', '3']
str.rsplit(sep=None, maxsplit=-1)  
Return a list of the words in the string, using sep as the delimiter string. If maxsplit is given, at most maxsplit splits are done, the rightmost ones. If sep is not specified or None, any whitespace string is a separator. Except for splitting from the right, rsplit() behaves like split() which is described in detail below.
pandas.DataFrame.clip   DataFrame.clip(lower=None, upper=None, axis=None, inplace=False, *args, **kwargs)[source]
 
Trim values at input threshold(s). Assigns values outside boundary to boundary values. Thresholds can be singular values or array like, and in the latter case the clipping is performed element-wise in the specified axis.  Parameters 
 
lower:float or array-like, default None


Minimum threshold value. All values below this threshold will be set to it. A missing threshold (e.g NA) will not clip the value.  
upper:float or array-like, default None


Maximum threshold value. All values above this threshold will be set to it. A missing threshold (e.g NA) will not clip the value.  
axis:int or str axis name, optional


Align object with lower and upper along the given axis.  
inplace:bool, default False


Whether to perform the operation in place on the data.  *args, **kwargs

Additional keywords have no effect but might be accepted for compatibility with numpy.    Returns 
 Series or DataFrame or None

Same type as calling object with the values outside the clip boundaries replaced or None if inplace=True.      See also  Series.clip

Trim values at input threshold in series.  DataFrame.clip

Trim values at input threshold in dataframe.  numpy.clip

Clip (limit) the values in an array.    Examples 
>>> data = {'col_0': [9, -3, 0, -1, 5], 'col_1': [-2, -7, 6, 8, -5]}
>>> df = pd.DataFrame(data)
>>> df
   col_0  col_1
0      9     -2
1     -3     -7
2      0      6
3     -1      8
4      5     -5
  Clips per column using lower and upper thresholds: 
>>> df.clip(-4, 6)
   col_0  col_1
0      6     -2
1     -3     -4
2      0      6
3     -1      6
4      5     -4
  Clips using specific lower and upper thresholds per column element: 
>>> t = pd.Series([2, -4, -1, 6, 3])
>>> t
0    2
1   -4
2   -1
3    6
4    3
dtype: int64
  
>>> df.clip(t, t + 4, axis=0)
   col_0  col_1
0      6      2
1     -3     -4
2      0      3
3      6      8
4      5      3
  Clips using specific lower threshold per column element, with missing values: 
>>> t = pd.Series([2, -4, np.NaN, 6, 3])
>>> t
0    2.0
1   -4.0
2    NaN
3    6.0
4    3.0
dtype: float64
  
>>> df.clip(t, axis=0)
col_0  col_1
0      9      2
1     -3     -4
2      0      6
3      6      8
4      5      3
pandas.Series.clip   Series.clip(lower=None, upper=None, axis=None, inplace=False, *args, **kwargs)[source]
 
Trim values at input threshold(s). Assigns values outside boundary to boundary values. Thresholds can be singular values or array like, and in the latter case the clipping is performed element-wise in the specified axis.  Parameters 
 
lower:float or array-like, default None


Minimum threshold value. All values below this threshold will be set to it. A missing threshold (e.g NA) will not clip the value.  
upper:float or array-like, default None


Maximum threshold value. All values above this threshold will be set to it. A missing threshold (e.g NA) will not clip the value.  
axis:int or str axis name, optional


Align object with lower and upper along the given axis.  
inplace:bool, default False


Whether to perform the operation in place on the data.  *args, **kwargs

Additional keywords have no effect but might be accepted for compatibility with numpy.    Returns 
 Series or DataFrame or None

Same type as calling object with the values outside the clip boundaries replaced or None if inplace=True.      See also  Series.clip

Trim values at input threshold in series.  DataFrame.clip

Trim values at input threshold in dataframe.  numpy.clip

Clip (limit) the values in an array.    Examples 
>>> data = {'col_0': [9, -3, 0, -1, 5], 'col_1': [-2, -7, 6, 8, -5]}
>>> df = pd.DataFrame(data)
>>> df
   col_0  col_1
0      9     -2
1     -3     -7
2      0      6
3     -1      8
4      5     -5
  Clips per column using lower and upper thresholds: 
>>> df.clip(-4, 6)
   col_0  col_1
0      6     -2
1     -3     -4
2      0      6
3     -1      6
4      5     -4
  Clips using specific lower and upper thresholds per column element: 
>>> t = pd.Series([2, -4, -1, 6, 3])
>>> t
0    2
1   -4
2   -1
3    6
4    3
dtype: int64
  
>>> df.clip(t, t + 4, axis=0)
   col_0  col_1
0      6      2
1     -3     -4
2      0      3
3      6      8
4      5      3
  Clips using specific lower threshold per column element, with missing values: 
>>> t = pd.Series([2, -4, np.NaN, 6, 3])
>>> t
0    2.0
1   -4.0
2    NaN
3    6.0
4    3.0
dtype: float64
  
>>> df.clip(t, axis=0)
col_0  col_1
0      9      2
1     -3     -4
2      0      6
3      6      8
4      5      3
def f_2195340():
    """split string 'Words, words, words.' using a regex '(\\W+)'
    """
    return  
 --------------------

def f_17977584():
    """open a file `Output.txt` in append mode
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
mpl_toolkits.mplot3d The mplot3d toolkit adds simple 3D plotting capabilities (scatter, surface, line, mesh, etc.) to Matplotlib by supplying an Axes object that can create a 2D projection of a 3D scene. The resulting graph will have the same look and feel as regular 2D plots. Not the fastest or most feature complete 3D library out there, but it ships with Matplotlib and thus may be a lighter weight solution for some use cases. See the mplot3d tutorial for more information.  The interactive backends also provide the ability to rotate and zoom the 3D scene. One can rotate the 3D scene by simply clicking-and-dragging the scene. Zooming is done by right-clicking the scene and dragging the mouse up and down (unlike 2D plots, the toolbar zoom button is not used).  
mplot3d FAQ How is mplot3d different from Mayavi? My 3D plot doesn't look right at certain viewing angles I don't like how the 3D plot is laid out, how do I change that?     Note pyplot cannot be used to add content to 3D plots, because its function signatures are strictly 2D and cannot handle the additional information needed for 3D. Instead, use the explicit API by calling the respective methods on the Axes3D object.   axes3d  Note 3D plotting in Matplotlib is still not as mature as the 2D case. Please report any functions that do not behave as expected as a bug. In addition, help and patches would be greatly appreciated!   
axes3d.Axes3D(fig[, rect, azim, elev, ...]) 3D axes object.     axis3d  Note See mpl_toolkits.mplot3d.axis3d._axinfo for a dictionary containing constants that may be modified for controlling the look and feel of mplot3d axes (e.g., label spacing, font colors and panel colors). Historically, axis3d has suffered from having hard-coded constants that precluded user adjustments, and this dictionary was implemented in version 1.1 as a stop-gap measure.   
axis3d.Axis(adir, v_intervalx, d_intervalx, ...) An Axis class for the 3D plots.     art3d  
art3d.Line3D(xs, ys, zs, *args, **kwargs) 3D line object.  
art3d.Line3DCollection(segments, *args[, zorder]) A collection of 3D lines.  
art3d.Patch3D(*args[, zs, zdir]) 3D patch object.  
art3d.Patch3DCollection(*args[, zs, zdir, ...]) A collection of 3D patches.  
art3d.Path3DCollection(*args[, zs, zdir, ...]) A collection of 3D paths.  
art3d.PathPatch3D(path, *[, zs, zdir]) 3D PathPatch object.  
art3d.Poly3DCollection(verts, *args[, zsort]) A collection of 3D polygons.  
art3d.Text3D([x, y, z, text, zdir]) Text object with 3D position and direction.  
art3d.get_dir_vector(zdir) Return a direction vector.  
art3d.juggle_axes(xs, ys, zs, zdir) Reorder coordinates so that 2D xs, ys can be plotted in the plane orthogonal to zdir.  
art3d.line_2d_to_3d(line[, zs, zdir]) Convert a 2D line to 3D.  
art3d.line_collection_2d_to_3d(col[, zs, zdir]) Convert a LineCollection to a Line3DCollection object.  
art3d.patch_2d_to_3d(patch[, z, zdir]) Convert a Patch to a Patch3D object.  
art3d.patch_collection_2d_to_3d(col[, zs, ...]) Convert a PatchCollection into a Patch3DCollection object (or a PathCollection into a Path3DCollection object).  
art3d.pathpatch_2d_to_3d(pathpatch[, z, zdir]) Convert a PathPatch to a PathPatch3D object.  
art3d.poly_collection_2d_to_3d(col[, zs, zdir]) Convert a PolyCollection to a Poly3DCollection object.  
art3d.rotate_axes(xs, ys, zs, zdir) Reorder coordinates so that the axes are rotated with zdir along the original z axis.  
art3d.text_2d_to_3d(obj[, z, zdir]) Convert a Text to a Text3D object.     proj3d  
proj3d.inv_transform(xs, ys, zs, M)   
proj3d.persp_transformation(zfront, zback)   
proj3d.proj_points(points, M)   
proj3d.proj_trans_points(points, M)   
proj3d.proj_transform(xs, ys, zs, M) Transform the points by the projection matrix  
proj3d.proj_transform_clip(xs, ys, zs, M) Transform the points by the projection matrix and return the clipping result returns txs, tys, tzs, tis  
proj3d.rot_x(V, alpha)   
proj3d.transform(xs, ys, zs, M) Transform the points by the projection matrix  
proj3d.view_transformation(E, R, V)   
proj3d.world_transformation(xmin, xmax, ...) Produce a matrix that scales homogeneous coords in the specified ranges to [0, 1], or [0, pb_aspect[i]] if the plotbox aspect ratio is specified.
stringprep.map_table_b3(code)  
Return the mapped value for code according to tableB.3 (Mapping for case-folding used with no normalization).
tf.raw_ops.BatchFFT3D  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.raw_ops.BatchFFT3D  
tf.raw_ops.BatchFFT3D(
    input, name=None
)

 


 Args
  input   A Tensor of type complex64.  
  name   A name for the operation (optional).   
 


 Returns   A Tensor of type complex64.
handler403
HTTPRedirectHandler.http_error_303(req, fp, code, msg, hdrs)  
The same as http_error_301(), but called for the ‘see other’ response.
def f_22676():
    """download a file "http://www.example.com/songs/mp3.mp3" over HTTP and save to "mp3.mp3"
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
http.client — HTTP protocol client Source code: Lib/http/client.py This module defines classes which implement the client side of the HTTP and HTTPS protocols. It is normally not used directly — the module urllib.request uses it to handle URLs that use HTTP and HTTPS.  See also The Requests package is recommended for a higher-level HTTP client interface.   Note HTTPS support is only available if Python was compiled with SSL support (through the ssl module).  The module provides the following classes:  
class http.client.HTTPConnection(host, port=None, [timeout, ]source_address=None, blocksize=8192)  
An HTTPConnection instance represents one transaction with an HTTP server. It should be instantiated passing it a host and optional port number. If no port number is passed, the port is extracted from the host string if it has the form host:port, else the default HTTP port (80) is used. If the optional timeout parameter is given, blocking operations (like connection attempts) will timeout after that many seconds (if it is not given, the global default timeout setting is used). The optional source_address parameter may be a tuple of a (host, port) to use as the source address the HTTP connection is made from. The optional blocksize parameter sets the buffer size in bytes for sending a file-like message body. For example, the following calls all create instances that connect to the server at the same host and port: >>> h1 = http.client.HTTPConnection('www.python.org')
>>> h2 = http.client.HTTPConnection('www.python.org:80')
>>> h3 = http.client.HTTPConnection('www.python.org', 80)
>>> h4 = http.client.HTTPConnection('www.python.org', 80, timeout=10)
  Changed in version 3.2: source_address was added.   Changed in version 3.4: The strict parameter was removed. HTTP 0.9-style “Simple Responses” are not longer supported.   Changed in version 3.7: blocksize parameter was added.  
  
class http.client.HTTPSConnection(host, port=None, key_file=None, cert_file=None, [timeout, ]source_address=None, *, context=None, check_hostname=None, blocksize=8192)  
A subclass of HTTPConnection that uses SSL for communication with secure servers. Default port is 443. If context is specified, it must be a ssl.SSLContext instance describing the various SSL options. Please read Security considerations for more information on best practices.  Changed in version 3.2: source_address, context and check_hostname were added.   Changed in version 3.2: This class now supports HTTPS virtual hosts if possible (that is, if ssl.HAS_SNI is true).   Changed in version 3.4: The strict parameter was removed. HTTP 0.9-style “Simple Responses” are no longer supported.   Changed in version 3.4.3: This class now performs all the necessary certificate and hostname checks by default. To revert to the previous, unverified, behavior ssl._create_unverified_context() can be passed to the context parameter.   Changed in version 3.8: This class now enables TLS 1.3 ssl.SSLContext.post_handshake_auth for the default context or when cert_file is passed with a custom context.   Deprecated since version 3.6: key_file and cert_file are deprecated in favor of context. Please use ssl.SSLContext.load_cert_chain() instead, or let ssl.create_default_context() select the system’s trusted CA certificates for you. The check_hostname parameter is also deprecated; the ssl.SSLContext.check_hostname attribute of context should be used instead.  
  
class http.client.HTTPResponse(sock, debuglevel=0, method=None, url=None)  
Class whose instances are returned upon successful connection. Not instantiated directly by user.  Changed in version 3.4: The strict parameter was removed. HTTP 0.9 style “Simple Responses” are no longer supported.  
 This module provides the following function:  
http.client.parse_headers(fp)  
Parse the headers from a file pointer fp representing a HTTP request/response. The file has to be a BufferedIOBase reader (i.e. not text) and must provide a valid RFC 2822 style header. This function returns an instance of http.client.HTTPMessage that holds the header fields, but no payload (the same as HTTPResponse.msg and http.server.BaseHTTPRequestHandler.headers). After returning, the file pointer fp is ready to read the HTTP body.  Note parse_headers() does not parse the start-line of a HTTP message; it only parses the Name: value lines. The file has to be ready to read these field lines, so the first line should already be consumed before calling the function.  
 The following exceptions are raised as appropriate:  
exception http.client.HTTPException  
The base class of the other exceptions in this module. It is a subclass of Exception. 
  
exception http.client.NotConnected  
A subclass of HTTPException. 
  
exception http.client.InvalidURL  
A subclass of HTTPException, raised if a port is given and is either non-numeric or empty. 
  
exception http.client.UnknownProtocol  
A subclass of HTTPException. 
  
exception http.client.UnknownTransferEncoding  
A subclass of HTTPException. 
  
exception http.client.UnimplementedFileMode  
A subclass of HTTPException. 
  
exception http.client.IncompleteRead  
A subclass of HTTPException. 
  
exception http.client.ImproperConnectionState  
A subclass of HTTPException. 
  
exception http.client.CannotSendRequest  
A subclass of ImproperConnectionState. 
  
exception http.client.CannotSendHeader  
A subclass of ImproperConnectionState. 
  
exception http.client.ResponseNotReady  
A subclass of ImproperConnectionState. 
  
exception http.client.BadStatusLine  
A subclass of HTTPException. Raised if a server responds with a HTTP status code that we don’t understand. 
  
exception http.client.LineTooLong  
A subclass of HTTPException. Raised if an excessively long line is received in the HTTP protocol from the server. 
  
exception http.client.RemoteDisconnected  
A subclass of ConnectionResetError and BadStatusLine. Raised by HTTPConnection.getresponse() when the attempt to read the response results in no data read from the connection, indicating that the remote end has closed the connection.  New in version 3.5: Previously, BadStatusLine('') was raised.  
 The constants defined in this module are:  
http.client.HTTP_PORT  
The default port for the HTTP protocol (always 80). 
  
http.client.HTTPS_PORT  
The default port for the HTTPS protocol (always 443). 
  
http.client.responses  
This dictionary maps the HTTP 1.1 status codes to the W3C names. Example: http.client.responses[http.client.NOT_FOUND] is 'Not Found'. 
 See HTTP status codes for a list of HTTP status codes that are available in this module as constants. HTTPConnection Objects HTTPConnection instances have the following methods:  
HTTPConnection.request(method, url, body=None, headers={}, *, encode_chunked=False)  
This will send a request to the server using the HTTP request method method and the selector url. If body is specified, the specified data is sent after the headers are finished. It may be a str, a bytes-like object, an open file object, or an iterable of bytes. If body is a string, it is encoded as ISO-8859-1, the default for HTTP. If it is a bytes-like object, the bytes are sent as is. If it is a file object, the contents of the file is sent; this file object should support at least the read() method. If the file object is an instance of io.TextIOBase, the data returned by the read() method will be encoded as ISO-8859-1, otherwise the data returned by read() is sent as is. If body is an iterable, the elements of the iterable are sent as is until the iterable is exhausted. The headers argument should be a mapping of extra HTTP headers to send with the request. If headers contains neither Content-Length nor Transfer-Encoding, but there is a request body, one of those header fields will be added automatically. If body is None, the Content-Length header is set to 0 for methods that expect a body (PUT, POST, and PATCH). If body is a string or a bytes-like object that is not also a file, the Content-Length header is set to its length. Any other type of body (files and iterables in general) will be chunk-encoded, and the Transfer-Encoding header will automatically be set instead of Content-Length. The encode_chunked argument is only relevant if Transfer-Encoding is specified in headers. If encode_chunked is False, the HTTPConnection object assumes that all encoding is handled by the calling code. If it is True, the body will be chunk-encoded.  Note Chunked transfer encoding has been added to the HTTP protocol version 1.1. Unless the HTTP server is known to handle HTTP 1.1, the caller must either specify the Content-Length, or must pass a str or bytes-like object that is not also a file as the body representation.   New in version 3.2: body can now be an iterable.   Changed in version 3.6: If neither Content-Length nor Transfer-Encoding are set in headers, file and iterable body objects are now chunk-encoded. The encode_chunked argument was added. No attempt is made to determine the Content-Length for file objects.  
  
HTTPConnection.getresponse()  
Should be called after a request is sent to get the response from the server. Returns an HTTPResponse instance.  Note Note that you must have read the whole response before you can send a new request to the server.   Changed in version 3.5: If a ConnectionError or subclass is raised, the HTTPConnection object will be ready to reconnect when a new request is sent.  
  
HTTPConnection.set_debuglevel(level)  
Set the debugging level. The default debug level is 0, meaning no debugging output is printed. Any value greater than 0 will cause all currently defined debug output to be printed to stdout. The debuglevel is passed to any new HTTPResponse objects that are created.  New in version 3.1.  
  
HTTPConnection.set_tunnel(host, port=None, headers=None)  
Set the host and the port for HTTP Connect Tunnelling. This allows running the connection through a proxy server. The host and port arguments specify the endpoint of the tunneled connection (i.e. the address included in the CONNECT request, not the address of the proxy server). The headers argument should be a mapping of extra HTTP headers to send with the CONNECT request. For example, to tunnel through a HTTPS proxy server running locally on port 8080, we would pass the address of the proxy to the HTTPSConnection constructor, and the address of the host that we eventually want to reach to the set_tunnel() method: >>> import http.client
>>> conn = http.client.HTTPSConnection("localhost", 8080)
>>> conn.set_tunnel("www.python.org")
>>> conn.request("HEAD","/index.html")
  New in version 3.2.  
  
HTTPConnection.connect()  
Connect to the server specified when the object was created. By default, this is called automatically when making a request if the client does not already have a connection. 
  
HTTPConnection.close()  
Close the connection to the server. 
  
HTTPConnection.blocksize  
Buffer size in bytes for sending a file-like message body.  New in version 3.7.  
 As an alternative to using the request() method described above, you can also send your request step by step, by using the four functions below.  
HTTPConnection.putrequest(method, url, skip_host=False, skip_accept_encoding=False)  
This should be the first call after the connection to the server has been made. It sends a line to the server consisting of the method string, the url string, and the HTTP version (HTTP/1.1). To disable automatic sending of Host: or Accept-Encoding: headers (for example to accept additional content encodings), specify skip_host or skip_accept_encoding with non-False values. 
  
HTTPConnection.putheader(header, argument[, ...])  
Send an RFC 822-style header to the server. It sends a line to the server consisting of the header, a colon and a space, and the first argument. If more arguments are given, continuation lines are sent, each consisting of a tab and an argument. 
  
HTTPConnection.endheaders(message_body=None, *, encode_chunked=False)  
Send a blank line to the server, signalling the end of the headers. The optional message_body argument can be used to pass a message body associated with the request. If encode_chunked is True, the result of each iteration of message_body will be chunk-encoded as specified in RFC 7230, Section 3.3.1. How the data is encoded is dependent on the type of message_body. If message_body implements the buffer interface the encoding will result in a single chunk. If message_body is a collections.abc.Iterable, each iteration of message_body will result in a chunk. If message_body is a file object, each call to .read() will result in a chunk. The method automatically signals the end of the chunk-encoded data immediately after message_body.  Note Due to the chunked encoding specification, empty chunks yielded by an iterator body will be ignored by the chunk-encoder. This is to avoid premature termination of the read of the request by the target server due to malformed encoding.   New in version 3.6: Chunked encoding support. The encode_chunked parameter was added.  
  
HTTPConnection.send(data)  
Send data to the server. This should be used directly only after the endheaders() method has been called and before getresponse() is called. 
 HTTPResponse Objects An HTTPResponse instance wraps the HTTP response from the server. It provides access to the request headers and the entity body. The response is an iterable object and can be used in a with statement.  Changed in version 3.5: The io.BufferedIOBase interface is now implemented and all of its reader operations are supported.   
HTTPResponse.read([amt])  
Reads and returns the response body, or up to the next amt bytes. 
  
HTTPResponse.readinto(b)  
Reads up to the next len(b) bytes of the response body into the buffer b. Returns the number of bytes read.  New in version 3.3.  
  
HTTPResponse.getheader(name, default=None)  
Return the value of the header name, or default if there is no header matching name. If there is more than one header with the name name, return all of the values joined by ‘, ‘. If ‘default’ is any iterable other than a single string, its elements are similarly returned joined by commas. 
  
HTTPResponse.getheaders()  
Return a list of (header, value) tuples. 
  
HTTPResponse.fileno()  
Return the fileno of the underlying socket. 
  
HTTPResponse.msg  
A http.client.HTTPMessage instance containing the response headers. http.client.HTTPMessage is a subclass of email.message.Message. 
  
HTTPResponse.version  
HTTP protocol version used by server. 10 for HTTP/1.0, 11 for HTTP/1.1. 
  
HTTPResponse.url  
URL of the resource retrieved, commonly used to determine if a redirect was followed. 
  
HTTPResponse.headers  
Headers of the response in the form of an email.message.EmailMessage instance. 
  
HTTPResponse.status  
Status code returned by server. 
  
HTTPResponse.reason  
Reason phrase returned by server. 
  
HTTPResponse.debuglevel  
A debugging hook. If debuglevel is greater than zero, messages will be printed to stdout as the response is read and parsed. 
  
HTTPResponse.closed  
Is True if the stream is closed. 
  
HTTPResponse.geturl()  
 Deprecated since version 3.9: Deprecated in favor of url.  
  
HTTPResponse.info()  
 Deprecated since version 3.9: Deprecated in favor of headers.  
  
HTTPResponse.getstatus()  
 Deprecated since version 3.9: Deprecated in favor of status.  
 Examples Here is an example session that uses the GET method: >>> import http.client
>>> conn = http.client.HTTPSConnection("www.python.org")
>>> conn.request("GET", "/")
>>> r1 = conn.getresponse()
>>> print(r1.status, r1.reason)
200 OK
>>> data1 = r1.read()  # This will return entire content.
>>> # The following example demonstrates reading data in chunks.
>>> conn.request("GET", "/")
>>> r1 = conn.getresponse()
>>> while chunk := r1.read(200):
...     print(repr(chunk))
b'<!doctype html>\n<!--[if"...
...
>>> # Example of an invalid request
>>> conn = http.client.HTTPSConnection("docs.python.org")
>>> conn.request("GET", "/parrot.spam")
>>> r2 = conn.getresponse()
>>> print(r2.status, r2.reason)
404 Not Found
>>> data2 = r2.read()
>>> conn.close()
 Here is an example session that uses the HEAD method. Note that the HEAD method never returns any data. >>> import http.client
>>> conn = http.client.HTTPSConnection("www.python.org")
>>> conn.request("HEAD", "/")
>>> res = conn.getresponse()
>>> print(res.status, res.reason)
200 OK
>>> data = res.read()
>>> print(len(data))
0
>>> data == b''
True
 Here is an example session that shows how to POST requests: >>> import http.client, urllib.parse
>>> params = urllib.parse.urlencode({'@number': 12524, '@type': 'issue', '@action': 'show'})
>>> headers = {"Content-type": "application/x-www-form-urlencoded",
...            "Accept": "text/plain"}
>>> conn = http.client.HTTPConnection("bugs.python.org")
>>> conn.request("POST", "", params, headers)
>>> response = conn.getresponse()
>>> print(response.status, response.reason)
302 Found
>>> data = response.read()
>>> data
b'Redirecting to <a href="http://bugs.python.org/issue12524">http://bugs.python.org/issue12524</a>'
>>> conn.close()
 Client side HTTP PUT requests are very similar to POST requests. The difference lies only the server side where HTTP server will allow resources to be created via PUT request. It should be noted that custom HTTP methods are also handled in urllib.request.Request by setting the appropriate method attribute. Here is an example session that shows how to send a PUT request using http.client: >>> # This creates an HTTP message
>>> # with the content of BODY as the enclosed representation
>>> # for the resource http://localhost:8080/file
...
>>> import http.client
>>> BODY = "***filecontents***"
>>> conn = http.client.HTTPConnection("localhost", 8080)
>>> conn.request("PUT", "/file", BODY)
>>> response = conn.getresponse()
>>> print(response.status, response.reason)
200, OK
 HTTPMessage Objects An http.client.HTTPMessage instance holds the headers from an HTTP response. It is implemented using the email.message.Message class.
class urllib.request.FileHandler  
Open local files.
cgi — Common Gateway Interface support Source code: Lib/cgi.py Support module for Common Gateway Interface (CGI) scripts. This module defines a number of utilities for use by CGI scripts written in Python. Introduction A CGI script is invoked by an HTTP server, usually to process user input submitted through an HTML <FORM> or <ISINDEX> element. Most often, CGI scripts live in the server’s special cgi-bin directory. The HTTP server places all sorts of information about the request (such as the client’s hostname, the requested URL, the query string, and lots of other goodies) in the script’s shell environment, executes the script, and sends the script’s output back to the client. The script’s input is connected to the client too, and sometimes the form data is read this way; at other times the form data is passed via the “query string” part of the URL. This module is intended to take care of the different cases and provide a simpler interface to the Python script. It also provides a number of utilities that help in debugging scripts, and the latest addition is support for file uploads from a form (if your browser supports it). The output of a CGI script should consist of two sections, separated by a blank line. The first section contains a number of headers, telling the client what kind of data is following. Python code to generate a minimal header section looks like this: print("Content-Type: text/html")    # HTML is following
print()                             # blank line, end of headers
 The second section is usually HTML, which allows the client software to display nicely formatted text with header, in-line images, etc. Here’s Python code that prints a simple piece of HTML: print("<TITLE>CGI script output</TITLE>")
print("<H1>This is my first CGI script</H1>")
print("Hello, world!")
 Using the cgi module Begin by writing import cgi. When you write a new script, consider adding these lines: import cgitb
cgitb.enable()
 This activates a special exception handler that will display detailed reports in the Web browser if any errors occur. If you’d rather not show the guts of your program to users of your script, you can have the reports saved to files instead, with code like this: import cgitb
cgitb.enable(display=0, logdir="/path/to/logdir")
 It’s very helpful to use this feature during script development. The reports produced by cgitb provide information that can save you a lot of time in tracking down bugs. You can always remove the cgitb line later when you have tested your script and are confident that it works correctly. To get at submitted form data, use the FieldStorage class. If the form contains non-ASCII characters, use the encoding keyword parameter set to the value of the encoding defined for the document. It is usually contained in the META tag in the HEAD section of the HTML document or by the Content-Type header). This reads the form contents from the standard input or the environment (depending on the value of various environment variables set according to the CGI standard). Since it may consume standard input, it should be instantiated only once. The FieldStorage instance can be indexed like a Python dictionary. It allows membership testing with the in operator, and also supports the standard dictionary method keys() and the built-in function len(). Form fields containing empty strings are ignored and do not appear in the dictionary; to keep such values, provide a true value for the optional keep_blank_values keyword parameter when creating the FieldStorage instance. For instance, the following code (which assumes that the Content-Type header and blank line have already been printed) checks that the fields name and addr are both set to a non-empty string: form = cgi.FieldStorage()
if "name" not in form or "addr" not in form:
    print("<H1>Error</H1>")
    print("Please fill in the name and addr fields.")
    return
print("<p>name:", form["name"].value)
print("<p>addr:", form["addr"].value)
...further form processing here...
 Here the fields, accessed through form[key], are themselves instances of FieldStorage (or MiniFieldStorage, depending on the form encoding). The value attribute of the instance yields the string value of the field. The getvalue() method returns this string value directly; it also accepts an optional second argument as a default to return if the requested key is not present. If the submitted form data contains more than one field with the same name, the object retrieved by form[key] is not a FieldStorage or MiniFieldStorage instance but a list of such instances. Similarly, in this situation, form.getvalue(key) would return a list of strings. If you expect this possibility (when your HTML form contains multiple fields with the same name), use the getlist() method, which always returns a list of values (so that you do not need to special-case the single item case). For example, this code concatenates any number of username fields, separated by commas: value = form.getlist("username")
usernames = ",".join(value)
 If a field represents an uploaded file, accessing the value via the value attribute or the getvalue() method reads the entire file in memory as bytes. This may not be what you want. You can test for an uploaded file by testing either the filename attribute or the file attribute. You can then read the data from the file attribute before it is automatically closed as part of the garbage collection of the FieldStorage instance (the read() and readline() methods will return bytes): fileitem = form["userfile"]
if fileitem.file:
    # It's an uploaded file; count lines
    linecount = 0
    while True:
        line = fileitem.file.readline()
        if not line: break
        linecount = linecount + 1
 FieldStorage objects also support being used in a with statement, which will automatically close them when done. If an error is encountered when obtaining the contents of an uploaded file (for example, when the user interrupts the form submission by clicking on a Back or Cancel button) the done attribute of the object for the field will be set to the value -1. The file upload draft standard entertains the possibility of uploading multiple files from one field (using a recursive multipart/* encoding). When this occurs, the item will be a dictionary-like FieldStorage item. This can be determined by testing its type attribute, which should be multipart/form-data (or perhaps another MIME type matching multipart/*). In this case, it can be iterated over recursively just like the top-level form object. When a form is submitted in the “old” format (as the query string or as a single data part of type application/x-www-form-urlencoded), the items will actually be instances of the class MiniFieldStorage. In this case, the list, file, and filename attributes are always None. A form submitted via POST that also has a query string will contain both FieldStorage and MiniFieldStorage items.  Changed in version 3.4: The file attribute is automatically closed upon the garbage collection of the creating FieldStorage instance.   Changed in version 3.5: Added support for the context management protocol to the FieldStorage class.  Higher Level Interface The previous section explains how to read CGI form data using the FieldStorage class. This section describes a higher level interface which was added to this class to allow one to do it in a more readable and intuitive way. The interface doesn’t make the techniques described in previous sections obsolete — they are still useful to process file uploads efficiently, for example. The interface consists of two simple methods. Using the methods you can process form data in a generic way, without the need to worry whether only one or more values were posted under one name. In the previous section, you learned to write following code anytime you expected a user to post more than one value under one name: item = form.getvalue("item")
if isinstance(item, list):
    # The user is requesting more than one item.
else:
    # The user is requesting only one item.
 This situation is common for example when a form contains a group of multiple checkboxes with the same name: <input type="checkbox" name="item" value="1" />
<input type="checkbox" name="item" value="2" />
 In most situations, however, there’s only one form control with a particular name in a form and then you expect and need only one value associated with this name. So you write a script containing for example this code: user = form.getvalue("user").upper()
 The problem with the code is that you should never expect that a client will provide valid input to your scripts. For example, if a curious user appends another user=foo pair to the query string, then the script would crash, because in this situation the getvalue("user") method call returns a list instead of a string. Calling the upper() method on a list is not valid (since lists do not have a method of this name) and results in an AttributeError exception. Therefore, the appropriate way to read form data values was to always use the code which checks whether the obtained value is a single value or a list of values. That’s annoying and leads to less readable scripts. A more convenient approach is to use the methods getfirst() and getlist() provided by this higher level interface.  
FieldStorage.getfirst(name, default=None)  
This method always returns only one value associated with form field name. The method returns only the first value in case that more values were posted under such name. Please note that the order in which the values are received may vary from browser to browser and should not be counted on. 1 If no such form field or value exists then the method returns the value specified by the optional parameter default. This parameter defaults to None if not specified. 
  
FieldStorage.getlist(name)  
This method always returns a list of values associated with form field name. The method returns an empty list if no such form field or value exists for name. It returns a list consisting of one item if only one such value exists. 
 Using these methods you can write nice compact code: import cgi
form = cgi.FieldStorage()
user = form.getfirst("user", "").upper()    # This way it's safe.
for item in form.getlist("item"):
    do_something(item)
 Functions These are useful if you want more control, or if you want to employ some of the algorithms implemented in this module in other circumstances.  
cgi.parse(fp=None, environ=os.environ, keep_blank_values=False, strict_parsing=False, separator="&")  
Parse a query in the environment or from a file (the file defaults to sys.stdin). The keep_blank_values, strict_parsing and separator parameters are passed to urllib.parse.parse_qs() unchanged. 
  
cgi.parse_multipart(fp, pdict, encoding="utf-8", errors="replace", separator="&")  
Parse input of type multipart/form-data (for file uploads). Arguments are fp for the input file, pdict for a dictionary containing other parameters in the Content-Type header, and encoding, the request encoding. Returns a dictionary just like urllib.parse.parse_qs(): keys are the field names, each value is a list of values for that field. For non-file fields, the value is a list of strings. This is easy to use but not much good if you are expecting megabytes to be uploaded — in that case, use the FieldStorage class instead which is much more flexible.  Changed in version 3.7: Added the encoding and errors parameters. For non-file fields, the value is now a list of strings, not bytes.   Changed in version 3.9.2: Added the separator parameter.  
  
cgi.parse_header(string)  
Parse a MIME header (such as Content-Type) into a main value and a dictionary of parameters. 
  
cgi.test()  
Robust test CGI script, usable as main program. Writes minimal HTTP headers and formats all information provided to the script in HTML form. 
  
cgi.print_environ()  
Format the shell environment in HTML. 
  
cgi.print_form(form)  
Format a form in HTML. 
  
cgi.print_directory()  
Format the current directory in HTML. 
  
cgi.print_environ_usage()  
Print a list of useful (used by CGI) environment variables in HTML. 
 Caring about security There’s one important rule: if you invoke an external program (via the os.system() or os.popen() functions. or others with similar functionality), make very sure you don’t pass arbitrary strings received from the client to the shell. This is a well-known security hole whereby clever hackers anywhere on the Web can exploit a gullible CGI script to invoke arbitrary shell commands. Even parts of the URL or field names cannot be trusted, since the request doesn’t have to come from your form! To be on the safe side, if you must pass a string gotten from a form to a shell command, you should make sure the string contains only alphanumeric characters, dashes, underscores, and periods. Installing your CGI script on a Unix system Read the documentation for your HTTP server and check with your local system administrator to find the directory where CGI scripts should be installed; usually this is in a directory cgi-bin in the server tree. Make sure that your script is readable and executable by “others”; the Unix file mode should be 0o755 octal (use chmod 0755 filename). Make sure that the first line of the script contains #! starting in column 1 followed by the pathname of the Python interpreter, for instance: #!/usr/local/bin/python
 Make sure the Python interpreter exists and is executable by “others”. Make sure that any files your script needs to read or write are readable or writable, respectively, by “others” — their mode should be 0o644 for readable and 0o666 for writable. This is because, for security reasons, the HTTP server executes your script as user “nobody”, without any special privileges. It can only read (write, execute) files that everybody can read (write, execute). The current directory at execution time is also different (it is usually the server’s cgi-bin directory) and the set of environment variables is also different from what you get when you log in. In particular, don’t count on the shell’s search path for executables (PATH) or the Python module search path (PYTHONPATH) to be set to anything interesting. If you need to load modules from a directory which is not on Python’s default module search path, you can change the path in your script, before importing other modules. For example: import sys
sys.path.insert(0, "/usr/home/joe/lib/python")
sys.path.insert(0, "/usr/local/lib/python")
 (This way, the directory inserted last will be searched first!) Instructions for non-Unix systems will vary; check your HTTP server’s documentation (it will usually have a section on CGI scripts). Testing your CGI script Unfortunately, a CGI script will generally not run when you try it from the command line, and a script that works perfectly from the command line may fail mysteriously when run from the server. There’s one reason why you should still test your script from the command line: if it contains a syntax error, the Python interpreter won’t execute it at all, and the HTTP server will most likely send a cryptic error to the client. Assuming your script has no syntax errors, yet it does not work, you have no choice but to read the next section. Debugging CGI scripts First of all, check for trivial installation errors — reading the section above on installing your CGI script carefully can save you a lot of time. If you wonder whether you have understood the installation procedure correctly, try installing a copy of this module file (cgi.py) as a CGI script. When invoked as a script, the file will dump its environment and the contents of the form in HTML form. Give it the right mode etc, and send it a request. If it’s installed in the standard cgi-bin directory, it should be possible to send it a request by entering a URL into your browser of the form: http://yourhostname/cgi-bin/cgi.py?name=Joe+Blow&addr=At+Home
 If this gives an error of type 404, the server cannot find the script – perhaps you need to install it in a different directory. If it gives another error, there’s an installation problem that you should fix before trying to go any further. If you get a nicely formatted listing of the environment and form content (in this example, the fields should be listed as “addr” with value “At Home” and “name” with value “Joe Blow”), the cgi.py script has been installed correctly. If you follow the same procedure for your own script, you should now be able to debug it. The next step could be to call the cgi module’s test() function from your script: replace its main code with the single statement cgi.test()
 This should produce the same results as those gotten from installing the cgi.py file itself. When an ordinary Python script raises an unhandled exception (for whatever reason: of a typo in a module name, a file that can’t be opened, etc.), the Python interpreter prints a nice traceback and exits. While the Python interpreter will still do this when your CGI script raises an exception, most likely the traceback will end up in one of the HTTP server’s log files, or be discarded altogether. Fortunately, once you have managed to get your script to execute some code, you can easily send tracebacks to the Web browser using the cgitb module. If you haven’t done so already, just add the lines: import cgitb
cgitb.enable()
 to the top of your script. Then try running it again; when a problem occurs, you should see a detailed report that will likely make apparent the cause of the crash. If you suspect that there may be a problem in importing the cgitb module, you can use an even more robust approach (which only uses built-in modules): import sys
sys.stderr = sys.stdout
print("Content-Type: text/plain")
print()
...your code here...
 This relies on the Python interpreter to print the traceback. The content type of the output is set to plain text, which disables all HTML processing. If your script works, the raw HTML will be displayed by your client. If it raises an exception, most likely after the first two lines have been printed, a traceback will be displayed. Because no HTML interpretation is going on, the traceback will be readable. Common problems and solutions  Most HTTP servers buffer the output from CGI scripts until the script is completed. This means that it is not possible to display a progress report on the client’s display while the script is running. Check the installation instructions above. Check the HTTP server’s log files. (tail -f logfile in a separate window may be useful!) Always check a script for syntax errors first, by doing something like python script.py. If your script does not have any syntax errors, try adding import cgitb;
cgitb.enable() to the top of the script. When invoking external programs, make sure they can be found. Usually, this means using absolute path names — PATH is usually not set to a very useful value in a CGI script. When reading or writing external files, make sure they can be read or written by the userid under which your CGI script will be running: this is typically the userid under which the web server is running, or some explicitly specified userid for a web server’s suexec feature. Don’t try to give a CGI script a set-uid mode. This doesn’t work on most systems, and is a security liability as well.  Footnotes  
1  
Note that some recent versions of the HTML specification do state what order the field values should be supplied in, but knowing whether a request was received from a conforming browser, or even from a browser at all, is tedious and error-prone.
http.client.HTTP_PORT  
The default port for the HTTP protocol (always 80).
urllib — URL handling modules Source code: Lib/urllib/ urllib is a package that collects several modules for working with URLs:  
urllib.request for opening and reading URLs 
urllib.error containing the exceptions raised by urllib.request
 
urllib.parse for parsing URLs 
urllib.robotparser for parsing robots.txt files
def f_22676(url):
    """download a file 'http://www.example.com/' over HTTP
    """
     
 --------------------

def f_22676(url):
    """download a file `url` over HTTP
    """
    return  
 --------------------

def f_22676(url):
    """download a file `url` over HTTP and save to "10MB"
    """
     
 --------------------
Please refer to the following documentation to generate the code:
make_svn_version_py(delete=True)[source]
 
Appends a data function to the data_files list that will generate __svn_version__.py file to the current package directory. Generate package __svn_version__.py file from SVN revision number, it will be removed after python exits but will be available when sdist, etc commands are executed. Notes If __svn_version__.py existed before, nothing is done. This is intended for working with source directories that are in an SVN repository.
sys.version_info  
A tuple containing the five components of the version number: major, minor, micro, releaselevel, and serial. All values except releaselevel are integers; the release level is 'alpha', 'beta', 'candidate', or 'final'. The version_info value corresponding to the Python version 2.0 is (2, 0, 0, 'final', 0). The components can also be accessed by name, so sys.version_info[0] is equivalent to sys.version_info.major and so on.  Changed in version 3.1: Added named component attributes.
platform.python_version()  
Returns the Python version as string 'major.minor.patchlevel'. Note that unlike the Python sys.version, the returned value will always include the patchlevel (it defaults to 0).
ArgumentParser.exit(status=0, message=None)  
This method terminates the program, exiting with the specified status and, if given, it prints a message before that. The user can override this method to handle these steps differently: class ErrorCatchingArgumentParser(argparse.ArgumentParser):
    def exit(self, status=0, message=None):
        if status:
            raise Exception(f'Exiting because of an error: {message}')
        exit(status)
sys.version  
A string containing the version number of the Python interpreter plus additional information on the build number and compiler used. This string is displayed when the interactive interpreter is started. Do not extract version information out of it, rather, use version_info and the functions provided by the platform module.
def f_15405636(parser):
    """argparse add argument with flag '--version' and version action of '%(prog)s 2.0' to parser `parser`
    """
    return  
 --------------------

def f_17665809(d):
    """remove key 'c' from dictionary `d`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
tf.raw_ops.BoostedTreesSparseCalculateBestFeatureSplit Calculates gains for each feature and returns the best possible split information for the feature.  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.raw_ops.BoostedTreesSparseCalculateBestFeatureSplit  
tf.raw_ops.BoostedTreesSparseCalculateBestFeatureSplit(
    node_id_range, stats_summary_indices, stats_summary_values, stats_summary_shape,
    l1, l2, tree_complexity, min_node_weight, logits_dimension,
    split_type='inequality', name=None
)
 The split information is the best threshold (bucket id), gains and left/right node contributions per node for each feature. It is possible that not all nodes can be split on each feature. Hence, the list of possible nodes can differ between the features. Therefore, we return node_ids_list for each feature, containing the list of nodes that this feature can be used to split. In this manner, the output is the best split per features and per node, so that it needs to be combined later to produce the best split for each node (among all possible features). The output shapes are compatible in a way that the first dimension of all tensors are the same and equal to the number of possible split nodes for each feature.
 


 Args
  node_id_range   A Tensor of type int32. A Rank 1 tensor (shape=[2]) to specify the range [first, last) of node ids to process within stats_summary_list. The nodes are iterated between the two nodes specified by the tensor, as like for node_id in range(node_id_range[0], node_id_range[1]) (Note that the last index node_id_range[1] is exclusive).  
  stats_summary_indices   A Tensor of type int32. A Rank 2 int64 tensor of dense shape N, 4 for accumulated stats summary (gradient/hessian) per node per bucket for each feature. The second dimension contains node id, feature dimension, bucket id, and stats dim. stats dim is the sum of logits dimension and hessian dimension, hessian dimension can either be logits dimension if diagonal hessian is used, or logits dimension^2 if full hessian is used.  
  stats_summary_values   A Tensor of type float32. A Rank 1 float tensor of dense shape N, which supplies the values for each element in summary_indices.  
  stats_summary_shape   A Tensor of type int32. A Rank 1 float tensor of dense shape [4], which specifies the dense shape of the sparse tensor, which is [num tree nodes, feature dimensions, num buckets, stats dim].  
  l1   A Tensor of type float32. l1 regularization factor on leaf weights, per instance based.  
  l2   A Tensor of type float32. l2 regularization factor on leaf weights, per instance based.  
  tree_complexity   A Tensor of type float32. adjustment to the gain, per leaf based.  
  min_node_weight   A Tensor of type float32. minimum avg of hessians in a node before required for the node to be considered for splitting.  
  logits_dimension   An int that is >= 1. The dimension of logit, i.e., number of classes.  
  split_type   An optional string from: "inequality". Defaults to "inequality". A string indicating if this Op should perform inequality split or equality split.  
  name   A name for the operation (optional).   
 


 Returns   A tuple of Tensor objects (node_ids, gains, feature_dimensions, thresholds, left_node_contribs, right_node_contribs, split_with_default_directions).     node_ids   A Tensor of type int32.  
  gains   A Tensor of type float32.  
  feature_dimensions   A Tensor of type int32.  
  thresholds   A Tensor of type int32.  
  left_node_contribs   A Tensor of type float32.  
  right_node_contribs   A Tensor of type float32.  
  split_with_default_directions   A Tensor of type string.
tf.raw_ops.BoostedTreesCalculateBestFeatureSplit Calculates gains for each feature and returns the best possible split information for the feature.  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.raw_ops.BoostedTreesCalculateBestFeatureSplit  
tf.raw_ops.BoostedTreesCalculateBestFeatureSplit(
    node_id_range, stats_summary, l1, l2, tree_complexity, min_node_weight,
    logits_dimension, split_type='inequality', name=None
)
 The split information is the best threshold (bucket id), gains and left/right node contributions per node for each feature. It is possible that not all nodes can be split on each feature. Hence, the list of possible nodes can differ between the features. Therefore, we return node_ids_list for each feature, containing the list of nodes that this feature can be used to split. In this manner, the output is the best split per features and per node, so that it needs to be combined later to produce the best split for each node (among all possible features). The output shapes are compatible in a way that the first dimension of all tensors are the same and equal to the number of possible split nodes for each feature.
 


 Args
  node_id_range   A Tensor of type int32. A Rank 1 tensor (shape=[2]) to specify the range [first, last) of node ids to process within stats_summary_list. The nodes are iterated between the two nodes specified by the tensor, as like for node_id in range(node_id_range[0], node_id_range[1]) (Note that the last index node_id_range[1] is exclusive).  
  stats_summary   A Tensor of type float32. A Rank 4 tensor (#shape=[max_splits, feature_dims, bucket, stats_dims]) for accumulated stats summary (gradient/hessian) per node, per dimension, per buckets for each feature. The first dimension of the tensor is the maximum number of splits, and thus not all elements of it will be used, but only the indexes specified by node_ids will be used.  
  l1   A Tensor of type float32. l1 regularization factor on leaf weights, per instance based.  
  l2   A Tensor of type float32. l2 regularization factor on leaf weights, per instance based.  
  tree_complexity   A Tensor of type float32. adjustment to the gain, per leaf based.  
  min_node_weight   A Tensor of type float32. minimum avg of hessians in a node before required for the node to be considered for splitting.  
  logits_dimension   An int that is >= 1. The dimension of logit, i.e., number of classes.  
  split_type   An optional string from: "inequality", "equality". Defaults to "inequality". A string indicating if this Op should perform inequality split or equality split.  
  name   A name for the operation (optional).   
 


 Returns   A tuple of Tensor objects (node_ids, gains, feature_dimensions, thresholds, left_node_contribs, right_node_contribs, split_with_default_directions).     node_ids   A Tensor of type int32.  
  gains   A Tensor of type float32.  
  feature_dimensions   A Tensor of type int32.  
  thresholds   A Tensor of type int32.  
  left_node_contribs   A Tensor of type float32.  
  right_node_contribs   A Tensor of type float32.  
  split_with_default_directions   A Tensor of type string.
tf.raw_ops.BoostedTreesCalculateBestFeatureSplitV2 Calculates gains for each feature and returns the best possible split information for each node. However, if no split is found, then no split information is returned for that node.  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.raw_ops.BoostedTreesCalculateBestFeatureSplitV2  
tf.raw_ops.BoostedTreesCalculateBestFeatureSplitV2(
    node_id_range, stats_summaries_list, split_types, candidate_feature_ids, l1, l2,
    tree_complexity, min_node_weight, logits_dimension, name=None
)
 The split information is the best threshold (bucket id), gains and left/right node contributions per node for each feature. It is possible that not all nodes can be split on each feature. Hence, the list of possible nodes can differ between the features. Therefore, we return node_ids_list for each feature, containing the list of nodes that this feature can be used to split. In this manner, the output is the best split per features and per node, so that it needs to be combined later to produce the best split for each node (among all possible features). The output shapes are compatible in a way that the first dimension of all tensors are the same and equal to the number of possible split nodes for each feature.
 


 Args
  node_id_range   A Tensor of type int32. A Rank 1 tensor (shape=[2]) to specify the range [first, last) of node ids to process within stats_summary_list. The nodes are iterated between the two nodes specified by the tensor, as like for node_id in range(node_id_range[0], node_id_range[1]) (Note that the last index node_id_range[1] is exclusive).  
  stats_summaries_list   A list of at least 1 Tensor objects with type float32. A list of Rank 4 tensor (#shape=[max_splits, feature_dims, bucket, stats_dims]) for accumulated stats summary (gradient/hessian) per node, per dimension, per buckets for each feature. The first dimension of the tensor is the maximum number of splits, and thus not all elements of it will be used, but only the indexes specified by node_ids will be used.  
  split_types   A Tensor of type string. A Rank 1 tensor indicating if this Op should perform inequality split or equality split per feature.  
  candidate_feature_ids   A Tensor of type int32. Rank 1 tensor with ids for each feature. This is the real id of the feature.  
  l1   A Tensor of type float32. l1 regularization factor on leaf weights, per instance based.  
  l2   A Tensor of type float32. l2 regularization factor on leaf weights, per instance based.  
  tree_complexity   A Tensor of type float32. adjustment to the gain, per leaf based.  
  min_node_weight   A Tensor of type float32. minimum avg of hessians in a node before required for the node to be considered for splitting.  
  logits_dimension   An int that is >= 1. The dimension of logit, i.e., number of classes.  
  name   A name for the operation (optional).   
 


 Returns   A tuple of Tensor objects (node_ids, gains, feature_ids, feature_dimensions, thresholds, left_node_contribs, right_node_contribs, split_with_default_directions).     node_ids   A Tensor of type int32.  
  gains   A Tensor of type float32.  
  feature_ids   A Tensor of type int32.  
  feature_dimensions   A Tensor of type int32.  
  thresholds   A Tensor of type int32.  
  left_node_contribs   A Tensor of type float32.  
  right_node_contribs   A Tensor of type float32.  
  split_with_default_directions   A Tensor of type string.
pandas.read_csv   pandas.read_csv(filepath_or_buffer, sep=NoDefault.no_default, delimiter=None, header='infer', names=NoDefault.no_default, index_col=None, usecols=None, squeeze=None, prefix=NoDefault.no_default, mangle_dupe_cols=True, dtype=None, engine=None, converters=None, true_values=None, false_values=None, skipinitialspace=False, skiprows=None, skipfooter=0, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=False, skip_blank_lines=True, parse_dates=None, infer_datetime_format=False, keep_date_col=False, date_parser=None, dayfirst=False, cache_dates=True, iterator=False, chunksize=None, compression='infer', thousands=None, decimal='.', lineterminator=None, quotechar='"', quoting=0, doublequote=True, escapechar=None, comment=None, encoding=None, encoding_errors='strict', dialect=None, error_bad_lines=None, warn_bad_lines=None, on_bad_lines=None, delim_whitespace=False, low_memory=True, memory_map=False, float_precision=None, storage_options=None)[source]
 
Read a comma-separated values (csv) file into DataFrame. Also supports optionally iterating or breaking of the file into chunks. Additional help can be found in the online docs for IO Tools.  Parameters 
 
filepath_or_buffer:str, path object or file-like object


Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv. If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handle (e.g. via builtin open function) or StringIO.  
sep:str, default ‘,’


Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python’s builtin sniffer tool, csv.Sniffer. In addition, separators longer than 1 character and different from '\s+' will be interpreted as regular expressions and will also force the use of the Python parsing engine. Note that regex delimiters are prone to ignoring quoted data. Regex example: '\r\t'.  
delimiter:str, default None


Alias for sep.  
header:int, list of int, None, default ‘infer’


Row number(s) to use as the column names, and the start of the data. Default behavior is to infer the column names: if no names are passed the behavior is identical to header=0 and column names are inferred from the first line of the file, if column names are passed explicitly then the behavior is identical to header=None. Explicitly pass header=0 to be able to replace existing names. The header can be a list of integers that specify row locations for a multi-index on the columns e.g. [0,1,3]. Intervening rows that are not specified will be skipped (e.g. 2 in this example is skipped). Note that this parameter ignores commented lines and empty lines if skip_blank_lines=True, so header=0 denotes the first line of data rather than the first line of the file.  
names:array-like, optional


List of column names to use. If the file contains a header row, then you should explicitly pass header=0 to override the column names. Duplicates in this list are not allowed.  
index_col:int, str, sequence of int / str, or False, optional, default None


Column(s) to use as the row labels of the DataFrame, either given as string name or column index. If a sequence of int / str is given, a MultiIndex is used. Note: index_col=False can be used to force pandas to not use the first column as the index, e.g. when you have a malformed file with delimiters at the end of each line.  
usecols:list-like or callable, optional


Return a subset of the columns. If list-like, all elements must either be positional (i.e. integer indices into the document columns) or strings that correspond to column names provided either by the user in names or inferred from the document header row(s). If names are given, the document header row(s) are not taken into account. For example, a valid list-like usecols parameter would be [0, 1, 2] or ['foo', 'bar', 'baz']. Element order is ignored, so usecols=[0, 1] is the same as [1, 0]. To instantiate a DataFrame from data with element order preserved use pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']] for columns in ['foo', 'bar'] order or pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']] for ['bar', 'foo'] order. If callable, the callable function will be evaluated against the column names, returning names where the callable function evaluates to True. An example of a valid callable argument would be lambda x: x.upper() in
['AAA', 'BBB', 'DDD']. Using this parameter results in much faster parsing time and lower memory usage.  
squeeze:bool, default False


If the parsed data only contains one column then return a Series.  Deprecated since version 1.4.0: Append .squeeze("columns") to the call to read_csv to squeeze the data.   
prefix:str, optional


Prefix to add to column numbers when no header, e.g. ‘X’ for X0, X1, …  Deprecated since version 1.4.0: Use a list comprehension on the DataFrame’s columns after calling read_csv.   
mangle_dupe_cols:bool, default True


Duplicate columns will be specified as ‘X’, ‘X.1’, …’X.N’, rather than ‘X’…’X’. Passing in False will cause data to be overwritten if there are duplicate names in the columns.  
dtype:Type name or dict of column -> type, optional


Data type for data or columns. E.g. {‘a’: np.float64, ‘b’: np.int32, ‘c’: ‘Int64’} Use str or object together with suitable na_values settings to preserve and not interpret dtype. If converters are specified, they will be applied INSTEAD of dtype conversion.  
engine:{‘c’, ‘python’, ‘pyarrow’}, optional


Parser engine to use. The C and pyarrow engines are faster, while the python engine is currently more feature-complete. Multithreading is currently only supported by the pyarrow engine.  New in version 1.4.0: The “pyarrow” engine was added as an experimental engine, and some features are unsupported, or may not work correctly, with this engine.   
converters:dict, optional


Dict of functions for converting values in certain columns. Keys can either be integers or column labels.  
true_values:list, optional


Values to consider as True.  
false_values:list, optional


Values to consider as False.  
skipinitialspace:bool, default False


Skip spaces after delimiter.  
skiprows:list-like, int or callable, optional


Line numbers to skip (0-indexed) or number of lines to skip (int) at the start of the file. If callable, the callable function will be evaluated against the row indices, returning True if the row should be skipped and False otherwise. An example of a valid callable argument would be lambda x: x in [0, 2].  
skipfooter:int, default 0


Number of lines at bottom of file to skip (Unsupported with engine=’c’).  
nrows:int, optional


Number of rows of file to read. Useful for reading pieces of large files.  
na_values:scalar, str, list-like, or dict, optional


Additional strings to recognize as NA/NaN. If dict passed, specific per-column NA values. By default the following values are interpreted as NaN: ‘’, ‘#N/A’, ‘#N/A N/A’, ‘#NA’, ‘-1.#IND’, ‘-1.#QNAN’, ‘-NaN’, ‘-nan’, ‘1.#IND’, ‘1.#QNAN’, ‘<NA>’, ‘N/A’, ‘NA’, ‘NULL’, ‘NaN’, ‘n/a’, ‘nan’, ‘null’.  
keep_default_na:bool, default True


Whether or not to include the default NaN values when parsing the data. Depending on whether na_values is passed in, the behavior is as follows:  If keep_default_na is True, and na_values are specified, na_values is appended to the default NaN values used for parsing. If keep_default_na is True, and na_values are not specified, only the default NaN values are used for parsing. If keep_default_na is False, and na_values are specified, only the NaN values specified na_values are used for parsing. If keep_default_na is False, and na_values are not specified, no strings will be parsed as NaN.  Note that if na_filter is passed in as False, the keep_default_na and na_values parameters will be ignored.  
na_filter:bool, default True


Detect missing value markers (empty strings and the value of na_values). In data without any NAs, passing na_filter=False can improve the performance of reading a large file.  
verbose:bool, default False


Indicate number of NA values placed in non-numeric columns.  
skip_blank_lines:bool, default True


If True, skip over blank lines rather than interpreting as NaN values.  
parse_dates:bool or list of int or names or list of lists or dict, default False


The behavior is as follows:  boolean. If True -> try parsing the index. list of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3 each as a separate date column. list of lists. e.g. If [[1, 3]] -> combine columns 1 and 3 and parse as a single date column. dict, e.g. {‘foo’ : [1, 3]} -> parse columns 1, 3 as date and call result ‘foo’  If a column or index cannot be represented as an array of datetimes, say because of an unparsable value or a mixture of timezones, the column or index will be returned unaltered as an object data type. For non-standard datetime parsing, use pd.to_datetime after pd.read_csv. To parse an index or column with a mixture of timezones, specify date_parser to be a partially-applied pandas.to_datetime() with utc=True. See Parsing a CSV with mixed timezones for more. Note: A fast-path exists for iso8601-formatted dates.  
infer_datetime_format:bool, default False


If True and parse_dates is enabled, pandas will attempt to infer the format of the datetime strings in the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the parsing speed by 5-10x.  
keep_date_col:bool, default False


If True and parse_dates specifies combining multiple columns then keep the original columns.  
date_parser:function, optional


Function to use for converting a sequence of string columns to an array of datetime instances. The default uses dateutil.parser.parser to do the conversion. Pandas will try to call date_parser in three different ways, advancing to the next if an exception occurs: 1) Pass one or more arrays (as defined by parse_dates) as arguments; 2) concatenate (row-wise) the string values from the columns defined by parse_dates into a single array and pass that; and 3) call date_parser once for each row using one or more strings (corresponding to the columns defined by parse_dates) as arguments.  
dayfirst:bool, default False


DD/MM format dates, international and European format.  
cache_dates:bool, default True


If True, use a cache of unique, converted dates to apply the datetime conversion. May produce significant speed-up when parsing duplicate date strings, especially ones with timezone offsets.  New in version 0.25.0.   
iterator:bool, default False


Return TextFileReader object for iteration or getting chunks with get_chunk().  Changed in version 1.2: TextFileReader is a context manager.   
chunksize:int, optional


Return TextFileReader object for iteration. See the IO Tools docs for more information on iterator and chunksize.  Changed in version 1.2: TextFileReader is a context manager.   
compression:str or dict, default ‘infer’


For on-the-fly decompression of on-disk data. If ‘infer’ and ‘%s’ is path-like, then detect compression from the following extensions: ‘.gz’, ‘.bz2’, ‘.zip’, ‘.xz’, or ‘.zst’ (otherwise no compression). If using ‘zip’, the ZIP file must contain only one data file to be read in. Set to None for no decompression. Can also be a dict with key 'method' set to one of {'zip', 'gzip', 'bz2', 'zstd'} and other key-value pairs are forwarded to zipfile.ZipFile, gzip.GzipFile, bz2.BZ2File, or zstandard.ZstdDecompressor, respectively. As an example, the following could be passed for Zstandard decompression using a custom compression dictionary: compression={'method': 'zstd', 'dict_data': my_compression_dict}.  Changed in version 1.4.0: Zstandard support.   
thousands:str, optional


Thousands separator.  
decimal:str, default ‘.’


Character to recognize as decimal point (e.g. use ‘,’ for European data).  
lineterminator:str (length 1), optional


Character to break file into lines. Only valid with C parser.  
quotechar:str (length 1), optional


The character used to denote the start and end of a quoted item. Quoted items can include the delimiter and it will be ignored.  
quoting:int or csv.QUOTE_* instance, default 0


Control field quoting behavior per csv.QUOTE_* constants. Use one of QUOTE_MINIMAL (0), QUOTE_ALL (1), QUOTE_NONNUMERIC (2) or QUOTE_NONE (3).  
doublequote:bool, default True


When quotechar is specified and quoting is not QUOTE_NONE, indicate whether or not to interpret two consecutive quotechar elements INSIDE a field as a single quotechar element.  
escapechar:str (length 1), optional


One-character string used to escape other characters.  
comment:str, optional


Indicates remainder of line should not be parsed. If found at the beginning of a line, the line will be ignored altogether. This parameter must be a single character. Like empty lines (as long as skip_blank_lines=True), fully commented lines are ignored by the parameter header but not by skiprows. For example, if comment='#', parsing #empty\na,b,c\n1,2,3 with header=0 will result in ‘a,b,c’ being treated as the header.  
encoding:str, optional


Encoding to use for UTF when reading/writing (ex. ‘utf-8’). List of Python standard encodings .  Changed in version 1.2: When encoding is None, errors="replace" is passed to open(). Otherwise, errors="strict" is passed to open(). This behavior was previously only the case for engine="python".   Changed in version 1.3.0: encoding_errors is a new argument. encoding has no longer an influence on how encoding errors are handled.   
encoding_errors:str, optional, default “strict”


How encoding errors are treated. List of possible values .  New in version 1.3.0.   
dialect:str or csv.Dialect, optional


If provided, this parameter will override values (default or not) for the following parameters: delimiter, doublequote, escapechar, skipinitialspace, quotechar, and quoting. If it is necessary to override values, a ParserWarning will be issued. See csv.Dialect documentation for more details.  
error_bad_lines:bool, optional, default None


Lines with too many fields (e.g. a csv line with too many commas) will by default cause an exception to be raised, and no DataFrame will be returned. If False, then these “bad lines” will be dropped from the DataFrame that is returned.  Deprecated since version 1.3.0: The on_bad_lines parameter should be used instead to specify behavior upon encountering a bad line instead.   
warn_bad_lines:bool, optional, default None


If error_bad_lines is False, and warn_bad_lines is True, a warning for each “bad line” will be output.  Deprecated since version 1.3.0: The on_bad_lines parameter should be used instead to specify behavior upon encountering a bad line instead.   
on_bad_lines:{‘error’, ‘warn’, ‘skip’} or callable, default ‘error’


Specifies what to do upon encountering a bad line (a line with too many fields). Allowed values are :  
 ‘error’, raise an Exception when a bad line is encountered. ‘warn’, raise a warning when a bad line is encountered and skip that line. ‘skip’, skip bad lines without raising or warning when they are encountered.  
  New in version 1.3.0:   callable, function with signature (bad_line: list[str]) -> list[str] | None that will process a single bad line. bad_line is a list of strings split by the sep. If the function returns None`, the bad line will be ignored.
If the function returns a new list of strings with more elements than
expected, a ``ParserWarning will be emitted while dropping extra elements. Only supported when engine="python"    New in version 1.4.0.   
delim_whitespace:bool, default False


Specifies whether or not whitespace (e.g. ' ' or '    ') will be used as the sep. Equivalent to setting sep='\s+'. If this option is set to True, nothing should be passed in for the delimiter parameter.  
low_memory:bool, default True


Internally process the file in chunks, resulting in lower memory use while parsing, but possibly mixed type inference. To ensure no mixed types either set False, or specify the type with the dtype parameter. Note that the entire file is read into a single DataFrame regardless, use the chunksize or iterator parameter to return the data in chunks. (Only valid with C parser).  
memory_map:bool, default False


If a filepath is provided for filepath_or_buffer, map the file object directly onto memory and access the data directly from there. Using this option can improve performance because there is no longer any I/O overhead.  
float_precision:str, optional


Specifies which converter the C engine should use for floating-point values. The options are None or ‘high’ for the ordinary converter, ‘legacy’ for the original lower precision pandas converter, and ‘round_trip’ for the round-trip converter.  Changed in version 1.2.   
storage_options:dict, optional


Extra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib as header options. For other URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are forwarded to fsspec. Please see fsspec and urllib for more details.  New in version 1.2.     Returns 
 DataFrame or TextParser

A comma-separated values (csv) file is returned as two-dimensional data structure with labeled axes.      See also  DataFrame.to_csv

Write DataFrame to a comma-separated values (csv) file.  read_csv

Read a comma-separated values (csv) file into DataFrame.  read_fwf

Read a table of fixed-width formatted lines into DataFrame.    Examples 
>>> pd.read_csv('data.csv')
pandas.merge_asof   pandas.merge_asof(left, right, on=None, left_on=None, right_on=None, left_index=False, right_index=False, by=None, left_by=None, right_by=None, suffixes=('_x', '_y'), tolerance=None, allow_exact_matches=True, direction='backward')[source]
 
Perform a merge by key distance. This is similar to a left-join except that we match on nearest key rather than equal keys. Both DataFrames must be sorted by the key. For each row in the left DataFrame:  
 A “backward” search selects the last row in the right DataFrame whose ‘on’ key is less than or equal to the left’s key. A “forward” search selects the first row in the right DataFrame whose ‘on’ key is greater than or equal to the left’s key. A “nearest” search selects the row in the right DataFrame whose ‘on’ key is closest in absolute distance to the left’s key.  
 The default is “backward” and is compatible in versions below 0.20.0. The direction parameter was added in version 0.20.0 and introduces “forward” and “nearest”. Optionally match on equivalent keys with ‘by’ before searching with ‘on’.  Parameters 
 
left:DataFrame or named Series


right:DataFrame or named Series


on:label


Field name to join on. Must be found in both DataFrames. The data MUST be ordered. Furthermore this must be a numeric column, such as datetimelike, integer, or float. On or left_on/right_on must be given.  
left_on:label


Field name to join on in left DataFrame.  
right_on:label


Field name to join on in right DataFrame.  
left_index:bool


Use the index of the left DataFrame as the join key.  
right_index:bool


Use the index of the right DataFrame as the join key.  
by:column name or list of column names


Match on these columns before performing merge operation.  
left_by:column name


Field names to match on in the left DataFrame.  
right_by:column name


Field names to match on in the right DataFrame.  
suffixes:2-length sequence (tuple, list, …)


Suffix to apply to overlapping column names in the left and right side, respectively.  
tolerance:int or Timedelta, optional, default None


Select asof tolerance within this range; must be compatible with the merge index.  
allow_exact_matches:bool, default True


 If True, allow matching with the same ‘on’ value (i.e. less-than-or-equal-to / greater-than-or-equal-to) If False, don’t match the same ‘on’ value (i.e., strictly less-than / strictly greater-than).   
direction:‘backward’ (default), ‘forward’, or ‘nearest’


Whether to search for prior, subsequent, or closest matches.    Returns 
 
merged:DataFrame

    See also  merge

Merge with a database-style join.  merge_ordered

Merge with optional filling/interpolation.    Examples 
>>> left = pd.DataFrame({"a": [1, 5, 10], "left_val": ["a", "b", "c"]})
>>> left
    a left_val
0   1        a
1   5        b
2  10        c
  
>>> right = pd.DataFrame({"a": [1, 2, 3, 6, 7], "right_val": [1, 2, 3, 6, 7]})
>>> right
   a  right_val
0  1          1
1  2          2
2  3          3
3  6          6
4  7          7
  
>>> pd.merge_asof(left, right, on="a")
    a left_val  right_val
0   1        a          1
1   5        b          3
2  10        c          7
  
>>> pd.merge_asof(left, right, on="a", allow_exact_matches=False)
    a left_val  right_val
0   1        a        NaN
1   5        b        3.0
2  10        c        7.0
  
>>> pd.merge_asof(left, right, on="a", direction="forward")
    a left_val  right_val
0   1        a        1.0
1   5        b        6.0
2  10        c        NaN
  
>>> pd.merge_asof(left, right, on="a", direction="nearest")
    a left_val  right_val
0   1        a          1
1   5        b          6
2  10        c          7
  We can use indexed DataFrames as well. 
>>> left = pd.DataFrame({"left_val": ["a", "b", "c"]}, index=[1, 5, 10])
>>> left
   left_val
1         a
5         b
10        c
  
>>> right = pd.DataFrame({"right_val": [1, 2, 3, 6, 7]}, index=[1, 2, 3, 6, 7])
>>> right
   right_val
1          1
2          2
3          3
6          6
7          7
  
>>> pd.merge_asof(left, right, left_index=True, right_index=True)
   left_val  right_val
1         a          1
5         b          3
10        c          7
  Here is a real-world times-series example 
>>> quotes = pd.DataFrame(
...     {
...         "time": [
...             pd.Timestamp("2016-05-25 13:30:00.023"),
...             pd.Timestamp("2016-05-25 13:30:00.023"),
...             pd.Timestamp("2016-05-25 13:30:00.030"),
...             pd.Timestamp("2016-05-25 13:30:00.041"),
...             pd.Timestamp("2016-05-25 13:30:00.048"),
...             pd.Timestamp("2016-05-25 13:30:00.049"),
...             pd.Timestamp("2016-05-25 13:30:00.072"),
...             pd.Timestamp("2016-05-25 13:30:00.075")
...         ],
...         "ticker": [
...                "GOOG",
...                "MSFT",
...                "MSFT",
...                "MSFT",
...                "GOOG",
...                "AAPL",
...                "GOOG",
...                "MSFT"
...            ],
...            "bid": [720.50, 51.95, 51.97, 51.99, 720.50, 97.99, 720.50, 52.01],
...            "ask": [720.93, 51.96, 51.98, 52.00, 720.93, 98.01, 720.88, 52.03]
...     }
... )
>>> quotes
                     time ticker     bid     ask
0 2016-05-25 13:30:00.023   GOOG  720.50  720.93
1 2016-05-25 13:30:00.023   MSFT   51.95   51.96
2 2016-05-25 13:30:00.030   MSFT   51.97   51.98
3 2016-05-25 13:30:00.041   MSFT   51.99   52.00
4 2016-05-25 13:30:00.048   GOOG  720.50  720.93
5 2016-05-25 13:30:00.049   AAPL   97.99   98.01
6 2016-05-25 13:30:00.072   GOOG  720.50  720.88
7 2016-05-25 13:30:00.075   MSFT   52.01   52.03
  
>>> trades = pd.DataFrame(
...        {
...            "time": [
...                pd.Timestamp("2016-05-25 13:30:00.023"),
...                pd.Timestamp("2016-05-25 13:30:00.038"),
...                pd.Timestamp("2016-05-25 13:30:00.048"),
...                pd.Timestamp("2016-05-25 13:30:00.048"),
...                pd.Timestamp("2016-05-25 13:30:00.048")
...            ],
...            "ticker": ["MSFT", "MSFT", "GOOG", "GOOG", "AAPL"],
...            "price": [51.95, 51.95, 720.77, 720.92, 98.0],
...            "quantity": [75, 155, 100, 100, 100]
...        }
...    )
>>> trades
                     time ticker   price  quantity
0 2016-05-25 13:30:00.023   MSFT   51.95        75
1 2016-05-25 13:30:00.038   MSFT   51.95       155
2 2016-05-25 13:30:00.048   GOOG  720.77       100
3 2016-05-25 13:30:00.048   GOOG  720.92       100
4 2016-05-25 13:30:00.048   AAPL   98.00       100
  By default we are taking the asof of the quotes 
>>> pd.merge_asof(trades, quotes, on="time", by="ticker")
                     time ticker   price  quantity     bid     ask
0 2016-05-25 13:30:00.023   MSFT   51.95        75   51.95   51.96
1 2016-05-25 13:30:00.038   MSFT   51.95       155   51.97   51.98
2 2016-05-25 13:30:00.048   GOOG  720.77       100  720.50  720.93
3 2016-05-25 13:30:00.048   GOOG  720.92       100  720.50  720.93
4 2016-05-25 13:30:00.048   AAPL   98.00       100     NaN     NaN
  We only asof within 2ms between the quote time and the trade time 
>>> pd.merge_asof(
...     trades, quotes, on="time", by="ticker", tolerance=pd.Timedelta("2ms")
... )
                     time ticker   price  quantity     bid     ask
0 2016-05-25 13:30:00.023   MSFT   51.95        75   51.95   51.96
1 2016-05-25 13:30:00.038   MSFT   51.95       155     NaN     NaN
2 2016-05-25 13:30:00.048   GOOG  720.77       100  720.50  720.93
3 2016-05-25 13:30:00.048   GOOG  720.92       100  720.50  720.93
4 2016-05-25 13:30:00.048   AAPL   98.00       100     NaN     NaN
  We only asof within 10ms between the quote time and the trade time and we exclude exact matches on time. However prior data will propagate forward 
>>> pd.merge_asof(
...     trades,
...     quotes,
...     on="time",
...     by="ticker",
...     tolerance=pd.Timedelta("10ms"),
...     allow_exact_matches=False
... )
                     time ticker   price  quantity     bid     ask
0 2016-05-25 13:30:00.023   MSFT   51.95        75     NaN     NaN
1 2016-05-25 13:30:00.038   MSFT   51.95       155   51.97   51.98
2 2016-05-25 13:30:00.048   GOOG  720.77       100     NaN     NaN
3 2016-05-25 13:30:00.048   GOOG  720.92       100     NaN     NaN
4 2016-05-25 13:30:00.048   AAPL   98.00       100     NaN     NaN
def f_41861705(split_df, csv_df):
    """Create new DataFrame object by merging columns "key" of  dataframes `split_df` and `csv_df` and rename the columns from dataframes `split_df` and `csv_df` with suffix `_left` and `_right` respectively
    """
    return  
 --------------------

def f_10697757(s):
    """Split a string `s` by space with `4` splits
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
DEBUG  
Whether debug mode is enabled. When using flask run to start the development server, an interactive debugger will be shown for unhandled exceptions, and the server will be reloaded when code changes. The debug attribute maps to this config key. This is enabled when ENV is 'development' and is overridden by the FLASK_DEBUG environment variable. It may not behave as expected if set in code. Do not enable debug mode when deploying in production. Default: True if ENV is 'development', or False otherwise.
class werkzeug.debug.DebuggedApplication(app, evalex=False, request_key='werkzeug.request', console_path='/console', console_init_func=None, show_hidden_frames=False, pin_security=True, pin_logging=True)  
Enables debugging support for a given application: from werkzeug.debug import DebuggedApplication
from myapp import app
app = DebuggedApplication(app, evalex=True)
 The evalex keyword argument allows evaluating expressions in a traceback’s frame context.  Parameters 
 
app (WSGIApplication) – the WSGI application to run debugged. 
evalex (bool) – enable exception evaluation feature (interactive debugging). This requires a non-forking server. 
request_key (str) – The key that points to the request object in ths environment. This parameter is ignored in current versions. 
console_path (str) – the URL for a general purpose console. 
console_init_func (Optional[Callable[[], Dict[str, Any]]]) – the function that is executed before starting the general purpose console. The return value is used as initial namespace. 
show_hidden_frames (bool) – by default hidden traceback frames are skipped. You can show them by setting this parameter to True. 
pin_security (bool) – can be used to disable the pin based security system. 
pin_logging (bool) – enables the logging of the pin system.   Return type 
None
debug()
loop.set_debug(enabled: bool)  
Set the debug mode of the event loop.  Changed in version 3.7: The new Python Development Mode can now also be used to enable the debug mode.
class werkzeug.middleware.lint.LintMiddleware(app)  
Warns about common errors in the WSGI and HTTP behavior of the server and wrapped application. Some of the issues it checks are:  invalid status codes non-bytes sent to the WSGI server strings returned from the WSGI application non-empty conditional responses unquoted etags relative URLs in the Location header unsafe calls to wsgi.input unclosed iterators  Error information is emitted using the warnings module.  Parameters 
app (WSGIApplication) – The WSGI application to wrap.  Return type 
None   from werkzeug.middleware.lint import LintMiddleware
app = LintMiddleware(app)
def f_16344756(app):
    """enable debug mode on Flask application `app`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
numpy.savetxt   numpy.savetxt(fname, X, fmt='%.18e', delimiter=' ', newline='\n', header='', footer='', comments='# ', encoding=None)[source]
 
Save an array to a text file.  Parameters 
 
fnamefilename or file handle


If the filename ends in .gz, the file is automatically saved in compressed gzip format. loadtxt understands gzipped files transparently.  
X1D or 2D array_like


Data to be saved to a text file.  
fmtstr or sequence of strs, optional


A single format (%10.5f), a sequence of formats, or a multi-format string, e.g. ‘Iteration %d – %10.5f’, in which case delimiter is ignored. For complex X, the legal options for fmt are:  a single specifier, fmt=’%.4e’, resulting in numbers formatted like ‘ (%s+%sj)’ % (fmt, fmt)
 a full string specifying every real and imaginary part, e.g. ‘ %.4e %+.4ej %.4e %+.4ej %.4e %+.4ej’ for 3 columns a list of specifiers, one per column - in this case, the real and imaginary part must have separate specifiers, e.g. [‘%.3e + %.3ej’, ‘(%.15e%+.15ej)’] for 2 columns   
delimiterstr, optional


String or character separating columns.  
newlinestr, optional


String or character separating lines.  New in version 1.5.0.   
headerstr, optional


String that will be written at the beginning of the file.  New in version 1.7.0.   
footerstr, optional


String that will be written at the end of the file.  New in version 1.7.0.   
commentsstr, optional


String that will be prepended to the header and footer strings, to mark them as comments. Default: ‘# ‘, as expected by e.g. numpy.loadtxt.  New in version 1.7.0.   
encoding{None, str}, optional


Encoding used to encode the outputfile. Does not apply to output streams. If the encoding is something other than ‘bytes’ or ‘latin1’ you will not be able to load the file in NumPy versions < 1.14. Default is ‘latin1’.  New in version 1.14.0.       See also  save

Save an array to a binary file in NumPy .npy format  savez

Save several arrays into an uncompressed .npz archive  savez_compressed

Save several arrays into a compressed .npz archive    Notes Further explanation of the fmt parameter (%[flag]width[.precision]specifier):  flags:

- : left justify + : Forces to precede result with + or -. 0 : Left pad the number with zeros instead of space (see width).  width:

Minimum number of characters to be printed. The value is not truncated if it has more characters.  precision:

 For integer specifiers (eg. d,i,o,x), the minimum number of digits. For e, E and f specifiers, the number of digits to print after the decimal point. For g and G, the maximum number of significant digits. For s, the maximum number of characters.   specifiers:

c : character d or i : signed decimal integer e or E : scientific notation with e or E. f : decimal floating point g,G : use the shorter of e,E or f o : signed octal s : string of characters u : unsigned decimal integer x,X : unsigned hexadecimal integer   This explanation of fmt is not complete, for an exhaustive specification see [1]. References  1 
Format Specification Mini-Language, Python Documentation.   Examples >>> x = y = z = np.arange(0.0,5.0,1.0)
>>> np.savetxt('test.out', x, delimiter=',')   # X is an array
>>> np.savetxt('test.out', (x,y,z))   # x,y,z equal sized 1D arrays
>>> np.savetxt('test.out', x, fmt='%1.4e')   # use exponential notation
handleError(record)  
This method should be called from handlers when an exception is encountered during an emit() call. If the module-level attribute raiseExceptions is False, exceptions get silently ignored. This is what is mostly wanted for a logging system - most users will not care about errors in the logging system, they are more interested in application errors. You could, however, replace this with a custom handler if you wish. The specified record is the one which was being processed when the exception occurred. (The default value of raiseExceptions is True, as that is more useful during development).
predict(X) [source]
 
Predict the labels for the data samples in X using trained model.  Parameters 
 
Xarray-like of shape (n_samples, n_features) 

List of n_features-dimensional data points. Each row corresponds to a single data point.    Returns 
 
labelsarray, shape (n_samples,) 

Component labels.
curses.doupdate()  
Update the physical screen. The curses library keeps two data structures, one representing the current physical screen contents and a virtual screen representing the desired next state. The doupdate() ground updates the physical screen to match the virtual screen. The virtual screen may be updated by a noutrefresh() call after write operations such as addstr() have been performed on a window. The normal refresh() call is simply noutrefresh() followed by doupdate(); if you have to update multiple windows, you can speed performance and perhaps reduce screen flicker by issuing noutrefresh() calls on all windows, followed by a single doupdate().
fit(X, y) [source]
 
Run score function on (X, y) and get the appropriate features.  Parameters 
 
Xarray-like of shape (n_samples, n_features) 

The training input samples.  
yarray-like of shape (n_samples,) 

The target values (class labels in classification, real numbers in regression).    Returns 
 
selfobject
def f_40133826(mylist):
    """python save list `mylist` to file object 'save.txt'
    """
     
 --------------------

def f_4490961(P, T):
    """Multiply a matrix `P` with a 3d tensor `T` in scipy
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
tf.experimental.numpy.zeros TensorFlow variant of NumPy's zeros. 
tf.experimental.numpy.zeros(
    shape, dtype=float
)
 See the NumPy documentation for numpy.zeros.
tf.zeros     View source on GitHub    Creates a tensor with all elements set to zero.  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.zeros  
tf.zeros(
    shape, dtype=tf.dtypes.float32, name=None
)
 See also tf.zeros_like, tf.ones, tf.fill, tf.eye. This operation returns a tensor of type dtype with shape shape and all elements set to zero. 
tf.zeros([3, 4], tf.int32)
<tf.Tensor: shape=(3, 4), dtype=int32, numpy=
array([[0, 0, 0, 0],
       [0, 0, 0, 0],
       [0, 0, 0, 0]], dtype=int32)>

 


 Args
  shape   A list of integers, a tuple of integers, or a 1-D Tensor of type int32.  
  dtype   The DType of an element in the resulting Tensor.  
  name   Optional string. A name for the operation.   
 


 Returns   A Tensor with all elements set to zero.
numpy.ma.zeros   ma.zeros(shape, dtype=float, order='C', *, like=None) = <numpy.ma.core._convert2ma object>
 
Return a new array of given shape and type, filled with zeros.  Parameters 
 
shapeint or tuple of ints


Shape of the new array, e.g., (2, 3) or 2.  
dtypedata-type, optional


The desired data-type for the array, e.g., numpy.int8. Default is numpy.float64.  
order{‘C’, ‘F’}, optional, default: ‘C’


Whether to store multi-dimensional data in row-major (C-style) or column-major (Fortran-style) order in memory.  
likearray_like


Reference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as like supports the __array_function__ protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.  New in version 1.20.0.     Returns 
 
outMaskedArray


Array of zeros with the given shape, dtype, and order.      See also  zeros_like

Return an array of zeros with shape and type of input.  empty

Return a new uninitialized array.  ones

Return a new array setting values to one.  full

Return a new array of given shape filled with value.    Examples >>> np.zeros(5)
array([ 0.,  0.,  0.,  0.,  0.])
 >>> np.zeros((5,), dtype=int)
array([0, 0, 0, 0, 0])
 >>> np.zeros((2, 1))
array([[ 0.],
       [ 0.]])
 >>> s = (2,2)
>>> np.zeros(s)
array([[ 0.,  0.],
       [ 0.,  0.]])
 >>> np.zeros((2,), dtype=[('x', 'i4'), ('y', 'i4')]) # custom dtype
array([(0, 0), (0, 0)],
      dtype=[('x', '<i4'), ('y', '<i4')])
numpy.zeros   numpy.zeros(shape, dtype=float, order='C', *, like=None)
 
Return a new array of given shape and type, filled with zeros.  Parameters 
 
shapeint or tuple of ints


Shape of the new array, e.g., (2, 3) or 2.  
dtypedata-type, optional


The desired data-type for the array, e.g., numpy.int8. Default is numpy.float64.  
order{‘C’, ‘F’}, optional, default: ‘C’


Whether to store multi-dimensional data in row-major (C-style) or column-major (Fortran-style) order in memory.  
likearray_like


Reference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as like supports the __array_function__ protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.  New in version 1.20.0.     Returns 
 
outndarray


Array of zeros with the given shape, dtype, and order.      See also  zeros_like

Return an array of zeros with shape and type of input.  empty

Return a new uninitialized array.  ones

Return a new array setting values to one.  full

Return a new array of given shape filled with value.    Examples >>> np.zeros(5)
array([ 0.,  0.,  0.,  0.,  0.])
 >>> np.zeros((5,), dtype=int)
array([0, 0, 0, 0, 0])
 >>> np.zeros((2, 1))
array([[ 0.],
       [ 0.]])
 >>> s = (2,2)
>>> np.zeros(s)
array([[ 0.,  0.],
       [ 0.,  0.]])
 >>> np.zeros((2,), dtype=[('x', 'i4'), ('y', 'i4')]) # custom dtype
array([(0, 0), (0, 0)],
      dtype=[('x', '<i4'), ('y', '<i4')])
numpy.zeros_like   numpy.zeros_like(a, dtype=None, order='K', subok=True, shape=None)[source]
 
Return an array of zeros with the same shape and type as a given array.  Parameters 
 
aarray_like


The shape and data-type of a define these same attributes of the returned array.  
dtypedata-type, optional


Overrides the data type of the result.  New in version 1.6.0.   
order{‘C’, ‘F’, ‘A’, or ‘K’}, optional


Overrides the memory layout of the result. ‘C’ means C-order, ‘F’ means F-order, ‘A’ means ‘F’ if a is Fortran contiguous, ‘C’ otherwise. ‘K’ means match the layout of a as closely as possible.  New in version 1.6.0.   
subokbool, optional.


If True, then the newly created array will use the sub-class type of a, otherwise it will be a base-class array. Defaults to True.  
shapeint or sequence of ints, optional.


Overrides the shape of the result. If order=’K’ and the number of dimensions is unchanged, will try to keep order, otherwise, order=’C’ is implied.  New in version 1.17.0.     Returns 
 
outndarray


Array of zeros with the same shape and type as a.      See also  empty_like

Return an empty array with shape and type of input.  ones_like

Return an array of ones with shape and type of input.  full_like

Return a new array with shape of input filled with value.  zeros

Return a new array setting values to zero.    Examples >>> x = np.arange(6)
>>> x = x.reshape((2, 3))
>>> x
array([[0, 1, 2],
       [3, 4, 5]])
>>> np.zeros_like(x)
array([[0, 0, 0],
       [0, 0, 0]])
 >>> y = np.arange(3, dtype=float)
>>> y
array([0., 1., 2.])
>>> np.zeros_like(y)
array([0.,  0.,  0.])
def f_2173087():
    """Create 3d array of zeroes of size `(3,3,3)`
    """
    return  
 --------------------

def f_6266727(content):
    """cut off the last word of a sentence `content`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
numpy.float32[source]
 
alias of numpy.single
tf.experimental.numpy.asarray TensorFlow variant of NumPy's asarray. 
tf.experimental.numpy.asarray(
    a, dtype=None
)
 Unsupported arguments: order. See the NumPy documentation for numpy.asarray.
tf.experimental.numpy.array TensorFlow variant of NumPy's array. 
tf.experimental.numpy.array(
    val, dtype=None, copy=True, ndmin=0
)
 Since Tensors are immutable, a copy is made only if val is placed on a different device than the current one. Even if copy is False, a new Tensor may need to be built to satisfy dtype and ndim. This is used only if val is an ndarray or a Tensor. See the NumPy documentation for numpy.array.
numpy.float64[source]
 
alias of numpy.double
sklearn.utils.as_float_array  
sklearn.utils.as_float_array(X, *, copy=True, force_all_finite=True) [source]
 
Converts an array-like to an array of floats. The new dtype will be np.float32 or np.float64, depending on the original type. The function can create a copy or modify the argument depending on the argument copy.  Parameters 
 
X{array-like, sparse matrix} 

copybool, default=True 

If True, a copy of X will be created. If False, a copy may still be returned if X’s dtype is not a floating point type.  
force_all_finitebool or ‘allow-nan’, default=True 

Whether to raise an error on np.inf, np.nan, pd.NA in X. The possibilities are:  True: Force all values of X to be finite. False: accepts np.inf, np.nan, pd.NA in X. ‘allow-nan’: accepts only np.nan and pd.NA values in X. Values cannot be infinite.   New in version 0.20: force_all_finite accepts the string 'allow-nan'.   Changed in version 0.23: Accepts pd.NA and converts it into np.nan     Returns 
 
XT{ndarray, sparse matrix} 

An array of type float.
def f_30385151(x):
    """convert scalar `x` to array
    """
     
 --------------------

def f_15856127(L):
    """sum all elements of nested list `L`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
classmethod float.fromhex(s)  
Class method to return the float represented by a hexadecimal string s. The string s may have leading and trailing whitespace.
class float([x])  
Return a floating point number constructed from a number or string x. If the argument is a string, it should contain a decimal number, optionally preceded by a sign, and optionally embedded in whitespace. The optional sign may be '+' or '-'; a '+' sign has no effect on the value produced. The argument may also be a string representing a NaN (not-a-number), or a positive or negative infinity. More precisely, the input must conform to the following grammar after leading and trailing whitespace characters are removed: 
sign           ::=  "+" | "-"
infinity       ::=  "Infinity" | "inf"
nan            ::=  "nan"
numeric_value  ::=  floatnumber | infinity | nan
numeric_string ::=  [sign] numeric_value
 Here floatnumber is the form of a Python floating-point literal, described in Floating point literals. Case is not significant, so, for example, “inf”, “Inf”, “INFINITY” and “iNfINity” are all acceptable spellings for positive infinity. Otherwise, if the argument is an integer or a floating point number, a floating point number with the same value (within Python’s floating point precision) is returned. If the argument is outside the range of a Python float, an OverflowError will be raised. For a general Python object x, float(x) delegates to x.__float__(). If __float__() is not defined then it falls back to __index__(). If no argument is given, 0.0 is returned. Examples: >>> float('+1.23')
1.23
>>> float('   -12345\n')
-12345.0
>>> float('1e-003')
0.001
>>> float('+1E6')
1000000.0
>>> float('-Infinity')
-inf
 The float type is described in Numeric Types — int, float, complex.  Changed in version 3.6: Grouping digits with underscores as in code literals is allowed.   Changed in version 3.7: x is now a positional-only parameter.   Changed in version 3.8: Falls back to __index__() if __float__() is not defined.
float.hex()  
Return a representation of a floating-point number as a hexadecimal string. For finite floating-point numbers, this representation will always include a leading 0x and a trailing p and exponent.
locale.atof(string)  
Converts a string to a floating point number, following the LC_NUMERIC settings.
classmethod fromhex(string)  
This bytes class method returns a bytes object, decoding the given string object. The string must contain two hexadecimal digits per byte, with ASCII whitespace being ignored. >>> bytes.fromhex('2Ef0 F1f2  ')
b'.\xf0\xf1\xf2'
  Changed in version 3.7: bytes.fromhex() now skips all ASCII whitespace in the string, not just spaces.
def f_1592158():
    """convert hex string '470FC614' to a float number
    """
    return  
 --------------------

def f_5010536(my_dict):
    """Multiple each value by `2` for all keys in a dictionary `my_dict`
    """
     
 --------------------
Please refer to the following documentation to generate the code:
curses.napms(ms)  
Sleep for ms milliseconds.
curses.def_shell_mode()  
Save the current terminal mode as the “shell” mode, the mode when the running program is not using curses. (Its counterpart is the “program” mode, when the program is using curses capabilities.) Subsequent calls to reset_shell_mode() will restore this mode.
time.sleep(secs)  
Suspend execution of the calling thread for the given number of seconds. The argument may be a floating point number to indicate a more precise sleep time. The actual suspension time may be less than that requested because any caught signal will terminate the sleep() following execution of that signal’s catching routine. Also, the suspension time may be longer than requested by an arbitrary amount because of the scheduling of other activity in the system.  Changed in version 3.5: The function now sleeps at least secs even if the sleep is interrupted by a signal, except if the signal handler raises an exception (see PEP 475 for the rationale).
math.sin(x)  
Return the sine of x radians.
IDLE Source code: Lib/idlelib/ IDLE is Python’s Integrated Development and Learning Environment. IDLE has the following features:  coded in 100% pure Python, using the tkinter GUI toolkit cross-platform: works mostly the same on Windows, Unix, and macOS Python shell window (interactive interpreter) with colorizing of code input, output, and error messages multi-window text editor with multiple undo, Python colorizing, smart indent, call tips, auto completion, and other features search within any window, replace within editor windows, and search through multiple files (grep) debugger with persistent breakpoints, stepping, and viewing of global and local namespaces configuration, browsers, and other dialogs  Menus IDLE has two main window types, the Shell window and the Editor window. It is possible to have multiple editor windows simultaneously. On Windows and Linux, each has its own top menu. Each menu documented below indicates which window type it is associated with. Output windows, such as used for Edit => Find in Files, are a subtype of editor window. They currently have the same top menu but a different default title and context menu. On macOS, there is one application menu. It dynamically changes according to the window currently selected. It has an IDLE menu, and some entries described below are moved around to conform to Apple guidelines. File menu (Shell and Editor)  New File

Create a new file editing window.  Open…

Open an existing file with an Open dialog.  Recent Files

Open a list of recent files. Click one to open it.  Open Module…

Open an existing module (searches sys.path).    Class Browser

Show functions, classes, and methods in the current Editor file in a tree structure. In the shell, open a module first.  Path Browser

Show sys.path directories, modules, functions, classes and methods in a tree structure.  Save

Save the current window to the associated file, if there is one. Windows that have been changed since being opened or last saved have a * before and after the window title. If there is no associated file, do Save As instead.  Save As…

Save the current window with a Save As dialog. The file saved becomes the new associated file for the window.  Save Copy As…

Save the current window to different file without changing the associated file.  Print Window

Print the current window to the default printer.  Close

Close the current window (ask to save if unsaved).  Exit

Close all windows and quit IDLE (ask to save unsaved windows).   Edit menu (Shell and Editor)  Undo

Undo the last change to the current window. A maximum of 1000 changes may be undone.  Redo

Redo the last undone change to the current window.  Cut

Copy selection into the system-wide clipboard; then delete the selection.  Copy

Copy selection into the system-wide clipboard.  Paste

Insert contents of the system-wide clipboard into the current window.   The clipboard functions are also available in context menus.  Select All

Select the entire contents of the current window.  Find…

Open a search dialog with many options  Find Again

Repeat the last search, if there is one.  Find Selection

Search for the currently selected string, if there is one.  Find in Files…

Open a file search dialog. Put results in a new output window.  Replace…

Open a search-and-replace dialog.  Go to Line

Move the cursor to the beginning of the line requested and make that line visible. A request past the end of the file goes to the end. Clear any selection and update the line and column status.  Show Completions

Open a scrollable list allowing selection of existing names. See Completions in the Editing and navigation section below.  Expand Word

Expand a prefix you have typed to match a full word in the same window; repeat to get a different expansion.  Show call tip

After an unclosed parenthesis for a function, open a small window with function parameter hints. See Calltips in the Editing and navigation section below.  Show surrounding parens

Highlight the surrounding parenthesis.   Format menu (Editor window only)  Indent Region

Shift selected lines right by the indent width (default 4 spaces).  Dedent Region

Shift selected lines left by the indent width (default 4 spaces).  Comment Out Region

Insert ## in front of selected lines.  Uncomment Region

Remove leading # or ## from selected lines.  Tabify Region

Turn leading stretches of spaces into tabs. (Note: We recommend using 4 space blocks to indent Python code.)  Untabify Region

Turn all tabs into the correct number of spaces.  Toggle Tabs

Open a dialog to switch between indenting with spaces and tabs.  New Indent Width

Open a dialog to change indent width. The accepted default by the Python community is 4 spaces.  Format Paragraph

Reformat the current blank-line-delimited paragraph in comment block or multiline string or selected line in a string. All lines in the paragraph will be formatted to less than N columns, where N defaults to 72.  Strip trailing whitespace

Remove trailing space and other whitespace characters after the last non-whitespace character of a line by applying str.rstrip to each line, including lines within multiline strings. Except for Shell windows, remove extra newlines at the end of the file.   Run menu (Editor window only)  Run Module

Do Check Module. If no error, restart the shell to clean the environment, then execute the module. Output is displayed in the Shell window. Note that output requires use of print or write. When execution is complete, the Shell retains focus and displays a prompt. At this point, one may interactively explore the result of execution. This is similar to executing a file with python -i file at a command line.    Run… Customized

Same as Run Module, but run the module with customized settings. Command Line Arguments extend sys.argv as if passed on a command line. The module can be run in the Shell without restarting.    Check Module

Check the syntax of the module currently open in the Editor window. If the module has not been saved IDLE will either prompt the user to save or autosave, as selected in the General tab of the Idle Settings dialog. If there is a syntax error, the approximate location is indicated in the Editor window.    Python Shell

Open or wake up the Python Shell window.   Shell menu (Shell window only)  View Last Restart

Scroll the shell window to the last Shell restart.  Restart Shell

Restart the shell to clean the environment and reset display and exception handling.  Previous History

Cycle through earlier commands in history which match the current entry.  Next History

Cycle through later commands in history which match the current entry.  Interrupt Execution

Stop a running program.   Debug menu (Shell window only)  Go to File/Line

Look on the current line. with the cursor, and the line above for a filename and line number. If found, open the file if not already open, and show the line. Use this to view source lines referenced in an exception traceback and lines found by Find in Files. Also available in the context menu of the Shell window and Output windows.    Debugger (toggle)

When activated, code entered in the Shell or run from an Editor will run under the debugger. In the Editor, breakpoints can be set with the context menu. This feature is still incomplete and somewhat experimental.  Stack Viewer

Show the stack traceback of the last exception in a tree widget, with access to locals and globals.  Auto-open Stack Viewer

Toggle automatically opening the stack viewer on an unhandled exception.   Options menu (Shell and Editor)  Configure IDLE

Open a configuration dialog and change preferences for the following: fonts, indentation, keybindings, text color themes, startup windows and size, additional help sources, and extensions. On macOS, open the configuration dialog by selecting Preferences in the application menu. For more details, see Setting preferences under Help and preferences.   Most configuration options apply to all windows or all future windows. The option items below only apply to the active window.  Show/Hide Code Context (Editor Window only)

Open a pane at the top of the edit window which shows the block context of the code which has scrolled above the top of the window. See Code Context in the Editing and Navigation section below.  Show/Hide Line Numbers (Editor Window only)

Open a column to the left of the edit window which shows the number of each line of text. The default is off, which may be changed in the preferences (see Setting preferences).  Zoom/Restore Height

Toggles the window between normal size and maximum height. The initial size defaults to 40 lines by 80 chars unless changed on the General tab of the Configure IDLE dialog. The maximum height for a screen is determined by momentarily maximizing a window the first time one is zoomed on the screen. Changing screen settings may invalidate the saved height. This toggle has no effect when a window is maximized.   Window menu (Shell and Editor) Lists the names of all open windows; select one to bring it to the foreground (deiconifying it if necessary). Help menu (Shell and Editor)  About IDLE

Display version, copyright, license, credits, and more.  IDLE Help

Display this IDLE document, detailing the menu options, basic editing and navigation, and other tips.  Python Docs

Access local Python documentation, if installed, or start a web browser and open docs.python.org showing the latest Python documentation.  Turtle Demo

Run the turtledemo module with example Python code and turtle drawings.   Additional help sources may be added here with the Configure IDLE dialog under the General tab. See the Help sources subsection below for more on Help menu choices. Context Menus Open a context menu by right-clicking in a window (Control-click on macOS). Context menus have the standard clipboard functions also on the Edit menu.  Cut

Copy selection into the system-wide clipboard; then delete the selection.  Copy

Copy selection into the system-wide clipboard.  Paste

Insert contents of the system-wide clipboard into the current window.   Editor windows also have breakpoint functions. Lines with a breakpoint set are specially marked. Breakpoints only have an effect when running under the debugger. Breakpoints for a file are saved in the user’s .idlerc directory.  Set Breakpoint

Set a breakpoint on the current line.  Clear Breakpoint

Clear the breakpoint on that line.   Shell and Output windows also have the following.  Go to file/line

Same as in Debug menu.   The Shell window also has an output squeezing facility explained in the Python Shell window subsection below.  Squeeze

If the cursor is over an output line, squeeze all the output between the code above and the prompt below down to a ‘Squeezed text’ label.   Editing and navigation Editor windows IDLE may open editor windows when it starts, depending on settings and how you start IDLE. Thereafter, use the File menu. There can be only one open editor window for a given file. The title bar contains the name of the file, the full path, and the version of Python and IDLE running the window. The status bar contains the line number (‘Ln’) and column number (‘Col’). Line numbers start with 1; column numbers with 0. IDLE assumes that files with a known .py* extension contain Python code and that other files do not. Run Python code with the Run menu. Key bindings In this section, ‘C’ refers to the Control key on Windows and Unix and the Command key on macOS.  
Backspace deletes to the left; Del deletes to the right 
C-Backspace delete word left; C-Del delete word to the right Arrow keys and Page Up/Page Down to move around 
C-LeftArrow and C-RightArrow moves by words 
Home/End go to begin/end of line 
C-Home/C-End go to begin/end of file 
Some useful Emacs bindings are inherited from Tcl/Tk:  
C-a beginning of line 
C-e end of line 
C-k kill line (but doesn’t put it in clipboard) 
C-l center window around the insertion point 
C-b go backward one character without deleting (usually you can also use the cursor key for this) 
C-f go forward one character without deleting (usually you can also use the cursor key for this) 
C-p go up one line (usually you can also use the cursor key for this) 
C-d delete next character    Standard keybindings (like C-c to copy and C-v to paste) may work. Keybindings are selected in the Configure IDLE dialog. Automatic indentation After a block-opening statement, the next line is indented by 4 spaces (in the Python Shell window by one tab). After certain keywords (break, return etc.) the next line is dedented. In leading indentation, Backspace deletes up to 4 spaces if they are there. Tab inserts spaces (in the Python Shell window one tab), number depends on Indent width. Currently, tabs are restricted to four spaces due to Tcl/Tk limitations. See also the indent/dedent region commands on the Format menu. Completions Completions are supplied, when requested and available, for module names, attributes of classes or functions, or filenames. Each request method displays a completion box with existing names. (See tab completions below for an exception.) For any box, change the name being completed and the item highlighted in the box by typing and deleting characters; by hitting Up, Down, PageUp, PageDown, Home, and End keys; and by a single click within the box. Close the box with Escape, Enter, and double Tab keys or clicks outside the box. A double click within the box selects and closes. One way to open a box is to type a key character and wait for a predefined interval. This defaults to 2 seconds; customize it in the settings dialog. (To prevent auto popups, set the delay to a large number of milliseconds, such as 100000000.) For imported module names or class or function attributes, type ‘.’. For filenames in the root directory, type os.sep or os.altsep immediately after an opening quote. (On Windows, one can specify a drive first.) Move into subdirectories by typing a directory name and a separator. Instead of waiting, or after a box is closed, open a completion box immediately with Show Completions on the Edit menu. The default hot key is C-space. If one types a prefix for the desired name before opening the box, the first match or near miss is made visible. The result is the same as if one enters a prefix after the box is displayed. Show Completions after a quote completes filenames in the current directory instead of a root directory. Hitting Tab after a prefix usually has the same effect as Show Completions. (With no prefix, it indents.) However, if there is only one match to the prefix, that match is immediately added to the editor text without opening a box. Invoking ‘Show Completions’, or hitting Tab after a prefix, outside of a string and without a preceding ‘.’ opens a box with keywords, builtin names, and available module-level names. When editing code in an editor (as oppose to Shell), increase the available module-level names by running your code and not restarting the Shell thereafter. This is especially useful after adding imports at the top of a file. This also increases possible attribute completions. Completion boxes intially exclude names beginning with ‘_’ or, for modules, not included in ‘__all__’. The hidden names can be accessed by typing ‘_’ after ‘.’, either before or after the box is opened. Calltips A calltip is shown automatically when one types ( after the name of an accessible function. A function name expression may include dots and subscripts. A calltip remains until it is clicked, the cursor is moved out of the argument area, or ) is typed. Whenever the cursor is in the argument part of a definition, select Edit and “Show Call Tip” on the menu or enter its shortcut to display a calltip. The calltip consists of the function’s signature and docstring up to the latter’s first blank line or the fifth non-blank line. (Some builtin functions lack an accessible signature.) A ‘/’ or ‘*’ in the signature indicates that the preceding or following arguments are passed by position or name (keyword) only. Details are subject to change. In Shell, the accessible functions depends on what modules have been imported into the user process, including those imported by Idle itself, and which definitions have been run, all since the last restart. For example, restart the Shell and enter itertools.count(. A calltip appears because Idle imports itertools into the user process for its own use. (This could change.) Enter turtle.write( and nothing appears. Idle does not itself import turtle. The menu entry and shortcut also do nothing. Enter import turtle. Thereafter, turtle.write( will display a calltip. In an editor, import statements have no effect until one runs the file. One might want to run a file after writing import statements, after adding function definitions, or after opening an existing file. Code Context Within an editor window containing Python code, code context can be toggled in order to show or hide a pane at the top of the window. When shown, this pane freezes the opening lines for block code, such as those beginning with class, def, or if keywords, that would have otherwise scrolled out of view. The size of the pane will be expanded and contracted as needed to show the all current levels of context, up to the maximum number of lines defined in the Configure IDLE dialog (which defaults to 15). If there are no current context lines and the feature is toggled on, a single blank line will display. Clicking on a line in the context pane will move that line to the top of the editor. The text and background colors for the context pane can be configured under the Highlights tab in the Configure IDLE dialog. Python Shell window With IDLE’s Shell, one enters, edits, and recalls complete statements. Most consoles and terminals only work with a single physical line at a time. When one pastes code into Shell, it is not compiled and possibly executed until one hits Return. One may edit pasted code first. If one pastes more that one statement into Shell, the result will be a SyntaxError when multiple statements are compiled as if they were one. The editing features described in previous subsections work when entering code interactively. IDLE’s Shell window also responds to the following keys.  
C-c interrupts executing command 
C-d sends end-of-file; closes window if typed at a >>> prompt 
Alt-/ (Expand word) is also useful to reduce typing Command history  
Alt-p retrieves previous command matching what you have typed. On macOS use C-p. 
Alt-n retrieves next. On macOS use C-n. 
Return while on any previous command retrieves that command    Text colors Idle defaults to black on white text, but colors text with special meanings. For the shell, these are shell output, shell error, user output, and user error. For Python code, at the shell prompt or in an editor, these are keywords, builtin class and function names, names following class and def, strings, and comments. For any text window, these are the cursor (when present), found text (when possible), and selected text. Text coloring is done in the background, so uncolorized text is occasionally visible. To change the color scheme, use the Configure IDLE dialog Highlighting tab. The marking of debugger breakpoint lines in the editor and text in popups and dialogs is not user-configurable. Startup and code execution Upon startup with the -s option, IDLE will execute the file referenced by the environment variables IDLESTARTUP or PYTHONSTARTUP. IDLE first checks for IDLESTARTUP; if IDLESTARTUP is present the file referenced is run. If IDLESTARTUP is not present, IDLE checks for PYTHONSTARTUP. Files referenced by these environment variables are convenient places to store functions that are used frequently from the IDLE shell, or for executing import statements to import common modules. In addition, Tk also loads a startup file if it is present. Note that the Tk file is loaded unconditionally. This additional file is .Idle.py and is looked for in the user’s home directory. Statements in this file will be executed in the Tk namespace, so this file is not useful for importing functions to be used from IDLE’s Python shell. Command line usage idle.py [-c command] [-d] [-e] [-h] [-i] [-r file] [-s] [-t title] [-] [arg] ...

-c command  run command in the shell window
-d          enable debugger and open shell window
-e          open editor window
-h          print help message with legal combinations and exit
-i          open shell window
-r file     run file in shell window
-s          run $IDLESTARTUP or $PYTHONSTARTUP first, in shell window
-t title    set title of shell window
-           run stdin in shell (- must be last option before args)
 If there are arguments:  If -, -c, or r is used, all arguments are placed in sys.argv[1:...] and sys.argv[0] is set to '', '-c', or '-r'. No editor window is opened, even if that is the default set in the Options dialog. Otherwise, arguments are files opened for editing and sys.argv reflects the arguments passed to IDLE itself.  Startup failure IDLE uses a socket to communicate between the IDLE GUI process and the user code execution process. A connection must be established whenever the Shell starts or restarts. (The latter is indicated by a divider line that says ‘RESTART’). If the user process fails to connect to the GUI process, it usually displays a Tk error box with a ‘cannot connect’ message that directs the user here. It then exits. One specific connection failure on Unix systems results from misconfigured masquerading rules somewhere in a system’s network setup. When IDLE is started from a terminal, one will see a message starting with ** Invalid host:. The valid value is 127.0.0.1 (idlelib.rpc.LOCALHOST). One can diagnose with tcpconnect -irv 127.0.0.1 6543 in one terminal window and tcplisten <same args> in another. A common cause of failure is a user-written file with the same name as a standard library module, such as random.py and tkinter.py. When such a file is located in the same directory as a file that is about to be run, IDLE cannot import the stdlib file. The current fix is to rename the user file. Though less common than in the past, an antivirus or firewall program may stop the connection. If the program cannot be taught to allow the connection, then it must be turned off for IDLE to work. It is safe to allow this internal connection because no data is visible on external ports. A similar problem is a network mis-configuration that blocks connections. Python installation issues occasionally stop IDLE: multiple versions can clash, or a single installation might need admin access. If one undo the clash, or cannot or does not want to run as admin, it might be easiest to completely remove Python and start over. A zombie pythonw.exe process could be a problem. On Windows, use Task Manager to check for one and stop it if there is. Sometimes a restart initiated by a program crash or Keyboard Interrupt (control-C) may fail to connect. Dismissing the error box or using Restart Shell on the Shell menu may fix a temporary problem. When IDLE first starts, it attempts to read user configuration files in ~/.idlerc/ (~ is one’s home directory). If there is a problem, an error message should be displayed. Leaving aside random disk glitches, this can be prevented by never editing the files by hand. Instead, use the configuration dialog, under Options. Once there is an error in a user configuration file, the best solution may be to delete it and start over with the settings dialog. If IDLE quits with no message, and it was not started from a console, try starting it from a console or terminal (python -m idlelib) and see if this results in an error message. On Unix-based systems with tcl/tk older than 8.6.11 (see About IDLE) certain characters of certain fonts can cause a tk failure with a message to the terminal. This can happen either if one starts IDLE to edit a file with such a character or later when entering such a character. If one cannot upgrade tcl/tk, then re-configure IDLE to use a font that works better. Running user code With rare exceptions, the result of executing Python code with IDLE is intended to be the same as executing the same code by the default method, directly with Python in a text-mode system console or terminal window. However, the different interface and operation occasionally affect visible results. For instance, sys.modules starts with more entries, and threading.active_count() returns 2 instead of 1. By default, IDLE runs user code in a separate OS process rather than in the user interface process that runs the shell and editor. In the execution process, it replaces sys.stdin, sys.stdout, and sys.stderr with objects that get input from and send output to the Shell window. The original values stored in sys.__stdin__, sys.__stdout__, and sys.__stderr__ are not touched, but may be None. Sending print output from one process to a text widget in another is slower than printing to a system terminal in the same process. This has the most effect when printing multiple arguments, as the string for each argument, each separator, the newline are sent separately. For development, this is usually not a problem, but if one wants to print faster in IDLE, format and join together everything one wants displayed together and then print a single string. Both format strings and str.join() can help combine fields and lines. IDLE’s standard stream replacements are not inherited by subprocesses created in the execution process, whether directly by user code or by modules such as multiprocessing. If such subprocess use input from sys.stdin or print or write to sys.stdout or sys.stderr, IDLE should be started in a command line window. The secondary subprocess will then be attached to that window for input and output. If sys is reset by user code, such as with importlib.reload(sys), IDLE’s changes are lost and input from the keyboard and output to the screen will not work correctly. When Shell has the focus, it controls the keyboard and screen. This is normally transparent, but functions that directly access the keyboard and screen will not work. These include system-specific functions that determine whether a key has been pressed and if so, which. The IDLE code running in the execution process adds frames to the call stack that would not be there otherwise. IDLE wraps sys.getrecursionlimit and sys.setrecursionlimit to reduce the effect of the additional stack frames. When user code raises SystemExit either directly or by calling sys.exit, IDLE returns to a Shell prompt instead of exiting. User output in Shell When a program outputs text, the result is determined by the corresponding output device. When IDLE executes user code, sys.stdout and sys.stderr are connected to the display area of IDLE’s Shell. Some of its features are inherited from the underlying Tk Text widget. Others are programmed additions. Where it matters, Shell is designed for development rather than production runs. For instance, Shell never throws away output. A program that sends unlimited output to Shell will eventually fill memory, resulting in a memory error. In contrast, some system text windows only keep the last n lines of output. A Windows console, for instance, keeps a user-settable 1 to 9999 lines, with 300 the default. A Tk Text widget, and hence IDLE’s Shell, displays characters (codepoints) in the BMP (Basic Multilingual Plane) subset of Unicode. Which characters are displayed with a proper glyph and which with a replacement box depends on the operating system and installed fonts. Tab characters cause the following text to begin after the next tab stop. (They occur every 8 ‘characters’). Newline characters cause following text to appear on a new line. Other control characters are ignored or displayed as a space, box, or something else, depending on the operating system and font. (Moving the text cursor through such output with arrow keys may exhibit some surprising spacing behavior.) >>> s = 'a\tb\a<\x02><\r>\bc\nd'  # Enter 22 chars.
>>> len(s)
14
>>> s  # Display repr(s)
'a\tb\x07<\x02><\r>\x08c\nd'
>>> print(s, end='')  # Display s as is.
# Result varies by OS and font.  Try it.
 The repr function is used for interactive echo of expression values. It returns an altered version of the input string in which control codes, some BMP codepoints, and all non-BMP codepoints are replaced with escape codes. As demonstrated above, it allows one to identify the characters in a string, regardless of how they are displayed. Normal and error output are generally kept separate (on separate lines) from code input and each other. They each get different highlight colors. For SyntaxError tracebacks, the normal ‘^’ marking where the error was detected is replaced by coloring the text with an error highlight. When code run from a file causes other exceptions, one may right click on a traceback line to jump to the corresponding line in an IDLE editor. The file will be opened if necessary. Shell has a special facility for squeezing output lines down to a ‘Squeezed text’ label. This is done automatically for output over N lines (N = 50 by default). N can be changed in the PyShell section of the General page of the Settings dialog. Output with fewer lines can be squeezed by right clicking on the output. This can be useful lines long enough to slow down scrolling. Squeezed output is expanded in place by double-clicking the label. It can also be sent to the clipboard or a separate view window by right-clicking the label. Developing tkinter applications IDLE is intentionally different from standard Python in order to facilitate development of tkinter programs. Enter import tkinter as tk;
root = tk.Tk() in standard Python and nothing appears. Enter the same in IDLE and a tk window appears. In standard Python, one must also enter root.update() to see the window. IDLE does the equivalent in the background, about 20 times a second, which is about every 50 milliseconds. Next enter b = tk.Button(root, text='button'); b.pack(). Again, nothing visibly changes in standard Python until one enters root.update(). Most tkinter programs run root.mainloop(), which usually does not return until the tk app is destroyed. If the program is run with python -i or from an IDLE editor, a >>> shell prompt does not appear until mainloop() returns, at which time there is nothing left to interact with. When running a tkinter program from an IDLE editor, one can comment out the mainloop call. One then gets a shell prompt immediately and can interact with the live application. One just has to remember to re-enable the mainloop call when running in standard Python. Running without a subprocess By default, IDLE executes user code in a separate subprocess via a socket, which uses the internal loopback interface. This connection is not externally visible and no data is sent to or received from the Internet. If firewall software complains anyway, you can ignore it. If the attempt to make the socket connection fails, Idle will notify you. Such failures are sometimes transient, but if persistent, the problem may be either a firewall blocking the connection or misconfiguration of a particular system. Until the problem is fixed, one can run Idle with the -n command line switch. If IDLE is started with the -n command line switch it will run in a single process and will not create the subprocess which runs the RPC Python execution server. This can be useful if Python cannot create the subprocess or the RPC socket interface on your platform. However, in this mode user code is not isolated from IDLE itself. Also, the environment is not restarted when Run/Run Module (F5) is selected. If your code has been modified, you must reload() the affected modules and re-import any specific items (e.g. from foo import baz) if the changes are to take effect. For these reasons, it is preferable to run IDLE with the default subprocess if at all possible.  Deprecated since version 3.4.  Help and preferences Help sources Help menu entry “IDLE Help” displays a formatted html version of the IDLE chapter of the Library Reference. The result, in a read-only tkinter text window, is close to what one sees in a web browser. Navigate through the text with a mousewheel, the scrollbar, or up and down arrow keys held down. Or click the TOC (Table of Contents) button and select a section header in the opened box. Help menu entry “Python Docs” opens the extensive sources of help, including tutorials, available at docs.python.org/x.y, where ‘x.y’ is the currently running Python version. If your system has an off-line copy of the docs (this may be an installation option), that will be opened instead. Selected URLs can be added or removed from the help menu at any time using the General tab of the Configure IDLE dialog. Setting preferences The font preferences, highlighting, keys, and general preferences can be changed via Configure IDLE on the Option menu. Non-default user settings are saved in a .idlerc directory in the user’s home directory. Problems caused by bad user configuration files are solved by editing or deleting one or more of the files in .idlerc. On the Font tab, see the text sample for the effect of font face and size on multiple characters in multiple languages. Edit the sample to add other characters of personal interest. Use the sample to select monospaced fonts. If particular characters have problems in Shell or an editor, add them to the top of the sample and try changing first size and then font. On the Highlights and Keys tab, select a built-in or custom color theme and key set. To use a newer built-in color theme or key set with older IDLEs, save it as a new custom theme or key set and it well be accessible to older IDLEs. IDLE on macOS Under System Preferences: Dock, one can set “Prefer tabs when opening documents” to “Always”. This setting is not compatible with the tk/tkinter GUI framework used by IDLE, and it breaks a few IDLE features. Extensions IDLE contains an extension facility. Preferences for extensions can be changed with the Extensions tab of the preferences dialog. See the beginning of config-extensions.def in the idlelib directory for further information. The only current default extension is zzdummy, an example also used for testing.
def f_13745648():
    """running bash script 'sleep.sh'
    """
    return  
 --------------------

def f_44778(l):
    """Join elements of list `l` with a comma `,`
    """
    return  
 --------------------

def f_44778(myList):
    """make a comma-separated string from a list `myList`
    """
     
 --------------------

def f_7286365():
    """reverse the list that contains 1 to 10
    """
    return  
 --------------------

def f_18454570():
    """remove substring 'bag,' from a string 'lamp, bag, mirror'
    """
    return  
 --------------------

def f_4357787(s):
    """Reverse the order of words, delimited by `.`, in string `s`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
time.ctime([secs])  
Convert a time expressed in seconds since the epoch to a string of a form: 'Sun Jun 20 23:21:05 1993' representing local time. The day field is two characters long and is space padded if the day is a single digit, e.g.: 'Wed Jun  9 04:26:40 1993'. If secs is not provided or None, the current time as returned by time() is used. ctime(secs) is equivalent to asctime(localtime(secs)). Locale information is not used by ctime().
http_date(epoch_seconds=None) [source]
 
Formats the time to match the RFC 1123#section-5.2.14 date format as specified by HTTP RFC 7231#section-7.1.1.1. Accepts a floating point number expressed in seconds since the epoch in UTC–such as that outputted by time.time(). If set to None, defaults to the current time. Outputs a string in the format Wdy, DD Mon YYYY HH:MM:SS GMT.
pandas.Timedelta.isoformat   Timedelta.isoformat()
 
Format Timedelta as ISO 8601 Duration like P[n]Y[n]M[n]DT[n]H[n]M[n]S, where the [n] s are replaced by the values. See https://en.wikipedia.org/wiki/ISO_8601#Durations.  Returns 
 str
    See also  Timestamp.isoformat

Function is used to convert the given Timestamp object into the ISO format.    Notes The longest component is days, whose value may be larger than 365. Every component is always included, even if its value is 0. Pandas uses nanosecond precision, so up to 9 decimal places may be included in the seconds component. Trailing 0’s are removed from the seconds component after the decimal. We do not 0 pad components, so it’s …T5H…, not …T05H… Examples 
>>> td = pd.Timedelta(days=6, minutes=50, seconds=3,
...                   milliseconds=10, microseconds=10, nanoseconds=12)
  
>>> td.isoformat()
'P6DT0H50M3.010010012S'
>>> pd.Timedelta(hours=1, seconds=10).isoformat()
'P0DT1H0M10S'
>>> pd.Timedelta(days=500.5).isoformat()
'P500DT12H0M0S'
shlex.quote(s)  
Return a shell-escaped version of the string s. The returned value is a string that can safely be used as one token in a shell command line, for cases where you cannot use a list. This idiom would be unsafe: >>> filename = 'somefile; rm -rf ~'
>>> command = 'ls -l {}'.format(filename)
>>> print(command)  # executed by a shell: boom!
ls -l somefile; rm -rf ~
 quote() lets you plug the security hole: >>> from shlex import quote
>>> command = 'ls -l {}'.format(quote(filename))
>>> print(command)
ls -l 'somefile; rm -rf ~'
>>> remote_command = 'ssh home {}'.format(quote(command))
>>> print(remote_command)
ssh home 'ls -l '"'"'somefile; rm -rf ~'"'"''
 The quoting is compatible with UNIX shells and with split(): >>> from shlex import split
>>> remote_command = split(remote_command)
>>> remote_command
['ssh', 'home', "ls -l 'somefile; rm -rf ~'"]
>>> command = split(remote_command[-1])
>>> command
['ls', '-l', 'somefile; rm -rf ~']
  New in version 3.3.
flask.escape()  
Replace the characters &, <, >, ', and " in the string with HTML-safe sequences. Use this if you need to display text that might contain such characters in HTML. If the object has an __html__ method, it is called and the return value is assumed to already be safe for HTML.  Parameters 
s – An object to be converted to a string and escaped.  Returns 
A Markup string with the escaped text.
def f_21787496(s):
    """convert epoch time represented as milliseconds `s` to string using format '%Y-%m-%d %H:%M:%S.%f'
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
http_date(epoch_seconds=None) [source]
 
Formats the time to match the RFC 1123#section-5.2.14 date format as specified by HTTP RFC 7231#section-7.1.1.1. Accepts a floating point number expressed in seconds since the epoch in UTC–such as that outputted by time.time(). If set to None, defaults to the current time. Outputs a string in the format Wdy, DD Mon YYYY HH:MM:SS GMT.
time.asctime([t])  
Convert a tuple or struct_time representing a time as returned by gmtime() or localtime() to a string of the following form: 'Sun Jun 20 23:21:05 1993'. The day field is two characters long and is space padded if the day is a single digit, e.g.: 'Wed Jun  9 04:26:40 1993'. If t is not provided, the current time as returned by localtime() is used. Locale information is not used by asctime().  Note Unlike the C function of the same name, asctime() does not add a trailing newline.
time.ctime([secs])  
Convert a time expressed in seconds since the epoch to a string of a form: 'Sun Jun 20 23:21:05 1993' representing local time. The day field is two characters long and is space padded if the day is a single digit, e.g.: 'Wed Jun  9 04:26:40 1993'. If secs is not provided or None, the current time as returned by time() is used. ctime(secs) is equivalent to asctime(localtime(secs)). Locale information is not used by ctime().
skimage.color.separate_stains(rgb, conv_matrix) [source]
 
RGB to stain color space conversion.  Parameters 
 
rgb(…, 3) array_like 

The image in RGB format. Final dimension denotes channels.  conv_matrix: ndarray

The stain separation matrix as described by G. Landini [1].    Returns 
 
out(…, 3) ndarray 

The image in stain color space. Same dimensions as input.    Raises 
 ValueError

If rgb is not at least 2-D with shape (…, 3).     Notes Stain separation matrices available in the color module and their respective colorspace:  
hed_from_rgb: Hematoxylin + Eosin + DAB 
hdx_from_rgb: Hematoxylin + DAB 
fgx_from_rgb: Feulgen + Light Green 
bex_from_rgb: Giemsa stain : Methyl Blue + Eosin 
rbd_from_rgb: FastRed + FastBlue + DAB 
gdx_from_rgb: Methyl Green + DAB 
hax_from_rgb: Hematoxylin + AEC 
bro_from_rgb: Blue matrix Anilline Blue + Red matrix Azocarmine + Orange matrix Orange-G 
bpx_from_rgb: Methyl Blue + Ponceau Fuchsin 
ahx_from_rgb: Alcian Blue + Hematoxylin 
hpx_from_rgb: Hematoxylin + PAS  This implementation borrows some ideas from DIPlib [2], e.g. the compensation using a small value to avoid log artifacts when calculating the Beer-Lambert law. References  
1  
https://web.archive.org/web/20160624145052/http://www.mecourse.com/landinig/software/cdeconv/cdeconv.html  
2  
https://github.com/DIPlib/diplib/  
3  
A. C. Ruifrok and D. A. Johnston, “Quantification of histochemical staining by color deconvolution,” Anal. Quant. Cytol. Histol., vol. 23, no. 4, pp. 291–299, Aug. 2001.   Examples >>> from skimage import data
>>> from skimage.color import separate_stains, hdx_from_rgb
>>> ihc = data.immunohistochemistry()
>>> ihc_hdx = separate_stains(ihc, hdx_from_rgb)
stat.IO_REPARSE_TAG_SYMLINK  
stat.IO_REPARSE_TAG_MOUNT_POINT  
stat.IO_REPARSE_TAG_APPEXECLINK  
 New in version 3.8.
def f_21787496():
    """parse milliseconds epoch time '1236472051807' to format '%Y-%m-%d %H:%M:%S'
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
get_prev_week(date)  
Returns a date object containing the first day of the week before the date provided. This function can also return None or raise an Http404 exception, depending on the values of allow_empty and allow_future.
get_next_week(date)  
Returns a date object containing the first day of the week after the date provided. This function can also return None or raise an Http404 exception, depending on the values of allow_empty and allow_future.
get_previous_day(date)  
Returns a date object containing the previous valid day. This function can also return None or raise an Http404 exception, depending on the values of allow_empty and allow_future.
str.removeprefix(prefix, /)  
If the string starts with the prefix string, return string[len(prefix):]. Otherwise, return a copy of the original string: >>> 'TestHook'.removeprefix('Test')
'Hook'
>>> 'BaseTestCase'.removeprefix('Test')
'BaseTestCase'
  New in version 3.9.
pandas.DataFrame.plot   DataFrame.plot(*args, **kwargs)[source]
 
Make plots of Series or DataFrame. Uses the backend specified by the option plotting.backend. By default, matplotlib is used.  Parameters 
 
data:Series or DataFrame


The object for which the method is called.  
x:label or position, default None


Only used if data is a DataFrame.  
y:label, position or list of label, positions, default None


Allows plotting of one column versus another. Only used if data is a DataFrame.  
kind:str


The kind of plot to produce:  ‘line’ : line plot (default) ‘bar’ : vertical bar plot ‘barh’ : horizontal bar plot ‘hist’ : histogram ‘box’ : boxplot ‘kde’ : Kernel Density Estimation plot ‘density’ : same as ‘kde’ ‘area’ : area plot ‘pie’ : pie plot ‘scatter’ : scatter plot (DataFrame only) ‘hexbin’ : hexbin plot (DataFrame only)   
ax:matplotlib axes object, default None


An axes of the current figure.  
subplots:bool, default False


Make separate subplots for each column.  
sharex:bool, default True if ax is None else False


In case subplots=True, share x axis and set some x axis labels to invisible; defaults to True if ax is None otherwise False if an ax is passed in; Be aware, that passing in both an ax and sharex=True will alter all x axis labels for all axis in a figure.  
sharey:bool, default False


In case subplots=True, share y axis and set some y axis labels to invisible.  
layout:tuple, optional


(rows, columns) for the layout of subplots.  
figsize:a tuple (width, height) in inches


Size of a figure object.  
use_index:bool, default True


Use index as ticks for x axis.  
title:str or list


Title to use for the plot. If a string is passed, print the string at the top of the figure. If a list is passed and subplots is True, print each item in the list above the corresponding subplot.  
grid:bool, default None (matlab style default)


Axis grid lines.  
legend:bool or {‘reverse’}


Place legend on axis subplots.  
style:list or dict


The matplotlib line style per column.  
logx:bool or ‘sym’, default False


Use log scaling or symlog scaling on x axis. .. versionchanged:: 0.25.0  
logy:bool or ‘sym’ default False


Use log scaling or symlog scaling on y axis. .. versionchanged:: 0.25.0  
loglog:bool or ‘sym’, default False


Use log scaling or symlog scaling on both x and y axes. .. versionchanged:: 0.25.0  
xticks:sequence


Values to use for the xticks.  
yticks:sequence


Values to use for the yticks.  
xlim:2-tuple/list


Set the x limits of the current axes.  
ylim:2-tuple/list


Set the y limits of the current axes.  
xlabel:label, optional


Name to use for the xlabel on x-axis. Default uses index name as xlabel, or the x-column name for planar plots.  New in version 1.1.0.   Changed in version 1.2.0: Now applicable to planar plots (scatter, hexbin).   
ylabel:label, optional


Name to use for the ylabel on y-axis. Default will show no ylabel, or the y-column name for planar plots.  New in version 1.1.0.   Changed in version 1.2.0: Now applicable to planar plots (scatter, hexbin).   
rot:int, default None


Rotation for ticks (xticks for vertical, yticks for horizontal plots).  
fontsize:int, default None


Font size for xticks and yticks.  
colormap:str or matplotlib colormap object, default None


Colormap to select colors from. If string, load colormap with that name from matplotlib.  
colorbar:bool, optional


If True, plot colorbar (only relevant for ‘scatter’ and ‘hexbin’ plots).  
position:float


Specify relative alignments for bar plot layout. From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5 (center).  
table:bool, Series or DataFrame, default False


If True, draw a table using the data in the DataFrame and the data will be transposed to meet matplotlib’s default layout. If a Series or DataFrame is passed, use passed data to draw a table.  
yerr:DataFrame, Series, array-like, dict and str


See Plotting with Error Bars for detail.  
xerr:DataFrame, Series, array-like, dict and str


Equivalent to yerr.  
stacked:bool, default False in line and bar plots, and True in area plot


If True, create stacked plot.  
sort_columns:bool, default False


Sort column names to determine plot ordering.  
secondary_y:bool or sequence, default False


Whether to plot on the secondary y-axis if a list/tuple, which columns to plot on secondary y-axis.  
mark_right:bool, default True


When using a secondary_y axis, automatically mark the column labels with “(right)” in the legend.  
include_bool:bool, default is False


If True, boolean values can be plotted.  
backend:str, default None


Backend to use instead of the backend specified in the option plotting.backend. For instance, ‘matplotlib’. Alternatively, to specify the plotting.backend for the whole session, set pd.options.plotting.backend.  New in version 1.0.0.   **kwargs

Options to pass to matplotlib plotting method.    Returns 
 
matplotlib.axes.Axes or numpy.ndarray of them

If the backend is not the default matplotlib one, the return value will be the object returned by the backend.     Notes  See matplotlib documentation online for more on this subject If kind = ‘bar’ or ‘barh’, you can specify relative alignments for bar plot layout by position keyword. From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5 (center)
def f_20573459():
    """get the date 7 days before the current date
    """
    return  
 --------------------

def f_15352457(column, data):
    """sum elements at index `column` of each list in list `data`
    """
    return  
 --------------------

def f_15352457(array):
    """sum columns of a list `array`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
base64.standard_b64encode(s)  
Encode bytes-like object s using the standard Base64 alphabet and return the encoded bytes.
binascii.a2b_base64(string)  
Convert a block of base64 data back to binary and return the binary data. More than one line may be passed at a time.
base64.b16encode(s)  
Encode the bytes-like object s using Base16 and return the encoded bytes.
tf.raw_ops.EncodeBase64 Encode strings into web-safe base64 format.  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.raw_ops.EncodeBase64  
tf.raw_ops.EncodeBase64(
    input, pad=False, name=None
)
 Refer to the following article for more information on base64 format: en.wikipedia.org/wiki/Base64. Base64 strings may have padding with '=' at the end so that the encoded has length multiple of 4. See Padding section of the link above. Web-safe means that the encoder uses - and _ instead of + and /.
 


 Args
  input   A Tensor of type string. Strings to be encoded.  
  pad   An optional bool. Defaults to False. Bool whether padding is applied at the ends.  
  name   A name for the operation (optional).   
 


 Returns   A Tensor of type string.
tf.io.encode_base64 Encode strings into web-safe base64 format.  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.encode_base64, tf.compat.v1.io.encode_base64  
tf.io.encode_base64(
    input, pad=False, name=None
)
 Refer to the following article for more information on base64 format: en.wikipedia.org/wiki/Base64. Base64 strings may have padding with '=' at the end so that the encoded has length multiple of 4. See Padding section of the link above. Web-safe means that the encoder uses - and _ instead of + and /.
 


 Args
  input   A Tensor of type string. Strings to be encoded.  
  pad   An optional bool. Defaults to False. Bool whether padding is applied at the ends.  
  name   A name for the operation (optional).   
 


 Returns   A Tensor of type string.
def f_23164058():
    """encode binary string 'your string' to base64 code
    """
    return  
 --------------------

def f_11533274(dicts):
    """combine list of dictionaries `dicts` with the same keys in each list to a single dictionary
    """
    return  
 --------------------

def f_11533274(dicts):
    """Merge a nested dictionary `dicts` into a flat dictionary by concatenating nested values with the same key `k`
    """
    return  
 --------------------

def f_14026704(request):
    """get the url parameter 'myParam' in a Flask view
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
IMAP4.myrights(mailbox)  
Show my ACLs for a mailbox (i.e. the rights that I have on mailbox).
update_viewlim()[source]
 
[Deprecated] Notes  Deprecated since version 3.4:
update_viewlim()[source]
 
[Deprecated] Notes  Deprecated since version 3.4:
array.fromlist(list)  
Append items from the list. This is equivalent to for x in list:
a.append(x) except that if there is a type error, the array is unchanged.
FieldStorage.getlist(name)  
This method always returns a list of values associated with form field name. The method returns an empty list if no such form field or value exists for name. It returns a list consisting of one item if only one such value exists.
def f_11236006(mylist):
    """identify duplicate values in list `mylist`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
SET_NULL  
Set the ForeignKey null; this is only possible if null is True.
none()
empty_result_set_value  
 New in Django 4.0.  Override empty_result_set_value to None since most aggregate functions result in NULL when applied to an empty result set.
empty_result_set_value  
 New in Django 4.0.  Tells Django which value should be returned when the expression is used to apply a function over an empty result set. Defaults to NotImplemented which forces the expression to be computed on the database.
None  
The sole value of the type NoneType. None is frequently used to represent the absence of a value, as when default arguments are not passed to a function. Assignments to None are illegal and raise a SyntaxError.
def f_20211942(db):
    """Insert a 'None' value into a SQLite3 table.
    """
    return  
 --------------------

def f_406121(list_of_menuitems):
    """flatten list `list_of_menuitems`
    """
    return  
 --------------------

def f_4741537(a, b):
    """append elements of a set `b` to a list `a`
    """
     
 --------------------

def f_15851568(x):
    """Split a string `x` by last occurrence of character `-`
    """
    return  
 --------------------

def f_15851568(x):
    """get the last part of a string before the character '-'
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
class urllib.request.FTPHandler  
Open FTP URLs.
FTP.storbinary(cmd, fp, blocksize=8192, callback=None, rest=None)  
Store a file in binary transfer mode. cmd should be an appropriate STOR command: "STOR filename". fp is a file object (opened in binary mode) which is read until EOF using its read() method in blocks of size blocksize to provide the data to be stored. The blocksize argument defaults to 8192. callback is an optional single parameter callable that is called on each block of data after it is sent. rest means the same thing as in the transfercmd() method.  Changed in version 3.2: rest parameter added.
FTP.storlines(cmd, fp, callback=None)  
Store a file in line mode. cmd should be an appropriate STOR command (see storbinary()). Lines are read until EOF from the file object fp (opened in binary mode) using its readline() method to provide the data to be stored. callback is an optional single parameter callable that is called on each line after it is sent.
FTP.login(user='anonymous', passwd='', acct='')  
Log in as the given user. The passwd and acct parameters are optional and default to the empty string. If no user is specified, it defaults to 'anonymous'. If user is 'anonymous', the default passwd is 'anonymous@'. This function should be called only once for each instance, after a connection has been established; it should not be called at all if a host and user were given when the instance was created. Most FTP commands are only allowed after the client has logged in. The acct parameter supplies “accounting information”; few systems implement this.
FTP.voidcmd(cmd)  
Send a simple command string to the server and handle the response. Return nothing if a response code corresponding to success (codes in the range 200–299) is received. Raise error_reply otherwise. Raises an auditing event ftplib.sendcmd with arguments self, cmd.
def f_17438096(filename, ftp):
    """upload file using FTP
    """
     
 --------------------
Please refer to the following documentation to generate the code:
maximum(other) → Tensor  
See torch.maximum()
numpy.matrix.max method   matrix.max(axis=None, out=None)[source]
 
Return the maximum value along an axis.  Parameters 
 See `amax` for complete descriptions
    See also  
amax, ndarray.max

  Notes This is the same as ndarray.max, but returns a matrix object where ndarray.max would return an ndarray. Examples >>> x = np.matrix(np.arange(12).reshape((3,4))); x
matrix([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11]])
>>> x.max()
11
>>> x.max(0)
matrix([[ 8,  9, 10, 11]])
>>> x.max(1)
matrix([[ 3],
        [ 7],
        [11]])
torch.maximum(input, other, *, out=None) → Tensor  
Computes the element-wise maximum of input and other.  Note If one of the elements being compared is a NaN, then that element is returned. maximum() is not supported for tensors with complex dtypes.   Parameters 
 
input (Tensor) – the input tensor. 
other (Tensor) – the second input tensor   Keyword Arguments 
out (Tensor, optional) – the output tensor.   Example: >>> a = torch.tensor((1, 2, -1))
>>> b = torch.tensor((3, 0, 4))
>>> torch.maximum(a, b)
tensor([3, 2, 4])
max(dim=None, keepdim=False) -> Tensor or (Tensor, Tensor)  
See torch.max()
torch.fmax(input, other, *, out=None) → Tensor  
Computes the element-wise maximum of input and other. This is like torch.maximum() except it handles NaNs differently: if exactly one of the two elements being compared is a NaN then the non-NaN element is taken as the maximum. Only if both elements are NaN is NaN propagated. This function is a wrapper around C++’s std::fmax and is similar to NumPy’s fmax function. Supports broadcasting to a common shape, type promotion, and integer and floating-point inputs.  Parameters 
 
input (Tensor) – the input tensor. 
other (Tensor) – the second input tensor   Keyword Arguments 
out (Tensor, optional) – the output tensor.   Example: >>> a = torch.tensor([9.7, float('nan'), 3.1, float('nan')])
>>> b = torch.tensor([-2.2, 0.5, float('nan'), float('nan')])
>>> torch.fmax(a, b)
tensor([9.7000, 0.5000, 3.1000,    nan])
def f_28742436():
    """create array containing the maximum value of respective elements of array `[2, 3, 4]` and array `[1, 5, 2]`
    """
    return  
 --------------------

def f_34280147(l):
    """print a list `l` and move first 3 elements to the end of the list
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
tf.config.LogicalDevice Abstraction for a logical device initialized by the runtime.  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.config.LogicalDevice  
tf.config.LogicalDevice(
    name, device_type
)
 A tf.config.LogicalDevice corresponds to an initialized logical device on a tf.config.PhysicalDevice or a remote device visible to the cluster. Tensors and operations can be placed on a specific logical device by calling tf.device with a specified tf.config.LogicalDevice. Fields:  
name: The fully qualified name of the device. Can be used for Op or function placement. 
device_type: String declaring the type of device such as "CPU" or "GPU". 
 


 Attributes
  name  
 
  device_type
matplotlib.axes.Axes.get_aspect   Axes.get_aspect()[source]
 
Return the aspect ratio of the axes scaling. This is either "auto" or a float giving the ratio of y/x-scale.
buffer_rgba()[source]
 
Get the image as a memoryview to the renderer's buffer. draw must be called at least once before this function will work and to update the renderer for any subsequent changes to the Figure.
remove_callback(oid)[source]
 
Remove a callback based on its observer id.  See also  add_callback
numpy.record.tolist method   record.tolist()
 
Scalar method identical to the corresponding array attribute. Please see ndarray.tolist.
def f_4172131():
    """create a random list of integers
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
time.microsecond  
In range(1000000).
datetime.microsecond  
In range(1000000).
pandas.Timestamp.microsecond   Timestamp.microsecond
pandas.Series.dt.microseconds   Series.dt.microseconds
 
Number of microseconds (>= 0 and less than 1 second) for each element.
pandas.DatetimeIndex.microsecond   propertyDatetimeIndex.microsecond
 
The microseconds of the datetime. Examples 
>>> datetime_series = pd.Series(
...     pd.date_range("2000-01-01", periods=3, freq="us")
... )
>>> datetime_series
0   2000-01-01 00:00:00.000000
1   2000-01-01 00:00:00.000001
2   2000-01-01 00:00:00.000002
dtype: datetime64[ns]
>>> datetime_series.dt.microsecond
0       0
1       1
2       2
dtype: int64
def f_6677332():
    """Using %f with strftime() in Python to get microseconds
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
pandas.DataFrame.filter   DataFrame.filter(items=None, like=None, regex=None, axis=None)[source]
 
Subset the dataframe rows or columns according to the specified index labels. Note that this routine does not filter a dataframe on its contents. The filter is applied to the labels of the index.  Parameters 
 
items:list-like


Keep labels from axis which are in items.  
like:str


Keep labels from axis for which “like in label == True”.  
regex:str (regular expression)


Keep labels from axis for which re.search(regex, label) == True.  
axis:{0 or ‘index’, 1 or ‘columns’, None}, default None


The axis to filter on, expressed either as an index (int) or axis name (str). By default this is the info axis, ‘index’ for Series, ‘columns’ for DataFrame.    Returns 
 same type as input object
    See also  DataFrame.loc

Access a group of rows and columns by label(s) or a boolean array.    Notes The items, like, and regex parameters are enforced to be mutually exclusive. axis defaults to the info axis that is used when indexing with []. Examples 
>>> df = pd.DataFrame(np.array(([1, 2, 3], [4, 5, 6])),
...                   index=['mouse', 'rabbit'],
...                   columns=['one', 'two', 'three'])
>>> df
        one  two  three
mouse     1    2      3
rabbit    4    5      6
  
>>> # select columns by name
>>> df.filter(items=['one', 'three'])
         one  three
mouse     1      3
rabbit    4      6
  
>>> # select columns by regular expression
>>> df.filter(regex='e$', axis=1)
         one  three
mouse     1      3
rabbit    4      6
  
>>> # select rows containing 'bbi'
>>> df.filter(like='bbi', axis=0)
         one  two  three
rabbit    4    5      6
pandas.Series.filter   Series.filter(items=None, like=None, regex=None, axis=None)[source]
 
Subset the dataframe rows or columns according to the specified index labels. Note that this routine does not filter a dataframe on its contents. The filter is applied to the labels of the index.  Parameters 
 
items:list-like


Keep labels from axis which are in items.  
like:str


Keep labels from axis for which “like in label == True”.  
regex:str (regular expression)


Keep labels from axis for which re.search(regex, label) == True.  
axis:{0 or ‘index’, 1 or ‘columns’, None}, default None


The axis to filter on, expressed either as an index (int) or axis name (str). By default this is the info axis, ‘index’ for Series, ‘columns’ for DataFrame.    Returns 
 same type as input object
    See also  DataFrame.loc

Access a group of rows and columns by label(s) or a boolean array.    Notes The items, like, and regex parameters are enforced to be mutually exclusive. axis defaults to the info axis that is used when indexing with []. Examples 
>>> df = pd.DataFrame(np.array(([1, 2, 3], [4, 5, 6])),
...                   index=['mouse', 'rabbit'],
...                   columns=['one', 'two', 'three'])
>>> df
        one  two  three
mouse     1    2      3
rabbit    4    5      6
  
>>> # select columns by name
>>> df.filter(items=['one', 'three'])
         one  three
mouse     1      3
rabbit    4      6
  
>>> # select columns by regular expression
>>> df.filter(regex='e$', axis=1)
         one  three
mouse     1      3
rabbit    4      6
  
>>> # select rows containing 'bbi'
>>> df.filter(like='bbi', axis=0)
         one  two  three
rabbit    4    5      6
pandas.Series.str.extractall   Series.str.extractall(pat, flags=0)[source]
 
Extract capture groups in the regex pat as columns in DataFrame. For each subject string in the Series, extract groups from all matches of regular expression pat. When each subject string in the Series has exactly one match, extractall(pat).xs(0, level=’match’) is the same as extract(pat).  Parameters 
 
pat:str


Regular expression pattern with capturing groups.  
flags:int, default 0 (no flags)


A re module flag, for example re.IGNORECASE. These allow to modify regular expression matching for things like case, spaces, etc. Multiple flags can be combined with the bitwise OR operator, for example re.IGNORECASE | re.MULTILINE.    Returns 
 DataFrame

A DataFrame with one row for each match, and one column for each group. Its rows have a MultiIndex with first levels that come from the subject Series. The last level is named ‘match’ and indexes the matches in each item of the Series. Any capture group names in regular expression pat will be used for column names; otherwise capture group numbers will be used.      See also  extract

Returns first match only (not all matches).    Examples A pattern with one group will return a DataFrame with one column. Indices with no matches will not appear in the result. 
>>> s = pd.Series(["a1a2", "b1", "c1"], index=["A", "B", "C"])
>>> s.str.extractall(r"[ab](\d)")
        0
match
A 0      1
  1      2
B 0      1
  Capture group names are used for column names of the result. 
>>> s.str.extractall(r"[ab](?P<digit>\d)")
        digit
match
A 0         1
  1         2
B 0         1
  A pattern with two groups will return a DataFrame with two columns. 
>>> s.str.extractall(r"(?P<letter>[ab])(?P<digit>\d)")
        letter digit
match
A 0          a     1
  1          a     2
B 0          b     1
  Optional groups that do not match are NaN in the result. 
>>> s.str.extractall(r"(?P<letter>[ab])?(?P<digit>\d)")
        letter digit
match
A 0          a     1
  1          a     2
B 0          b     1
C 0        NaN     1
frame_type  
This attribute is set to 'ROWS'.
class RTrim(expression, **extra)
def f_15325182(df):
    """filter rows in pandas starting with alphabet 'f' using regular expression.
    """
    return  
 --------------------

def f_583557(tab):
    """print a 2 dimensional list `tab` as a table with delimiters
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
pandas.DataFrame.drop   DataFrame.drop(labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise')[source]
 
Drop specified labels from rows or columns. Remove rows or columns by specifying label names and corresponding axis, or by specifying directly index or column names. When using a multi-index, labels on different levels can be removed by specifying the level. See the user guide <advanced.shown_levels> for more information about the now unused levels.  Parameters 
 
labels:single label or list-like


Index or column labels to drop. A tuple will be used as a single label and not treated as a list-like.  
axis:{0 or ‘index’, 1 or ‘columns’}, default 0


Whether to drop labels from the index (0 or ‘index’) or columns (1 or ‘columns’).  
index:single label or list-like


Alternative to specifying axis (labels, axis=0 is equivalent to index=labels).  
columns:single label or list-like


Alternative to specifying axis (labels, axis=1 is equivalent to columns=labels).  
level:int or level name, optional


For MultiIndex, level from which the labels will be removed.  
inplace:bool, default False


If False, return a copy. Otherwise, do operation inplace and return None.  
errors:{‘ignore’, ‘raise’}, default ‘raise’


If ‘ignore’, suppress error and only existing labels are dropped.    Returns 
 DataFrame or None

DataFrame without the removed index or column labels or None if inplace=True.    Raises 
 KeyError

If any of the labels is not found in the selected axis.      See also  DataFrame.loc

Label-location based indexer for selection by label.  DataFrame.dropna

Return DataFrame with labels on given axis omitted where (all or any) data are missing.  DataFrame.drop_duplicates

Return DataFrame with duplicate rows removed, optionally only considering certain columns.  Series.drop

Return Series with specified index labels removed.    Examples 
>>> df = pd.DataFrame(np.arange(12).reshape(3, 4),
...                   columns=['A', 'B', 'C', 'D'])
>>> df
   A  B   C   D
0  0  1   2   3
1  4  5   6   7
2  8  9  10  11
  Drop columns 
>>> df.drop(['B', 'C'], axis=1)
   A   D
0  0   3
1  4   7
2  8  11
  
>>> df.drop(columns=['B', 'C'])
   A   D
0  0   3
1  4   7
2  8  11
  Drop a row by index 
>>> df.drop([0, 1])
   A  B   C   D
2  8  9  10  11
  Drop columns and/or rows of MultiIndex DataFrame 
>>> midx = pd.MultiIndex(levels=[['lama', 'cow', 'falcon'],
...                              ['speed', 'weight', 'length']],
...                      codes=[[0, 0, 0, 1, 1, 1, 2, 2, 2],
...                             [0, 1, 2, 0, 1, 2, 0, 1, 2]])
>>> df = pd.DataFrame(index=midx, columns=['big', 'small'],
...                   data=[[45, 30], [200, 100], [1.5, 1], [30, 20],
...                         [250, 150], [1.5, 0.8], [320, 250],
...                         [1, 0.8], [0.3, 0.2]])
>>> df
                big     small
lama    speed   45.0    30.0
        weight  200.0   100.0
        length  1.5     1.0
cow     speed   30.0    20.0
        weight  250.0   150.0
        length  1.5     0.8
falcon  speed   320.0   250.0
        weight  1.0     0.8
        length  0.3     0.2
  Drop a specific index combination from the MultiIndex DataFrame, i.e., drop the combination 'falcon' and 'weight', which deletes only the corresponding row 
>>> df.drop(index=('falcon', 'weight'))
                big     small
lama    speed   45.0    30.0
        weight  200.0   100.0
        length  1.5     1.0
cow     speed   30.0    20.0
        weight  250.0   150.0
        length  1.5     0.8
falcon  speed   320.0   250.0
        length  0.3     0.2
  
>>> df.drop(index='cow', columns='small')
                big
lama    speed   45.0
        weight  200.0
        length  1.5
falcon  speed   320.0
        weight  1.0
        length  0.3
  
>>> df.drop(index='length', level=1)
                big     small
lama    speed   45.0    30.0
        weight  200.0   100.0
cow     speed   30.0    20.0
        weight  250.0   150.0
falcon  speed   320.0   250.0
        weight  1.0     0.8
pandas.DataFrame.dropna   DataFrame.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)[source]
 
Remove missing values. See the User Guide for more on which values are considered missing, and how to work with missing data.  Parameters 
 
axis:{0 or ‘index’, 1 or ‘columns’}, default 0


Determine if rows or columns which contain missing values are removed.  0, or ‘index’ : Drop rows which contain missing values. 1, or ‘columns’ : Drop columns which contain missing value.   Changed in version 1.0.0: Pass tuple or list to drop on multiple axes. Only a single axis is allowed.   
how:{‘any’, ‘all’}, default ‘any’


Determine if row or column is removed from DataFrame, when we have at least one NA or all NA.  ‘any’ : If any NA values are present, drop that row or column. ‘all’ : If all values are NA, drop that row or column.   
thresh:int, optional


Require that many non-NA values.  
subset:column label or sequence of labels, optional


Labels along other axis to consider, e.g. if you are dropping rows these would be a list of columns to include.  
inplace:bool, default False


If True, do operation inplace and return None.    Returns 
 DataFrame or None

DataFrame with NA entries dropped from it or None if inplace=True.      See also  DataFrame.isna

Indicate missing values.  DataFrame.notna

Indicate existing (non-missing) values.  DataFrame.fillna

Replace missing values.  Series.dropna

Drop missing values.  Index.dropna

Drop missing indices.    Examples 
>>> df = pd.DataFrame({"name": ['Alfred', 'Batman', 'Catwoman'],
...                    "toy": [np.nan, 'Batmobile', 'Bullwhip'],
...                    "born": [pd.NaT, pd.Timestamp("1940-04-25"),
...                             pd.NaT]})
>>> df
       name        toy       born
0    Alfred        NaN        NaT
1    Batman  Batmobile 1940-04-25
2  Catwoman   Bullwhip        NaT
  Drop the rows where at least one element is missing. 
>>> df.dropna()
     name        toy       born
1  Batman  Batmobile 1940-04-25
  Drop the columns where at least one element is missing. 
>>> df.dropna(axis='columns')
       name
0    Alfred
1    Batman
2  Catwoman
  Drop the rows where all elements are missing. 
>>> df.dropna(how='all')
       name        toy       born
0    Alfred        NaN        NaT
1    Batman  Batmobile 1940-04-25
2  Catwoman   Bullwhip        NaT
  Keep only the rows with at least 2 non-NA values. 
>>> df.dropna(thresh=2)
       name        toy       born
1    Batman  Batmobile 1940-04-25
2  Catwoman   Bullwhip        NaT
  Define in which columns to look for missing values. 
>>> df.dropna(subset=['name', 'toy'])
       name        toy       born
1    Batman  Batmobile 1940-04-25
2  Catwoman   Bullwhip        NaT
  Keep the DataFrame with valid entries in the same variable. 
>>> df.dropna(inplace=True)
>>> df
     name        toy       born
1  Batman  Batmobile 1940-04-25
pandas.DataFrame.drop_duplicates   DataFrame.drop_duplicates(subset=None, keep='first', inplace=False, ignore_index=False)[source]
 
Return DataFrame with duplicate rows removed. Considering certain columns is optional. Indexes, including time indexes are ignored.  Parameters 
 
subset:column label or sequence of labels, optional


Only consider certain columns for identifying duplicates, by default use all of the columns.  
keep:{‘first’, ‘last’, False}, default ‘first’


Determines which duplicates (if any) to keep. - first : Drop duplicates except for the first occurrence. - last : Drop duplicates except for the last occurrence. - False : Drop all duplicates.  
inplace:bool, default False


Whether to drop duplicates in place or to return a copy.  
ignore_index:bool, default False


If True, the resulting axis will be labeled 0, 1, …, n - 1.  New in version 1.0.0.     Returns 
 DataFrame or None

DataFrame with duplicates removed or None if inplace=True.      See also  DataFrame.value_counts

Count unique combinations of columns.    Examples Consider dataset containing ramen rating. 
>>> df = pd.DataFrame({
...     'brand': ['Yum Yum', 'Yum Yum', 'Indomie', 'Indomie', 'Indomie'],
...     'style': ['cup', 'cup', 'cup', 'pack', 'pack'],
...     'rating': [4, 4, 3.5, 15, 5]
... })
>>> df
    brand style  rating
0  Yum Yum   cup     4.0
1  Yum Yum   cup     4.0
2  Indomie   cup     3.5
3  Indomie  pack    15.0
4  Indomie  pack     5.0
  By default, it removes duplicate rows based on all columns. 
>>> df.drop_duplicates()
    brand style  rating
0  Yum Yum   cup     4.0
2  Indomie   cup     3.5
3  Indomie  pack    15.0
4  Indomie  pack     5.0
  To remove duplicates on specific column(s), use subset. 
>>> df.drop_duplicates(subset=['brand'])
    brand style  rating
0  Yum Yum   cup     4.0
2  Indomie   cup     3.5
  To remove duplicates and keep last occurrences, use keep. 
>>> df.drop_duplicates(subset=['brand', 'style'], keep='last')
    brand style  rating
1  Yum Yum   cup     4.0
2  Indomie   cup     3.5
4  Indomie  pack     5.0
Cookbook This is a repository for short and sweet examples and links for useful pandas recipes. We encourage users to add to this documentation. Adding interesting links and/or inline examples to this section is a great First Pull Request. Simplified, condensed, new-user friendly, in-line examples have been inserted where possible to augment the Stack-Overflow and GitHub links. Many of the links contain expanded information, above what the in-line examples offer. pandas (pd) and NumPy (np) are the only two abbreviated imported modules. The rest are kept explicitly imported for newer users.  Idioms These are some neat pandas idioms if-then/if-then-else on one column, and assignment to another one or more columns: 
In [1]: df = pd.DataFrame(
   ...:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ...: )
   ...: 

In [2]: df
Out[2]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50
   if-then… An if-then on one column 
In [3]: df.loc[df.AAA >= 5, "BBB"] = -1

In [4]: df
Out[4]: 
   AAA  BBB  CCC
0    4   10  100
1    5   -1   50
2    6   -1  -30
3    7   -1  -50
  An if-then with assignment to 2 columns: 
In [5]: df.loc[df.AAA >= 5, ["BBB", "CCC"]] = 555

In [6]: df
Out[6]: 
   AAA  BBB  CCC
0    4   10  100
1    5  555  555
2    6  555  555
3    7  555  555
  Add another line with different logic, to do the -else 
In [7]: df.loc[df.AAA < 5, ["BBB", "CCC"]] = 2000

In [8]: df
Out[8]: 
   AAA   BBB   CCC
0    4  2000  2000
1    5   555   555
2    6   555   555
3    7   555   555
  Or use pandas where after you’ve set up a mask 
In [9]: df_mask = pd.DataFrame(
   ...:     {"AAA": [True] * 4, "BBB": [False] * 4, "CCC": [True, False] * 2}
   ...: )
   ...: 

In [10]: df.where(df_mask, -1000)
Out[10]: 
   AAA   BBB   CCC
0    4 -1000  2000
1    5 -1000 -1000
2    6 -1000   555
3    7 -1000 -1000
  if-then-else using NumPy’s where() 
In [11]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ....: )
   ....: 

In [12]: df
Out[12]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50

In [13]: df["logic"] = np.where(df["AAA"] > 5, "high", "low")

In [14]: df
Out[14]: 
   AAA  BBB  CCC logic
0    4   10  100   low
1    5   20   50   low
2    6   30  -30  high
3    7   40  -50  high
    Splitting Split a frame with a boolean criterion 
In [15]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ....: )
   ....: 

In [16]: df
Out[16]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50

In [17]: df[df.AAA <= 5]
Out[17]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50

In [18]: df[df.AAA > 5]
Out[18]: 
   AAA  BBB  CCC
2    6   30  -30
3    7   40  -50
    Building criteria Select with multi-column criteria 
In [19]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ....: )
   ....: 

In [20]: df
Out[20]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50
  …and (without assignment returns a Series) 
In [21]: df.loc[(df["BBB"] < 25) & (df["CCC"] >= -40), "AAA"]
Out[21]: 
0    4
1    5
Name: AAA, dtype: int64
  …or (without assignment returns a Series) 
In [22]: df.loc[(df["BBB"] > 25) | (df["CCC"] >= -40), "AAA"]
Out[22]: 
0    4
1    5
2    6
3    7
Name: AAA, dtype: int64
  …or (with assignment modifies the DataFrame.) 
In [23]: df.loc[(df["BBB"] > 25) | (df["CCC"] >= 75), "AAA"] = 0.1

In [24]: df
Out[24]: 
   AAA  BBB  CCC
0  0.1   10  100
1  5.0   20   50
2  0.1   30  -30
3  0.1   40  -50
  Select rows with data closest to certain value using argsort 
In [25]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ....: )
   ....: 

In [26]: df
Out[26]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50

In [27]: aValue = 43.0

In [28]: df.loc[(df.CCC - aValue).abs().argsort()]
Out[28]: 
   AAA  BBB  CCC
1    5   20   50
0    4   10  100
2    6   30  -30
3    7   40  -50
  Dynamically reduce a list of criteria using a binary operators 
In [29]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ....: )
   ....: 

In [30]: df
Out[30]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50

In [31]: Crit1 = df.AAA <= 5.5

In [32]: Crit2 = df.BBB == 10.0

In [33]: Crit3 = df.CCC > -40.0
  One could hard code: 
In [34]: AllCrit = Crit1 & Crit2 & Crit3
  …Or it can be done with a list of dynamically built criteria 
In [35]: import functools

In [36]: CritList = [Crit1, Crit2, Crit3]

In [37]: AllCrit = functools.reduce(lambda x, y: x & y, CritList)

In [38]: df[AllCrit]
Out[38]: 
   AAA  BBB  CCC
0    4   10  100
     Selection  Dataframes The indexing docs. Using both row labels and value conditionals 
In [39]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ....: )
   ....: 

In [40]: df
Out[40]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50

In [41]: df[(df.AAA <= 6) & (df.index.isin([0, 2, 4]))]
Out[41]: 
   AAA  BBB  CCC
0    4   10  100
2    6   30  -30
  Use loc for label-oriented slicing and iloc positional slicing GH2904 
In [42]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]},
   ....:     index=["foo", "bar", "boo", "kar"],
   ....: )
   ....: 
  There are 2 explicit slicing methods, with a third general case  Positional-oriented (Python slicing style : exclusive of end) Label-oriented (Non-Python slicing style : inclusive of end) General (Either slicing style : depends on if the slice contains labels or positions)  
In [43]: df.loc["bar":"kar"]  # Label
Out[43]: 
     AAA  BBB  CCC
bar    5   20   50
boo    6   30  -30
kar    7   40  -50

# Generic
In [44]: df[0:3]
Out[44]: 
     AAA  BBB  CCC
foo    4   10  100
bar    5   20   50
boo    6   30  -30

In [45]: df["bar":"kar"]
Out[45]: 
     AAA  BBB  CCC
bar    5   20   50
boo    6   30  -30
kar    7   40  -50
  Ambiguity arises when an index consists of integers with a non-zero start or non-unit increment. 
In [46]: data = {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}

In [47]: df2 = pd.DataFrame(data=data, index=[1, 2, 3, 4])  # Note index starts at 1.

In [48]: df2.iloc[1:3]  # Position-oriented
Out[48]: 
   AAA  BBB  CCC
2    5   20   50
3    6   30  -30

In [49]: df2.loc[1:3]  # Label-oriented
Out[49]: 
   AAA  BBB  CCC
1    4   10  100
2    5   20   50
3    6   30  -30
  Using inverse operator (~) to take the complement of a mask 
In [50]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ....: )
   ....: 

In [51]: df
Out[51]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50

In [52]: df[~((df.AAA <= 6) & (df.index.isin([0, 2, 4])))]
Out[52]: 
   AAA  BBB  CCC
1    5   20   50
3    7   40  -50
    New columns Efficiently and dynamically creating new columns using applymap 
In [53]: df = pd.DataFrame({"AAA": [1, 2, 1, 3], "BBB": [1, 1, 2, 2], "CCC": [2, 1, 3, 1]})

In [54]: df
Out[54]: 
   AAA  BBB  CCC
0    1    1    2
1    2    1    1
2    1    2    3
3    3    2    1

In [55]: source_cols = df.columns  # Or some subset would work too

In [56]: new_cols = [str(x) + "_cat" for x in source_cols]

In [57]: categories = {1: "Alpha", 2: "Beta", 3: "Charlie"}

In [58]: df[new_cols] = df[source_cols].applymap(categories.get)

In [59]: df
Out[59]: 
   AAA  BBB  CCC  AAA_cat BBB_cat  CCC_cat
0    1    1    2    Alpha   Alpha     Beta
1    2    1    1     Beta   Alpha    Alpha
2    1    2    3    Alpha    Beta  Charlie
3    3    2    1  Charlie    Beta    Alpha
  Keep other columns when using min() with groupby 
In [60]: df = pd.DataFrame(
   ....:     {"AAA": [1, 1, 1, 2, 2, 2, 3, 3], "BBB": [2, 1, 3, 4, 5, 1, 2, 3]}
   ....: )
   ....: 

In [61]: df
Out[61]: 
   AAA  BBB
0    1    2
1    1    1
2    1    3
3    2    4
4    2    5
5    2    1
6    3    2
7    3    3
  Method 1 : idxmin() to get the index of the minimums 
In [62]: df.loc[df.groupby("AAA")["BBB"].idxmin()]
Out[62]: 
   AAA  BBB
1    1    1
5    2    1
6    3    2
  Method 2 : sort then take first of each 
In [63]: df.sort_values(by="BBB").groupby("AAA", as_index=False).first()
Out[63]: 
   AAA  BBB
0    1    1
1    2    1
2    3    2
  Notice the same results, with the exception of the index.    Multiindexing The multindexing docs. Creating a MultiIndex from a labeled frame 
In [64]: df = pd.DataFrame(
   ....:     {
   ....:         "row": [0, 1, 2],
   ....:         "One_X": [1.1, 1.1, 1.1],
   ....:         "One_Y": [1.2, 1.2, 1.2],
   ....:         "Two_X": [1.11, 1.11, 1.11],
   ....:         "Two_Y": [1.22, 1.22, 1.22],
   ....:     }
   ....: )
   ....: 

In [65]: df
Out[65]: 
   row  One_X  One_Y  Two_X  Two_Y
0    0    1.1    1.2   1.11   1.22
1    1    1.1    1.2   1.11   1.22
2    2    1.1    1.2   1.11   1.22

# As Labelled Index
In [66]: df = df.set_index("row")

In [67]: df
Out[67]: 
     One_X  One_Y  Two_X  Two_Y
row                            
0      1.1    1.2   1.11   1.22
1      1.1    1.2   1.11   1.22
2      1.1    1.2   1.11   1.22

# With Hierarchical Columns
In [68]: df.columns = pd.MultiIndex.from_tuples([tuple(c.split("_")) for c in df.columns])

In [69]: df
Out[69]: 
     One        Two      
       X    Y     X     Y
row                      
0    1.1  1.2  1.11  1.22
1    1.1  1.2  1.11  1.22
2    1.1  1.2  1.11  1.22

# Now stack & Reset
In [70]: df = df.stack(0).reset_index(1)

In [71]: df
Out[71]: 
    level_1     X     Y
row                    
0       One  1.10  1.20
0       Two  1.11  1.22
1       One  1.10  1.20
1       Two  1.11  1.22
2       One  1.10  1.20
2       Two  1.11  1.22

# And fix the labels (Notice the label 'level_1' got added automatically)
In [72]: df.columns = ["Sample", "All_X", "All_Y"]

In [73]: df
Out[73]: 
    Sample  All_X  All_Y
row                     
0      One   1.10   1.20
0      Two   1.11   1.22
1      One   1.10   1.20
1      Two   1.11   1.22
2      One   1.10   1.20
2      Two   1.11   1.22
   Arithmetic Performing arithmetic with a MultiIndex that needs broadcasting 
In [74]: cols = pd.MultiIndex.from_tuples(
   ....:     [(x, y) for x in ["A", "B", "C"] for y in ["O", "I"]]
   ....: )
   ....: 

In [75]: df = pd.DataFrame(np.random.randn(2, 6), index=["n", "m"], columns=cols)

In [76]: df
Out[76]: 
          A                   B                   C          
          O         I         O         I         O         I
n  0.469112 -0.282863 -1.509059 -1.135632  1.212112 -0.173215
m  0.119209 -1.044236 -0.861849 -2.104569 -0.494929  1.071804

In [77]: df = df.div(df["C"], level=1)

In [78]: df
Out[78]: 
          A                   B              C     
          O         I         O         I    O    I
n  0.387021  1.633022 -1.244983  6.556214  1.0  1.0
m -0.240860 -0.974279  1.741358 -1.963577  1.0  1.0
    Slicing Slicing a MultiIndex with xs 
In [79]: coords = [("AA", "one"), ("AA", "six"), ("BB", "one"), ("BB", "two"), ("BB", "six")]

In [80]: index = pd.MultiIndex.from_tuples(coords)

In [81]: df = pd.DataFrame([11, 22, 33, 44, 55], index, ["MyData"])

In [82]: df
Out[82]: 
        MyData
AA one      11
   six      22
BB one      33
   two      44
   six      55
  To take the cross section of the 1st level and 1st axis the index: 
# Note : level and axis are optional, and default to zero
In [83]: df.xs("BB", level=0, axis=0)
Out[83]: 
     MyData
one      33
two      44
six      55
  …and now the 2nd level of the 1st axis. 
In [84]: df.xs("six", level=1, axis=0)
Out[84]: 
    MyData
AA      22
BB      55
  Slicing a MultiIndex with xs, method #2 
In [85]: import itertools

In [86]: index = list(itertools.product(["Ada", "Quinn", "Violet"], ["Comp", "Math", "Sci"]))

In [87]: headr = list(itertools.product(["Exams", "Labs"], ["I", "II"]))

In [88]: indx = pd.MultiIndex.from_tuples(index, names=["Student", "Course"])

In [89]: cols = pd.MultiIndex.from_tuples(headr)  # Notice these are un-named

In [90]: data = [[70 + x + y + (x * y) % 3 for x in range(4)] for y in range(9)]

In [91]: df = pd.DataFrame(data, indx, cols)

In [92]: df
Out[92]: 
               Exams     Labs    
                   I  II    I  II
Student Course                   
Ada     Comp      70  71   72  73
        Math      71  73   75  74
        Sci       72  75   75  75
Quinn   Comp      73  74   75  76
        Math      74  76   78  77
        Sci       75  78   78  78
Violet  Comp      76  77   78  79
        Math      77  79   81  80
        Sci       78  81   81  81

In [93]: All = slice(None)

In [94]: df.loc["Violet"]
Out[94]: 
       Exams     Labs    
           I  II    I  II
Course                   
Comp      76  77   78  79
Math      77  79   81  80
Sci       78  81   81  81

In [95]: df.loc[(All, "Math"), All]
Out[95]: 
               Exams     Labs    
                   I  II    I  II
Student Course                   
Ada     Math      71  73   75  74
Quinn   Math      74  76   78  77
Violet  Math      77  79   81  80

In [96]: df.loc[(slice("Ada", "Quinn"), "Math"), All]
Out[96]: 
               Exams     Labs    
                   I  II    I  II
Student Course                   
Ada     Math      71  73   75  74
Quinn   Math      74  76   78  77

In [97]: df.loc[(All, "Math"), ("Exams")]
Out[97]: 
                 I  II
Student Course        
Ada     Math    71  73
Quinn   Math    74  76
Violet  Math    77  79

In [98]: df.loc[(All, "Math"), (All, "II")]
Out[98]: 
               Exams Labs
                  II   II
Student Course           
Ada     Math      73   74
Quinn   Math      76   77
Violet  Math      79   80
  Setting portions of a MultiIndex with xs   Sorting Sort by specific column or an ordered list of columns, with a MultiIndex 
In [99]: df.sort_values(by=("Labs", "II"), ascending=False)
Out[99]: 
               Exams     Labs    
                   I  II    I  II
Student Course                   
Violet  Sci       78  81   81  81
        Math      77  79   81  80
        Comp      76  77   78  79
Quinn   Sci       75  78   78  78
        Math      74  76   78  77
        Comp      73  74   75  76
Ada     Sci       72  75   75  75
        Math      71  73   75  74
        Comp      70  71   72  73
  Partial selection, the need for sortedness GH2995   Levels Prepending a level to a multiindex Flatten Hierarchical columns    Missing data The missing data docs. Fill forward a reversed timeseries 
In [100]: df = pd.DataFrame(
   .....:     np.random.randn(6, 1),
   .....:     index=pd.date_range("2013-08-01", periods=6, freq="B"),
   .....:     columns=list("A"),
   .....: )
   .....: 

In [101]: df.loc[df.index[3], "A"] = np.nan

In [102]: df
Out[102]: 
                   A
2013-08-01  0.721555
2013-08-02 -0.706771
2013-08-05 -1.039575
2013-08-06       NaN
2013-08-07 -0.424972
2013-08-08  0.567020

In [103]: df.reindex(df.index[::-1]).ffill()
Out[103]: 
                   A
2013-08-08  0.567020
2013-08-07 -0.424972
2013-08-06 -0.424972
2013-08-05 -1.039575
2013-08-02 -0.706771
2013-08-01  0.721555
  cumsum reset at NaN values  Replace Using replace with backrefs    Grouping The grouping docs. Basic grouping with apply Unlike agg, apply’s callable is passed a sub-DataFrame which gives you access to all the columns 
In [104]: df = pd.DataFrame(
   .....:     {
   .....:         "animal": "cat dog cat fish dog cat cat".split(),
   .....:         "size": list("SSMMMLL"),
   .....:         "weight": [8, 10, 11, 1, 20, 12, 12],
   .....:         "adult": [False] * 5 + [True] * 2,
   .....:     }
   .....: )
   .....: 

In [105]: df
Out[105]: 
  animal size  weight  adult
0    cat    S       8  False
1    dog    S      10  False
2    cat    M      11  False
3   fish    M       1  False
4    dog    M      20  False
5    cat    L      12   True
6    cat    L      12   True

# List the size of the animals with the highest weight.
In [106]: df.groupby("animal").apply(lambda subf: subf["size"][subf["weight"].idxmax()])
Out[106]: 
animal
cat     L
dog     M
fish    M
dtype: object
  Using get_group 
In [107]: gb = df.groupby(["animal"])

In [108]: gb.get_group("cat")
Out[108]: 
  animal size  weight  adult
0    cat    S       8  False
2    cat    M      11  False
5    cat    L      12   True
6    cat    L      12   True
  Apply to different items in a group 
In [109]: def GrowUp(x):
   .....:     avg_weight = sum(x[x["size"] == "S"].weight * 1.5)
   .....:     avg_weight += sum(x[x["size"] == "M"].weight * 1.25)
   .....:     avg_weight += sum(x[x["size"] == "L"].weight)
   .....:     avg_weight /= len(x)
   .....:     return pd.Series(["L", avg_weight, True], index=["size", "weight", "adult"])
   .....: 

In [110]: expected_df = gb.apply(GrowUp)

In [111]: expected_df
Out[111]: 
       size   weight  adult
animal                     
cat       L  12.4375   True
dog       L  20.0000   True
fish      L   1.2500   True
  Expanding apply 
In [112]: S = pd.Series([i / 100.0 for i in range(1, 11)])

In [113]: def cum_ret(x, y):
   .....:     return x * (1 + y)
   .....: 

In [114]: def red(x):
   .....:     return functools.reduce(cum_ret, x, 1.0)
   .....: 

In [115]: S.expanding().apply(red, raw=True)
Out[115]: 
0    1.010000
1    1.030200
2    1.061106
3    1.103550
4    1.158728
5    1.228251
6    1.314229
7    1.419367
8    1.547110
9    1.701821
dtype: float64
  Replacing some values with mean of the rest of a group 
In [116]: df = pd.DataFrame({"A": [1, 1, 2, 2], "B": [1, -1, 1, 2]})

In [117]: gb = df.groupby("A")

In [118]: def replace(g):
   .....:     mask = g < 0
   .....:     return g.where(mask, g[~mask].mean())
   .....: 

In [119]: gb.transform(replace)
Out[119]: 
     B
0  1.0
1 -1.0
2  1.5
3  1.5
  Sort groups by aggregated data 
In [120]: df = pd.DataFrame(
   .....:     {
   .....:         "code": ["foo", "bar", "baz"] * 2,
   .....:         "data": [0.16, -0.21, 0.33, 0.45, -0.59, 0.62],
   .....:         "flag": [False, True] * 3,
   .....:     }
   .....: )
   .....: 

In [121]: code_groups = df.groupby("code")

In [122]: agg_n_sort_order = code_groups[["data"]].transform(sum).sort_values(by="data")

In [123]: sorted_df = df.loc[agg_n_sort_order.index]

In [124]: sorted_df
Out[124]: 
  code  data   flag
1  bar -0.21   True
4  bar -0.59  False
0  foo  0.16  False
3  foo  0.45   True
2  baz  0.33  False
5  baz  0.62   True
  Create multiple aggregated columns 
In [125]: rng = pd.date_range(start="2014-10-07", periods=10, freq="2min")

In [126]: ts = pd.Series(data=list(range(10)), index=rng)

In [127]: def MyCust(x):
   .....:     if len(x) > 2:
   .....:         return x[1] * 1.234
   .....:     return pd.NaT
   .....: 

In [128]: mhc = {"Mean": np.mean, "Max": np.max, "Custom": MyCust}

In [129]: ts.resample("5min").apply(mhc)
Out[129]: 
                     Mean  Max Custom
2014-10-07 00:00:00   1.0    2  1.234
2014-10-07 00:05:00   3.5    4    NaT
2014-10-07 00:10:00   6.0    7  7.404
2014-10-07 00:15:00   8.5    9    NaT

In [130]: ts
Out[130]: 
2014-10-07 00:00:00    0
2014-10-07 00:02:00    1
2014-10-07 00:04:00    2
2014-10-07 00:06:00    3
2014-10-07 00:08:00    4
2014-10-07 00:10:00    5
2014-10-07 00:12:00    6
2014-10-07 00:14:00    7
2014-10-07 00:16:00    8
2014-10-07 00:18:00    9
Freq: 2T, dtype: int64
  Create a value counts column and reassign back to the DataFrame 
In [131]: df = pd.DataFrame(
   .....:     {"Color": "Red Red Red Blue".split(), "Value": [100, 150, 50, 50]}
   .....: )
   .....: 

In [132]: df
Out[132]: 
  Color  Value
0   Red    100
1   Red    150
2   Red     50
3  Blue     50

In [133]: df["Counts"] = df.groupby(["Color"]).transform(len)

In [134]: df
Out[134]: 
  Color  Value  Counts
0   Red    100       3
1   Red    150       3
2   Red     50       3
3  Blue     50       1
  Shift groups of the values in a column based on the index 
In [135]: df = pd.DataFrame(
   .....:     {"line_race": [10, 10, 8, 10, 10, 8], "beyer": [99, 102, 103, 103, 88, 100]},
   .....:     index=[
   .....:         "Last Gunfighter",
   .....:         "Last Gunfighter",
   .....:         "Last Gunfighter",
   .....:         "Paynter",
   .....:         "Paynter",
   .....:         "Paynter",
   .....:     ],
   .....: )
   .....: 

In [136]: df
Out[136]: 
                 line_race  beyer
Last Gunfighter         10     99
Last Gunfighter         10    102
Last Gunfighter          8    103
Paynter                 10    103
Paynter                 10     88
Paynter                  8    100

In [137]: df["beyer_shifted"] = df.groupby(level=0)["beyer"].shift(1)

In [138]: df
Out[138]: 
                 line_race  beyer  beyer_shifted
Last Gunfighter         10     99            NaN
Last Gunfighter         10    102           99.0
Last Gunfighter          8    103          102.0
Paynter                 10    103            NaN
Paynter                 10     88          103.0
Paynter                  8    100           88.0
  Select row with maximum value from each group 
In [139]: df = pd.DataFrame(
   .....:     {
   .....:         "host": ["other", "other", "that", "this", "this"],
   .....:         "service": ["mail", "web", "mail", "mail", "web"],
   .....:         "no": [1, 2, 1, 2, 1],
   .....:     }
   .....: ).set_index(["host", "service"])
   .....: 

In [140]: mask = df.groupby(level=0).agg("idxmax")

In [141]: df_count = df.loc[mask["no"]].reset_index()

In [142]: df_count
Out[142]: 
    host service  no
0  other     web   2
1   that    mail   1
2   this    mail   2
  Grouping like Python’s itertools.groupby 
In [143]: df = pd.DataFrame([0, 1, 0, 1, 1, 1, 0, 1, 1], columns=["A"])

In [144]: df["A"].groupby((df["A"] != df["A"].shift()).cumsum()).groups
Out[144]: {1: [0], 2: [1], 3: [2], 4: [3, 4, 5], 5: [6], 6: [7, 8]}

In [145]: df["A"].groupby((df["A"] != df["A"].shift()).cumsum()).cumsum()
Out[145]: 
0    0
1    1
2    0
3    1
4    2
5    3
6    0
7    1
8    2
Name: A, dtype: int64
   Expanding data Alignment and to-date Rolling Computation window based on values instead of counts Rolling Mean by Time Interval   Splitting Splitting a frame Create a list of dataframes, split using a delineation based on logic included in rows. 
In [146]: df = pd.DataFrame(
   .....:     data={
   .....:         "Case": ["A", "A", "A", "B", "A", "A", "B", "A", "A"],
   .....:         "Data": np.random.randn(9),
   .....:     }
   .....: )
   .....: 

In [147]: dfs = list(
   .....:     zip(
   .....:         *df.groupby(
   .....:             (1 * (df["Case"] == "B"))
   .....:             .cumsum()
   .....:             .rolling(window=3, min_periods=1)
   .....:             .median()
   .....:         )
   .....:     )
   .....: )[-1]
   .....: 

In [148]: dfs[0]
Out[148]: 
  Case      Data
0    A  0.276232
1    A -1.087401
2    A -0.673690
3    B  0.113648

In [149]: dfs[1]
Out[149]: 
  Case      Data
4    A -1.478427
5    A  0.524988
6    B  0.404705

In [150]: dfs[2]
Out[150]: 
  Case      Data
7    A  0.577046
8    A -1.715002
    Pivot The Pivot docs. Partial sums and subtotals 
In [151]: df = pd.DataFrame(
   .....:     data={
   .....:         "Province": ["ON", "QC", "BC", "AL", "AL", "MN", "ON"],
   .....:         "City": [
   .....:             "Toronto",
   .....:             "Montreal",
   .....:             "Vancouver",
   .....:             "Calgary",
   .....:             "Edmonton",
   .....:             "Winnipeg",
   .....:             "Windsor",
   .....:         ],
   .....:         "Sales": [13, 6, 16, 8, 4, 3, 1],
   .....:     }
   .....: )
   .....: 

In [152]: table = pd.pivot_table(
   .....:     df,
   .....:     values=["Sales"],
   .....:     index=["Province"],
   .....:     columns=["City"],
   .....:     aggfunc=np.sum,
   .....:     margins=True,
   .....: )
   .....: 

In [153]: table.stack("City")
Out[153]: 
                    Sales
Province City            
AL       All         12.0
         Calgary      8.0
         Edmonton     4.0
BC       All         16.0
         Vancouver   16.0
...                   ...
All      Montreal     6.0
         Toronto     13.0
         Vancouver   16.0
         Windsor      1.0
         Winnipeg     3.0

[20 rows x 1 columns]
  Frequency table like plyr in R 
In [154]: grades = [48, 99, 75, 80, 42, 80, 72, 68, 36, 78]

In [155]: df = pd.DataFrame(
   .....:     {
   .....:         "ID": ["x%d" % r for r in range(10)],
   .....:         "Gender": ["F", "M", "F", "M", "F", "M", "F", "M", "M", "M"],
   .....:         "ExamYear": [
   .....:             "2007",
   .....:             "2007",
   .....:             "2007",
   .....:             "2008",
   .....:             "2008",
   .....:             "2008",
   .....:             "2008",
   .....:             "2009",
   .....:             "2009",
   .....:             "2009",
   .....:         ],
   .....:         "Class": [
   .....:             "algebra",
   .....:             "stats",
   .....:             "bio",
   .....:             "algebra",
   .....:             "algebra",
   .....:             "stats",
   .....:             "stats",
   .....:             "algebra",
   .....:             "bio",
   .....:             "bio",
   .....:         ],
   .....:         "Participated": [
   .....:             "yes",
   .....:             "yes",
   .....:             "yes",
   .....:             "yes",
   .....:             "no",
   .....:             "yes",
   .....:             "yes",
   .....:             "yes",
   .....:             "yes",
   .....:             "yes",
   .....:         ],
   .....:         "Passed": ["yes" if x > 50 else "no" for x in grades],
   .....:         "Employed": [
   .....:             True,
   .....:             True,
   .....:             True,
   .....:             False,
   .....:             False,
   .....:             False,
   .....:             False,
   .....:             True,
   .....:             True,
   .....:             False,
   .....:         ],
   .....:         "Grade": grades,
   .....:     }
   .....: )
   .....: 

In [156]: df.groupby("ExamYear").agg(
   .....:     {
   .....:         "Participated": lambda x: x.value_counts()["yes"],
   .....:         "Passed": lambda x: sum(x == "yes"),
   .....:         "Employed": lambda x: sum(x),
   .....:         "Grade": lambda x: sum(x) / len(x),
   .....:     }
   .....: )
   .....: 
Out[156]: 
          Participated  Passed  Employed      Grade
ExamYear                                           
2007                 3       2         3  74.000000
2008                 3       3         0  68.500000
2009                 3       2         2  60.666667
  Plot pandas DataFrame with year over year data To create year and month cross tabulation: 
In [157]: df = pd.DataFrame(
   .....:     {"value": np.random.randn(36)},
   .....:     index=pd.date_range("2011-01-01", freq="M", periods=36),
   .....: )
   .....: 

In [158]: pd.pivot_table(
   .....:     df, index=df.index.month, columns=df.index.year, values="value", aggfunc="sum"
   .....: )
   .....: 
Out[158]: 
        2011      2012      2013
1  -1.039268 -0.968914  2.565646
2  -0.370647 -1.294524  1.431256
3  -1.157892  0.413738  1.340309
4  -1.344312  0.276662 -1.170299
5   0.844885 -0.472035 -0.226169
6   1.075770 -0.013960  0.410835
7  -0.109050 -0.362543  0.813850
8   1.643563 -0.006154  0.132003
9  -1.469388 -0.923061 -0.827317
10  0.357021  0.895717 -0.076467
11 -0.674600  0.805244 -1.187678
12 -1.776904 -1.206412  1.130127
    Apply Rolling apply to organize - Turning embedded lists into a MultiIndex frame 
In [159]: df = pd.DataFrame(
   .....:     data={
   .....:         "A": [[2, 4, 8, 16], [100, 200], [10, 20, 30]],
   .....:         "B": [["a", "b", "c"], ["jj", "kk"], ["ccc"]],
   .....:     },
   .....:     index=["I", "II", "III"],
   .....: )
   .....: 

In [160]: def SeriesFromSubList(aList):
   .....:     return pd.Series(aList)
   .....: 

In [161]: df_orgz = pd.concat(
   .....:     {ind: row.apply(SeriesFromSubList) for ind, row in df.iterrows()}
   .....: )
   .....: 

In [162]: df_orgz
Out[162]: 
         0     1     2     3
I   A    2     4     8  16.0
    B    a     b     c   NaN
II  A  100   200   NaN   NaN
    B   jj    kk   NaN   NaN
III A   10  20.0  30.0   NaN
    B  ccc   NaN   NaN   NaN
  Rolling apply with a DataFrame returning a Series Rolling Apply to multiple columns where function calculates a Series before a Scalar from the Series is returned 
In [163]: df = pd.DataFrame(
   .....:     data=np.random.randn(2000, 2) / 10000,
   .....:     index=pd.date_range("2001-01-01", periods=2000),
   .....:     columns=["A", "B"],
   .....: )
   .....: 

In [164]: df
Out[164]: 
                   A         B
2001-01-01 -0.000144 -0.000141
2001-01-02  0.000161  0.000102
2001-01-03  0.000057  0.000088
2001-01-04 -0.000221  0.000097
2001-01-05 -0.000201 -0.000041
...              ...       ...
2006-06-19  0.000040 -0.000235
2006-06-20 -0.000123 -0.000021
2006-06-21 -0.000113  0.000114
2006-06-22  0.000136  0.000109
2006-06-23  0.000027  0.000030

[2000 rows x 2 columns]

In [165]: def gm(df, const):
   .....:     v = ((((df["A"] + df["B"]) + 1).cumprod()) - 1) * const
   .....:     return v.iloc[-1]
   .....: 

In [166]: s = pd.Series(
   .....:     {
   .....:         df.index[i]: gm(df.iloc[i: min(i + 51, len(df) - 1)], 5)
   .....:         for i in range(len(df) - 50)
   .....:     }
   .....: )
   .....: 

In [167]: s
Out[167]: 
2001-01-01    0.000930
2001-01-02    0.002615
2001-01-03    0.001281
2001-01-04    0.001117
2001-01-05    0.002772
                ...   
2006-04-30    0.003296
2006-05-01    0.002629
2006-05-02    0.002081
2006-05-03    0.004247
2006-05-04    0.003928
Length: 1950, dtype: float64
  Rolling apply with a DataFrame returning a Scalar Rolling Apply to multiple columns where function returns a Scalar (Volume Weighted Average Price) 
In [168]: rng = pd.date_range(start="2014-01-01", periods=100)

In [169]: df = pd.DataFrame(
   .....:     {
   .....:         "Open": np.random.randn(len(rng)),
   .....:         "Close": np.random.randn(len(rng)),
   .....:         "Volume": np.random.randint(100, 2000, len(rng)),
   .....:     },
   .....:     index=rng,
   .....: )
   .....: 

In [170]: df
Out[170]: 
                Open     Close  Volume
2014-01-01 -1.611353 -0.492885    1219
2014-01-02 -3.000951  0.445794    1054
2014-01-03 -0.138359 -0.076081    1381
2014-01-04  0.301568  1.198259    1253
2014-01-05  0.276381 -0.669831    1728
...              ...       ...     ...
2014-04-06 -0.040338  0.937843    1188
2014-04-07  0.359661 -0.285908    1864
2014-04-08  0.060978  1.714814     941
2014-04-09  1.759055 -0.455942    1065
2014-04-10  0.138185 -1.147008    1453

[100 rows x 3 columns]

In [171]: def vwap(bars):
   .....:     return (bars.Close * bars.Volume).sum() / bars.Volume.sum()
   .....: 

In [172]: window = 5

In [173]: s = pd.concat(
   .....:     [
   .....:         (pd.Series(vwap(df.iloc[i: i + window]), index=[df.index[i + window]]))
   .....:         for i in range(len(df) - window)
   .....:     ]
   .....: )
   .....: 

In [174]: s.round(2)
Out[174]: 
2014-01-06    0.02
2014-01-07    0.11
2014-01-08    0.10
2014-01-09    0.07
2014-01-10   -0.29
              ... 
2014-04-06   -0.63
2014-04-07   -0.02
2014-04-08   -0.03
2014-04-09    0.34
2014-04-10    0.29
Length: 95, dtype: float64
     Timeseries Between times Using indexer between time Constructing a datetime range that excludes weekends and includes only certain times Vectorized Lookup Aggregation and plotting time series Turn a matrix with hours in columns and days in rows into a continuous row sequence in the form of a time series. How to rearrange a Python pandas DataFrame? Dealing with duplicates when reindexing a timeseries to a specified frequency Calculate the first day of the month for each entry in a DatetimeIndex 
In [175]: dates = pd.date_range("2000-01-01", periods=5)

In [176]: dates.to_period(freq="M").to_timestamp()
Out[176]: 
DatetimeIndex(['2000-01-01', '2000-01-01', '2000-01-01', '2000-01-01',
               '2000-01-01'],
              dtype='datetime64[ns]', freq=None)
   Resampling The Resample docs. Using Grouper instead of TimeGrouper for time grouping of values Time grouping with some missing values Valid frequency arguments to Grouper Timeseries Grouping using a MultiIndex Using TimeGrouper and another grouping to create subgroups, then apply a custom function GH3791 Resampling with custom periods Resample intraday frame without adding new days Resample minute data Resample with groupby    Merge The Join docs. Concatenate two dataframes with overlapping index (emulate R rbind) 
In [177]: rng = pd.date_range("2000-01-01", periods=6)

In [178]: df1 = pd.DataFrame(np.random.randn(6, 3), index=rng, columns=["A", "B", "C"])

In [179]: df2 = df1.copy()
  Depending on df construction, ignore_index may be needed 
In [180]: df = pd.concat([df1, df2], ignore_index=True)

In [181]: df
Out[181]: 
           A         B         C
0  -0.870117 -0.479265 -0.790855
1   0.144817  1.726395 -0.464535
2  -0.821906  1.597605  0.187307
3  -0.128342 -1.511638 -0.289858
4   0.399194 -1.430030 -0.639760
5   1.115116 -2.012600  1.810662
6  -0.870117 -0.479265 -0.790855
7   0.144817  1.726395 -0.464535
8  -0.821906  1.597605  0.187307
9  -0.128342 -1.511638 -0.289858
10  0.399194 -1.430030 -0.639760
11  1.115116 -2.012600  1.810662
  Self Join of a DataFrame GH2996 
In [182]: df = pd.DataFrame(
   .....:     data={
   .....:         "Area": ["A"] * 5 + ["C"] * 2,
   .....:         "Bins": [110] * 2 + [160] * 3 + [40] * 2,
   .....:         "Test_0": [0, 1, 0, 1, 2, 0, 1],
   .....:         "Data": np.random.randn(7),
   .....:     }
   .....: )
   .....: 

In [183]: df
Out[183]: 
  Area  Bins  Test_0      Data
0    A   110       0 -0.433937
1    A   110       1 -0.160552
2    A   160       0  0.744434
3    A   160       1  1.754213
4    A   160       2  0.000850
5    C    40       0  0.342243
6    C    40       1  1.070599

In [184]: df["Test_1"] = df["Test_0"] - 1

In [185]: pd.merge(
   .....:     df,
   .....:     df,
   .....:     left_on=["Bins", "Area", "Test_0"],
   .....:     right_on=["Bins", "Area", "Test_1"],
   .....:     suffixes=("_L", "_R"),
   .....: )
   .....: 
Out[185]: 
  Area  Bins  Test_0_L    Data_L  Test_1_L  Test_0_R    Data_R  Test_1_R
0    A   110         0 -0.433937        -1         1 -0.160552         0
1    A   160         0  0.744434        -1         1  1.754213         0
2    A   160         1  1.754213         0         2  0.000850         1
3    C    40         0  0.342243        -1         1  1.070599         0
  How to set the index and join KDB like asof join Join with a criteria based on the values Using searchsorted to merge based on values inside a range   Plotting The Plotting docs. Make Matplotlib look like R Setting x-axis major and minor labels Plotting multiple charts in an IPython Jupyter notebook Creating a multi-line plot Plotting a heatmap Annotate a time-series plot Annotate a time-series plot #2 Generate Embedded plots in excel files using Pandas, Vincent and xlsxwriter Boxplot for each quartile of a stratifying variable 
In [186]: df = pd.DataFrame(
   .....:     {
   .....:         "stratifying_var": np.random.uniform(0, 100, 20),
   .....:         "price": np.random.normal(100, 5, 20),
   .....:     }
   .....: )
   .....: 

In [187]: df["quartiles"] = pd.qcut(
   .....:     df["stratifying_var"], 4, labels=["0-25%", "25-50%", "50-75%", "75-100%"]
   .....: )
   .....: 

In [188]: df.boxplot(column="price", by="quartiles")
Out[188]: <AxesSubplot:title={'center':'price'}, xlabel='quartiles'>
     Data in/out Performance comparison of SQL vs HDF5  CSV The CSV docs read_csv in action appending to a csv Reading a csv chunk-by-chunk Reading only certain rows of a csv chunk-by-chunk Reading the first few lines of a frame Reading a file that is compressed but not by gzip/bz2 (the native compressed formats which read_csv understands). This example shows a WinZipped file, but is a general application of opening the file within a context manager and using that handle to read. See here Inferring dtypes from a file Dealing with bad lines GH2886 Write a multi-row index CSV without writing duplicates  Reading multiple files to create a single DataFrame The best way to combine multiple files into a single DataFrame is to read the individual frames one by one, put all of the individual frames into a list, and then combine the frames in the list using pd.concat(): 
In [189]: for i in range(3):
   .....:     data = pd.DataFrame(np.random.randn(10, 4))
   .....:     data.to_csv("file_{}.csv".format(i))
   .....: 

In [190]: files = ["file_0.csv", "file_1.csv", "file_2.csv"]

In [191]: result = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)
  You can use the same approach to read all files matching a pattern. Here is an example using glob: 
In [192]: import glob

In [193]: import os

In [194]: files = glob.glob("file_*.csv")

In [195]: result = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)
  Finally, this strategy will work with the other pd.read_*(...) functions described in the io docs.   Parsing date components in multi-columns Parsing date components in multi-columns is faster with a format 
In [196]: i = pd.date_range("20000101", periods=10000)

In [197]: df = pd.DataFrame({"year": i.year, "month": i.month, "day": i.day})

In [198]: df.head()
Out[198]: 
   year  month  day
0  2000      1    1
1  2000      1    2
2  2000      1    3
3  2000      1    4
4  2000      1    5

In [199]: %timeit pd.to_datetime(df.year * 10000 + df.month * 100 + df.day, format='%Y%m%d')
   .....: ds = df.apply(lambda x: "%04d%02d%02d" % (x["year"], x["month"], x["day"]), axis=1)
   .....: ds.head()
   .....: %timeit pd.to_datetime(ds)
   .....: 
8.7 ms +- 765 us per loop (mean +- std. dev. of 7 runs, 100 loops each)
2.1 ms +- 419 us per loop (mean +- std. dev. of 7 runs, 100 loops each)
    Skip row between header and data 
In [200]: data = """;;;;
   .....:  ;;;;
   .....:  ;;;;
   .....:  ;;;;
   .....:  ;;;;
   .....:  ;;;;
   .....: ;;;;
   .....:  ;;;;
   .....:  ;;;;
   .....: ;;;;
   .....: date;Param1;Param2;Param4;Param5
   .....:     ;m²;°C;m²;m
   .....: ;;;;
   .....: 01.01.1990 00:00;1;1;2;3
   .....: 01.01.1990 01:00;5;3;4;5
   .....: 01.01.1990 02:00;9;5;6;7
   .....: 01.01.1990 03:00;13;7;8;9
   .....: 01.01.1990 04:00;17;9;10;11
   .....: 01.01.1990 05:00;21;11;12;13
   .....: """
   .....: 
   Option 1: pass rows explicitly to skip rows 
In [201]: from io import StringIO

In [202]: pd.read_csv(
   .....:     StringIO(data),
   .....:     sep=";",
   .....:     skiprows=[11, 12],
   .....:     index_col=0,
   .....:     parse_dates=True,
   .....:     header=10,
   .....: )
   .....: 
Out[202]: 
                     Param1  Param2  Param4  Param5
date                                               
1990-01-01 00:00:00       1       1       2       3
1990-01-01 01:00:00       5       3       4       5
1990-01-01 02:00:00       9       5       6       7
1990-01-01 03:00:00      13       7       8       9
1990-01-01 04:00:00      17       9      10      11
1990-01-01 05:00:00      21      11      12      13
    Option 2: read column names and then data 
In [203]: pd.read_csv(StringIO(data), sep=";", header=10, nrows=10).columns
Out[203]: Index(['date', 'Param1', 'Param2', 'Param4', 'Param5'], dtype='object')

In [204]: columns = pd.read_csv(StringIO(data), sep=";", header=10, nrows=10).columns

In [205]: pd.read_csv(
   .....:     StringIO(data), sep=";", index_col=0, header=12, parse_dates=True, names=columns
   .....: )
   .....: 
Out[205]: 
                     Param1  Param2  Param4  Param5
date                                               
1990-01-01 00:00:00       1       1       2       3
1990-01-01 01:00:00       5       3       4       5
1990-01-01 02:00:00       9       5       6       7
1990-01-01 03:00:00      13       7       8       9
1990-01-01 04:00:00      17       9      10      11
1990-01-01 05:00:00      21      11      12      13
      SQL The SQL docs Reading from databases with SQL   Excel The Excel docs Reading from a filelike handle Modifying formatting in XlsxWriter output Loading only visible sheets GH19842#issuecomment-892150745   HTML Reading HTML tables from a server that cannot handle the default request header   HDFStore The HDFStores docs Simple queries with a Timestamp Index Managing heterogeneous data using a linked multiple table hierarchy GH3032 Merging on-disk tables with millions of rows Avoiding inconsistencies when writing to a store from multiple processes/threads De-duplicating a large store by chunks, essentially a recursive reduction operation. Shows a function for taking in data from csv file and creating a store by chunks, with date parsing as well. See here Creating a store chunk-by-chunk from a csv file Appending to a store, while creating a unique index Large Data work flows Reading in a sequence of files, then providing a global unique index to a store while appending Groupby on a HDFStore with low group density Groupby on a HDFStore with high group density Hierarchical queries on a HDFStore Counting with a HDFStore Troubleshoot HDFStore exceptions Setting min_itemsize with strings Using ptrepack to create a completely-sorted-index on a store Storing Attributes to a group node 
In [206]: df = pd.DataFrame(np.random.randn(8, 3))

In [207]: store = pd.HDFStore("test.h5")

In [208]: store.put("df", df)

# you can store an arbitrary Python object via pickle
In [209]: store.get_storer("df").attrs.my_attribute = {"A": 10}

In [210]: store.get_storer("df").attrs.my_attribute
Out[210]: {'A': 10}
  You can create or load a HDFStore in-memory by passing the driver parameter to PyTables. Changes are only written to disk when the HDFStore is closed. 
In [211]: store = pd.HDFStore("test.h5", "w", driver="H5FD_CORE")

In [212]: df = pd.DataFrame(np.random.randn(8, 3))

In [213]: store["test"] = df

# only after closing the store, data is written to disk:
In [214]: store.close()
    Binary files pandas readily accepts NumPy record arrays, if you need to read in a binary file consisting of an array of C structs. For example, given this C program in a file called main.c compiled with gcc main.c -std=gnu99 on a 64-bit machine, 
#include <stdio.h>
#include <stdint.h>

typedef struct _Data
{
    int32_t count;
    double avg;
    float scale;
} Data;

int main(int argc, const char *argv[])
{
    size_t n = 10;
    Data d[n];

    for (int i = 0; i < n; ++i)
    {
        d[i].count = i;
        d[i].avg = i + 1.0;
        d[i].scale = (float) i + 2.0f;
    }

    FILE *file = fopen("binary.dat", "wb");
    fwrite(&d, sizeof(Data), n, file);
    fclose(file);

    return 0;
}
  the following Python code will read the binary file 'binary.dat' into a pandas DataFrame, where each element of the struct corresponds to a column in the frame: 
names = "count", "avg", "scale"

# note that the offsets are larger than the size of the type because of
# struct padding
offsets = 0, 8, 16
formats = "i4", "f8", "f4"
dt = np.dtype({"names": names, "offsets": offsets, "formats": formats}, align=True)
df = pd.DataFrame(np.fromfile("binary.dat", dt))
   Note The offsets of the structure elements may be different depending on the architecture of the machine on which the file was created. Using a raw binary file format like this for general data storage is not recommended, as it is not cross platform. We recommended either HDF5 or parquet, both of which are supported by pandas’ IO facilities.     Computation Numerical integration (sample-based) of a time series  Correlation Often it’s useful to obtain the lower (or upper) triangular form of a correlation matrix calculated from DataFrame.corr(). This can be achieved by passing a boolean mask to where as follows: 
In [215]: df = pd.DataFrame(np.random.random(size=(100, 5)))

In [216]: corr_mat = df.corr()

In [217]: mask = np.tril(np.ones_like(corr_mat, dtype=np.bool_), k=-1)

In [218]: corr_mat.where(mask)
Out[218]: 
          0         1         2        3   4
0       NaN       NaN       NaN      NaN NaN
1 -0.079861       NaN       NaN      NaN NaN
2 -0.236573  0.183801       NaN      NaN NaN
3 -0.013795 -0.051975  0.037235      NaN NaN
4 -0.031974  0.118342 -0.073499 -0.02063 NaN
  The method argument within DataFrame.corr can accept a callable in addition to the named correlation types. Here we compute the distance correlation matrix for a DataFrame object. 
In [219]: def distcorr(x, y):
   .....:     n = len(x)
   .....:     a = np.zeros(shape=(n, n))
   .....:     b = np.zeros(shape=(n, n))
   .....:     for i in range(n):
   .....:         for j in range(i + 1, n):
   .....:             a[i, j] = abs(x[i] - x[j])
   .....:             b[i, j] = abs(y[i] - y[j])
   .....:     a += a.T
   .....:     b += b.T
   .....:     a_bar = np.vstack([np.nanmean(a, axis=0)] * n)
   .....:     b_bar = np.vstack([np.nanmean(b, axis=0)] * n)
   .....:     A = a - a_bar - a_bar.T + np.full(shape=(n, n), fill_value=a_bar.mean())
   .....:     B = b - b_bar - b_bar.T + np.full(shape=(n, n), fill_value=b_bar.mean())
   .....:     cov_ab = np.sqrt(np.nansum(A * B)) / n
   .....:     std_a = np.sqrt(np.sqrt(np.nansum(A ** 2)) / n)
   .....:     std_b = np.sqrt(np.sqrt(np.nansum(B ** 2)) / n)
   .....:     return cov_ab / std_a / std_b
   .....: 

In [220]: df = pd.DataFrame(np.random.normal(size=(100, 3)))

In [221]: df.corr(method=distcorr)
Out[221]: 
          0         1         2
0  1.000000  0.197613  0.216328
1  0.197613  1.000000  0.208749
2  0.216328  0.208749  1.000000
     Timedeltas The Timedeltas docs. Using timedeltas 
In [222]: import datetime

In [223]: s = pd.Series(pd.date_range("2012-1-1", periods=3, freq="D"))

In [224]: s - s.max()
Out[224]: 
0   -2 days
1   -1 days
2    0 days
dtype: timedelta64[ns]

In [225]: s.max() - s
Out[225]: 
0   2 days
1   1 days
2   0 days
dtype: timedelta64[ns]

In [226]: s - datetime.datetime(2011, 1, 1, 3, 5)
Out[226]: 
0   364 days 20:55:00
1   365 days 20:55:00
2   366 days 20:55:00
dtype: timedelta64[ns]

In [227]: s + datetime.timedelta(minutes=5)
Out[227]: 
0   2012-01-01 00:05:00
1   2012-01-02 00:05:00
2   2012-01-03 00:05:00
dtype: datetime64[ns]

In [228]: datetime.datetime(2011, 1, 1, 3, 5) - s
Out[228]: 
0   -365 days +03:05:00
1   -366 days +03:05:00
2   -367 days +03:05:00
dtype: timedelta64[ns]

In [229]: datetime.timedelta(minutes=5) + s
Out[229]: 
0   2012-01-01 00:05:00
1   2012-01-02 00:05:00
2   2012-01-03 00:05:00
dtype: datetime64[ns]
  Adding and subtracting deltas and dates 
In [230]: deltas = pd.Series([datetime.timedelta(days=i) for i in range(3)])

In [231]: df = pd.DataFrame({"A": s, "B": deltas})

In [232]: df
Out[232]: 
           A      B
0 2012-01-01 0 days
1 2012-01-02 1 days
2 2012-01-03 2 days

In [233]: df["New Dates"] = df["A"] + df["B"]

In [234]: df["Delta"] = df["A"] - df["New Dates"]

In [235]: df
Out[235]: 
           A      B  New Dates   Delta
0 2012-01-01 0 days 2012-01-01  0 days
1 2012-01-02 1 days 2012-01-03 -1 days
2 2012-01-03 2 days 2012-01-05 -2 days

In [236]: df.dtypes
Out[236]: 
A             datetime64[ns]
B            timedelta64[ns]
New Dates     datetime64[ns]
Delta        timedelta64[ns]
dtype: object
  Another example Values can be set to NaT using np.nan, similar to datetime 
In [237]: y = s - s.shift()

In [238]: y
Out[238]: 
0      NaT
1   1 days
2   1 days
dtype: timedelta64[ns]

In [239]: y[1] = np.nan

In [240]: y
Out[240]: 
0      NaT
1      NaT
2   1 days
dtype: timedelta64[ns]
    Creating example data To create a dataframe from every combination of some given values, like R’s expand.grid() function, we can create a dict where the keys are column names and the values are lists of the data values: 
In [241]: def expand_grid(data_dict):
   .....:     rows = itertools.product(*data_dict.values())
   .....:     return pd.DataFrame.from_records(rows, columns=data_dict.keys())
   .....: 

In [242]: df = expand_grid(
   .....:     {"height": [60, 70], "weight": [100, 140, 180], "sex": ["Male", "Female"]}
   .....: )
   .....: 

In [243]: df
Out[243]: 
    height  weight     sex
0       60     100    Male
1       60     100  Female
2       60     140    Male
3       60     140  Female
4       60     180    Male
5       60     180  Female
6       70     100    Male
7       70     100  Female
8       70     140    Male
9       70     140  Female
10      70     180    Male
11      70     180  Female
pygame.event.Event() 
 create a new event object Event(type, dict) -> EventType instance Event(type, **attributes) -> EventType instance  Creates a new event with the given type and attributes. The attributes can come from a dictionary argument with string keys or from keyword arguments.
def f_38535931(df, tuples):
    """pandas: delete rows in dataframe `df` based on multiple columns values
    """
    return  
 --------------------

def f_13945749(goals, penalties):
    """format the variables `goals` and `penalties` using string formatting
    """
    return  
 --------------------

def f_13945749(goals, penalties):
    """format string "({} goals, ${})" with variables `goals` and `penalties`
    """
    return  
 --------------------

def f_18524642(L):
    """convert list of lists `L` to list of integers
    """
    return  
 --------------------

def f_18524642(L):
    """convert a list of lists `L` to list of integers
    """
     
 --------------------

def f_7138686(lines, myfile):
    """write the elements of list `lines` concatenated by special character '\n' to file `myfile`
    """
     
 --------------------
Please refer to the following documentation to generate the code:
comment(text)  
Creates a comment with the given text. If insert_comments is true, this will also add it to the tree.  New in version 3.8.
text  
The source code text involved in the error.
token_generator  
Instance of the class to check the password. This will default to default_token_generator, it’s an instance of django.contrib.auth.tokens.PasswordResetTokenGenerator.
token_generator  
Instance of the class to check the one time link. This will default to default_token_generator, it’s an instance of django.contrib.auth.tokens.PasswordResetTokenGenerator.
textwrap.dedent(text)  
Remove any common leading whitespace from every line in text. This can be used to make triple-quoted strings line up with the left edge of the display, while still presenting them in the source code in indented form. Note that tabs and spaces are both treated as whitespace, but they are not equal: the lines "  hello" and "\thello" are considered to have no common leading whitespace. Lines containing only whitespace are ignored in the input and normalized to a single newline character in the output. For example: def test():
    # end first line with \ to avoid the empty line!
    s = '''\
    hello
      world
    '''
    print(repr(s))          # prints '    hello\n      world\n    '
    print(repr(dedent(s)))  # prints 'hello\n  world\n'
def f_17238587(text):
    """Remove duplicate words from a string `text` using regex
    """
     
 --------------------
Please refer to the following documentation to generate the code:
pandas.DataFrame.count   DataFrame.count(axis=0, level=None, numeric_only=False)[source]
 
Count non-NA cells for each column or row. The values None, NaN, NaT, and optionally numpy.inf (depending on pandas.options.mode.use_inf_as_na) are considered NA.  Parameters 
 
axis:{0 or ‘index’, 1 or ‘columns’}, default 0


If 0 or ‘index’ counts are generated for each column. If 1 or ‘columns’ counts are generated for each row.  
level:int or str, optional


If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a DataFrame. A str specifies the level name.  
numeric_only:bool, default False


Include only float, int or boolean data.    Returns 
 Series or DataFrame

For each column/row the number of non-NA/null entries. If level is specified returns a DataFrame.      See also  Series.count

Number of non-NA elements in a Series.  DataFrame.value_counts

Count unique combinations of columns.  DataFrame.shape

Number of DataFrame rows and columns (including NA elements).  DataFrame.isna

Boolean same-sized DataFrame showing places of NA elements.    Examples Constructing DataFrame from a dictionary: 
>>> df = pd.DataFrame({"Person":
...                    ["John", "Myla", "Lewis", "John", "Myla"],
...                    "Age": [24., np.nan, 21., 33, 26],
...                    "Single": [False, True, True, True, False]})
>>> df
   Person   Age  Single
0    John  24.0   False
1    Myla   NaN    True
2   Lewis  21.0    True
3    John  33.0    True
4    Myla  26.0   False
  Notice the uncounted NA values: 
>>> df.count()
Person    5
Age       4
Single    5
dtype: int64
  Counts for each row: 
>>> df.count(axis='columns')
0    3
1    2
2    3
3    3
4    3
dtype: int64
pandas.DataFrame.value_counts   DataFrame.value_counts(subset=None, normalize=False, sort=True, ascending=False, dropna=True)[source]
 
Return a Series containing counts of unique rows in the DataFrame.  New in version 1.1.0.   Parameters 
 
subset:list-like, optional


Columns to use when counting unique combinations.  
normalize:bool, default False


Return proportions rather than frequencies.  
sort:bool, default True


Sort by frequencies.  
ascending:bool, default False


Sort in ascending order.  
dropna:bool, default True


Don’t include counts of rows that contain NA values.  New in version 1.3.0.     Returns 
 Series
    See also  Series.value_counts

Equivalent method on Series.    Notes The returned Series will have a MultiIndex with one level per input column. By default, rows that contain any NA values are omitted from the result. By default, the resulting Series will be in descending order so that the first element is the most frequently-occurring row. Examples 
>>> df = pd.DataFrame({'num_legs': [2, 4, 4, 6],
...                    'num_wings': [2, 0, 0, 0]},
...                   index=['falcon', 'dog', 'cat', 'ant'])
>>> df
        num_legs  num_wings
falcon         2          2
dog            4          0
cat            4          0
ant            6          0
  
>>> df.value_counts()
num_legs  num_wings
4         0            2
2         2            1
6         0            1
dtype: int64
  
>>> df.value_counts(sort=False)
num_legs  num_wings
2         2            1
4         0            2
6         0            1
dtype: int64
  
>>> df.value_counts(ascending=True)
num_legs  num_wings
2         2            1
6         0            1
4         0            2
dtype: int64
  
>>> df.value_counts(normalize=True)
num_legs  num_wings
4         0            0.50
2         2            0.25
6         0            0.25
dtype: float64
  With dropna set to False we can also count rows with NA values. 
>>> df = pd.DataFrame({'first_name': ['John', 'Anne', 'John', 'Beth'],
...                    'middle_name': ['Smith', pd.NA, pd.NA, 'Louise']})
>>> df
  first_name middle_name
0       John       Smith
1       Anne        <NA>
2       John        <NA>
3       Beth      Louise
  
>>> df.value_counts()
first_name  middle_name
Beth        Louise         1
John        Smith          1
dtype: int64
  
>>> df.value_counts(dropna=False)
first_name  middle_name
Anne        NaN            1
Beth        Louise         1
John        Smith          1
            NaN            1
dtype: int64
pandas.Series.count   Series.count(level=None)[source]
 
Return number of non-NA/null observations in the Series.  Parameters 
 
level:int or level name, default None


If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a smaller Series.    Returns 
 int or Series (if level specified)

Number of non-null values in the Series.      See also  DataFrame.count

Count non-NA cells for each column or row.    Examples 
>>> s = pd.Series([0.0, 1.0, np.nan])
>>> s.count()
2
pandas.Series.value_counts   Series.value_counts(normalize=False, sort=True, ascending=False, bins=None, dropna=True)[source]
 
Return a Series containing counts of unique values. The resulting object will be in descending order so that the first element is the most frequently-occurring element. Excludes NA values by default.  Parameters 
 
normalize:bool, default False


If True then the object returned will contain the relative frequencies of the unique values.  
sort:bool, default True


Sort by frequencies.  
ascending:bool, default False


Sort in ascending order.  
bins:int, optional


Rather than count values, group them into half-open bins, a convenience for pd.cut, only works with numeric data.  
dropna:bool, default True


Don’t include counts of NaN.    Returns 
 Series
    See also  Series.count

Number of non-NA elements in a Series.  DataFrame.count

Number of non-NA elements in a DataFrame.  DataFrame.value_counts

Equivalent method on DataFrames.    Examples 
>>> index = pd.Index([3, 1, 2, 3, 4, np.nan])
>>> index.value_counts()
3.0    2
1.0    1
2.0    1
4.0    1
dtype: int64
  With normalize set to True, returns the relative frequency by dividing all values by the sum of values. 
>>> s = pd.Series([3, 1, 2, 3, 4, np.nan])
>>> s.value_counts(normalize=True)
3.0    0.4
1.0    0.2
2.0    0.2
4.0    0.2
dtype: float64
  bins Bins can be useful for going from a continuous variable to a categorical variable; instead of counting unique apparitions of values, divide the index in the specified number of half-open bins. 
>>> s.value_counts(bins=3)
(0.996, 2.0]    2
(2.0, 3.0]      2
(3.0, 4.0]      1
dtype: int64
  dropna With dropna set to False we can also see NaN index values. 
>>> s.value_counts(dropna=False)
3.0    2
1.0    1
2.0    1
4.0    1
NaN    1
dtype: int64
pandas.core.groupby.DataFrameGroupBy.nunique   DataFrameGroupBy.nunique(dropna=True)[source]
 
Return DataFrame with counts of unique elements in each position.  Parameters 
 
dropna:bool, default True


Don’t include NaN in the counts.    Returns 
 nunique: DataFrame
   Examples 
>>> df = pd.DataFrame({'id': ['spam', 'egg', 'egg', 'spam',
...                           'ham', 'ham'],
...                    'value1': [1, 5, 5, 2, 5, 5],
...                    'value2': list('abbaxy')})
>>> df
     id  value1 value2
0  spam       1      a
1   egg       5      b
2   egg       5      b
3  spam       2      a
4   ham       5      x
5   ham       5      y
  
>>> df.groupby('id').nunique()
      value1  value2
id
egg        1       1
ham        1       2
spam       2       1
  Check for rows with the same id but conflicting values: 
>>> df.groupby('id').filter(lambda g: (g.nunique() > 1).any())
     id  value1 value2
0  spam       1      a
3  spam       2      a
4   ham       5      x
5   ham       5      y
def f_26053849(df):
    """count non zero values in each column in pandas data frame `df`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
winreg.ExpandEnvironmentStrings(str)  
Expands environment variable placeholders %NAME% in strings like REG_EXPAND_SZ: >>> ExpandEnvironmentStrings('%windir%')
'C:\\Windows'
 Raises an auditing event winreg.ExpandEnvironmentStrings with argument str.
glob.escape(pathname)  
Escape all special characters ('?', '*' and '['). This is useful if you want to match an arbitrary literal string that may have special characters in it. Special characters in drive/UNC sharepoints are not escaped, e.g. on Windows escape('//?/c:/Quo vadis?.txt') returns '//?/c:/Quo vadis[?].txt'.  New in version 3.4.
email.utils.quote(str)  
Return a new string with backslashes in str replaced by two backslashes, and double quotes replaced by backslash-double quote.
numpy.distutils.misc_util.cyg2win32(path: str) → str[source]
 
Convert a path from Cygwin-native to Windows-native. Uses the cygpath utility (part of the Base install) to do the actual conversion. Falls back to returning the original path if this fails. Handles the default /cygdrive mount prefix as well as the /proc/cygdrive portable prefix, custom cygdrive prefixes such as / or /mnt, and absolute paths such as /usr/src/ or /home/username  Parameters 
 
pathstr


The path to convert    Returns 
 
converted_pathstr


The converted path     Notes Documentation for cygpath utility: https://cygwin.com/cygwin-ug-net/cygpath.html Documentation for the C function it wraps: https://cygwin.com/cygwin-api/func-cygwin-conv-path.html
winreg.HKEY_DYN_DATA  
This key is not used in versions of Windows after 98.
def f_15534223():
    """search for string that matches regular expression pattern '(?<!Distillr)\\\\AcroTray\\.exe' in string 'C:\\SomeDir\\AcroTray.exe'
    """
    return  
 --------------------

def f_5453026():
    """split string 'QH QD JC KD JS' into a list on white spaces
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
xml.parsers.expat.errors.XML_ERROR_TAG_MISMATCH  
An end tag did not match the innermost open start tag.
HTMLParser.get_starttag_text()  
Return the text of the most recently opened start tag. This should not normally be needed for structured processing, but may be useful in dealing with HTML “as deployed” or for re-generating input with minimal changes (whitespace between attributes can be preserved, etc.).
SimpleTestCase.assertInHTML(needle, haystack, count=None, msg_prefix='')  
Asserts that the HTML fragment needle is contained in the haystack one. If the count integer argument is specified, then additionally the number of needle occurrences will be strictly verified. Whitespace in most cases is ignored, and attribute ordering is not significant. See assertHTMLEqual() for more details.
findtext(match, default=None, namespaces=None)  
Same as Element.findtext(), starting at the root of the tree.
iterfind(match, namespaces=None)  
Same as Element.iterfind(), starting at the root of the tree.  New in version 3.2.
def f_18168684(line):
    """search for occurrences of regex pattern '>.*<' in xml string `line`
    """
    return  
 --------------------

def f_4914277(filename):
    """erase all the contents of a file `filename`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
classmethod datetime.strptime(date_string, format)  
Return a datetime corresponding to date_string, parsed according to format. This is equivalent to: datetime(*(time.strptime(date_string, format)[0:6]))
 ValueError is raised if the date_string and format can’t be parsed by time.strptime() or if it returns a value which isn’t a time tuple. For a complete list of formatting directives, see strftime() and strptime() Behavior.
classmethod date.fromisoformat(date_string)  
Return a date corresponding to a date_string given in the format YYYY-MM-DD: >>> from datetime import date
>>> date.fromisoformat('2019-12-04')
datetime.date(2019, 12, 4)
 This is the inverse of date.isoformat(). It only supports the format YYYY-MM-DD.  New in version 3.7.
pandas.StringDtype   classpandas.StringDtype(storage=None)[source]
 
Extension dtype for string data.  New in version 1.0.0.   Warning StringDtype is considered experimental. The implementation and parts of the API may change without warning. In particular, StringDtype.na_value may change to no longer be numpy.nan.   Parameters 
 
storage:{“python”, “pyarrow”}, optional


If not given, the value of pd.options.mode.string_storage.     Examples 
>>> pd.StringDtype()
string[python]
  
>>> pd.StringDtype(storage="pyarrow")
string[pyarrow]
  Attributes       
None     Methods       
None
classmethod datetime.fromisoformat(date_string)  
Return a datetime corresponding to a date_string in one of the formats emitted by date.isoformat() and datetime.isoformat(). Specifically, this function supports strings in the format: YYYY-MM-DD[*HH[:MM[:SS[.fff[fff]]]][+HH:MM[:SS[.ffffff]]]]
 where * can match any single character.  Caution This does not support parsing arbitrary ISO 8601 strings - it is only intended as the inverse operation of datetime.isoformat(). A more full-featured ISO 8601 parser, dateutil.parser.isoparse is available in the third-party package dateutil.  Examples: >>> from datetime import datetime
>>> datetime.fromisoformat('2011-11-04')
datetime.datetime(2011, 11, 4, 0, 0)
>>> datetime.fromisoformat('2011-11-04T00:05:23')
datetime.datetime(2011, 11, 4, 0, 5, 23)
>>> datetime.fromisoformat('2011-11-04 00:05:23.283')
datetime.datetime(2011, 11, 4, 0, 5, 23, 283000)
>>> datetime.fromisoformat('2011-11-04 00:05:23.283+00:00')
datetime.datetime(2011, 11, 4, 0, 5, 23, 283000, tzinfo=datetime.timezone.utc)
>>> datetime.fromisoformat('2011-11-04T00:05:23+04:00')   
datetime.datetime(2011, 11, 4, 0, 5, 23,
    tzinfo=datetime.timezone(datetime.timedelta(seconds=14400)))
  New in version 3.7.
tf.keras.layers.Maximum     View source on GitHub    Layer that computes the maximum (element-wise) a list of inputs. Inherits From: Layer, Module  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.keras.layers.Maximum  
tf.keras.layers.Maximum(
    **kwargs
)
 It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape). 
tf.keras.layers.Maximum()([np.arange(5).reshape(5, 1),
                           np.arange(5, 10).reshape(5, 1)])
<tf.Tensor: shape=(5, 1), dtype=int64, numpy=
array([[5],
     [6],
     [7],
     [8],
     [9]])>
 
x1 = tf.keras.layers.Dense(8)(np.arange(10).reshape(5, 2))
x2 = tf.keras.layers.Dense(8)(np.arange(10, 20).reshape(5, 2))
maxed = tf.keras.layers.Maximum()([x1, x2])
maxed.shape
TensorShape([5, 8])

 


 Arguments
  **kwargs   standard layer keyword arguments.
def f_19068269(string_date):
    """convert a string `string_date` into datetime using the format '%Y-%m-%d %H:%M:%S.%f'
    """
    return  
 --------------------

def f_20683167(thelist):
    """find the index of a list with the first element equal to '332' within the list of lists `thelist`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
tf.strings.lower Converts all uppercase characters into their respective lowercase replacements.  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.strings.lower  
tf.strings.lower(
    input, encoding='', name=None
)
 Example: 
tf.strings.lower("CamelCase string and ALL CAPS")
<tf.Tensor: shape=(), dtype=string, numpy=b'camelcase string and all caps'>

 


 Args
  input   A Tensor of type string.  
  encoding   An optional string. Defaults to "".  
  name   A name for the operation (optional).   
 


 Returns   A Tensor of type string.
numpy.char.lower   char.lower(a)[source]
 
Return an array with the elements converted to lowercase. Call str.lower element-wise. For 8-bit strings, this method is locale-dependent.  Parameters 
 
aarray_like, {str, unicode}


Input array.    Returns 
 
outndarray, {str, unicode}


Output array of str or unicode, depending on input type      See also  str.lower
  Examples >>> c = np.array(['A1B C', '1BCA', 'BCA1']); c
array(['A1B C', '1BCA', 'BCA1'], dtype='<U5')
>>> np.char.lower(c)
array(['a1b c', '1bca', 'bca1'], dtype='<U5')
operator.ilshift(a, b)  
operator.__ilshift__(a, b)  
a = ilshift(a, b) is equivalent to a <<= b.
operator.ilshift(a, b)  
operator.__ilshift__(a, b)  
a = ilshift(a, b) is equivalent to a <<= b.
str.isalnum()  
Return True if all characters in the string are alphanumeric and there is at least one character, False otherwise. A character c is alphanumeric if one of the following returns True: c.isalpha(), c.isdecimal(), c.isdigit(), or c.isnumeric().
def f_30693804(text):
    """lower a string `text` and remove non-alphanumeric characters aside from space
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
operator.ilshift(a, b)  
operator.__ilshift__(a, b)  
a = ilshift(a, b) is equivalent to a <<= b.
operator.ilshift(a, b)  
operator.__ilshift__(a, b)  
a = ilshift(a, b) is equivalent to a <<= b.
str.isalnum()  
Return True if all characters in the string are alphanumeric and there is at least one character, False otherwise. A character c is alphanumeric if one of the following returns True: c.isalpha(), c.isdecimal(), c.isdigit(), or c.isnumeric().
numpy.genfromtxt   numpy.genfromtxt(fname, dtype=<class 'float'>, comments='#', delimiter=None, skip_header=0, skip_footer=0, converters=None, missing_values=None, filling_values=None, usecols=None, names=None, excludelist=None, deletechars=" !#$%&'()*+, -./:;<=>?@[\\]^{|}~", replace_space='_', autostrip=False, case_sensitive=True, defaultfmt='f%i', unpack=None, usemask=False, loose=True, invalid_raise=True, max_rows=None, encoding='bytes', *, like=None)[source]
 
Load data from a text file, with missing values handled as specified. Each line past the first skip_header lines is split at the delimiter character, and characters following the comments character are discarded.  Parameters 
 
fnamefile, str, pathlib.Path, list of str, generator


File, filename, list, or generator to read. If the filename extension is .gz or .bz2, the file is first decompressed. Note that generators must return bytes or strings. The strings in a list or produced by a generator are treated as lines.  
dtypedtype, optional


Data type of the resulting array. If None, the dtypes will be determined by the contents of each column, individually.  
commentsstr, optional


The character used to indicate the start of a comment. All the characters occurring on a line after a comment are discarded.  
delimiterstr, int, or sequence, optional


The string used to separate values. By default, any consecutive whitespaces act as delimiter. An integer or sequence of integers can also be provided as width(s) of each field.  
skiprowsint, optional


skiprows was removed in numpy 1.10. Please use skip_header instead.  
skip_headerint, optional


The number of lines to skip at the beginning of the file.  
skip_footerint, optional


The number of lines to skip at the end of the file.  
convertersvariable, optional


The set of functions that convert the data of a column to a value. The converters can also be used to provide a default value for missing data: converters = {3: lambda s: float(s or 0)}.  
missingvariable, optional


missing was removed in numpy 1.10. Please use missing_values instead.  
missing_valuesvariable, optional


The set of strings corresponding to missing data.  
filling_valuesvariable, optional


The set of values to be used as default when the data are missing.  
usecolssequence, optional


Which columns to read, with 0 being the first. For example, usecols = (1, 4, 5) will extract the 2nd, 5th and 6th columns.  
names{None, True, str, sequence}, optional


If names is True, the field names are read from the first line after the first skip_header lines. This line can optionally be preceded by a comment delimiter. If names is a sequence or a single-string of comma-separated names, the names will be used to define the field names in a structured dtype. If names is None, the names of the dtype fields will be used, if any.  
excludelistsequence, optional


A list of names to exclude. This list is appended to the default list [‘return’,’file’,’print’]. Excluded names are appended with an underscore: for example, file would become file_.  
deletecharsstr, optional


A string combining invalid characters that must be deleted from the names.  
defaultfmtstr, optional


A format used to define default field names, such as “f%i” or “f_%02i”.  
autostripbool, optional


Whether to automatically strip white spaces from the variables.  
replace_spacechar, optional


Character(s) used in replacement of white spaces in the variable names. By default, use a ‘_’.  
case_sensitive{True, False, ‘upper’, ‘lower’}, optional


If True, field names are case sensitive. If False or ‘upper’, field names are converted to upper case. If ‘lower’, field names are converted to lower case.  
unpackbool, optional


If True, the returned array is transposed, so that arguments may be unpacked using x, y, z = genfromtxt(...). When used with a structured data-type, arrays are returned for each field. Default is False.  
usemaskbool, optional


If True, return a masked array. If False, return a regular array.  
loosebool, optional


If True, do not raise errors for invalid values.  
invalid_raisebool, optional


If True, an exception is raised if an inconsistency is detected in the number of columns. If False, a warning is emitted and the offending lines are skipped.  
max_rowsint, optional


The maximum number of rows to read. Must not be used with skip_footer at the same time. If given, the value must be at least 1. Default is to read the entire file.  New in version 1.10.0.   
encodingstr, optional


Encoding used to decode the inputfile. Does not apply when fname is a file object. The special value ‘bytes’ enables backward compatibility workarounds that ensure that you receive byte arrays when possible and passes latin1 encoded strings to converters. Override this value to receive unicode arrays and pass strings as input to converters. If set to None the system default is used. The default value is ‘bytes’.  New in version 1.14.0.   
likearray_like


Reference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as like supports the __array_function__ protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.  New in version 1.20.0.     Returns 
 
outndarray


Data read from the text file. If usemask is True, this is a masked array.      See also  numpy.loadtxt

equivalent function when no data is missing.    Notes  When spaces are used as delimiters, or when no delimiter has been given as input, there should not be any missing data between two fields. When the variables are named (either by a flexible dtype or with names), there must not be any header in the file (else a ValueError exception is raised). Individual values are not stripped of spaces by default. When using a custom converter, make sure the function does remove spaces.  References  1 
NumPy User Guide, section I/O with NumPy.   Examples >>> from io import StringIO
>>> import numpy as np
 Comma delimited file with mixed dtype >>> s = StringIO(u"1,1.3,abcde")
>>> data = np.genfromtxt(s, dtype=[('myint','i8'),('myfloat','f8'),
... ('mystring','S5')], delimiter=",")
>>> data
array((1, 1.3, b'abcde'),
      dtype=[('myint', '<i8'), ('myfloat', '<f8'), ('mystring', 'S5')])
 Using dtype = None >>> _ = s.seek(0) # needed for StringIO example only
>>> data = np.genfromtxt(s, dtype=None,
... names = ['myint','myfloat','mystring'], delimiter=",")
>>> data
array((1, 1.3, b'abcde'),
      dtype=[('myint', '<i8'), ('myfloat', '<f8'), ('mystring', 'S5')])
 Specifying dtype and names >>> _ = s.seek(0)
>>> data = np.genfromtxt(s, dtype="i8,f8,S5",
... names=['myint','myfloat','mystring'], delimiter=",")
>>> data
array((1, 1.3, b'abcde'),
      dtype=[('myint', '<i8'), ('myfloat', '<f8'), ('mystring', 'S5')])
 An example with fixed-width columns >>> s = StringIO(u"11.3abcde")
>>> data = np.genfromtxt(s, dtype=None, names=['intvar','fltvar','strvar'],
...     delimiter=[1,3,5])
>>> data
array((1, 1.3, b'abcde'),
      dtype=[('intvar', '<i8'), ('fltvar', '<f8'), ('strvar', 'S5')])
 An example to show comments >>> f = StringIO('''
... text,# of chars
... hello world,11
... numpy,5''')
>>> np.genfromtxt(f, dtype='S12,S12', delimiter=',')
array([(b'text', b''), (b'hello world', b'11'), (b'numpy', b'5')],
  dtype=[('f0', 'S12'), ('f1', 'S12')])
numpy.char.lower   char.lower(a)[source]
 
Return an array with the elements converted to lowercase. Call str.lower element-wise. For 8-bit strings, this method is locale-dependent.  Parameters 
 
aarray_like, {str, unicode}


Input array.    Returns 
 
outndarray, {str, unicode}


Output array of str or unicode, depending on input type      See also  str.lower
  Examples >>> c = np.array(['A1B C', '1BCA', 'BCA1']); c
array(['A1B C', '1BCA', 'BCA1'], dtype='<U5')
>>> np.char.lower(c)
array(['a1b c', '1bca', 'bca1'], dtype='<U5')
def f_30693804(text):
    """remove all non-alphanumeric characters except space from a string `text` and lower it
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
fmt_ds='$%d.%s^\\mathrm{h}$'
deg_mark='^\\mathrm{h}'
fmt_d='$%d^\\mathrm{h}$'
matplotlib.pyplot.semilogx   matplotlib.pyplot.semilogx(*args, **kwargs)[source]
 
Make a plot with log scaling on the x axis. Call signatures: semilogx([x], y, [fmt], data=None, **kwargs)
semilogx([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)
 This is just a thin wrapper around plot which additionally changes the x-axis to log scaling. All of the concepts and parameters of plot can be used here as well. The additional parameters base, subs, and nonpositive control the x-axis properties. They are just forwarded to Axes.set_xscale.  Parameters 
 
basefloat, default: 10


Base of the x logarithm.  
subsarray-like, optional


The location of the minor xticks. If None, reasonable locations are automatically chosen depending on the number of decades in the plot. See Axes.set_xscale for details.  
nonpositive{'mask', 'clip'}, default: 'mask'


Non-positive values in x can be masked as invalid, or clipped to a very small positive number.  **kwargs

All parameters supported by plot.    Returns 
 list of Line2D


Objects representing the plotted data.
matplotlib.axes.Axes.semilogx   Axes.semilogx(*args, **kwargs)[source]
 
Make a plot with log scaling on the x axis. Call signatures: semilogx([x], y, [fmt], data=None, **kwargs)
semilogx([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)
 This is just a thin wrapper around plot which additionally changes the x-axis to log scaling. All of the concepts and parameters of plot can be used here as well. The additional parameters base, subs, and nonpositive control the x-axis properties. They are just forwarded to Axes.set_xscale.  Parameters 
 
basefloat, default: 10


Base of the x logarithm.  
subsarray-like, optional


The location of the minor xticks. If None, reasonable locations are automatically chosen depending on the number of decades in the plot. See Axes.set_xscale for details.  
nonpositive{'mask', 'clip'}, default: 'mask'


Non-positive values in x can be masked as invalid, or clipped to a very small positive number.  **kwargs

All parameters supported by plot.    Returns 
 list of Line2D


Objects representing the plotted data.     
  Examples using matplotlib.axes.Axes.semilogx
 
   Log Demo   

   Log Axis   

   Transformations Tutorial
def f_17138464(x, y):
    """subscript text 'H20' with '2' as subscripted in matplotlib labels for arrays 'x' and 'y'.
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
fmt_ds='$%d.%s^\\mathrm{h}$'
deg_mark='^\\mathrm{h}'
fmt_d='$%d^\\mathrm{h}$'
matplotlib.pyplot.semilogx   matplotlib.pyplot.semilogx(*args, **kwargs)[source]
 
Make a plot with log scaling on the x axis. Call signatures: semilogx([x], y, [fmt], data=None, **kwargs)
semilogx([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)
 This is just a thin wrapper around plot which additionally changes the x-axis to log scaling. All of the concepts and parameters of plot can be used here as well. The additional parameters base, subs, and nonpositive control the x-axis properties. They are just forwarded to Axes.set_xscale.  Parameters 
 
basefloat, default: 10


Base of the x logarithm.  
subsarray-like, optional


The location of the minor xticks. If None, reasonable locations are automatically chosen depending on the number of decades in the plot. See Axes.set_xscale for details.  
nonpositive{'mask', 'clip'}, default: 'mask'


Non-positive values in x can be masked as invalid, or clipped to a very small positive number.  **kwargs

All parameters supported by plot.    Returns 
 list of Line2D


Objects representing the plotted data.
matplotlib.axes.Axes.semilogx   Axes.semilogx(*args, **kwargs)[source]
 
Make a plot with log scaling on the x axis. Call signatures: semilogx([x], y, [fmt], data=None, **kwargs)
semilogx([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)
 This is just a thin wrapper around plot which additionally changes the x-axis to log scaling. All of the concepts and parameters of plot can be used here as well. The additional parameters base, subs, and nonpositive control the x-axis properties. They are just forwarded to Axes.set_xscale.  Parameters 
 
basefloat, default: 10


Base of the x logarithm.  
subsarray-like, optional


The location of the minor xticks. If None, reasonable locations are automatically chosen depending on the number of decades in the plot. See Axes.set_xscale for details.  
nonpositive{'mask', 'clip'}, default: 'mask'


Non-positive values in x can be masked as invalid, or clipped to a very small positive number.  **kwargs

All parameters supported by plot.    Returns 
 list of Line2D


Objects representing the plotted data.     
  Examples using matplotlib.axes.Axes.semilogx
 
   Log Demo   

   Log Axis   

   Transformations Tutorial
def f_17138464(x, y):
    """subscript text 'H20' with '2' as subscripted in matplotlib labels for arrays 'x' and 'y'.
    """
    return  
 --------------------

def f_9138112(mylist):
    """loop over a list `mylist` if sublists length equals 3
    """
    return  
 --------------------

def f_1807026():
    """initialize a list `lst` of 100 objects Object()
    """
     
 --------------------
Please refer to the following documentation to generate the code:
tf.sparse.sparse_dense_matmul     View source on GitHub    Multiply SparseTensor (or dense Matrix) (of rank 2) "A" by dense matrix  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.sparse.matmul, tf.compat.v1.sparse.sparse_dense_matmul, tf.compat.v1.sparse_tensor_dense_matmul  
tf.sparse.sparse_dense_matmul(
    sp_a, b, adjoint_a=False, adjoint_b=False, name=None
)
 (or SparseTensor) "B". Please note that one and only one of the inputs MUST be a SparseTensor and the other MUST be a dense matrix. No validity checking is performed on the indices of A. However, the following input format is recommended for optimal behavior:  If adjoint_a == false: A should be sorted in lexicographically increasing order. Use sparse.reorder if you're not sure. If adjoint_a == true: A should be sorted in order of increasing dimension 1 (i.e., "column major" order instead of "row major" order).  Using tf.nn.embedding_lookup_sparse for sparse multiplication: It's not obvious but you can consider embedding_lookup_sparse as another sparse and dense multiplication. In some situations, you may prefer to use embedding_lookup_sparse even though you're not dealing with embeddings. There are two questions to ask in the decision process: Do you need gradients computed as sparse too? Is your sparse data represented as two SparseTensors: ids and values? There is more explanation about data format below. If you answer any of these questions as yes, consider using tf.nn.embedding_lookup_sparse. Following explains differences between the expected SparseTensors: For example if dense form of your sparse data has shape [3, 5] and values: [[  a      ]
 [b       c]
 [    d    ]]
 SparseTensor format expected by sparse_tensor_dense_matmul: sp_a (indices, values): [0, 1]: a
[1, 0]: b
[1, 4]: c
[2, 2]: d
 SparseTensor format expected by embedding_lookup_sparse: sp_ids sp_weights [0, 0]: 1                [0, 0]: a
[1, 0]: 0                [1, 0]: b
[1, 1]: 4                [1, 1]: c
[2, 0]: 2                [2, 0]: d
 Deciding when to use sparse_tensor_dense_matmul vs. matmul(a_is_sparse=True): There are a number of questions to ask in the decision process, including:  Will the SparseTensor A fit in memory if densified? Is the column count of the product large (>> 1)? Is the density of A larger than approximately 15%?  If the answer to several of these questions is yes, consider converting the SparseTensor to a dense one and using tf.matmul with a_is_sparse=True. This operation tends to perform well when A is more sparse, if the column size of the product is small (e.g. matrix-vector multiplication), if sp_a.dense_shape takes on large values. Below is a rough speed comparison between sparse_tensor_dense_matmul, labeled 'sparse', and matmul(a_is_sparse=True), labeled 'dense'. For purposes of the comparison, the time spent converting from a SparseTensor to a dense Tensor is not included, so it is overly conservative with respect to the time ratio. Benchmark system: CPU: Intel Ivybridge with HyperThreading (6 cores) dL1:32KB dL2:256KB dL3:12MB GPU: NVidia Tesla k40c Compiled with: -c opt --config=cuda --copt=-mavx tensorflow/python/sparse_tensor_dense_matmul_op_test --benchmarks
A sparse [m, k] with % nonzero values between 1% and 80%
B dense [k, n]

% nnz  n   gpu   m     k     dt(dense)     dt(sparse)   dt(sparse)/dt(dense)
0.01   1   True  100   100   0.000221166   0.00010154   0.459112
0.01   1   True  100   1000  0.00033858    0.000109275  0.322745
0.01   1   True  1000  100   0.000310557   9.85661e-05  0.317385
0.01   1   True  1000  1000  0.0008721     0.000100875  0.115669
0.01   1   False 100   100   0.000208085   0.000107603  0.51711
0.01   1   False 100   1000  0.000327112   9.51118e-05  0.290762
0.01   1   False 1000  100   0.000308222   0.00010345   0.335635
0.01   1   False 1000  1000  0.000865721   0.000101397  0.117124
0.01   10  True  100   100   0.000218522   0.000105537  0.482958
0.01   10  True  100   1000  0.000340882   0.000111641  0.327506
0.01   10  True  1000  100   0.000315472   0.000117376  0.372064
0.01   10  True  1000  1000  0.000905493   0.000123263  0.136128
0.01   10  False 100   100   0.000221529   9.82571e-05  0.44354
0.01   10  False 100   1000  0.000330552   0.000112615  0.340687
0.01   10  False 1000  100   0.000341277   0.000114097  0.334324
0.01   10  False 1000  1000  0.000819944   0.000120982  0.147549
0.01   25  True  100   100   0.000207806   0.000105977  0.509981
0.01   25  True  100   1000  0.000322879   0.00012921   0.400181
0.01   25  True  1000  100   0.00038262    0.00014158   0.370035
0.01   25  True  1000  1000  0.000865438   0.000202083  0.233504
0.01   25  False 100   100   0.000209401   0.000104696  0.499979
0.01   25  False 100   1000  0.000321161   0.000130737  0.407076
0.01   25  False 1000  100   0.000377012   0.000136801  0.362856
0.01   25  False 1000  1000  0.000861125   0.00020272   0.235413
0.2    1   True  100   100   0.000206952   9.69219e-05  0.46833
0.2    1   True  100   1000  0.000348674   0.000147475  0.422959
0.2    1   True  1000  100   0.000336908   0.00010122   0.300439
0.2    1   True  1000  1000  0.001022      0.000203274  0.198898
0.2    1   False 100   100   0.000207532   9.5412e-05   0.459746
0.2    1   False 100   1000  0.000356127   0.000146824  0.41228
0.2    1   False 1000  100   0.000322664   0.000100918  0.312764
0.2    1   False 1000  1000  0.000998987   0.000203442  0.203648
0.2    10  True  100   100   0.000211692   0.000109903  0.519165
0.2    10  True  100   1000  0.000372819   0.000164321  0.440753
0.2    10  True  1000  100   0.000338651   0.000144806  0.427596
0.2    10  True  1000  1000  0.00108312    0.000758876  0.70064
0.2    10  False 100   100   0.000215727   0.000110502  0.512231
0.2    10  False 100   1000  0.000375419   0.0001613    0.429653
0.2    10  False 1000  100   0.000336999   0.000145628  0.432132
0.2    10  False 1000  1000  0.00110502    0.000762043  0.689618
0.2    25  True  100   100   0.000218705   0.000129913  0.594009
0.2    25  True  100   1000  0.000394794   0.00029428   0.745402
0.2    25  True  1000  100   0.000404483   0.0002693    0.665788
0.2    25  True  1000  1000  0.0012002     0.00194494   1.62052
0.2    25  False 100   100   0.000221494   0.0001306    0.589632
0.2    25  False 100   1000  0.000396436   0.000297204  0.74969
0.2    25  False 1000  100   0.000409346   0.000270068  0.659754
0.2    25  False 1000  1000  0.00121051    0.00193737   1.60046
0.5    1   True  100   100   0.000214981   9.82111e-05  0.456836
0.5    1   True  100   1000  0.000415328   0.000223073  0.537101
0.5    1   True  1000  100   0.000358324   0.00011269   0.314492
0.5    1   True  1000  1000  0.00137612    0.000437401  0.317851
0.5    1   False 100   100   0.000224196   0.000101423  0.452386
0.5    1   False 100   1000  0.000400987   0.000223286  0.556841
0.5    1   False 1000  100   0.000368825   0.00011224   0.304318
0.5    1   False 1000  1000  0.00136036    0.000429369  0.31563
0.5    10  True  100   100   0.000222125   0.000112308  0.505608
0.5    10  True  100   1000  0.000461088   0.00032357   0.701753
0.5    10  True  1000  100   0.000394624   0.000225497  0.571422
0.5    10  True  1000  1000  0.00158027    0.00190898   1.20801
0.5    10  False 100   100   0.000232083   0.000114978  0.495418
0.5    10  False 100   1000  0.000454574   0.000324632  0.714146
0.5    10  False 1000  100   0.000379097   0.000227768  0.600817
0.5    10  False 1000  1000  0.00160292    0.00190168   1.18638
0.5    25  True  100   100   0.00023429    0.000151703  0.647501
0.5    25  True  100   1000  0.000497462   0.000598873  1.20386
0.5    25  True  1000  100   0.000460778   0.000557038  1.20891
0.5    25  True  1000  1000  0.00170036    0.00467336   2.74845
0.5    25  False 100   100   0.000228981   0.000155334  0.678371
0.5    25  False 100   1000  0.000496139   0.000620789  1.25124
0.5    25  False 1000  100   0.00045473    0.000551528  1.21287
0.5    25  False 1000  1000  0.00171793    0.00467152   2.71927
0.8    1   True  100   100   0.000222037   0.000105301  0.47425
0.8    1   True  100   1000  0.000410804   0.000329327  0.801664
0.8    1   True  1000  100   0.000349735   0.000131225  0.375212
0.8    1   True  1000  1000  0.00139219    0.000677065  0.48633
0.8    1   False 100   100   0.000214079   0.000107486  0.502085
0.8    1   False 100   1000  0.000413746   0.000323244  0.781261
0.8    1   False 1000  100   0.000348983   0.000131983  0.378193
0.8    1   False 1000  1000  0.00136296    0.000685325  0.50282
0.8    10  True  100   100   0.000229159   0.00011825   0.516017
0.8    10  True  100   1000  0.000498845   0.000532618  1.0677
0.8    10  True  1000  100   0.000383126   0.00029935   0.781336
0.8    10  True  1000  1000  0.00162866    0.00307312   1.88689
0.8    10  False 100   100   0.000230783   0.000124958  0.541452
0.8    10  False 100   1000  0.000493393   0.000550654  1.11606
0.8    10  False 1000  100   0.000377167   0.000298581  0.791642
0.8    10  False 1000  1000  0.00165795    0.00305103   1.84024
0.8    25  True  100   100   0.000233496   0.000175241  0.75051
0.8    25  True  100   1000  0.00055654    0.00102658   1.84458
0.8    25  True  1000  100   0.000463814   0.000783267  1.68875
0.8    25  True  1000  1000  0.00186905    0.00755344   4.04132
0.8    25  False 100   100   0.000240243   0.000175047  0.728625
0.8    25  False 100   1000  0.000578102   0.00104499   1.80763
0.8    25  False 1000  100   0.000485113   0.000776849  1.60138
0.8    25  False 1000  1000  0.00211448    0.00752736   3.55992

 


 Args
  sp_a   SparseTensor (or dense Matrix) A, of rank 2.  
  b   dense Matrix (or SparseTensor) B, with the same dtype as sp_a.  
  adjoint_a   Use the adjoint of A in the matrix multiply. If A is complex, this is transpose(conj(A)). Otherwise it's transpose(A).  
  adjoint_b   Use the adjoint of B in the matrix multiply. If B is complex, this is transpose(conj(B)). Otherwise it's transpose(B).  
  name   A name prefix for the returned tensors (optional)   
 


 Returns   A dense matrix (pseudo-code in dense np.matrix notation): A = A.H if adjoint_a else A B = B.H if adjoint_b else B return A*B
classmethod path_hook(*loader_details)  
A class method which returns a closure for use on sys.path_hooks. An instance of FileFinder is returned by the closure using the path argument given to the closure directly and loader_details indirectly. If the argument to the closure is not an existing directory, ImportError is raised.
read([n])  
Return a bytes containing up to n bytes starting from the current file position. If the argument is omitted, None or negative, return all bytes from the current file position to the end of the mapping. The file position is updated to point after the bytes that were returned.  Changed in version 3.3: Argument can be omitted or None.
read1([size])  
In BytesIO, this is the same as read().  Changed in version 3.7: The size argument is now optional.
tf.compat.v1.reduce_sum Computes the sum of elements across dimensions of a tensor. (deprecated arguments)  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.math.reduce_sum  
tf.compat.v1.reduce_sum(
    input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None,
    keep_dims=None
)
 Warning: SOME ARGUMENTS ARE DEPRECATED: (keep_dims). They will be removed in a future version. Instructions for updating: keep_dims is deprecated, use keepdims instead Reduces input_tensor along the dimensions given in axis. Unless keepdims is true, the rank of the tensor is reduced by 1 for each of the entries in axis, which must be unique. If keepdims is true, the reduced dimensions are retained with length 1. If axis is None, all dimensions are reduced, and a tensor with a single element is returned. For example: x = tf.constant([[1, 1, 1], [1, 1, 1]])
tf.reduce_sum(x)  # 6
tf.reduce_sum(x, 0)  # [2, 2, 2]
tf.reduce_sum(x, 1)  # [3, 3]
tf.reduce_sum(x, 1, keepdims=True)  # [[3], [3]]
tf.reduce_sum(x, [0, 1])  # 6

 


 Args
  input_tensor   The tensor to reduce. Should have numeric type.  
  axis   The dimensions to reduce. If None (the default), reduces all dimensions. Must be in the range [-rank(input_tensor), rank(input_tensor)).  
  keepdims   If true, retains reduced dimensions with length 1.  
  name   A name for the operation (optional).  
  reduction_indices   The old (deprecated) name for axis.  
  keep_dims   Deprecated alias for keepdims.   
 


 Returns   The reduced tensor, of the same dtype as the input_tensor.  
 Numpy Compatibility Equivalent to np.sum apart the fact that numpy upcast uint8 and int32 to int64 while tensorflow returns the same dtype as the input.
def f_13793321(df1, df2):
    """joining data from dataframe `df1` with data from dataframe `df2` based on matching values of column 'Date_Time' in both dataframes
    """
    return  
 --------------------

def f_3367288(str1):
    """use `%s` operator to print variable values `str1` inside a string
    """
    return  
 --------------------

def f_3475251():
    """Split a string '2.MATCHES $$TEXT$$ STRING' by a delimiter '$$TEXT$$'
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
turtle.back(distance)  
turtle.bk(distance)  
turtle.backward(distance)  
 Parameters 
distance – a number   Move the turtle backward by distance, opposite to the direction the turtle is headed. Do not change the turtle’s heading. >>> turtle.position()
(0.00,0.00)
>>> turtle.backward(30)
>>> turtle.position()
(-30.00,0.00)
turtle.back(distance)  
turtle.bk(distance)  
turtle.backward(distance)  
 Parameters 
distance – a number   Move the turtle backward by distance, opposite to the direction the turtle is headed. Do not change the turtle’s heading. >>> turtle.position()
(0.00,0.00)
>>> turtle.backward(30)
>>> turtle.position()
(-30.00,0.00)
turtle.back(distance)  
turtle.bk(distance)  
turtle.backward(distance)  
 Parameters 
distance – a number   Move the turtle backward by distance, opposite to the direction the turtle is headed. Do not change the turtle’s heading. >>> turtle.position()
(0.00,0.00)
>>> turtle.backward(30)
>>> turtle.position()
(-30.00,0.00)
SSLSocket.version()  
Return the actual SSL protocol version negotiated by the connection as a string, or None is no secure connection is established. As of this writing, possible return values include "SSLv2", "SSLv3", "TLSv1", "TLSv1.1" and "TLSv1.2". Recent OpenSSL versions may define more return values.  New in version 3.5.
urllib.request.pathname2url(path)  
Convert the pathname path from the local syntax for a path to the form used in the path component of a URL. This does not produce a complete URL. The return value will already be quoted using the quote() function.
def f_273192(directory):
    """check if directory `directory ` exists and create it if necessary
    """
     
 --------------------
Please refer to the following documentation to generate the code:
tf.io.gfile.makedirs     View source on GitHub    Creates a directory and all parent/intermediate directories.  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.io.gfile.makedirs  
tf.io.gfile.makedirs(
    path
)
 It succeeds if path already exists and is writable.
 


 Args
  path   string, name of the directory to be created   
 


 Raises
  errors.OpError   If the operation fails.
tf.io.gfile.mkdir     View source on GitHub    Creates a directory with the name given by path.  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.io.gfile.mkdir  
tf.io.gfile.mkdir(
    path
)

 


 Args
  path   string, name of the directory to be created    Notes: The parent directories need to exist. Use tf.io.gfile.makedirs instead if there is the possibility that the parent dirs don't exist.
 


 Raises
  errors.OpError   If the operation fails.
class asyncio.DatagramTransport(BaseTransport)  
A transport for datagram (UDP) connections. Instances of the DatagramTransport class are returned from the loop.create_datagram_endpoint() event loop method.
os.path.normcase(path)  
Normalize the case of a pathname. On Windows, convert all characters in the pathname to lowercase, and also convert forward slashes to backward slashes. On other operating systems, return the path unchanged.  Changed in version 3.6: Accepts a path-like object.
curses.is_term_resized(nlines, ncols)  
Return True if resize_term() would modify the window structure, False otherwise.
def f_273192(path):
    """check if a directory `path` exists and create it if necessary
    """
     
 --------------------
Please refer to the following documentation to generate the code:
tf.io.gfile.makedirs     View source on GitHub    Creates a directory and all parent/intermediate directories.  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.io.gfile.makedirs  
tf.io.gfile.makedirs(
    path
)
 It succeeds if path already exists and is writable.
 


 Args
  path   string, name of the directory to be created   
 


 Raises
  errors.OpError   If the operation fails.
tf.io.gfile.mkdir     View source on GitHub    Creates a directory with the name given by path.  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.io.gfile.mkdir  
tf.io.gfile.mkdir(
    path
)

 


 Args
  path   string, name of the directory to be created    Notes: The parent directories need to exist. Use tf.io.gfile.makedirs instead if there is the possibility that the parent dirs don't exist.
 


 Raises
  errors.OpError   If the operation fails.
class asyncio.DatagramTransport(BaseTransport)  
A transport for datagram (UDP) connections. Instances of the DatagramTransport class are returned from the loop.create_datagram_endpoint() event loop method.
os.path.normcase(path)  
Normalize the case of a pathname. On Windows, convert all characters in the pathname to lowercase, and also convert forward slashes to backward slashes. On other operating systems, return the path unchanged.  Changed in version 3.6: Accepts a path-like object.
curses.is_term_resized(nlines, ncols)  
Return True if resize_term() would modify the window structure, False otherwise.
def f_273192(path):
    """check if a directory `path` exists and create it if necessary
    """
     
 --------------------
Please refer to the following documentation to generate the code:
stringprep.map_table_b3(code)  
Return the mapped value for code according to tableB.3 (Mapping for case-folding used with no normalization).
class email.policy.Compat32(**kw)  
This concrete Policy is the backward compatibility policy. It replicates the behavior of the email package in Python 3.2. The policy module also defines an instance of this class, compat32, that is used as the default policy. Thus the default behavior of the email package is to maintain compatibility with Python 3.2. The following attributes have values that are different from the Policy default:  
mangle_from_  
The default is True. 
 The class provides the following concrete implementations of the abstract methods of Policy:  
header_source_parse(sourcelines)  
The name is parsed as everything up to the ‘:’ and returned unmodified. The value is determined by stripping leading whitespace off the remainder of the first line, joining all subsequent lines together, and stripping any trailing carriage return or linefeed characters. 
  
header_store_parse(name, value)  
The name and value are returned unmodified. 
  
header_fetch_parse(name, value)  
If the value contains binary data, it is converted into a Header object using the unknown-8bit charset. Otherwise it is returned unmodified. 
  
fold(name, value)  
Headers are folded using the Header folding algorithm, which preserves existing line breaks in the value, and wraps each resulting line to the max_line_length. Non-ASCII binary data are CTE encoded using the unknown-8bit charset. 
  
fold_binary(name, value)  
Headers are folded using the Header folding algorithm, which preserves existing line breaks in the value, and wraps each resulting line to the max_line_length. If cte_type is 7bit, non-ascii binary data is CTE encoded using the unknown-8bit charset. Otherwise the original source header is used, with its existing line breaks and any (RFC invalid) binary data it may contain.
CookiePolicy.hide_cookie2  
Don’t add Cookie2 header to requests (the presence of this header indicates to the server that we understand RFC 2965 cookies).
mpl_toolkits.axes_grid1.axes_divider.HBoxDivider   classmpl_toolkits.axes_grid1.axes_divider.HBoxDivider(fig, *args, horizontal=None, vertical=None, aspect=None, anchor='C')[source]
 
Bases: mpl_toolkits.axes_grid1.axes_divider.SubplotDivider A SubplotDivider for laying out axes horizontally, while ensuring that they have equal heights. Examples (Source code, png, pdf)     Parameters 
 
figmatplotlib.figure.Figure


*argstuple (nrows, ncols, index) or int


The array of subplots in the figure has dimensions (nrows,
ncols), and index is the index of the subplot being created. index starts at 1 in the upper left corner and increases to the right. If nrows, ncols, and index are all single digit numbers, then args can be passed as a single 3-digit number (e.g. 234 for (2, 3, 4)).       locate(nx, ny, nx1=None, ny1=None, axes=None, renderer=None)[source]
 
 Parameters 
 
nx, nx1int


Integers specifying the column-position of the cell. When nx1 is None, a single nx-th column is specified. Otherwise location of columns spanning between nx to nx1 (but excluding nx1-th column) is specified.  
ny, ny1int


Same as nx and nx1, but for row positions.  axes
renderer
   
   new_locator(nx, nx1=None)[source]
 
Create a new AxesLocator for the specified cell.  Parameters 
 
nx, nx1int


Integers specifying the column-position of the cell. When nx1 is None, a single nx-th column is specified. Otherwise location of columns spanning between nx to nx1 (but excluding nx1-th column) is specified.     
 
  Examples using mpl_toolkits.axes_grid1.axes_divider.HBoxDivider
 
   HBoxDivider demo
email.policy.compat32  
An instance of Compat32, providing backward compatibility with the behavior of the email package in Python 3.2.
def f_18785032(text):
    """Replace a separate word 'H3' by 'H1' in a string 'text'
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
encodings.idna.ToASCII(label)  
Convert a label to ASCII, as specified in RFC 3490. UseSTD3ASCIIRules is assumed to be false.
string.ascii_letters  
The concatenation of the ascii_lowercase and ascii_uppercase constants described below. This value is not locale-dependent.
ascii(object)  
As repr(), return a string containing a printable representation of an object, but escape the non-ASCII characters in the string returned by repr() using \x, \u or \U escapes. This generates a string similar to that returned by repr() in Python 2.
re.A  
re.ASCII  
Make \w, \W, \b, \B, \d, \D, \s and \S perform ASCII-only matching instead of full Unicode matching. This is only meaningful for Unicode patterns, and is ignored for byte patterns. Corresponds to the inline flag (?a). Note that for backward compatibility, the re.U flag still exists (as well as its synonym re.UNICODE and its embedded counterpart (?u)), but these are redundant in Python 3 since matches are Unicode by default for strings (and Unicode matching isn’t allowed for bytes).
string.ascii_lowercase  
The lowercase letters 'abcdefghijklmnopqrstuvwxyz'. This value is not locale-dependent and will not change.
def f_1450897():
    """substitute ASCII letters in string 'aas30dsa20' with empty string ''
    """
    return  
 --------------------

def f_1450897():
    """get digits only from a string `aas30dsa20` using lambda function
    """
    return  
 --------------------

def f_14435268(soup):
    """access a tag called "name" in beautifulsoup `soup`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
concat_matrix=b'cm'[source]
pandas.api.extensions.ExtensionArray._concat_same_type   classmethodExtensionArray._concat_same_type(to_concat)[source]
 
Concatenate multiple array of this dtype.  Parameters 
 
to_concat:sequence of this type

  Returns 
 ExtensionArray
pandas.DataFrame.join   DataFrame.join(other, on=None, how='left', lsuffix='', rsuffix='', sort=False)[source]
 
Join columns of another DataFrame. Join columns with other DataFrame either on index or on a key column. Efficiently join multiple DataFrame objects by index at once by passing a list.  Parameters 
 
other:DataFrame, Series, or list of DataFrame


Index should be similar to one of the columns in this one. If a Series is passed, its name attribute must be set, and that will be used as the column name in the resulting joined DataFrame.  
on:str, list of str, or array-like, optional


Column or index level name(s) in the caller to join on the index in other, otherwise joins index-on-index. If multiple values given, the other DataFrame must have a MultiIndex. Can pass an array as the join key if it is not already contained in the calling DataFrame. Like an Excel VLOOKUP operation.  
how:{‘left’, ‘right’, ‘outer’, ‘inner’}, default ‘left’


How to handle the operation of the two objects.  left: use calling frame’s index (or column if on is specified) right: use other’s index. outer: form union of calling frame’s index (or column if on is specified) with other’s index, and sort it. lexicographically. inner: form intersection of calling frame’s index (or column if on is specified) with other’s index, preserving the order of the calling’s one. 
cross: creates the cartesian product from both frames, preserves the order of the left keys.  New in version 1.2.0.     
lsuffix:str, default ‘’


Suffix to use from left frame’s overlapping columns.  
rsuffix:str, default ‘’


Suffix to use from right frame’s overlapping columns.  
sort:bool, default False


Order result DataFrame lexicographically by the join key. If False, the order of the join key depends on the join type (how keyword).    Returns 
 DataFrame

A dataframe containing columns from both the caller and other.      See also  DataFrame.merge

For column(s)-on-column(s) operations.    Notes Parameters on, lsuffix, and rsuffix are not supported when passing a list of DataFrame objects. Support for specifying index levels as the on parameter was added in version 0.23.0. Examples 
>>> df = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3', 'K4', 'K5'],
...                    'A': ['A0', 'A1', 'A2', 'A3', 'A4', 'A5']})
  
>>> df
  key   A
0  K0  A0
1  K1  A1
2  K2  A2
3  K3  A3
4  K4  A4
5  K5  A5
  
>>> other = pd.DataFrame({'key': ['K0', 'K1', 'K2'],
...                       'B': ['B0', 'B1', 'B2']})
  
>>> other
  key   B
0  K0  B0
1  K1  B1
2  K2  B2
  Join DataFrames using their indexes. 
>>> df.join(other, lsuffix='_caller', rsuffix='_other')
  key_caller   A key_other    B
0         K0  A0        K0   B0
1         K1  A1        K1   B1
2         K2  A2        K2   B2
3         K3  A3       NaN  NaN
4         K4  A4       NaN  NaN
5         K5  A5       NaN  NaN
  If we want to join using the key columns, we need to set key to be the index in both df and other. The joined DataFrame will have key as its index. 
>>> df.set_index('key').join(other.set_index('key'))
      A    B
key
K0   A0   B0
K1   A1   B1
K2   A2   B2
K3   A3  NaN
K4   A4  NaN
K5   A5  NaN
  Another option to join using the key columns is to use the on parameter. DataFrame.join always uses other’s index but we can use any column in df. This method preserves the original DataFrame’s index in the result. 
>>> df.join(other.set_index('key'), on='key')
  key   A    B
0  K0  A0   B0
1  K1  A1   B1
2  K2  A2   B2
3  K3  A3  NaN
4  K4  A4  NaN
5  K5  A5  NaN
  Using non-unique key values shows how they are matched. 
>>> df = pd.DataFrame({'key': ['K0', 'K1', 'K1', 'K3', 'K0', 'K1'],
...                    'A': ['A0', 'A1', 'A2', 'A3', 'A4', 'A5']})
  
>>> df
  key   A
0  K0  A0
1  K1  A1
2  K1  A2
3  K3  A3
4  K0  A4
5  K1  A5
  
>>> df.join(other.set_index('key'), on='key')
  key   A    B
0  K0  A0   B0
1  K1  A1   B1
2  K1  A2   B1
3  K3  A3  NaN
4  K0  A4   B0
5  K1  A5   B1
operator.concat(a, b)  
operator.__concat__(a, b)  
Return a + b for a and b sequences.
classBarAB(widthA=1.0, angleA=0, widthB=1.0, angleB=0)[source]
 
Bases: matplotlib.patches.ArrowStyle._Curve An arrow with vertical bars | at both ends.  Parameters 
 
widthA, widthBfloat, default: 1.0


Width of the bracket.  
angleA, angleBfloat, default: 0 degrees


Orientation of the bracket, as a counterclockwise angle. 0 degrees means perpendicular to the line.       arrow='|-|'
def f_20180210(A, B):
    """Create new matrix object  by concatenating data from matrix A and matrix B
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
numpy.c_   numpy.c_ = <numpy.lib.index_tricks.CClass object>
 
Translates slice objects to concatenation along the second axis. This is short-hand for np.r_['-1,2,0', index expression], which is useful because of its common occurrence. In particular, arrays will be stacked along their last axis after being upgraded to at least 2-D with 1’s post-pended to the shape (column vectors made out of 1-D arrays).  See also  column_stack

Stack 1-D arrays as columns into a 2-D array.  r_

For more detailed documentation.    Examples >>> np.c_[np.array([1,2,3]), np.array([4,5,6])]
array([[1, 4],
       [2, 5],
       [3, 6]])
>>> np.c_[np.array([[1,2,3]]), 0, 0, np.array([[4,5,6]])]
array([[1, 2, 3, ..., 4, 5, 6]])
numpy.ma.append   ma.append(a, b, axis=None)[source]
 
Append values to the end of an array.  New in version 1.9.0.   Parameters 
 
aarray_like


Values are appended to a copy of this array.  
barray_like


These values are appended to a copy of a. It must be of the correct shape (the same shape as a, excluding axis). If axis is not specified, b can be any shape and will be flattened before use.  
axisint, optional


The axis along which v are appended. If axis is not given, both a and b are flattened before use.    Returns 
 
appendMaskedArray


A copy of a with b appended to axis. Note that append does not occur in-place: a new array is allocated and filled. If axis is None, the result is a flattened array.      See also  numpy.append

Equivalent function in the top-level NumPy module.    Examples >>> import numpy.ma as ma
>>> a = ma.masked_values([1, 2, 3], 2)
>>> b = ma.masked_values([[4, 5, 6], [7, 8, 9]], 7)
>>> ma.append(a, b)
masked_array(data=[1, --, 3, 4, 5, 6, --, 8, 9],
             mask=[False,  True, False, False, False, False,  True, False,
                   False],
       fill_value=999999)
numpy.concatenate   numpy.concatenate((a1, a2, ...), axis=0, out=None, dtype=None, casting="same_kind")
 
Join a sequence of arrays along an existing axis.  Parameters 
 
a1, a2, …sequence of array_like


The arrays must have the same shape, except in the dimension corresponding to axis (the first, by default).  
axisint, optional


The axis along which the arrays will be joined. If axis is None, arrays are flattened before use. Default is 0.  
outndarray, optional


If provided, the destination to place the result. The shape must be correct, matching that of what concatenate would have returned if no out argument were specified.  
dtypestr or dtype


If provided, the destination array will have this dtype. Cannot be provided together with out.  New in version 1.20.0.   
casting{‘no’, ‘equiv’, ‘safe’, ‘same_kind’, ‘unsafe’}, optional


Controls what kind of data casting may occur. Defaults to ‘same_kind’.  New in version 1.20.0.     Returns 
 
resndarray


The concatenated array.      See also  ma.concatenate

Concatenate function that preserves input masks.  array_split

Split an array into multiple sub-arrays of equal or near-equal size.  split

Split array into a list of multiple sub-arrays of equal size.  hsplit

Split array into multiple sub-arrays horizontally (column wise).  vsplit

Split array into multiple sub-arrays vertically (row wise).  dsplit

Split array into multiple sub-arrays along the 3rd axis (depth).  stack

Stack a sequence of arrays along a new axis.  block

Assemble arrays from blocks.  hstack

Stack arrays in sequence horizontally (column wise).  vstack

Stack arrays in sequence vertically (row wise).  dstack

Stack arrays in sequence depth wise (along third dimension).  column_stack

Stack 1-D arrays as columns into a 2-D array.    Notes When one or more of the arrays to be concatenated is a MaskedArray, this function will return a MaskedArray object instead of an ndarray, but the input masks are not preserved. In cases where a MaskedArray is expected as input, use the ma.concatenate function from the masked array module instead. Examples >>> a = np.array([[1, 2], [3, 4]])
>>> b = np.array([[5, 6]])
>>> np.concatenate((a, b), axis=0)
array([[1, 2],
       [3, 4],
       [5, 6]])
>>> np.concatenate((a, b.T), axis=1)
array([[1, 2, 5],
       [3, 4, 6]])
>>> np.concatenate((a, b), axis=None)
array([1, 2, 3, 4, 5, 6])
 This function will not preserve masking of MaskedArray inputs. >>> a = np.ma.arange(3)
>>> a[1] = np.ma.masked
>>> b = np.arange(2, 5)
>>> a
masked_array(data=[0, --, 2],
             mask=[False,  True, False],
       fill_value=999999)
>>> b
array([2, 3, 4])
>>> np.concatenate([a, b])
masked_array(data=[0, 1, 2, 2, 3, 4],
             mask=False,
       fill_value=999999)
>>> np.ma.concatenate([a, b])
masked_array(data=[0, --, 2, 2, 3, 4],
             mask=[False,  True, False, False, False, False],
       fill_value=999999)
numpy.ufunc.outer method   ufunc.outer(A, B, /, **kwargs)
 
Apply the ufunc op to all pairs (a, b) with a in A and b in B. Let M = A.ndim, N = B.ndim. Then the result, C, of op.outer(A, B) is an array of dimension M + N such that:  \[C[i_0, ..., i_{M-1}, j_0, ..., j_{N-1}] = op(A[i_0, ..., i_{M-1}], B[j_0, ..., j_{N-1}])\] For A and B one-dimensional, this is equivalent to: r = empty(len(A),len(B))
for i in range(len(A)):
    for j in range(len(B)):
        r[i,j] = op(A[i], B[j])  # op = ufunc in question
  Parameters 
 
Aarray_like


First array  
Barray_like


Second array  
kwargsany


Arguments to pass on to the ufunc. Typically dtype or out. See ufunc for a comprehensive overview of all available arguments.    Returns 
 
rndarray


Output array      See also  numpy.outer

A less powerful version of np.multiply.outer that ravels all inputs to 1D. This exists primarily for compatibility with old code.  tensordot

np.tensordot(a, b, axes=((), ())) and np.multiply.outer(a, b) behave same for all dimensions of a and b.    Examples >>> np.multiply.outer([1, 2, 3], [4, 5, 6])
array([[ 4,  5,  6],
       [ 8, 10, 12],
       [12, 15, 18]])
 A multi-dimensional example: >>> A = np.array([[1, 2, 3], [4, 5, 6]])
>>> A.shape
(2, 3)
>>> B = np.array([[1, 2, 3, 4]])
>>> B.shape
(1, 4)
>>> C = np.multiply.outer(A, B)
>>> C.shape; C
(2, 3, 1, 4)
array([[[[ 1,  2,  3,  4]],
        [[ 2,  4,  6,  8]],
        [[ 3,  6,  9, 12]]],
       [[[ 4,  8, 12, 16]],
        [[ 5, 10, 15, 20]],
        [[ 6, 12, 18, 24]]]])
numpy.ma.concatenate   ma.concatenate(arrays, axis=0)[source]
 
Concatenate a sequence of arrays along the given axis.  Parameters 
 
arrayssequence of array_like


The arrays must have the same shape, except in the dimension corresponding to axis (the first, by default).  
axisint, optional


The axis along which the arrays will be joined. Default is 0.    Returns 
 
resultMaskedArray


The concatenated array with any masked entries preserved.      See also  numpy.concatenate

Equivalent function in the top-level NumPy module.    Examples >>> import numpy.ma as ma
>>> a = ma.arange(3)
>>> a[1] = ma.masked
>>> b = ma.arange(2, 5)
>>> a
masked_array(data=[0, --, 2],
             mask=[False,  True, False],
       fill_value=999999)
>>> b
masked_array(data=[2, 3, 4],
             mask=False,
       fill_value=999999)
>>> ma.concatenate([a, b])
masked_array(data=[0, --, 2, 2, 3, 4],
             mask=[False,  True, False, False, False, False],
       fill_value=999999)
def f_20180210(A, B):
    """concat two matrices `A` and `B` in numpy
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
fileinput.filelineno()  
Return the line number in the current file. Before the first line has been read, returns 0. After the last line of the last file has been read, returns the line number of that line within the file.
lines  
Height of the terminal window in characters.
tell()  
Return the current stream position as an opaque number. The number does not usually represent a number of bytes in the underlying binary storage.
fileinput.lineno()  
Return the cumulative line number of the line that has just been read. Before the first line has been read, returns 0. After the last line of the last file has been read, returns the line number of that line.
readfp(fp, filename=None)  
 Deprecated since version 3.2: Use read_file() instead.   Changed in version 3.2: readfp() now iterates on fp instead of calling fp.readline().  For existing code calling readfp() with arguments which don’t support iteration, the following generator may be used as a wrapper around the file-like object: def readline_generator(fp):
    line = fp.readline()
    while line:
        yield line
        line = fp.readline()
 Instead of parser.readfp(fp) use parser.read_file(readline_generator(fp)).
def f_2011048(filepath):
    """Get the characters count in a file `filepath`
    """
    return  
 --------------------

def f_2600191(l):
    """count the occurrences of item "a" in list `l`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
count(value)  
Returns the number of occurrences of value.
operator.countOf(a, b)  
Return the number of occurrences of b in a.
tf.raw_ops.BatchCholeskyGrad  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.raw_ops.BatchCholeskyGrad  
tf.raw_ops.BatchCholeskyGrad(
    l, grad, name=None
)

 


 Args
  l   A Tensor. Must be one of the following types: float32, float64.  
  grad   A Tensor. Must have the same type as l.  
  name   A name for the operation (optional).   
 


 Returns   A Tensor. Has the same type as l.
pandas.Series.str.count   Series.str.count(pat, flags=0)[source]
 
Count occurrences of pattern in each string of the Series/Index. This function is used to count the number of times a particular regex pattern is repeated in each of the string elements of the Series.  Parameters 
 
pat:str


Valid regular expression.  
flags:int, default 0, meaning no flags


Flags for the re module. For a complete list, see here.  **kwargs

For compatibility with other string methods. Not used.    Returns 
 Series or Index

Same type as the calling object containing the integer counts.      See also  re

Standard library module for regular expressions.  str.count

Standard library version, without regular expression support.    Notes Some characters need to be escaped when passing in pat. eg. '$' has a special meaning in regex and must be escaped when finding this literal character. Examples 
>>> s = pd.Series(['A', 'B', 'Aaba', 'Baca', np.nan, 'CABA', 'cat'])
>>> s.str.count('a')
0    0.0
1    0.0
2    2.0
3    2.0
4    NaN
5    0.0
6    1.0
dtype: float64
  Escape '$' to find the literal dollar sign. 
>>> s = pd.Series(['$', 'B', 'Aab$', '$$ca', 'C$B$', 'cat'])
>>> s.str.count('\\$')
0    1
1    0
2    1
3    2
4    2
5    0
dtype: int64
  This is also available on Index 
>>> pd.Index(['A', 'A', 'Aaba', 'cat']).str.count('a')
Int64Index([0, 0, 2, 1], dtype='int64')
torch.cuda.device_count() [source]
 
Returns the number of GPUs available.
def f_2600191(l):
    """count the occurrences of items in list `l`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
count(value)  
Returns the number of occurrences of value.
operator.countOf(a, b)  
Return the number of occurrences of b in a.
tf.raw_ops.BatchCholeskyGrad  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.raw_ops.BatchCholeskyGrad  
tf.raw_ops.BatchCholeskyGrad(
    l, grad, name=None
)

 


 Args
  l   A Tensor. Must be one of the following types: float32, float64.  
  grad   A Tensor. Must have the same type as l.  
  name   A name for the operation (optional).   
 


 Returns   A Tensor. Has the same type as l.
pandas.Series.str.count   Series.str.count(pat, flags=0)[source]
 
Count occurrences of pattern in each string of the Series/Index. This function is used to count the number of times a particular regex pattern is repeated in each of the string elements of the Series.  Parameters 
 
pat:str


Valid regular expression.  
flags:int, default 0, meaning no flags


Flags for the re module. For a complete list, see here.  **kwargs

For compatibility with other string methods. Not used.    Returns 
 Series or Index

Same type as the calling object containing the integer counts.      See also  re

Standard library module for regular expressions.  str.count

Standard library version, without regular expression support.    Notes Some characters need to be escaped when passing in pat. eg. '$' has a special meaning in regex and must be escaped when finding this literal character. Examples 
>>> s = pd.Series(['A', 'B', 'Aaba', 'Baca', np.nan, 'CABA', 'cat'])
>>> s.str.count('a')
0    0.0
1    0.0
2    2.0
3    2.0
4    NaN
5    0.0
6    1.0
dtype: float64
  Escape '$' to find the literal dollar sign. 
>>> s = pd.Series(['$', 'B', 'Aab$', '$$ca', 'C$B$', 'cat'])
>>> s.str.count('\\$')
0    1
1    0
2    1
3    2
4    2
5    0
dtype: int64
  This is also available on Index 
>>> pd.Index(['A', 'A', 'Aaba', 'cat']).str.count('a')
Int64Index([0, 0, 2, 1], dtype='int64')
torch.cuda.device_count() [source]
 
Returns the number of GPUs available.
def f_2600191(l):
    """count the occurrences of items in list `l`
    """
    return  
 --------------------

def f_2600191(l):
    """count the occurrences of items in list `l`
    """
    return  
 --------------------

def f_2600191(l):
    """count the occurrences of item "b" in list `l`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
shutil.copyfile(src, dst, *, follow_symlinks=True)  
Copy the contents (no metadata) of the file named src to a file named dst and return dst in the most efficient way possible. src and dst are path-like objects or path names given as strings. dst must be the complete target file name; look at copy() for a copy that accepts a target directory path. If src and dst specify the same file, SameFileError is raised. The destination location must be writable; otherwise, an OSError exception will be raised. If dst already exists, it will be replaced. Special files such as character or block devices and pipes cannot be copied with this function. If follow_symlinks is false and src is a symbolic link, a new symbolic link will be created instead of copying the file src points to. Raises an auditing event shutil.copyfile with arguments src, dst.  Changed in version 3.3: IOError used to be raised instead of OSError. Added follow_symlinks argument. Now returns dst.   Changed in version 3.4: Raise SameFileError instead of Error. Since the former is a subclass of the latter, this change is backward compatible.   Changed in version 3.8: Platform-specific fast-copy syscalls may be used internally in order to copy the file more efficiently. See Platform-dependent efficient copy operations section.
Template.copy(infile, outfile)  
Copy infile to outfile through the pipe.
exception shutil.SameFileError  
This exception is raised if source and destination in copyfile() are the same file.  New in version 3.4.
shutil.copyfileobj(fsrc, fdst[, length])  
Copy the contents of the file-like object fsrc to the file-like object fdst. The integer length, if given, is the buffer size. In particular, a negative length value means to copy the data without looping over the source data in chunks; by default the data is read in chunks to avoid uncontrolled memory consumption. Note that if the current file position of the fsrc object is not 0, only the contents from the current file position to the end of the file will be copied.
test.support.findfile(filename, subdir=None)  
Return the path to the file named filename. If no match is found filename is returned. This does not equal a failure since it could be the path to the file. Setting subdir indicates a relative path to use to find the file rather than looking directly in the path directories.
def f_12842997(srcfile, dstdir):
    """copy file `srcfile` to directory `dstdir`
    """
     
 --------------------

def f_1555968(x):
    """find the key associated with the largest value in dictionary `x` whilst key is non-zero value
    """
    return  
 --------------------

def f_17021863(file):
    """Put the curser at beginning of the file
    """
     
 --------------------
Please refer to the following documentation to generate the code:
pandas.DataFrame.combine_first   DataFrame.combine_first(other)[source]
 
Update null elements with value in the same location in other. Combine two DataFrame objects by filling null values in one DataFrame with non-null values from other DataFrame. The row and column indexes of the resulting DataFrame will be the union of the two.  Parameters 
 
other:DataFrame


Provided DataFrame to use to fill null values.    Returns 
 DataFrame

The result of combining the provided DataFrame with the other object.      See also  DataFrame.combine

Perform series-wise operation on two DataFrames using a given function.    Examples 
>>> df1 = pd.DataFrame({'A': [None, 0], 'B': [None, 4]})
>>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})
>>> df1.combine_first(df2)
     A    B
0  1.0  3.0
1  0.0  4.0
  Null values still persist if the location of that null value does not exist in other 
>>> df1 = pd.DataFrame({'A': [None, 0], 'B': [4, None]})
>>> df2 = pd.DataFrame({'B': [3, 3], 'C': [1, 1]}, index=[1, 2])
>>> df1.combine_first(df2)
     A    B    C
0  NaN  4.0  NaN
1  0.0  3.0  1.0
2  NaN  3.0  1.0
pandas.DataFrame.combine   DataFrame.combine(other, func, fill_value=None, overwrite=True)[source]
 
Perform column-wise combine with another DataFrame. Combines a DataFrame with other DataFrame using func to element-wise combine columns. The row and column indexes of the resulting DataFrame will be the union of the two.  Parameters 
 
other:DataFrame


The DataFrame to merge column-wise.  
func:function


Function that takes two series as inputs and return a Series or a scalar. Used to merge the two dataframes column by columns.  
fill_value:scalar value, default None


The value to fill NaNs with prior to passing any column to the merge func.  
overwrite:bool, default True


If True, columns in self that do not exist in other will be overwritten with NaNs.    Returns 
 DataFrame

Combination of the provided DataFrames.      See also  DataFrame.combine_first

Combine two DataFrame objects and default to non-null values in frame calling the method.    Examples Combine using a simple function that chooses the smaller column. 
>>> df1 = pd.DataFrame({'A': [0, 0], 'B': [4, 4]})
>>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})
>>> take_smaller = lambda s1, s2: s1 if s1.sum() < s2.sum() else s2
>>> df1.combine(df2, take_smaller)
   A  B
0  0  3
1  0  3
  Example using a true element-wise combine function. 
>>> df1 = pd.DataFrame({'A': [5, 0], 'B': [2, 4]})
>>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})
>>> df1.combine(df2, np.minimum)
   A  B
0  1  2
1  0  3
  Using fill_value fills Nones prior to passing the column to the merge function. 
>>> df1 = pd.DataFrame({'A': [0, 0], 'B': [None, 4]})
>>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})
>>> df1.combine(df2, take_smaller, fill_value=-5)
   A    B
0  0 -5.0
1  0  4.0
  However, if the same element in both dataframes is None, that None is preserved 
>>> df1 = pd.DataFrame({'A': [0, 0], 'B': [None, 4]})
>>> df2 = pd.DataFrame({'A': [1, 1], 'B': [None, 3]})
>>> df1.combine(df2, take_smaller, fill_value=-5)
    A    B
0  0 -5.0
1  0  3.0
  Example that demonstrates the use of overwrite and behavior when the axis differ between the dataframes. 
>>> df1 = pd.DataFrame({'A': [0, 0], 'B': [4, 4]})
>>> df2 = pd.DataFrame({'B': [3, 3], 'C': [-10, 1], }, index=[1, 2])
>>> df1.combine(df2, take_smaller)
     A    B     C
0  NaN  NaN   NaN
1  NaN  3.0 -10.0
2  NaN  3.0   1.0
  
>>> df1.combine(df2, take_smaller, overwrite=False)
     A    B     C
0  0.0  NaN   NaN
1  0.0  3.0 -10.0
2  NaN  3.0   1.0
  Demonstrating the preference of the passed in dataframe. 
>>> df2 = pd.DataFrame({'B': [3, 3], 'C': [1, 1], }, index=[1, 2])
>>> df2.combine(df1, take_smaller)
   A    B   C
0  0.0  NaN NaN
1  0.0  3.0 NaN
2  NaN  3.0 NaN
  
>>> df2.combine(df1, take_smaller, overwrite=False)
     A    B   C
0  0.0  NaN NaN
1  0.0  3.0 1.0
2  NaN  3.0 1.0
Cookbook This is a repository for short and sweet examples and links for useful pandas recipes. We encourage users to add to this documentation. Adding interesting links and/or inline examples to this section is a great First Pull Request. Simplified, condensed, new-user friendly, in-line examples have been inserted where possible to augment the Stack-Overflow and GitHub links. Many of the links contain expanded information, above what the in-line examples offer. pandas (pd) and NumPy (np) are the only two abbreviated imported modules. The rest are kept explicitly imported for newer users.  Idioms These are some neat pandas idioms if-then/if-then-else on one column, and assignment to another one or more columns: 
In [1]: df = pd.DataFrame(
   ...:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ...: )
   ...: 

In [2]: df
Out[2]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50
   if-then… An if-then on one column 
In [3]: df.loc[df.AAA >= 5, "BBB"] = -1

In [4]: df
Out[4]: 
   AAA  BBB  CCC
0    4   10  100
1    5   -1   50
2    6   -1  -30
3    7   -1  -50
  An if-then with assignment to 2 columns: 
In [5]: df.loc[df.AAA >= 5, ["BBB", "CCC"]] = 555

In [6]: df
Out[6]: 
   AAA  BBB  CCC
0    4   10  100
1    5  555  555
2    6  555  555
3    7  555  555
  Add another line with different logic, to do the -else 
In [7]: df.loc[df.AAA < 5, ["BBB", "CCC"]] = 2000

In [8]: df
Out[8]: 
   AAA   BBB   CCC
0    4  2000  2000
1    5   555   555
2    6   555   555
3    7   555   555
  Or use pandas where after you’ve set up a mask 
In [9]: df_mask = pd.DataFrame(
   ...:     {"AAA": [True] * 4, "BBB": [False] * 4, "CCC": [True, False] * 2}
   ...: )
   ...: 

In [10]: df.where(df_mask, -1000)
Out[10]: 
   AAA   BBB   CCC
0    4 -1000  2000
1    5 -1000 -1000
2    6 -1000   555
3    7 -1000 -1000
  if-then-else using NumPy’s where() 
In [11]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ....: )
   ....: 

In [12]: df
Out[12]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50

In [13]: df["logic"] = np.where(df["AAA"] > 5, "high", "low")

In [14]: df
Out[14]: 
   AAA  BBB  CCC logic
0    4   10  100   low
1    5   20   50   low
2    6   30  -30  high
3    7   40  -50  high
    Splitting Split a frame with a boolean criterion 
In [15]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ....: )
   ....: 

In [16]: df
Out[16]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50

In [17]: df[df.AAA <= 5]
Out[17]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50

In [18]: df[df.AAA > 5]
Out[18]: 
   AAA  BBB  CCC
2    6   30  -30
3    7   40  -50
    Building criteria Select with multi-column criteria 
In [19]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ....: )
   ....: 

In [20]: df
Out[20]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50
  …and (without assignment returns a Series) 
In [21]: df.loc[(df["BBB"] < 25) & (df["CCC"] >= -40), "AAA"]
Out[21]: 
0    4
1    5
Name: AAA, dtype: int64
  …or (without assignment returns a Series) 
In [22]: df.loc[(df["BBB"] > 25) | (df["CCC"] >= -40), "AAA"]
Out[22]: 
0    4
1    5
2    6
3    7
Name: AAA, dtype: int64
  …or (with assignment modifies the DataFrame.) 
In [23]: df.loc[(df["BBB"] > 25) | (df["CCC"] >= 75), "AAA"] = 0.1

In [24]: df
Out[24]: 
   AAA  BBB  CCC
0  0.1   10  100
1  5.0   20   50
2  0.1   30  -30
3  0.1   40  -50
  Select rows with data closest to certain value using argsort 
In [25]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ....: )
   ....: 

In [26]: df
Out[26]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50

In [27]: aValue = 43.0

In [28]: df.loc[(df.CCC - aValue).abs().argsort()]
Out[28]: 
   AAA  BBB  CCC
1    5   20   50
0    4   10  100
2    6   30  -30
3    7   40  -50
  Dynamically reduce a list of criteria using a binary operators 
In [29]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ....: )
   ....: 

In [30]: df
Out[30]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50

In [31]: Crit1 = df.AAA <= 5.5

In [32]: Crit2 = df.BBB == 10.0

In [33]: Crit3 = df.CCC > -40.0
  One could hard code: 
In [34]: AllCrit = Crit1 & Crit2 & Crit3
  …Or it can be done with a list of dynamically built criteria 
In [35]: import functools

In [36]: CritList = [Crit1, Crit2, Crit3]

In [37]: AllCrit = functools.reduce(lambda x, y: x & y, CritList)

In [38]: df[AllCrit]
Out[38]: 
   AAA  BBB  CCC
0    4   10  100
     Selection  Dataframes The indexing docs. Using both row labels and value conditionals 
In [39]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ....: )
   ....: 

In [40]: df
Out[40]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50

In [41]: df[(df.AAA <= 6) & (df.index.isin([0, 2, 4]))]
Out[41]: 
   AAA  BBB  CCC
0    4   10  100
2    6   30  -30
  Use loc for label-oriented slicing and iloc positional slicing GH2904 
In [42]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]},
   ....:     index=["foo", "bar", "boo", "kar"],
   ....: )
   ....: 
  There are 2 explicit slicing methods, with a third general case  Positional-oriented (Python slicing style : exclusive of end) Label-oriented (Non-Python slicing style : inclusive of end) General (Either slicing style : depends on if the slice contains labels or positions)  
In [43]: df.loc["bar":"kar"]  # Label
Out[43]: 
     AAA  BBB  CCC
bar    5   20   50
boo    6   30  -30
kar    7   40  -50

# Generic
In [44]: df[0:3]
Out[44]: 
     AAA  BBB  CCC
foo    4   10  100
bar    5   20   50
boo    6   30  -30

In [45]: df["bar":"kar"]
Out[45]: 
     AAA  BBB  CCC
bar    5   20   50
boo    6   30  -30
kar    7   40  -50
  Ambiguity arises when an index consists of integers with a non-zero start or non-unit increment. 
In [46]: data = {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}

In [47]: df2 = pd.DataFrame(data=data, index=[1, 2, 3, 4])  # Note index starts at 1.

In [48]: df2.iloc[1:3]  # Position-oriented
Out[48]: 
   AAA  BBB  CCC
2    5   20   50
3    6   30  -30

In [49]: df2.loc[1:3]  # Label-oriented
Out[49]: 
   AAA  BBB  CCC
1    4   10  100
2    5   20   50
3    6   30  -30
  Using inverse operator (~) to take the complement of a mask 
In [50]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ....: )
   ....: 

In [51]: df
Out[51]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50

In [52]: df[~((df.AAA <= 6) & (df.index.isin([0, 2, 4])))]
Out[52]: 
   AAA  BBB  CCC
1    5   20   50
3    7   40  -50
    New columns Efficiently and dynamically creating new columns using applymap 
In [53]: df = pd.DataFrame({"AAA": [1, 2, 1, 3], "BBB": [1, 1, 2, 2], "CCC": [2, 1, 3, 1]})

In [54]: df
Out[54]: 
   AAA  BBB  CCC
0    1    1    2
1    2    1    1
2    1    2    3
3    3    2    1

In [55]: source_cols = df.columns  # Or some subset would work too

In [56]: new_cols = [str(x) + "_cat" for x in source_cols]

In [57]: categories = {1: "Alpha", 2: "Beta", 3: "Charlie"}

In [58]: df[new_cols] = df[source_cols].applymap(categories.get)

In [59]: df
Out[59]: 
   AAA  BBB  CCC  AAA_cat BBB_cat  CCC_cat
0    1    1    2    Alpha   Alpha     Beta
1    2    1    1     Beta   Alpha    Alpha
2    1    2    3    Alpha    Beta  Charlie
3    3    2    1  Charlie    Beta    Alpha
  Keep other columns when using min() with groupby 
In [60]: df = pd.DataFrame(
   ....:     {"AAA": [1, 1, 1, 2, 2, 2, 3, 3], "BBB": [2, 1, 3, 4, 5, 1, 2, 3]}
   ....: )
   ....: 

In [61]: df
Out[61]: 
   AAA  BBB
0    1    2
1    1    1
2    1    3
3    2    4
4    2    5
5    2    1
6    3    2
7    3    3
  Method 1 : idxmin() to get the index of the minimums 
In [62]: df.loc[df.groupby("AAA")["BBB"].idxmin()]
Out[62]: 
   AAA  BBB
1    1    1
5    2    1
6    3    2
  Method 2 : sort then take first of each 
In [63]: df.sort_values(by="BBB").groupby("AAA", as_index=False).first()
Out[63]: 
   AAA  BBB
0    1    1
1    2    1
2    3    2
  Notice the same results, with the exception of the index.    Multiindexing The multindexing docs. Creating a MultiIndex from a labeled frame 
In [64]: df = pd.DataFrame(
   ....:     {
   ....:         "row": [0, 1, 2],
   ....:         "One_X": [1.1, 1.1, 1.1],
   ....:         "One_Y": [1.2, 1.2, 1.2],
   ....:         "Two_X": [1.11, 1.11, 1.11],
   ....:         "Two_Y": [1.22, 1.22, 1.22],
   ....:     }
   ....: )
   ....: 

In [65]: df
Out[65]: 
   row  One_X  One_Y  Two_X  Two_Y
0    0    1.1    1.2   1.11   1.22
1    1    1.1    1.2   1.11   1.22
2    2    1.1    1.2   1.11   1.22

# As Labelled Index
In [66]: df = df.set_index("row")

In [67]: df
Out[67]: 
     One_X  One_Y  Two_X  Two_Y
row                            
0      1.1    1.2   1.11   1.22
1      1.1    1.2   1.11   1.22
2      1.1    1.2   1.11   1.22

# With Hierarchical Columns
In [68]: df.columns = pd.MultiIndex.from_tuples([tuple(c.split("_")) for c in df.columns])

In [69]: df
Out[69]: 
     One        Two      
       X    Y     X     Y
row                      
0    1.1  1.2  1.11  1.22
1    1.1  1.2  1.11  1.22
2    1.1  1.2  1.11  1.22

# Now stack & Reset
In [70]: df = df.stack(0).reset_index(1)

In [71]: df
Out[71]: 
    level_1     X     Y
row                    
0       One  1.10  1.20
0       Two  1.11  1.22
1       One  1.10  1.20
1       Two  1.11  1.22
2       One  1.10  1.20
2       Two  1.11  1.22

# And fix the labels (Notice the label 'level_1' got added automatically)
In [72]: df.columns = ["Sample", "All_X", "All_Y"]

In [73]: df
Out[73]: 
    Sample  All_X  All_Y
row                     
0      One   1.10   1.20
0      Two   1.11   1.22
1      One   1.10   1.20
1      Two   1.11   1.22
2      One   1.10   1.20
2      Two   1.11   1.22
   Arithmetic Performing arithmetic with a MultiIndex that needs broadcasting 
In [74]: cols = pd.MultiIndex.from_tuples(
   ....:     [(x, y) for x in ["A", "B", "C"] for y in ["O", "I"]]
   ....: )
   ....: 

In [75]: df = pd.DataFrame(np.random.randn(2, 6), index=["n", "m"], columns=cols)

In [76]: df
Out[76]: 
          A                   B                   C          
          O         I         O         I         O         I
n  0.469112 -0.282863 -1.509059 -1.135632  1.212112 -0.173215
m  0.119209 -1.044236 -0.861849 -2.104569 -0.494929  1.071804

In [77]: df = df.div(df["C"], level=1)

In [78]: df
Out[78]: 
          A                   B              C     
          O         I         O         I    O    I
n  0.387021  1.633022 -1.244983  6.556214  1.0  1.0
m -0.240860 -0.974279  1.741358 -1.963577  1.0  1.0
    Slicing Slicing a MultiIndex with xs 
In [79]: coords = [("AA", "one"), ("AA", "six"), ("BB", "one"), ("BB", "two"), ("BB", "six")]

In [80]: index = pd.MultiIndex.from_tuples(coords)

In [81]: df = pd.DataFrame([11, 22, 33, 44, 55], index, ["MyData"])

In [82]: df
Out[82]: 
        MyData
AA one      11
   six      22
BB one      33
   two      44
   six      55
  To take the cross section of the 1st level and 1st axis the index: 
# Note : level and axis are optional, and default to zero
In [83]: df.xs("BB", level=0, axis=0)
Out[83]: 
     MyData
one      33
two      44
six      55
  …and now the 2nd level of the 1st axis. 
In [84]: df.xs("six", level=1, axis=0)
Out[84]: 
    MyData
AA      22
BB      55
  Slicing a MultiIndex with xs, method #2 
In [85]: import itertools

In [86]: index = list(itertools.product(["Ada", "Quinn", "Violet"], ["Comp", "Math", "Sci"]))

In [87]: headr = list(itertools.product(["Exams", "Labs"], ["I", "II"]))

In [88]: indx = pd.MultiIndex.from_tuples(index, names=["Student", "Course"])

In [89]: cols = pd.MultiIndex.from_tuples(headr)  # Notice these are un-named

In [90]: data = [[70 + x + y + (x * y) % 3 for x in range(4)] for y in range(9)]

In [91]: df = pd.DataFrame(data, indx, cols)

In [92]: df
Out[92]: 
               Exams     Labs    
                   I  II    I  II
Student Course                   
Ada     Comp      70  71   72  73
        Math      71  73   75  74
        Sci       72  75   75  75
Quinn   Comp      73  74   75  76
        Math      74  76   78  77
        Sci       75  78   78  78
Violet  Comp      76  77   78  79
        Math      77  79   81  80
        Sci       78  81   81  81

In [93]: All = slice(None)

In [94]: df.loc["Violet"]
Out[94]: 
       Exams     Labs    
           I  II    I  II
Course                   
Comp      76  77   78  79
Math      77  79   81  80
Sci       78  81   81  81

In [95]: df.loc[(All, "Math"), All]
Out[95]: 
               Exams     Labs    
                   I  II    I  II
Student Course                   
Ada     Math      71  73   75  74
Quinn   Math      74  76   78  77
Violet  Math      77  79   81  80

In [96]: df.loc[(slice("Ada", "Quinn"), "Math"), All]
Out[96]: 
               Exams     Labs    
                   I  II    I  II
Student Course                   
Ada     Math      71  73   75  74
Quinn   Math      74  76   78  77

In [97]: df.loc[(All, "Math"), ("Exams")]
Out[97]: 
                 I  II
Student Course        
Ada     Math    71  73
Quinn   Math    74  76
Violet  Math    77  79

In [98]: df.loc[(All, "Math"), (All, "II")]
Out[98]: 
               Exams Labs
                  II   II
Student Course           
Ada     Math      73   74
Quinn   Math      76   77
Violet  Math      79   80
  Setting portions of a MultiIndex with xs   Sorting Sort by specific column or an ordered list of columns, with a MultiIndex 
In [99]: df.sort_values(by=("Labs", "II"), ascending=False)
Out[99]: 
               Exams     Labs    
                   I  II    I  II
Student Course                   
Violet  Sci       78  81   81  81
        Math      77  79   81  80
        Comp      76  77   78  79
Quinn   Sci       75  78   78  78
        Math      74  76   78  77
        Comp      73  74   75  76
Ada     Sci       72  75   75  75
        Math      71  73   75  74
        Comp      70  71   72  73
  Partial selection, the need for sortedness GH2995   Levels Prepending a level to a multiindex Flatten Hierarchical columns    Missing data The missing data docs. Fill forward a reversed timeseries 
In [100]: df = pd.DataFrame(
   .....:     np.random.randn(6, 1),
   .....:     index=pd.date_range("2013-08-01", periods=6, freq="B"),
   .....:     columns=list("A"),
   .....: )
   .....: 

In [101]: df.loc[df.index[3], "A"] = np.nan

In [102]: df
Out[102]: 
                   A
2013-08-01  0.721555
2013-08-02 -0.706771
2013-08-05 -1.039575
2013-08-06       NaN
2013-08-07 -0.424972
2013-08-08  0.567020

In [103]: df.reindex(df.index[::-1]).ffill()
Out[103]: 
                   A
2013-08-08  0.567020
2013-08-07 -0.424972
2013-08-06 -0.424972
2013-08-05 -1.039575
2013-08-02 -0.706771
2013-08-01  0.721555
  cumsum reset at NaN values  Replace Using replace with backrefs    Grouping The grouping docs. Basic grouping with apply Unlike agg, apply’s callable is passed a sub-DataFrame which gives you access to all the columns 
In [104]: df = pd.DataFrame(
   .....:     {
   .....:         "animal": "cat dog cat fish dog cat cat".split(),
   .....:         "size": list("SSMMMLL"),
   .....:         "weight": [8, 10, 11, 1, 20, 12, 12],
   .....:         "adult": [False] * 5 + [True] * 2,
   .....:     }
   .....: )
   .....: 

In [105]: df
Out[105]: 
  animal size  weight  adult
0    cat    S       8  False
1    dog    S      10  False
2    cat    M      11  False
3   fish    M       1  False
4    dog    M      20  False
5    cat    L      12   True
6    cat    L      12   True

# List the size of the animals with the highest weight.
In [106]: df.groupby("animal").apply(lambda subf: subf["size"][subf["weight"].idxmax()])
Out[106]: 
animal
cat     L
dog     M
fish    M
dtype: object
  Using get_group 
In [107]: gb = df.groupby(["animal"])

In [108]: gb.get_group("cat")
Out[108]: 
  animal size  weight  adult
0    cat    S       8  False
2    cat    M      11  False
5    cat    L      12   True
6    cat    L      12   True
  Apply to different items in a group 
In [109]: def GrowUp(x):
   .....:     avg_weight = sum(x[x["size"] == "S"].weight * 1.5)
   .....:     avg_weight += sum(x[x["size"] == "M"].weight * 1.25)
   .....:     avg_weight += sum(x[x["size"] == "L"].weight)
   .....:     avg_weight /= len(x)
   .....:     return pd.Series(["L", avg_weight, True], index=["size", "weight", "adult"])
   .....: 

In [110]: expected_df = gb.apply(GrowUp)

In [111]: expected_df
Out[111]: 
       size   weight  adult
animal                     
cat       L  12.4375   True
dog       L  20.0000   True
fish      L   1.2500   True
  Expanding apply 
In [112]: S = pd.Series([i / 100.0 for i in range(1, 11)])

In [113]: def cum_ret(x, y):
   .....:     return x * (1 + y)
   .....: 

In [114]: def red(x):
   .....:     return functools.reduce(cum_ret, x, 1.0)
   .....: 

In [115]: S.expanding().apply(red, raw=True)
Out[115]: 
0    1.010000
1    1.030200
2    1.061106
3    1.103550
4    1.158728
5    1.228251
6    1.314229
7    1.419367
8    1.547110
9    1.701821
dtype: float64
  Replacing some values with mean of the rest of a group 
In [116]: df = pd.DataFrame({"A": [1, 1, 2, 2], "B": [1, -1, 1, 2]})

In [117]: gb = df.groupby("A")

In [118]: def replace(g):
   .....:     mask = g < 0
   .....:     return g.where(mask, g[~mask].mean())
   .....: 

In [119]: gb.transform(replace)
Out[119]: 
     B
0  1.0
1 -1.0
2  1.5
3  1.5
  Sort groups by aggregated data 
In [120]: df = pd.DataFrame(
   .....:     {
   .....:         "code": ["foo", "bar", "baz"] * 2,
   .....:         "data": [0.16, -0.21, 0.33, 0.45, -0.59, 0.62],
   .....:         "flag": [False, True] * 3,
   .....:     }
   .....: )
   .....: 

In [121]: code_groups = df.groupby("code")

In [122]: agg_n_sort_order = code_groups[["data"]].transform(sum).sort_values(by="data")

In [123]: sorted_df = df.loc[agg_n_sort_order.index]

In [124]: sorted_df
Out[124]: 
  code  data   flag
1  bar -0.21   True
4  bar -0.59  False
0  foo  0.16  False
3  foo  0.45   True
2  baz  0.33  False
5  baz  0.62   True
  Create multiple aggregated columns 
In [125]: rng = pd.date_range(start="2014-10-07", periods=10, freq="2min")

In [126]: ts = pd.Series(data=list(range(10)), index=rng)

In [127]: def MyCust(x):
   .....:     if len(x) > 2:
   .....:         return x[1] * 1.234
   .....:     return pd.NaT
   .....: 

In [128]: mhc = {"Mean": np.mean, "Max": np.max, "Custom": MyCust}

In [129]: ts.resample("5min").apply(mhc)
Out[129]: 
                     Mean  Max Custom
2014-10-07 00:00:00   1.0    2  1.234
2014-10-07 00:05:00   3.5    4    NaT
2014-10-07 00:10:00   6.0    7  7.404
2014-10-07 00:15:00   8.5    9    NaT

In [130]: ts
Out[130]: 
2014-10-07 00:00:00    0
2014-10-07 00:02:00    1
2014-10-07 00:04:00    2
2014-10-07 00:06:00    3
2014-10-07 00:08:00    4
2014-10-07 00:10:00    5
2014-10-07 00:12:00    6
2014-10-07 00:14:00    7
2014-10-07 00:16:00    8
2014-10-07 00:18:00    9
Freq: 2T, dtype: int64
  Create a value counts column and reassign back to the DataFrame 
In [131]: df = pd.DataFrame(
   .....:     {"Color": "Red Red Red Blue".split(), "Value": [100, 150, 50, 50]}
   .....: )
   .....: 

In [132]: df
Out[132]: 
  Color  Value
0   Red    100
1   Red    150
2   Red     50
3  Blue     50

In [133]: df["Counts"] = df.groupby(["Color"]).transform(len)

In [134]: df
Out[134]: 
  Color  Value  Counts
0   Red    100       3
1   Red    150       3
2   Red     50       3
3  Blue     50       1
  Shift groups of the values in a column based on the index 
In [135]: df = pd.DataFrame(
   .....:     {"line_race": [10, 10, 8, 10, 10, 8], "beyer": [99, 102, 103, 103, 88, 100]},
   .....:     index=[
   .....:         "Last Gunfighter",
   .....:         "Last Gunfighter",
   .....:         "Last Gunfighter",
   .....:         "Paynter",
   .....:         "Paynter",
   .....:         "Paynter",
   .....:     ],
   .....: )
   .....: 

In [136]: df
Out[136]: 
                 line_race  beyer
Last Gunfighter         10     99
Last Gunfighter         10    102
Last Gunfighter          8    103
Paynter                 10    103
Paynter                 10     88
Paynter                  8    100

In [137]: df["beyer_shifted"] = df.groupby(level=0)["beyer"].shift(1)

In [138]: df
Out[138]: 
                 line_race  beyer  beyer_shifted
Last Gunfighter         10     99            NaN
Last Gunfighter         10    102           99.0
Last Gunfighter          8    103          102.0
Paynter                 10    103            NaN
Paynter                 10     88          103.0
Paynter                  8    100           88.0
  Select row with maximum value from each group 
In [139]: df = pd.DataFrame(
   .....:     {
   .....:         "host": ["other", "other", "that", "this", "this"],
   .....:         "service": ["mail", "web", "mail", "mail", "web"],
   .....:         "no": [1, 2, 1, 2, 1],
   .....:     }
   .....: ).set_index(["host", "service"])
   .....: 

In [140]: mask = df.groupby(level=0).agg("idxmax")

In [141]: df_count = df.loc[mask["no"]].reset_index()

In [142]: df_count
Out[142]: 
    host service  no
0  other     web   2
1   that    mail   1
2   this    mail   2
  Grouping like Python’s itertools.groupby 
In [143]: df = pd.DataFrame([0, 1, 0, 1, 1, 1, 0, 1, 1], columns=["A"])

In [144]: df["A"].groupby((df["A"] != df["A"].shift()).cumsum()).groups
Out[144]: {1: [0], 2: [1], 3: [2], 4: [3, 4, 5], 5: [6], 6: [7, 8]}

In [145]: df["A"].groupby((df["A"] != df["A"].shift()).cumsum()).cumsum()
Out[145]: 
0    0
1    1
2    0
3    1
4    2
5    3
6    0
7    1
8    2
Name: A, dtype: int64
   Expanding data Alignment and to-date Rolling Computation window based on values instead of counts Rolling Mean by Time Interval   Splitting Splitting a frame Create a list of dataframes, split using a delineation based on logic included in rows. 
In [146]: df = pd.DataFrame(
   .....:     data={
   .....:         "Case": ["A", "A", "A", "B", "A", "A", "B", "A", "A"],
   .....:         "Data": np.random.randn(9),
   .....:     }
   .....: )
   .....: 

In [147]: dfs = list(
   .....:     zip(
   .....:         *df.groupby(
   .....:             (1 * (df["Case"] == "B"))
   .....:             .cumsum()
   .....:             .rolling(window=3, min_periods=1)
   .....:             .median()
   .....:         )
   .....:     )
   .....: )[-1]
   .....: 

In [148]: dfs[0]
Out[148]: 
  Case      Data
0    A  0.276232
1    A -1.087401
2    A -0.673690
3    B  0.113648

In [149]: dfs[1]
Out[149]: 
  Case      Data
4    A -1.478427
5    A  0.524988
6    B  0.404705

In [150]: dfs[2]
Out[150]: 
  Case      Data
7    A  0.577046
8    A -1.715002
    Pivot The Pivot docs. Partial sums and subtotals 
In [151]: df = pd.DataFrame(
   .....:     data={
   .....:         "Province": ["ON", "QC", "BC", "AL", "AL", "MN", "ON"],
   .....:         "City": [
   .....:             "Toronto",
   .....:             "Montreal",
   .....:             "Vancouver",
   .....:             "Calgary",
   .....:             "Edmonton",
   .....:             "Winnipeg",
   .....:             "Windsor",
   .....:         ],
   .....:         "Sales": [13, 6, 16, 8, 4, 3, 1],
   .....:     }
   .....: )
   .....: 

In [152]: table = pd.pivot_table(
   .....:     df,
   .....:     values=["Sales"],
   .....:     index=["Province"],
   .....:     columns=["City"],
   .....:     aggfunc=np.sum,
   .....:     margins=True,
   .....: )
   .....: 

In [153]: table.stack("City")
Out[153]: 
                    Sales
Province City            
AL       All         12.0
         Calgary      8.0
         Edmonton     4.0
BC       All         16.0
         Vancouver   16.0
...                   ...
All      Montreal     6.0
         Toronto     13.0
         Vancouver   16.0
         Windsor      1.0
         Winnipeg     3.0

[20 rows x 1 columns]
  Frequency table like plyr in R 
In [154]: grades = [48, 99, 75, 80, 42, 80, 72, 68, 36, 78]

In [155]: df = pd.DataFrame(
   .....:     {
   .....:         "ID": ["x%d" % r for r in range(10)],
   .....:         "Gender": ["F", "M", "F", "M", "F", "M", "F", "M", "M", "M"],
   .....:         "ExamYear": [
   .....:             "2007",
   .....:             "2007",
   .....:             "2007",
   .....:             "2008",
   .....:             "2008",
   .....:             "2008",
   .....:             "2008",
   .....:             "2009",
   .....:             "2009",
   .....:             "2009",
   .....:         ],
   .....:         "Class": [
   .....:             "algebra",
   .....:             "stats",
   .....:             "bio",
   .....:             "algebra",
   .....:             "algebra",
   .....:             "stats",
   .....:             "stats",
   .....:             "algebra",
   .....:             "bio",
   .....:             "bio",
   .....:         ],
   .....:         "Participated": [
   .....:             "yes",
   .....:             "yes",
   .....:             "yes",
   .....:             "yes",
   .....:             "no",
   .....:             "yes",
   .....:             "yes",
   .....:             "yes",
   .....:             "yes",
   .....:             "yes",
   .....:         ],
   .....:         "Passed": ["yes" if x > 50 else "no" for x in grades],
   .....:         "Employed": [
   .....:             True,
   .....:             True,
   .....:             True,
   .....:             False,
   .....:             False,
   .....:             False,
   .....:             False,
   .....:             True,
   .....:             True,
   .....:             False,
   .....:         ],
   .....:         "Grade": grades,
   .....:     }
   .....: )
   .....: 

In [156]: df.groupby("ExamYear").agg(
   .....:     {
   .....:         "Participated": lambda x: x.value_counts()["yes"],
   .....:         "Passed": lambda x: sum(x == "yes"),
   .....:         "Employed": lambda x: sum(x),
   .....:         "Grade": lambda x: sum(x) / len(x),
   .....:     }
   .....: )
   .....: 
Out[156]: 
          Participated  Passed  Employed      Grade
ExamYear                                           
2007                 3       2         3  74.000000
2008                 3       3         0  68.500000
2009                 3       2         2  60.666667
  Plot pandas DataFrame with year over year data To create year and month cross tabulation: 
In [157]: df = pd.DataFrame(
   .....:     {"value": np.random.randn(36)},
   .....:     index=pd.date_range("2011-01-01", freq="M", periods=36),
   .....: )
   .....: 

In [158]: pd.pivot_table(
   .....:     df, index=df.index.month, columns=df.index.year, values="value", aggfunc="sum"
   .....: )
   .....: 
Out[158]: 
        2011      2012      2013
1  -1.039268 -0.968914  2.565646
2  -0.370647 -1.294524  1.431256
3  -1.157892  0.413738  1.340309
4  -1.344312  0.276662 -1.170299
5   0.844885 -0.472035 -0.226169
6   1.075770 -0.013960  0.410835
7  -0.109050 -0.362543  0.813850
8   1.643563 -0.006154  0.132003
9  -1.469388 -0.923061 -0.827317
10  0.357021  0.895717 -0.076467
11 -0.674600  0.805244 -1.187678
12 -1.776904 -1.206412  1.130127
    Apply Rolling apply to organize - Turning embedded lists into a MultiIndex frame 
In [159]: df = pd.DataFrame(
   .....:     data={
   .....:         "A": [[2, 4, 8, 16], [100, 200], [10, 20, 30]],
   .....:         "B": [["a", "b", "c"], ["jj", "kk"], ["ccc"]],
   .....:     },
   .....:     index=["I", "II", "III"],
   .....: )
   .....: 

In [160]: def SeriesFromSubList(aList):
   .....:     return pd.Series(aList)
   .....: 

In [161]: df_orgz = pd.concat(
   .....:     {ind: row.apply(SeriesFromSubList) for ind, row in df.iterrows()}
   .....: )
   .....: 

In [162]: df_orgz
Out[162]: 
         0     1     2     3
I   A    2     4     8  16.0
    B    a     b     c   NaN
II  A  100   200   NaN   NaN
    B   jj    kk   NaN   NaN
III A   10  20.0  30.0   NaN
    B  ccc   NaN   NaN   NaN
  Rolling apply with a DataFrame returning a Series Rolling Apply to multiple columns where function calculates a Series before a Scalar from the Series is returned 
In [163]: df = pd.DataFrame(
   .....:     data=np.random.randn(2000, 2) / 10000,
   .....:     index=pd.date_range("2001-01-01", periods=2000),
   .....:     columns=["A", "B"],
   .....: )
   .....: 

In [164]: df
Out[164]: 
                   A         B
2001-01-01 -0.000144 -0.000141
2001-01-02  0.000161  0.000102
2001-01-03  0.000057  0.000088
2001-01-04 -0.000221  0.000097
2001-01-05 -0.000201 -0.000041
...              ...       ...
2006-06-19  0.000040 -0.000235
2006-06-20 -0.000123 -0.000021
2006-06-21 -0.000113  0.000114
2006-06-22  0.000136  0.000109
2006-06-23  0.000027  0.000030

[2000 rows x 2 columns]

In [165]: def gm(df, const):
   .....:     v = ((((df["A"] + df["B"]) + 1).cumprod()) - 1) * const
   .....:     return v.iloc[-1]
   .....: 

In [166]: s = pd.Series(
   .....:     {
   .....:         df.index[i]: gm(df.iloc[i: min(i + 51, len(df) - 1)], 5)
   .....:         for i in range(len(df) - 50)
   .....:     }
   .....: )
   .....: 

In [167]: s
Out[167]: 
2001-01-01    0.000930
2001-01-02    0.002615
2001-01-03    0.001281
2001-01-04    0.001117
2001-01-05    0.002772
                ...   
2006-04-30    0.003296
2006-05-01    0.002629
2006-05-02    0.002081
2006-05-03    0.004247
2006-05-04    0.003928
Length: 1950, dtype: float64
  Rolling apply with a DataFrame returning a Scalar Rolling Apply to multiple columns where function returns a Scalar (Volume Weighted Average Price) 
In [168]: rng = pd.date_range(start="2014-01-01", periods=100)

In [169]: df = pd.DataFrame(
   .....:     {
   .....:         "Open": np.random.randn(len(rng)),
   .....:         "Close": np.random.randn(len(rng)),
   .....:         "Volume": np.random.randint(100, 2000, len(rng)),
   .....:     },
   .....:     index=rng,
   .....: )
   .....: 

In [170]: df
Out[170]: 
                Open     Close  Volume
2014-01-01 -1.611353 -0.492885    1219
2014-01-02 -3.000951  0.445794    1054
2014-01-03 -0.138359 -0.076081    1381
2014-01-04  0.301568  1.198259    1253
2014-01-05  0.276381 -0.669831    1728
...              ...       ...     ...
2014-04-06 -0.040338  0.937843    1188
2014-04-07  0.359661 -0.285908    1864
2014-04-08  0.060978  1.714814     941
2014-04-09  1.759055 -0.455942    1065
2014-04-10  0.138185 -1.147008    1453

[100 rows x 3 columns]

In [171]: def vwap(bars):
   .....:     return (bars.Close * bars.Volume).sum() / bars.Volume.sum()
   .....: 

In [172]: window = 5

In [173]: s = pd.concat(
   .....:     [
   .....:         (pd.Series(vwap(df.iloc[i: i + window]), index=[df.index[i + window]]))
   .....:         for i in range(len(df) - window)
   .....:     ]
   .....: )
   .....: 

In [174]: s.round(2)
Out[174]: 
2014-01-06    0.02
2014-01-07    0.11
2014-01-08    0.10
2014-01-09    0.07
2014-01-10   -0.29
              ... 
2014-04-06   -0.63
2014-04-07   -0.02
2014-04-08   -0.03
2014-04-09    0.34
2014-04-10    0.29
Length: 95, dtype: float64
     Timeseries Between times Using indexer between time Constructing a datetime range that excludes weekends and includes only certain times Vectorized Lookup Aggregation and plotting time series Turn a matrix with hours in columns and days in rows into a continuous row sequence in the form of a time series. How to rearrange a Python pandas DataFrame? Dealing with duplicates when reindexing a timeseries to a specified frequency Calculate the first day of the month for each entry in a DatetimeIndex 
In [175]: dates = pd.date_range("2000-01-01", periods=5)

In [176]: dates.to_period(freq="M").to_timestamp()
Out[176]: 
DatetimeIndex(['2000-01-01', '2000-01-01', '2000-01-01', '2000-01-01',
               '2000-01-01'],
              dtype='datetime64[ns]', freq=None)
   Resampling The Resample docs. Using Grouper instead of TimeGrouper for time grouping of values Time grouping with some missing values Valid frequency arguments to Grouper Timeseries Grouping using a MultiIndex Using TimeGrouper and another grouping to create subgroups, then apply a custom function GH3791 Resampling with custom periods Resample intraday frame without adding new days Resample minute data Resample with groupby    Merge The Join docs. Concatenate two dataframes with overlapping index (emulate R rbind) 
In [177]: rng = pd.date_range("2000-01-01", periods=6)

In [178]: df1 = pd.DataFrame(np.random.randn(6, 3), index=rng, columns=["A", "B", "C"])

In [179]: df2 = df1.copy()
  Depending on df construction, ignore_index may be needed 
In [180]: df = pd.concat([df1, df2], ignore_index=True)

In [181]: df
Out[181]: 
           A         B         C
0  -0.870117 -0.479265 -0.790855
1   0.144817  1.726395 -0.464535
2  -0.821906  1.597605  0.187307
3  -0.128342 -1.511638 -0.289858
4   0.399194 -1.430030 -0.639760
5   1.115116 -2.012600  1.810662
6  -0.870117 -0.479265 -0.790855
7   0.144817  1.726395 -0.464535
8  -0.821906  1.597605  0.187307
9  -0.128342 -1.511638 -0.289858
10  0.399194 -1.430030 -0.639760
11  1.115116 -2.012600  1.810662
  Self Join of a DataFrame GH2996 
In [182]: df = pd.DataFrame(
   .....:     data={
   .....:         "Area": ["A"] * 5 + ["C"] * 2,
   .....:         "Bins": [110] * 2 + [160] * 3 + [40] * 2,
   .....:         "Test_0": [0, 1, 0, 1, 2, 0, 1],
   .....:         "Data": np.random.randn(7),
   .....:     }
   .....: )
   .....: 

In [183]: df
Out[183]: 
  Area  Bins  Test_0      Data
0    A   110       0 -0.433937
1    A   110       1 -0.160552
2    A   160       0  0.744434
3    A   160       1  1.754213
4    A   160       2  0.000850
5    C    40       0  0.342243
6    C    40       1  1.070599

In [184]: df["Test_1"] = df["Test_0"] - 1

In [185]: pd.merge(
   .....:     df,
   .....:     df,
   .....:     left_on=["Bins", "Area", "Test_0"],
   .....:     right_on=["Bins", "Area", "Test_1"],
   .....:     suffixes=("_L", "_R"),
   .....: )
   .....: 
Out[185]: 
  Area  Bins  Test_0_L    Data_L  Test_1_L  Test_0_R    Data_R  Test_1_R
0    A   110         0 -0.433937        -1         1 -0.160552         0
1    A   160         0  0.744434        -1         1  1.754213         0
2    A   160         1  1.754213         0         2  0.000850         1
3    C    40         0  0.342243        -1         1  1.070599         0
  How to set the index and join KDB like asof join Join with a criteria based on the values Using searchsorted to merge based on values inside a range   Plotting The Plotting docs. Make Matplotlib look like R Setting x-axis major and minor labels Plotting multiple charts in an IPython Jupyter notebook Creating a multi-line plot Plotting a heatmap Annotate a time-series plot Annotate a time-series plot #2 Generate Embedded plots in excel files using Pandas, Vincent and xlsxwriter Boxplot for each quartile of a stratifying variable 
In [186]: df = pd.DataFrame(
   .....:     {
   .....:         "stratifying_var": np.random.uniform(0, 100, 20),
   .....:         "price": np.random.normal(100, 5, 20),
   .....:     }
   .....: )
   .....: 

In [187]: df["quartiles"] = pd.qcut(
   .....:     df["stratifying_var"], 4, labels=["0-25%", "25-50%", "50-75%", "75-100%"]
   .....: )
   .....: 

In [188]: df.boxplot(column="price", by="quartiles")
Out[188]: <AxesSubplot:title={'center':'price'}, xlabel='quartiles'>
     Data in/out Performance comparison of SQL vs HDF5  CSV The CSV docs read_csv in action appending to a csv Reading a csv chunk-by-chunk Reading only certain rows of a csv chunk-by-chunk Reading the first few lines of a frame Reading a file that is compressed but not by gzip/bz2 (the native compressed formats which read_csv understands). This example shows a WinZipped file, but is a general application of opening the file within a context manager and using that handle to read. See here Inferring dtypes from a file Dealing with bad lines GH2886 Write a multi-row index CSV without writing duplicates  Reading multiple files to create a single DataFrame The best way to combine multiple files into a single DataFrame is to read the individual frames one by one, put all of the individual frames into a list, and then combine the frames in the list using pd.concat(): 
In [189]: for i in range(3):
   .....:     data = pd.DataFrame(np.random.randn(10, 4))
   .....:     data.to_csv("file_{}.csv".format(i))
   .....: 

In [190]: files = ["file_0.csv", "file_1.csv", "file_2.csv"]

In [191]: result = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)
  You can use the same approach to read all files matching a pattern. Here is an example using glob: 
In [192]: import glob

In [193]: import os

In [194]: files = glob.glob("file_*.csv")

In [195]: result = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)
  Finally, this strategy will work with the other pd.read_*(...) functions described in the io docs.   Parsing date components in multi-columns Parsing date components in multi-columns is faster with a format 
In [196]: i = pd.date_range("20000101", periods=10000)

In [197]: df = pd.DataFrame({"year": i.year, "month": i.month, "day": i.day})

In [198]: df.head()
Out[198]: 
   year  month  day
0  2000      1    1
1  2000      1    2
2  2000      1    3
3  2000      1    4
4  2000      1    5

In [199]: %timeit pd.to_datetime(df.year * 10000 + df.month * 100 + df.day, format='%Y%m%d')
   .....: ds = df.apply(lambda x: "%04d%02d%02d" % (x["year"], x["month"], x["day"]), axis=1)
   .....: ds.head()
   .....: %timeit pd.to_datetime(ds)
   .....: 
8.7 ms +- 765 us per loop (mean +- std. dev. of 7 runs, 100 loops each)
2.1 ms +- 419 us per loop (mean +- std. dev. of 7 runs, 100 loops each)
    Skip row between header and data 
In [200]: data = """;;;;
   .....:  ;;;;
   .....:  ;;;;
   .....:  ;;;;
   .....:  ;;;;
   .....:  ;;;;
   .....: ;;;;
   .....:  ;;;;
   .....:  ;;;;
   .....: ;;;;
   .....: date;Param1;Param2;Param4;Param5
   .....:     ;m²;°C;m²;m
   .....: ;;;;
   .....: 01.01.1990 00:00;1;1;2;3
   .....: 01.01.1990 01:00;5;3;4;5
   .....: 01.01.1990 02:00;9;5;6;7
   .....: 01.01.1990 03:00;13;7;8;9
   .....: 01.01.1990 04:00;17;9;10;11
   .....: 01.01.1990 05:00;21;11;12;13
   .....: """
   .....: 
   Option 1: pass rows explicitly to skip rows 
In [201]: from io import StringIO

In [202]: pd.read_csv(
   .....:     StringIO(data),
   .....:     sep=";",
   .....:     skiprows=[11, 12],
   .....:     index_col=0,
   .....:     parse_dates=True,
   .....:     header=10,
   .....: )
   .....: 
Out[202]: 
                     Param1  Param2  Param4  Param5
date                                               
1990-01-01 00:00:00       1       1       2       3
1990-01-01 01:00:00       5       3       4       5
1990-01-01 02:00:00       9       5       6       7
1990-01-01 03:00:00      13       7       8       9
1990-01-01 04:00:00      17       9      10      11
1990-01-01 05:00:00      21      11      12      13
    Option 2: read column names and then data 
In [203]: pd.read_csv(StringIO(data), sep=";", header=10, nrows=10).columns
Out[203]: Index(['date', 'Param1', 'Param2', 'Param4', 'Param5'], dtype='object')

In [204]: columns = pd.read_csv(StringIO(data), sep=";", header=10, nrows=10).columns

In [205]: pd.read_csv(
   .....:     StringIO(data), sep=";", index_col=0, header=12, parse_dates=True, names=columns
   .....: )
   .....: 
Out[205]: 
                     Param1  Param2  Param4  Param5
date                                               
1990-01-01 00:00:00       1       1       2       3
1990-01-01 01:00:00       5       3       4       5
1990-01-01 02:00:00       9       5       6       7
1990-01-01 03:00:00      13       7       8       9
1990-01-01 04:00:00      17       9      10      11
1990-01-01 05:00:00      21      11      12      13
      SQL The SQL docs Reading from databases with SQL   Excel The Excel docs Reading from a filelike handle Modifying formatting in XlsxWriter output Loading only visible sheets GH19842#issuecomment-892150745   HTML Reading HTML tables from a server that cannot handle the default request header   HDFStore The HDFStores docs Simple queries with a Timestamp Index Managing heterogeneous data using a linked multiple table hierarchy GH3032 Merging on-disk tables with millions of rows Avoiding inconsistencies when writing to a store from multiple processes/threads De-duplicating a large store by chunks, essentially a recursive reduction operation. Shows a function for taking in data from csv file and creating a store by chunks, with date parsing as well. See here Creating a store chunk-by-chunk from a csv file Appending to a store, while creating a unique index Large Data work flows Reading in a sequence of files, then providing a global unique index to a store while appending Groupby on a HDFStore with low group density Groupby on a HDFStore with high group density Hierarchical queries on a HDFStore Counting with a HDFStore Troubleshoot HDFStore exceptions Setting min_itemsize with strings Using ptrepack to create a completely-sorted-index on a store Storing Attributes to a group node 
In [206]: df = pd.DataFrame(np.random.randn(8, 3))

In [207]: store = pd.HDFStore("test.h5")

In [208]: store.put("df", df)

# you can store an arbitrary Python object via pickle
In [209]: store.get_storer("df").attrs.my_attribute = {"A": 10}

In [210]: store.get_storer("df").attrs.my_attribute
Out[210]: {'A': 10}
  You can create or load a HDFStore in-memory by passing the driver parameter to PyTables. Changes are only written to disk when the HDFStore is closed. 
In [211]: store = pd.HDFStore("test.h5", "w", driver="H5FD_CORE")

In [212]: df = pd.DataFrame(np.random.randn(8, 3))

In [213]: store["test"] = df

# only after closing the store, data is written to disk:
In [214]: store.close()
    Binary files pandas readily accepts NumPy record arrays, if you need to read in a binary file consisting of an array of C structs. For example, given this C program in a file called main.c compiled with gcc main.c -std=gnu99 on a 64-bit machine, 
#include <stdio.h>
#include <stdint.h>

typedef struct _Data
{
    int32_t count;
    double avg;
    float scale;
} Data;

int main(int argc, const char *argv[])
{
    size_t n = 10;
    Data d[n];

    for (int i = 0; i < n; ++i)
    {
        d[i].count = i;
        d[i].avg = i + 1.0;
        d[i].scale = (float) i + 2.0f;
    }

    FILE *file = fopen("binary.dat", "wb");
    fwrite(&d, sizeof(Data), n, file);
    fclose(file);

    return 0;
}
  the following Python code will read the binary file 'binary.dat' into a pandas DataFrame, where each element of the struct corresponds to a column in the frame: 
names = "count", "avg", "scale"

# note that the offsets are larger than the size of the type because of
# struct padding
offsets = 0, 8, 16
formats = "i4", "f8", "f4"
dt = np.dtype({"names": names, "offsets": offsets, "formats": formats}, align=True)
df = pd.DataFrame(np.fromfile("binary.dat", dt))
   Note The offsets of the structure elements may be different depending on the architecture of the machine on which the file was created. Using a raw binary file format like this for general data storage is not recommended, as it is not cross platform. We recommended either HDF5 or parquet, both of which are supported by pandas’ IO facilities.     Computation Numerical integration (sample-based) of a time series  Correlation Often it’s useful to obtain the lower (or upper) triangular form of a correlation matrix calculated from DataFrame.corr(). This can be achieved by passing a boolean mask to where as follows: 
In [215]: df = pd.DataFrame(np.random.random(size=(100, 5)))

In [216]: corr_mat = df.corr()

In [217]: mask = np.tril(np.ones_like(corr_mat, dtype=np.bool_), k=-1)

In [218]: corr_mat.where(mask)
Out[218]: 
          0         1         2        3   4
0       NaN       NaN       NaN      NaN NaN
1 -0.079861       NaN       NaN      NaN NaN
2 -0.236573  0.183801       NaN      NaN NaN
3 -0.013795 -0.051975  0.037235      NaN NaN
4 -0.031974  0.118342 -0.073499 -0.02063 NaN
  The method argument within DataFrame.corr can accept a callable in addition to the named correlation types. Here we compute the distance correlation matrix for a DataFrame object. 
In [219]: def distcorr(x, y):
   .....:     n = len(x)
   .....:     a = np.zeros(shape=(n, n))
   .....:     b = np.zeros(shape=(n, n))
   .....:     for i in range(n):
   .....:         for j in range(i + 1, n):
   .....:             a[i, j] = abs(x[i] - x[j])
   .....:             b[i, j] = abs(y[i] - y[j])
   .....:     a += a.T
   .....:     b += b.T
   .....:     a_bar = np.vstack([np.nanmean(a, axis=0)] * n)
   .....:     b_bar = np.vstack([np.nanmean(b, axis=0)] * n)
   .....:     A = a - a_bar - a_bar.T + np.full(shape=(n, n), fill_value=a_bar.mean())
   .....:     B = b - b_bar - b_bar.T + np.full(shape=(n, n), fill_value=b_bar.mean())
   .....:     cov_ab = np.sqrt(np.nansum(A * B)) / n
   .....:     std_a = np.sqrt(np.sqrt(np.nansum(A ** 2)) / n)
   .....:     std_b = np.sqrt(np.sqrt(np.nansum(B ** 2)) / n)
   .....:     return cov_ab / std_a / std_b
   .....: 

In [220]: df = pd.DataFrame(np.random.normal(size=(100, 3)))

In [221]: df.corr(method=distcorr)
Out[221]: 
          0         1         2
0  1.000000  0.197613  0.216328
1  0.197613  1.000000  0.208749
2  0.216328  0.208749  1.000000
     Timedeltas The Timedeltas docs. Using timedeltas 
In [222]: import datetime

In [223]: s = pd.Series(pd.date_range("2012-1-1", periods=3, freq="D"))

In [224]: s - s.max()
Out[224]: 
0   -2 days
1   -1 days
2    0 days
dtype: timedelta64[ns]

In [225]: s.max() - s
Out[225]: 
0   2 days
1   1 days
2   0 days
dtype: timedelta64[ns]

In [226]: s - datetime.datetime(2011, 1, 1, 3, 5)
Out[226]: 
0   364 days 20:55:00
1   365 days 20:55:00
2   366 days 20:55:00
dtype: timedelta64[ns]

In [227]: s + datetime.timedelta(minutes=5)
Out[227]: 
0   2012-01-01 00:05:00
1   2012-01-02 00:05:00
2   2012-01-03 00:05:00
dtype: datetime64[ns]

In [228]: datetime.datetime(2011, 1, 1, 3, 5) - s
Out[228]: 
0   -365 days +03:05:00
1   -366 days +03:05:00
2   -367 days +03:05:00
dtype: timedelta64[ns]

In [229]: datetime.timedelta(minutes=5) + s
Out[229]: 
0   2012-01-01 00:05:00
1   2012-01-02 00:05:00
2   2012-01-03 00:05:00
dtype: datetime64[ns]
  Adding and subtracting deltas and dates 
In [230]: deltas = pd.Series([datetime.timedelta(days=i) for i in range(3)])

In [231]: df = pd.DataFrame({"A": s, "B": deltas})

In [232]: df
Out[232]: 
           A      B
0 2012-01-01 0 days
1 2012-01-02 1 days
2 2012-01-03 2 days

In [233]: df["New Dates"] = df["A"] + df["B"]

In [234]: df["Delta"] = df["A"] - df["New Dates"]

In [235]: df
Out[235]: 
           A      B  New Dates   Delta
0 2012-01-01 0 days 2012-01-01  0 days
1 2012-01-02 1 days 2012-01-03 -1 days
2 2012-01-03 2 days 2012-01-05 -2 days

In [236]: df.dtypes
Out[236]: 
A             datetime64[ns]
B            timedelta64[ns]
New Dates     datetime64[ns]
Delta        timedelta64[ns]
dtype: object
  Another example Values can be set to NaT using np.nan, similar to datetime 
In [237]: y = s - s.shift()

In [238]: y
Out[238]: 
0      NaT
1   1 days
2   1 days
dtype: timedelta64[ns]

In [239]: y[1] = np.nan

In [240]: y
Out[240]: 
0      NaT
1      NaT
2   1 days
dtype: timedelta64[ns]
    Creating example data To create a dataframe from every combination of some given values, like R’s expand.grid() function, we can create a dict where the keys are column names and the values are lists of the data values: 
In [241]: def expand_grid(data_dict):
   .....:     rows = itertools.product(*data_dict.values())
   .....:     return pd.DataFrame.from_records(rows, columns=data_dict.keys())
   .....: 

In [242]: df = expand_grid(
   .....:     {"height": [60, 70], "weight": [100, 140, 180], "sex": ["Male", "Female"]}
   .....: )
   .....: 

In [243]: df
Out[243]: 
    height  weight     sex
0       60     100    Male
1       60     100  Female
2       60     140    Male
3       60     140  Female
4       60     180    Male
5       60     180  Female
6       70     100    Male
7       70     100  Female
8       70     140    Male
9       70     140  Female
10      70     180    Male
11      70     180  Female
pandas.Series.combine_first   Series.combine_first(other)[source]
 
Update null elements with value in the same location in ‘other’. Combine two Series objects by filling null values in one Series with non-null values from the other Series. Result index will be the union of the two indexes.  Parameters 
 
other:Series


The value(s) to be used for filling null values.    Returns 
 Series

The result of combining the provided Series with the other object.      See also  Series.combine

Perform element-wise operation on two Series using a given function.    Examples 
>>> s1 = pd.Series([1, np.nan])
>>> s2 = pd.Series([3, 4, 5])
>>> s1.combine_first(s2)
0    1.0
1    4.0
2    5.0
dtype: float64
  Null values still persist if the location of that null value does not exist in other 
>>> s1 = pd.Series({'falcon': np.nan, 'eagle': 160.0})
>>> s2 = pd.Series({'eagle': 200.0, 'duck': 30.0})
>>> s1.combine_first(s2)
duck       30.0
eagle     160.0
falcon      NaN
dtype: float64
pandas.wide_to_long   pandas.wide_to_long(df, stubnames, i, j, sep='', suffix='\\d+')[source]
 
Unpivot a DataFrame from wide to long format. Less flexible but more user-friendly than melt. With stubnames [‘A’, ‘B’], this function expects to find one or more group of columns with format A-suffix1, A-suffix2,…, B-suffix1, B-suffix2,… You specify what you want to call this suffix in the resulting long format with j (for example j=’year’) Each row of these wide variables are assumed to be uniquely identified by i (can be a single column name or a list of column names) All remaining variables in the data frame are left intact.  Parameters 
 
df:DataFrame


The wide-format DataFrame.  
stubnames:str or list-like


The stub name(s). The wide format variables are assumed to start with the stub names.  
i:str or list-like


Column(s) to use as id variable(s).  
j:str


The name of the sub-observation variable. What you wish to name your suffix in the long format.  
sep:str, default “”


A character indicating the separation of the variable names in the wide format, to be stripped from the names in the long format. For example, if your column names are A-suffix1, A-suffix2, you can strip the hyphen by specifying sep=’-’.  
suffix:str, default ‘\d+’


A regular expression capturing the wanted suffixes. ‘\d+’ captures numeric suffixes. Suffixes with no numbers could be specified with the negated character class ‘\D+’. You can also further disambiguate suffixes, for example, if your wide variables are of the form A-one, B-two,.., and you have an unrelated column A-rating, you can ignore the last one by specifying suffix=’(!?one|two)’. When all suffixes are numeric, they are cast to int64/float64.    Returns 
 DataFrame

A DataFrame that contains each stub name as a variable, with new index (i, j).      See also  melt

Unpivot a DataFrame from wide to long format, optionally leaving identifiers set.  pivot

Create a spreadsheet-style pivot table as a DataFrame.  DataFrame.pivot

Pivot without aggregation that can handle non-numeric data.  DataFrame.pivot_table

Generalization of pivot that can handle duplicate values for one index/column pair.  DataFrame.unstack

Pivot based on the index values instead of a column.    Notes All extra variables are left untouched. This simply uses pandas.melt under the hood, but is hard-coded to “do the right thing” in a typical case. Examples 
>>> np.random.seed(123)
>>> df = pd.DataFrame({"A1970" : {0 : "a", 1 : "b", 2 : "c"},
...                    "A1980" : {0 : "d", 1 : "e", 2 : "f"},
...                    "B1970" : {0 : 2.5, 1 : 1.2, 2 : .7},
...                    "B1980" : {0 : 3.2, 1 : 1.3, 2 : .1},
...                    "X"     : dict(zip(range(3), np.random.randn(3)))
...                   })
>>> df["id"] = df.index
>>> df
  A1970 A1980  B1970  B1980         X  id
0     a     d    2.5    3.2 -1.085631   0
1     b     e    1.2    1.3  0.997345   1
2     c     f    0.7    0.1  0.282978   2
>>> pd.wide_to_long(df, ["A", "B"], i="id", j="year")
... 
                X  A    B
id year
0  1970 -1.085631  a  2.5
1  1970  0.997345  b  1.2
2  1970  0.282978  c  0.7
0  1980 -1.085631  d  3.2
1  1980  0.997345  e  1.3
2  1980  0.282978  f  0.1
  With multiple id columns 
>>> df = pd.DataFrame({
...     'famid': [1, 1, 1, 2, 2, 2, 3, 3, 3],
...     'birth': [1, 2, 3, 1, 2, 3, 1, 2, 3],
...     'ht1': [2.8, 2.9, 2.2, 2, 1.8, 1.9, 2.2, 2.3, 2.1],
...     'ht2': [3.4, 3.8, 2.9, 3.2, 2.8, 2.4, 3.3, 3.4, 2.9]
... })
>>> df
   famid  birth  ht1  ht2
0      1      1  2.8  3.4
1      1      2  2.9  3.8
2      1      3  2.2  2.9
3      2      1  2.0  3.2
4      2      2  1.8  2.8
5      2      3  1.9  2.4
6      3      1  2.2  3.3
7      3      2  2.3  3.4
8      3      3  2.1  2.9
>>> l = pd.wide_to_long(df, stubnames='ht', i=['famid', 'birth'], j='age')
>>> l
... 
                  ht
famid birth age
1     1     1    2.8
            2    3.4
      2     1    2.9
            2    3.8
      3     1    2.2
            2    2.9
2     1     1    2.0
            2    3.2
      2     1    1.8
            2    2.8
      3     1    1.9
            2    2.4
3     1     1    2.2
            2    3.3
      2     1    2.3
            2    3.4
      3     1    2.1
            2    2.9
  Going from long back to wide just takes some creative use of unstack 
>>> w = l.unstack()
>>> w.columns = w.columns.map('{0[0]}{0[1]}'.format)
>>> w.reset_index()
   famid  birth  ht1  ht2
0      1      1  2.8  3.4
1      1      2  2.9  3.8
2      1      3  2.2  2.9
3      2      1  2.0  3.2
4      2      2  1.8  2.8
5      2      3  1.9  2.4
6      3      1  2.2  3.3
7      3      2  2.3  3.4
8      3      3  2.1  2.9
  Less wieldy column names are also handled 
>>> np.random.seed(0)
>>> df = pd.DataFrame({'A(weekly)-2010': np.random.rand(3),
...                    'A(weekly)-2011': np.random.rand(3),
...                    'B(weekly)-2010': np.random.rand(3),
...                    'B(weekly)-2011': np.random.rand(3),
...                    'X' : np.random.randint(3, size=3)})
>>> df['id'] = df.index
>>> df 
   A(weekly)-2010  A(weekly)-2011  B(weekly)-2010  B(weekly)-2011  X  id
0        0.548814        0.544883        0.437587        0.383442  0   0
1        0.715189        0.423655        0.891773        0.791725  1   1
2        0.602763        0.645894        0.963663        0.528895  1   2
  
>>> pd.wide_to_long(df, ['A(weekly)', 'B(weekly)'], i='id',
...                 j='year', sep='-')
... 
         X  A(weekly)  B(weekly)
id year
0  2010  0   0.548814   0.437587
1  2010  1   0.715189   0.891773
2  2010  1   0.602763   0.963663
0  2011  0   0.544883   0.383442
1  2011  1   0.423655   0.791725
2  2011  1   0.645894   0.528895
  If we have many columns, we could also use a regex to find our stubnames and pass that list on to wide_to_long 
>>> stubnames = sorted(
...     set([match[0] for match in df.columns.str.findall(
...         r'[A-B]\(.*\)').values if match != []])
... )
>>> list(stubnames)
['A(weekly)', 'B(weekly)']
  All of the above examples have integers as suffixes. It is possible to have non-integers as suffixes. 
>>> df = pd.DataFrame({
...     'famid': [1, 1, 1, 2, 2, 2, 3, 3, 3],
...     'birth': [1, 2, 3, 1, 2, 3, 1, 2, 3],
...     'ht_one': [2.8, 2.9, 2.2, 2, 1.8, 1.9, 2.2, 2.3, 2.1],
...     'ht_two': [3.4, 3.8, 2.9, 3.2, 2.8, 2.4, 3.3, 3.4, 2.9]
... })
>>> df
   famid  birth  ht_one  ht_two
0      1      1     2.8     3.4
1      1      2     2.9     3.8
2      1      3     2.2     2.9
3      2      1     2.0     3.2
4      2      2     1.8     2.8
5      2      3     1.9     2.4
6      3      1     2.2     3.3
7      3      2     2.3     3.4
8      3      3     2.1     2.9
  
>>> l = pd.wide_to_long(df, stubnames='ht', i=['famid', 'birth'], j='age',
...                     sep='_', suffix=r'\w+')
>>> l
... 
                  ht
famid birth age
1     1     one  2.8
            two  3.4
      2     one  2.9
            two  3.8
      3     one  2.2
            two  2.9
2     1     one  2.0
            two  3.2
      2     one  1.8
            two  2.8
      3     one  1.9
            two  2.4
3     1     one  2.2
            two  3.3
      2     one  2.3
            two  3.4
      3     one  2.1
            two  2.9
def f_38152389(df):
    """combine values from column 'b' and column 'a' of dataframe `df`  into column 'c' of datafram `df`
    """
     
 --------------------

def f_4175686(d):
    """remove key 'ele' from dictionary `d`
    """
     
 --------------------

def f_11574195():
    """merge list `['it']` and list `['was']` and list `['annoying']` into one list
    """
    return  
 --------------------

def f_587647(x):
    """increment a value with leading zeroes in a number `x`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
pandas.Index.is_monotonic_increasing   propertyIndex.is_monotonic_increasing
 
Return if the index is monotonic increasing (only equal or increasing) values. Examples 
>>> Index([1, 2, 3]).is_monotonic_increasing
True
>>> Index([1, 2, 2]).is_monotonic_increasing
True
>>> Index([1, 3, 2]).is_monotonic_increasing
False
pandas.Index.is_monotonic_decreasing   propertyIndex.is_monotonic_decreasing
 
Return if the index is monotonic decreasing (only equal or decreasing) values. Examples 
>>> Index([3, 2, 1]).is_monotonic_decreasing
True
>>> Index([3, 2, 2]).is_monotonic_decreasing
True
>>> Index([3, 1, 2]).is_monotonic_decreasing
False
pandas.Index.is_unique   Index.is_unique
 
Return if the index has unique values.
pandas.Index.is_monotonic   propertyIndex.is_monotonic
 
Alias for is_monotonic_increasing.
pandas.Series.is_monotonic_increasing   propertySeries.is_monotonic_increasing
 
Alias for is_monotonic.
def f_17315881(df):
    """check if a pandas dataframe `df`'s index is sorted
    """
    return  
 --------------------

def f_16296643(t):
    """Convert tuple `t` to list
    """
    return  
 --------------------

def f_16296643(t):
    """Convert list `t` to tuple
    """
    return  
 --------------------

def f_16296643(level1):
    """Convert tuple `level1` to list
    """
     
 --------------------
Please refer to the following documentation to generate the code:
PrettyPrinter.pprint(object)  
Print the formatted representation of object on the configured stream, followed by a newline.
trigger(*args, **kwargs)[source]
 
Called when this tool gets used. This method is called by ToolManager.trigger_tool.  Parameters 
 
eventEvent


The canvas event that caused this tool to be called.  
senderobject


Object that requested the tool to be triggered.  
dataobject


Extra data.
use_xobject=b'Do'[source]
numpy.ma.masked_object   ma.masked_object(x, value, copy=True, shrink=True)[source]
 
Mask the array x where the data are exactly equal to value. This function is similar to masked_values, but only suitable for object arrays: for floating point, use masked_values instead.  Parameters 
 
xarray_like


Array to mask  
valueobject


Comparison value  
copy{True, False}, optional


Whether to return a copy of x.  
shrink{True, False}, optional


Whether to collapse a mask full of False to nomask    Returns 
 
resultMaskedArray


The result of masking x where equal to value.      See also  masked_where

Mask where a condition is met.  masked_equal

Mask where equal to a given value (integers).  masked_values

Mask using floating point equality.    Examples >>> import numpy.ma as ma
>>> food = np.array(['green_eggs', 'ham'], dtype=object)
>>> # don't eat spoiled food
>>> eat = ma.masked_object(food, 'green_eggs')
>>> eat
masked_array(data=[--, 'ham'],
             mask=[ True, False],
       fill_value='green_eggs',
            dtype=object)
>>> # plain ol` ham is boring
>>> fresh_food = np.array(['cheese', 'ham', 'pineapple'], dtype=object)
>>> eat = ma.masked_object(fresh_food, 'green_eggs')
>>> eat
masked_array(data=['cheese', 'ham', 'pineapple'],
             mask=False,
       fill_value='green_eggs',
            dtype=object)
 Note that mask is set to nomask if possible. >>> eat
masked_array(data=['cheese', 'ham', 'pineapple'],
             mask=False,
       fill_value='green_eggs',
            dtype=object)
trigger(sender, event, data=None)[source]
 
Called when this tool gets used. This method is called by ToolManager.trigger_tool.  Parameters 
 
eventEvent


The canvas event that caused this tool to be called.  
senderobject


Object that requested the tool to be triggered.  
dataobject


Extra data.
def f_3880399(dataobject, logFile):
    """send the output of pprint object `dataobject` to file `logFile`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
pandas.BooleanDtype   classpandas.BooleanDtype[source]
 
Extension dtype for boolean data.  New in version 1.0.0.   Warning BooleanDtype is considered experimental. The implementation and parts of the API may change without warning.  Examples 
>>> pd.BooleanDtype()
BooleanDtype
  Attributes       
None     Methods       
None
numpy.matrix.nonzero method   matrix.nonzero()
 
Return the indices of the elements that are non-zero. Refer to numpy.nonzero for full documentation.  See also  numpy.nonzero

equivalent function
numpy.recarray.nonzero method   recarray.nonzero()
 
Return the indices of the elements that are non-zero. Refer to numpy.nonzero for full documentation.  See also  numpy.nonzero

equivalent function
numpy.ndarray.nonzero method   ndarray.nonzero()
 
Return the indices of the elements that are non-zero. Refer to numpy.nonzero for full documentation.  See also  numpy.nonzero

equivalent function
numpy.ma.nonzero   ma.nonzero(self) = <numpy.ma.core._frommethod object>
 
Return the indices of unmasked elements that are not zero. Returns a tuple of arrays, one for each dimension, containing the indices of the non-zero elements in that dimension. The corresponding non-zero values can be obtained with: a[a.nonzero()]
 To group the indices by element, rather than dimension, use instead: np.transpose(a.nonzero())
 The result of this is always a 2d array, with a row for each non-zero element.  Parameters 
 None
  Returns 
 
tuple_of_arraystuple


Indices of elements that are non-zero.      See also  numpy.nonzero

Function operating on ndarrays.  flatnonzero

Return indices that are non-zero in the flattened version of the input array.  numpy.ndarray.nonzero

Equivalent ndarray method.  count_nonzero

Counts the number of non-zero elements in the input array.    Examples >>> import numpy.ma as ma
>>> x = ma.array(np.eye(3))
>>> x
masked_array(
  data=[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]],
  mask=False,
  fill_value=1e+20)
>>> x.nonzero()
(array([0, 1, 2]), array([0, 1, 2]))
 Masked elements are ignored. >>> x[1, 1] = ma.masked
>>> x
masked_array(
  data=[[1.0, 0.0, 0.0],
        [0.0, --, 0.0],
        [0.0, 0.0, 1.0]],
  mask=[[False, False, False],
        [False,  True, False],
        [False, False, False]],
  fill_value=1e+20)
>>> x.nonzero()
(array([0, 2]), array([0, 2]))
 Indices can also be grouped by element. >>> np.transpose(x.nonzero())
array([[0, 0],
       [2, 2]])
 A common use for nonzero is to find the indices of an array, where a condition is True. Given an array a, the condition a > 3 is a boolean array and since False is interpreted as 0, ma.nonzero(a > 3) yields the indices of the a where the condition is true. >>> a = ma.array([[1,2,3],[4,5,6],[7,8,9]])
>>> a > 3
masked_array(
  data=[[False, False, False],
        [ True,  True,  True],
        [ True,  True,  True]],
  mask=False,
  fill_value=True)
>>> ma.nonzero(a > 3)
(array([1, 1, 1, 2, 2, 2]), array([0, 1, 2, 0, 1, 2]))
 The nonzero method of the condition array can also be called. >>> (a > 3).nonzero()
(array([1, 1, 1, 2, 2, 2]), array([0, 1, 2, 0, 1, 2]))
def f_21800169(df):
    """get index of rows in column 'BoolCol'
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
pandas.Index.asof_locs   Index.asof_locs(where, mask)[source]
 
Return the locations (indices) of labels in the index. As in the asof function, if the label (a particular entry in where) is not in the index, the latest index label up to the passed label is chosen and its index returned. If all of the labels in the index are later than a label in where, -1 is returned. mask is used to ignore NA values in the index during calculation.  Parameters 
 
where:Index


An Index consisting of an array of timestamps.  
mask:np.ndarray[bool]


Array of booleans denoting where values in the original data are not NA.    Returns 
 np.ndarray[np.intp]

An array of locations (indices) of the labels from the Index which correspond to the return values of the asof function for every element in where.
pandas.DataFrame.lookup   DataFrame.lookup(row_labels, col_labels)[source]
 
Label-based “fancy indexing” function for DataFrame. Given equal-length arrays of row and column labels, return an array of the values corresponding to each (row, col) pair.  Deprecated since version 1.2.0: DataFrame.lookup is deprecated, use DataFrame.melt and DataFrame.loc instead. For further details see Looking up values by index/column labels.   Parameters 
 
row_labels:sequence


The row labels to use for lookup.  
col_labels:sequence


The column labels to use for lookup.    Returns 
 numpy.ndarray

The found values.
numpy.matrix.nonzero method   matrix.nonzero()
 
Return the indices of the elements that are non-zero. Refer to numpy.nonzero for full documentation.  See also  numpy.nonzero

equivalent function
numpy.ma.nonzero   ma.nonzero(self) = <numpy.ma.core._frommethod object>
 
Return the indices of unmasked elements that are not zero. Returns a tuple of arrays, one for each dimension, containing the indices of the non-zero elements in that dimension. The corresponding non-zero values can be obtained with: a[a.nonzero()]
 To group the indices by element, rather than dimension, use instead: np.transpose(a.nonzero())
 The result of this is always a 2d array, with a row for each non-zero element.  Parameters 
 None
  Returns 
 
tuple_of_arraystuple


Indices of elements that are non-zero.      See also  numpy.nonzero

Function operating on ndarrays.  flatnonzero

Return indices that are non-zero in the flattened version of the input array.  numpy.ndarray.nonzero

Equivalent ndarray method.  count_nonzero

Counts the number of non-zero elements in the input array.    Examples >>> import numpy.ma as ma
>>> x = ma.array(np.eye(3))
>>> x
masked_array(
  data=[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]],
  mask=False,
  fill_value=1e+20)
>>> x.nonzero()
(array([0, 1, 2]), array([0, 1, 2]))
 Masked elements are ignored. >>> x[1, 1] = ma.masked
>>> x
masked_array(
  data=[[1.0, 0.0, 0.0],
        [0.0, --, 0.0],
        [0.0, 0.0, 1.0]],
  mask=[[False, False, False],
        [False,  True, False],
        [False, False, False]],
  fill_value=1e+20)
>>> x.nonzero()
(array([0, 2]), array([0, 2]))
 Indices can also be grouped by element. >>> np.transpose(x.nonzero())
array([[0, 0],
       [2, 2]])
 A common use for nonzero is to find the indices of an array, where a condition is True. Given an array a, the condition a > 3 is a boolean array and since False is interpreted as 0, ma.nonzero(a > 3) yields the indices of the a where the condition is true. >>> a = ma.array([[1,2,3],[4,5,6],[7,8,9]])
>>> a > 3
masked_array(
  data=[[False, False, False],
        [ True,  True,  True],
        [ True,  True,  True]],
  mask=False,
  fill_value=True)
>>> ma.nonzero(a > 3)
(array([1, 1, 1, 2, 2, 2]), array([0, 1, 2, 0, 1, 2]))
 The nonzero method of the condition array can also be called. >>> (a > 3).nonzero()
(array([1, 1, 1, 2, 2, 2]), array([0, 1, 2, 0, 1, 2]))
pandas.DataFrame.bool   DataFrame.bool()[source]
 
Return the bool of a single element Series or DataFrame. This must be a boolean scalar value, either True or False. It will raise a ValueError if the Series or DataFrame does not have exactly 1 element, or that element is not boolean (integer values 0 and 1 will also raise an exception).  Returns 
 bool

The value in the Series or DataFrame.      See also  Series.astype

Change the data type of a Series, including to boolean.  DataFrame.astype

Change the data type of a DataFrame, including to boolean.  numpy.bool_

NumPy boolean data type, used by pandas for boolean values.    Examples The method will only work for single element objects with a boolean value: 
>>> pd.Series([True]).bool()
True
>>> pd.Series([False]).bool()
False
  
>>> pd.DataFrame({'col': [True]}).bool()
True
>>> pd.DataFrame({'col': [False]}).bool()
False
def f_21800169(df):
    """Create a list containing the indexes of rows where the value of column 'BoolCol' in dataframe `df` are equal to True
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
pandas.DataFrame.lookup   DataFrame.lookup(row_labels, col_labels)[source]
 
Label-based “fancy indexing” function for DataFrame. Given equal-length arrays of row and column labels, return an array of the values corresponding to each (row, col) pair.  Deprecated since version 1.2.0: DataFrame.lookup is deprecated, use DataFrame.melt and DataFrame.loc instead. For further details see Looking up values by index/column labels.   Parameters 
 
row_labels:sequence


The row labels to use for lookup.  
col_labels:sequence


The column labels to use for lookup.    Returns 
 numpy.ndarray

The found values.
pandas.Index.asof_locs   Index.asof_locs(where, mask)[source]
 
Return the locations (indices) of labels in the index. As in the asof function, if the label (a particular entry in where) is not in the index, the latest index label up to the passed label is chosen and its index returned. If all of the labels in the index are later than a label in where, -1 is returned. mask is used to ignore NA values in the index during calculation.  Parameters 
 
where:Index


An Index consisting of an array of timestamps.  
mask:np.ndarray[bool]


Array of booleans denoting where values in the original data are not NA.    Returns 
 np.ndarray[np.intp]

An array of locations (indices) of the labels from the Index which correspond to the return values of the asof function for every element in where.
Cookbook This is a repository for short and sweet examples and links for useful pandas recipes. We encourage users to add to this documentation. Adding interesting links and/or inline examples to this section is a great First Pull Request. Simplified, condensed, new-user friendly, in-line examples have been inserted where possible to augment the Stack-Overflow and GitHub links. Many of the links contain expanded information, above what the in-line examples offer. pandas (pd) and NumPy (np) are the only two abbreviated imported modules. The rest are kept explicitly imported for newer users.  Idioms These are some neat pandas idioms if-then/if-then-else on one column, and assignment to another one or more columns: 
In [1]: df = pd.DataFrame(
   ...:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ...: )
   ...: 

In [2]: df
Out[2]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50
   if-then… An if-then on one column 
In [3]: df.loc[df.AAA >= 5, "BBB"] = -1

In [4]: df
Out[4]: 
   AAA  BBB  CCC
0    4   10  100
1    5   -1   50
2    6   -1  -30
3    7   -1  -50
  An if-then with assignment to 2 columns: 
In [5]: df.loc[df.AAA >= 5, ["BBB", "CCC"]] = 555

In [6]: df
Out[6]: 
   AAA  BBB  CCC
0    4   10  100
1    5  555  555
2    6  555  555
3    7  555  555
  Add another line with different logic, to do the -else 
In [7]: df.loc[df.AAA < 5, ["BBB", "CCC"]] = 2000

In [8]: df
Out[8]: 
   AAA   BBB   CCC
0    4  2000  2000
1    5   555   555
2    6   555   555
3    7   555   555
  Or use pandas where after you’ve set up a mask 
In [9]: df_mask = pd.DataFrame(
   ...:     {"AAA": [True] * 4, "BBB": [False] * 4, "CCC": [True, False] * 2}
   ...: )
   ...: 

In [10]: df.where(df_mask, -1000)
Out[10]: 
   AAA   BBB   CCC
0    4 -1000  2000
1    5 -1000 -1000
2    6 -1000   555
3    7 -1000 -1000
  if-then-else using NumPy’s where() 
In [11]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ....: )
   ....: 

In [12]: df
Out[12]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50

In [13]: df["logic"] = np.where(df["AAA"] > 5, "high", "low")

In [14]: df
Out[14]: 
   AAA  BBB  CCC logic
0    4   10  100   low
1    5   20   50   low
2    6   30  -30  high
3    7   40  -50  high
    Splitting Split a frame with a boolean criterion 
In [15]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ....: )
   ....: 

In [16]: df
Out[16]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50

In [17]: df[df.AAA <= 5]
Out[17]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50

In [18]: df[df.AAA > 5]
Out[18]: 
   AAA  BBB  CCC
2    6   30  -30
3    7   40  -50
    Building criteria Select with multi-column criteria 
In [19]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ....: )
   ....: 

In [20]: df
Out[20]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50
  …and (without assignment returns a Series) 
In [21]: df.loc[(df["BBB"] < 25) & (df["CCC"] >= -40), "AAA"]
Out[21]: 
0    4
1    5
Name: AAA, dtype: int64
  …or (without assignment returns a Series) 
In [22]: df.loc[(df["BBB"] > 25) | (df["CCC"] >= -40), "AAA"]
Out[22]: 
0    4
1    5
2    6
3    7
Name: AAA, dtype: int64
  …or (with assignment modifies the DataFrame.) 
In [23]: df.loc[(df["BBB"] > 25) | (df["CCC"] >= 75), "AAA"] = 0.1

In [24]: df
Out[24]: 
   AAA  BBB  CCC
0  0.1   10  100
1  5.0   20   50
2  0.1   30  -30
3  0.1   40  -50
  Select rows with data closest to certain value using argsort 
In [25]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ....: )
   ....: 

In [26]: df
Out[26]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50

In [27]: aValue = 43.0

In [28]: df.loc[(df.CCC - aValue).abs().argsort()]
Out[28]: 
   AAA  BBB  CCC
1    5   20   50
0    4   10  100
2    6   30  -30
3    7   40  -50
  Dynamically reduce a list of criteria using a binary operators 
In [29]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ....: )
   ....: 

In [30]: df
Out[30]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50

In [31]: Crit1 = df.AAA <= 5.5

In [32]: Crit2 = df.BBB == 10.0

In [33]: Crit3 = df.CCC > -40.0
  One could hard code: 
In [34]: AllCrit = Crit1 & Crit2 & Crit3
  …Or it can be done with a list of dynamically built criteria 
In [35]: import functools

In [36]: CritList = [Crit1, Crit2, Crit3]

In [37]: AllCrit = functools.reduce(lambda x, y: x & y, CritList)

In [38]: df[AllCrit]
Out[38]: 
   AAA  BBB  CCC
0    4   10  100
     Selection  Dataframes The indexing docs. Using both row labels and value conditionals 
In [39]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ....: )
   ....: 

In [40]: df
Out[40]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50

In [41]: df[(df.AAA <= 6) & (df.index.isin([0, 2, 4]))]
Out[41]: 
   AAA  BBB  CCC
0    4   10  100
2    6   30  -30
  Use loc for label-oriented slicing and iloc positional slicing GH2904 
In [42]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]},
   ....:     index=["foo", "bar", "boo", "kar"],
   ....: )
   ....: 
  There are 2 explicit slicing methods, with a third general case  Positional-oriented (Python slicing style : exclusive of end) Label-oriented (Non-Python slicing style : inclusive of end) General (Either slicing style : depends on if the slice contains labels or positions)  
In [43]: df.loc["bar":"kar"]  # Label
Out[43]: 
     AAA  BBB  CCC
bar    5   20   50
boo    6   30  -30
kar    7   40  -50

# Generic
In [44]: df[0:3]
Out[44]: 
     AAA  BBB  CCC
foo    4   10  100
bar    5   20   50
boo    6   30  -30

In [45]: df["bar":"kar"]
Out[45]: 
     AAA  BBB  CCC
bar    5   20   50
boo    6   30  -30
kar    7   40  -50
  Ambiguity arises when an index consists of integers with a non-zero start or non-unit increment. 
In [46]: data = {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}

In [47]: df2 = pd.DataFrame(data=data, index=[1, 2, 3, 4])  # Note index starts at 1.

In [48]: df2.iloc[1:3]  # Position-oriented
Out[48]: 
   AAA  BBB  CCC
2    5   20   50
3    6   30  -30

In [49]: df2.loc[1:3]  # Label-oriented
Out[49]: 
   AAA  BBB  CCC
1    4   10  100
2    5   20   50
3    6   30  -30
  Using inverse operator (~) to take the complement of a mask 
In [50]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ....: )
   ....: 

In [51]: df
Out[51]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50

In [52]: df[~((df.AAA <= 6) & (df.index.isin([0, 2, 4])))]
Out[52]: 
   AAA  BBB  CCC
1    5   20   50
3    7   40  -50
    New columns Efficiently and dynamically creating new columns using applymap 
In [53]: df = pd.DataFrame({"AAA": [1, 2, 1, 3], "BBB": [1, 1, 2, 2], "CCC": [2, 1, 3, 1]})

In [54]: df
Out[54]: 
   AAA  BBB  CCC
0    1    1    2
1    2    1    1
2    1    2    3
3    3    2    1

In [55]: source_cols = df.columns  # Or some subset would work too

In [56]: new_cols = [str(x) + "_cat" for x in source_cols]

In [57]: categories = {1: "Alpha", 2: "Beta", 3: "Charlie"}

In [58]: df[new_cols] = df[source_cols].applymap(categories.get)

In [59]: df
Out[59]: 
   AAA  BBB  CCC  AAA_cat BBB_cat  CCC_cat
0    1    1    2    Alpha   Alpha     Beta
1    2    1    1     Beta   Alpha    Alpha
2    1    2    3    Alpha    Beta  Charlie
3    3    2    1  Charlie    Beta    Alpha
  Keep other columns when using min() with groupby 
In [60]: df = pd.DataFrame(
   ....:     {"AAA": [1, 1, 1, 2, 2, 2, 3, 3], "BBB": [2, 1, 3, 4, 5, 1, 2, 3]}
   ....: )
   ....: 

In [61]: df
Out[61]: 
   AAA  BBB
0    1    2
1    1    1
2    1    3
3    2    4
4    2    5
5    2    1
6    3    2
7    3    3
  Method 1 : idxmin() to get the index of the minimums 
In [62]: df.loc[df.groupby("AAA")["BBB"].idxmin()]
Out[62]: 
   AAA  BBB
1    1    1
5    2    1
6    3    2
  Method 2 : sort then take first of each 
In [63]: df.sort_values(by="BBB").groupby("AAA", as_index=False).first()
Out[63]: 
   AAA  BBB
0    1    1
1    2    1
2    3    2
  Notice the same results, with the exception of the index.    Multiindexing The multindexing docs. Creating a MultiIndex from a labeled frame 
In [64]: df = pd.DataFrame(
   ....:     {
   ....:         "row": [0, 1, 2],
   ....:         "One_X": [1.1, 1.1, 1.1],
   ....:         "One_Y": [1.2, 1.2, 1.2],
   ....:         "Two_X": [1.11, 1.11, 1.11],
   ....:         "Two_Y": [1.22, 1.22, 1.22],
   ....:     }
   ....: )
   ....: 

In [65]: df
Out[65]: 
   row  One_X  One_Y  Two_X  Two_Y
0    0    1.1    1.2   1.11   1.22
1    1    1.1    1.2   1.11   1.22
2    2    1.1    1.2   1.11   1.22

# As Labelled Index
In [66]: df = df.set_index("row")

In [67]: df
Out[67]: 
     One_X  One_Y  Two_X  Two_Y
row                            
0      1.1    1.2   1.11   1.22
1      1.1    1.2   1.11   1.22
2      1.1    1.2   1.11   1.22

# With Hierarchical Columns
In [68]: df.columns = pd.MultiIndex.from_tuples([tuple(c.split("_")) for c in df.columns])

In [69]: df
Out[69]: 
     One        Two      
       X    Y     X     Y
row                      
0    1.1  1.2  1.11  1.22
1    1.1  1.2  1.11  1.22
2    1.1  1.2  1.11  1.22

# Now stack & Reset
In [70]: df = df.stack(0).reset_index(1)

In [71]: df
Out[71]: 
    level_1     X     Y
row                    
0       One  1.10  1.20
0       Two  1.11  1.22
1       One  1.10  1.20
1       Two  1.11  1.22
2       One  1.10  1.20
2       Two  1.11  1.22

# And fix the labels (Notice the label 'level_1' got added automatically)
In [72]: df.columns = ["Sample", "All_X", "All_Y"]

In [73]: df
Out[73]: 
    Sample  All_X  All_Y
row                     
0      One   1.10   1.20
0      Two   1.11   1.22
1      One   1.10   1.20
1      Two   1.11   1.22
2      One   1.10   1.20
2      Two   1.11   1.22
   Arithmetic Performing arithmetic with a MultiIndex that needs broadcasting 
In [74]: cols = pd.MultiIndex.from_tuples(
   ....:     [(x, y) for x in ["A", "B", "C"] for y in ["O", "I"]]
   ....: )
   ....: 

In [75]: df = pd.DataFrame(np.random.randn(2, 6), index=["n", "m"], columns=cols)

In [76]: df
Out[76]: 
          A                   B                   C          
          O         I         O         I         O         I
n  0.469112 -0.282863 -1.509059 -1.135632  1.212112 -0.173215
m  0.119209 -1.044236 -0.861849 -2.104569 -0.494929  1.071804

In [77]: df = df.div(df["C"], level=1)

In [78]: df
Out[78]: 
          A                   B              C     
          O         I         O         I    O    I
n  0.387021  1.633022 -1.244983  6.556214  1.0  1.0
m -0.240860 -0.974279  1.741358 -1.963577  1.0  1.0
    Slicing Slicing a MultiIndex with xs 
In [79]: coords = [("AA", "one"), ("AA", "six"), ("BB", "one"), ("BB", "two"), ("BB", "six")]

In [80]: index = pd.MultiIndex.from_tuples(coords)

In [81]: df = pd.DataFrame([11, 22, 33, 44, 55], index, ["MyData"])

In [82]: df
Out[82]: 
        MyData
AA one      11
   six      22
BB one      33
   two      44
   six      55
  To take the cross section of the 1st level and 1st axis the index: 
# Note : level and axis are optional, and default to zero
In [83]: df.xs("BB", level=0, axis=0)
Out[83]: 
     MyData
one      33
two      44
six      55
  …and now the 2nd level of the 1st axis. 
In [84]: df.xs("six", level=1, axis=0)
Out[84]: 
    MyData
AA      22
BB      55
  Slicing a MultiIndex with xs, method #2 
In [85]: import itertools

In [86]: index = list(itertools.product(["Ada", "Quinn", "Violet"], ["Comp", "Math", "Sci"]))

In [87]: headr = list(itertools.product(["Exams", "Labs"], ["I", "II"]))

In [88]: indx = pd.MultiIndex.from_tuples(index, names=["Student", "Course"])

In [89]: cols = pd.MultiIndex.from_tuples(headr)  # Notice these are un-named

In [90]: data = [[70 + x + y + (x * y) % 3 for x in range(4)] for y in range(9)]

In [91]: df = pd.DataFrame(data, indx, cols)

In [92]: df
Out[92]: 
               Exams     Labs    
                   I  II    I  II
Student Course                   
Ada     Comp      70  71   72  73
        Math      71  73   75  74
        Sci       72  75   75  75
Quinn   Comp      73  74   75  76
        Math      74  76   78  77
        Sci       75  78   78  78
Violet  Comp      76  77   78  79
        Math      77  79   81  80
        Sci       78  81   81  81

In [93]: All = slice(None)

In [94]: df.loc["Violet"]
Out[94]: 
       Exams     Labs    
           I  II    I  II
Course                   
Comp      76  77   78  79
Math      77  79   81  80
Sci       78  81   81  81

In [95]: df.loc[(All, "Math"), All]
Out[95]: 
               Exams     Labs    
                   I  II    I  II
Student Course                   
Ada     Math      71  73   75  74
Quinn   Math      74  76   78  77
Violet  Math      77  79   81  80

In [96]: df.loc[(slice("Ada", "Quinn"), "Math"), All]
Out[96]: 
               Exams     Labs    
                   I  II    I  II
Student Course                   
Ada     Math      71  73   75  74
Quinn   Math      74  76   78  77

In [97]: df.loc[(All, "Math"), ("Exams")]
Out[97]: 
                 I  II
Student Course        
Ada     Math    71  73
Quinn   Math    74  76
Violet  Math    77  79

In [98]: df.loc[(All, "Math"), (All, "II")]
Out[98]: 
               Exams Labs
                  II   II
Student Course           
Ada     Math      73   74
Quinn   Math      76   77
Violet  Math      79   80
  Setting portions of a MultiIndex with xs   Sorting Sort by specific column or an ordered list of columns, with a MultiIndex 
In [99]: df.sort_values(by=("Labs", "II"), ascending=False)
Out[99]: 
               Exams     Labs    
                   I  II    I  II
Student Course                   
Violet  Sci       78  81   81  81
        Math      77  79   81  80
        Comp      76  77   78  79
Quinn   Sci       75  78   78  78
        Math      74  76   78  77
        Comp      73  74   75  76
Ada     Sci       72  75   75  75
        Math      71  73   75  74
        Comp      70  71   72  73
  Partial selection, the need for sortedness GH2995   Levels Prepending a level to a multiindex Flatten Hierarchical columns    Missing data The missing data docs. Fill forward a reversed timeseries 
In [100]: df = pd.DataFrame(
   .....:     np.random.randn(6, 1),
   .....:     index=pd.date_range("2013-08-01", periods=6, freq="B"),
   .....:     columns=list("A"),
   .....: )
   .....: 

In [101]: df.loc[df.index[3], "A"] = np.nan

In [102]: df
Out[102]: 
                   A
2013-08-01  0.721555
2013-08-02 -0.706771
2013-08-05 -1.039575
2013-08-06       NaN
2013-08-07 -0.424972
2013-08-08  0.567020

In [103]: df.reindex(df.index[::-1]).ffill()
Out[103]: 
                   A
2013-08-08  0.567020
2013-08-07 -0.424972
2013-08-06 -0.424972
2013-08-05 -1.039575
2013-08-02 -0.706771
2013-08-01  0.721555
  cumsum reset at NaN values  Replace Using replace with backrefs    Grouping The grouping docs. Basic grouping with apply Unlike agg, apply’s callable is passed a sub-DataFrame which gives you access to all the columns 
In [104]: df = pd.DataFrame(
   .....:     {
   .....:         "animal": "cat dog cat fish dog cat cat".split(),
   .....:         "size": list("SSMMMLL"),
   .....:         "weight": [8, 10, 11, 1, 20, 12, 12],
   .....:         "adult": [False] * 5 + [True] * 2,
   .....:     }
   .....: )
   .....: 

In [105]: df
Out[105]: 
  animal size  weight  adult
0    cat    S       8  False
1    dog    S      10  False
2    cat    M      11  False
3   fish    M       1  False
4    dog    M      20  False
5    cat    L      12   True
6    cat    L      12   True

# List the size of the animals with the highest weight.
In [106]: df.groupby("animal").apply(lambda subf: subf["size"][subf["weight"].idxmax()])
Out[106]: 
animal
cat     L
dog     M
fish    M
dtype: object
  Using get_group 
In [107]: gb = df.groupby(["animal"])

In [108]: gb.get_group("cat")
Out[108]: 
  animal size  weight  adult
0    cat    S       8  False
2    cat    M      11  False
5    cat    L      12   True
6    cat    L      12   True
  Apply to different items in a group 
In [109]: def GrowUp(x):
   .....:     avg_weight = sum(x[x["size"] == "S"].weight * 1.5)
   .....:     avg_weight += sum(x[x["size"] == "M"].weight * 1.25)
   .....:     avg_weight += sum(x[x["size"] == "L"].weight)
   .....:     avg_weight /= len(x)
   .....:     return pd.Series(["L", avg_weight, True], index=["size", "weight", "adult"])
   .....: 

In [110]: expected_df = gb.apply(GrowUp)

In [111]: expected_df
Out[111]: 
       size   weight  adult
animal                     
cat       L  12.4375   True
dog       L  20.0000   True
fish      L   1.2500   True
  Expanding apply 
In [112]: S = pd.Series([i / 100.0 for i in range(1, 11)])

In [113]: def cum_ret(x, y):
   .....:     return x * (1 + y)
   .....: 

In [114]: def red(x):
   .....:     return functools.reduce(cum_ret, x, 1.0)
   .....: 

In [115]: S.expanding().apply(red, raw=True)
Out[115]: 
0    1.010000
1    1.030200
2    1.061106
3    1.103550
4    1.158728
5    1.228251
6    1.314229
7    1.419367
8    1.547110
9    1.701821
dtype: float64
  Replacing some values with mean of the rest of a group 
In [116]: df = pd.DataFrame({"A": [1, 1, 2, 2], "B": [1, -1, 1, 2]})

In [117]: gb = df.groupby("A")

In [118]: def replace(g):
   .....:     mask = g < 0
   .....:     return g.where(mask, g[~mask].mean())
   .....: 

In [119]: gb.transform(replace)
Out[119]: 
     B
0  1.0
1 -1.0
2  1.5
3  1.5
  Sort groups by aggregated data 
In [120]: df = pd.DataFrame(
   .....:     {
   .....:         "code": ["foo", "bar", "baz"] * 2,
   .....:         "data": [0.16, -0.21, 0.33, 0.45, -0.59, 0.62],
   .....:         "flag": [False, True] * 3,
   .....:     }
   .....: )
   .....: 

In [121]: code_groups = df.groupby("code")

In [122]: agg_n_sort_order = code_groups[["data"]].transform(sum).sort_values(by="data")

In [123]: sorted_df = df.loc[agg_n_sort_order.index]

In [124]: sorted_df
Out[124]: 
  code  data   flag
1  bar -0.21   True
4  bar -0.59  False
0  foo  0.16  False
3  foo  0.45   True
2  baz  0.33  False
5  baz  0.62   True
  Create multiple aggregated columns 
In [125]: rng = pd.date_range(start="2014-10-07", periods=10, freq="2min")

In [126]: ts = pd.Series(data=list(range(10)), index=rng)

In [127]: def MyCust(x):
   .....:     if len(x) > 2:
   .....:         return x[1] * 1.234
   .....:     return pd.NaT
   .....: 

In [128]: mhc = {"Mean": np.mean, "Max": np.max, "Custom": MyCust}

In [129]: ts.resample("5min").apply(mhc)
Out[129]: 
                     Mean  Max Custom
2014-10-07 00:00:00   1.0    2  1.234
2014-10-07 00:05:00   3.5    4    NaT
2014-10-07 00:10:00   6.0    7  7.404
2014-10-07 00:15:00   8.5    9    NaT

In [130]: ts
Out[130]: 
2014-10-07 00:00:00    0
2014-10-07 00:02:00    1
2014-10-07 00:04:00    2
2014-10-07 00:06:00    3
2014-10-07 00:08:00    4
2014-10-07 00:10:00    5
2014-10-07 00:12:00    6
2014-10-07 00:14:00    7
2014-10-07 00:16:00    8
2014-10-07 00:18:00    9
Freq: 2T, dtype: int64
  Create a value counts column and reassign back to the DataFrame 
In [131]: df = pd.DataFrame(
   .....:     {"Color": "Red Red Red Blue".split(), "Value": [100, 150, 50, 50]}
   .....: )
   .....: 

In [132]: df
Out[132]: 
  Color  Value
0   Red    100
1   Red    150
2   Red     50
3  Blue     50

In [133]: df["Counts"] = df.groupby(["Color"]).transform(len)

In [134]: df
Out[134]: 
  Color  Value  Counts
0   Red    100       3
1   Red    150       3
2   Red     50       3
3  Blue     50       1
  Shift groups of the values in a column based on the index 
In [135]: df = pd.DataFrame(
   .....:     {"line_race": [10, 10, 8, 10, 10, 8], "beyer": [99, 102, 103, 103, 88, 100]},
   .....:     index=[
   .....:         "Last Gunfighter",
   .....:         "Last Gunfighter",
   .....:         "Last Gunfighter",
   .....:         "Paynter",
   .....:         "Paynter",
   .....:         "Paynter",
   .....:     ],
   .....: )
   .....: 

In [136]: df
Out[136]: 
                 line_race  beyer
Last Gunfighter         10     99
Last Gunfighter         10    102
Last Gunfighter          8    103
Paynter                 10    103
Paynter                 10     88
Paynter                  8    100

In [137]: df["beyer_shifted"] = df.groupby(level=0)["beyer"].shift(1)

In [138]: df
Out[138]: 
                 line_race  beyer  beyer_shifted
Last Gunfighter         10     99            NaN
Last Gunfighter         10    102           99.0
Last Gunfighter          8    103          102.0
Paynter                 10    103            NaN
Paynter                 10     88          103.0
Paynter                  8    100           88.0
  Select row with maximum value from each group 
In [139]: df = pd.DataFrame(
   .....:     {
   .....:         "host": ["other", "other", "that", "this", "this"],
   .....:         "service": ["mail", "web", "mail", "mail", "web"],
   .....:         "no": [1, 2, 1, 2, 1],
   .....:     }
   .....: ).set_index(["host", "service"])
   .....: 

In [140]: mask = df.groupby(level=0).agg("idxmax")

In [141]: df_count = df.loc[mask["no"]].reset_index()

In [142]: df_count
Out[142]: 
    host service  no
0  other     web   2
1   that    mail   1
2   this    mail   2
  Grouping like Python’s itertools.groupby 
In [143]: df = pd.DataFrame([0, 1, 0, 1, 1, 1, 0, 1, 1], columns=["A"])

In [144]: df["A"].groupby((df["A"] != df["A"].shift()).cumsum()).groups
Out[144]: {1: [0], 2: [1], 3: [2], 4: [3, 4, 5], 5: [6], 6: [7, 8]}

In [145]: df["A"].groupby((df["A"] != df["A"].shift()).cumsum()).cumsum()
Out[145]: 
0    0
1    1
2    0
3    1
4    2
5    3
6    0
7    1
8    2
Name: A, dtype: int64
   Expanding data Alignment and to-date Rolling Computation window based on values instead of counts Rolling Mean by Time Interval   Splitting Splitting a frame Create a list of dataframes, split using a delineation based on logic included in rows. 
In [146]: df = pd.DataFrame(
   .....:     data={
   .....:         "Case": ["A", "A", "A", "B", "A", "A", "B", "A", "A"],
   .....:         "Data": np.random.randn(9),
   .....:     }
   .....: )
   .....: 

In [147]: dfs = list(
   .....:     zip(
   .....:         *df.groupby(
   .....:             (1 * (df["Case"] == "B"))
   .....:             .cumsum()
   .....:             .rolling(window=3, min_periods=1)
   .....:             .median()
   .....:         )
   .....:     )
   .....: )[-1]
   .....: 

In [148]: dfs[0]
Out[148]: 
  Case      Data
0    A  0.276232
1    A -1.087401
2    A -0.673690
3    B  0.113648

In [149]: dfs[1]
Out[149]: 
  Case      Data
4    A -1.478427
5    A  0.524988
6    B  0.404705

In [150]: dfs[2]
Out[150]: 
  Case      Data
7    A  0.577046
8    A -1.715002
    Pivot The Pivot docs. Partial sums and subtotals 
In [151]: df = pd.DataFrame(
   .....:     data={
   .....:         "Province": ["ON", "QC", "BC", "AL", "AL", "MN", "ON"],
   .....:         "City": [
   .....:             "Toronto",
   .....:             "Montreal",
   .....:             "Vancouver",
   .....:             "Calgary",
   .....:             "Edmonton",
   .....:             "Winnipeg",
   .....:             "Windsor",
   .....:         ],
   .....:         "Sales": [13, 6, 16, 8, 4, 3, 1],
   .....:     }
   .....: )
   .....: 

In [152]: table = pd.pivot_table(
   .....:     df,
   .....:     values=["Sales"],
   .....:     index=["Province"],
   .....:     columns=["City"],
   .....:     aggfunc=np.sum,
   .....:     margins=True,
   .....: )
   .....: 

In [153]: table.stack("City")
Out[153]: 
                    Sales
Province City            
AL       All         12.0
         Calgary      8.0
         Edmonton     4.0
BC       All         16.0
         Vancouver   16.0
...                   ...
All      Montreal     6.0
         Toronto     13.0
         Vancouver   16.0
         Windsor      1.0
         Winnipeg     3.0

[20 rows x 1 columns]
  Frequency table like plyr in R 
In [154]: grades = [48, 99, 75, 80, 42, 80, 72, 68, 36, 78]

In [155]: df = pd.DataFrame(
   .....:     {
   .....:         "ID": ["x%d" % r for r in range(10)],
   .....:         "Gender": ["F", "M", "F", "M", "F", "M", "F", "M", "M", "M"],
   .....:         "ExamYear": [
   .....:             "2007",
   .....:             "2007",
   .....:             "2007",
   .....:             "2008",
   .....:             "2008",
   .....:             "2008",
   .....:             "2008",
   .....:             "2009",
   .....:             "2009",
   .....:             "2009",
   .....:         ],
   .....:         "Class": [
   .....:             "algebra",
   .....:             "stats",
   .....:             "bio",
   .....:             "algebra",
   .....:             "algebra",
   .....:             "stats",
   .....:             "stats",
   .....:             "algebra",
   .....:             "bio",
   .....:             "bio",
   .....:         ],
   .....:         "Participated": [
   .....:             "yes",
   .....:             "yes",
   .....:             "yes",
   .....:             "yes",
   .....:             "no",
   .....:             "yes",
   .....:             "yes",
   .....:             "yes",
   .....:             "yes",
   .....:             "yes",
   .....:         ],
   .....:         "Passed": ["yes" if x > 50 else "no" for x in grades],
   .....:         "Employed": [
   .....:             True,
   .....:             True,
   .....:             True,
   .....:             False,
   .....:             False,
   .....:             False,
   .....:             False,
   .....:             True,
   .....:             True,
   .....:             False,
   .....:         ],
   .....:         "Grade": grades,
   .....:     }
   .....: )
   .....: 

In [156]: df.groupby("ExamYear").agg(
   .....:     {
   .....:         "Participated": lambda x: x.value_counts()["yes"],
   .....:         "Passed": lambda x: sum(x == "yes"),
   .....:         "Employed": lambda x: sum(x),
   .....:         "Grade": lambda x: sum(x) / len(x),
   .....:     }
   .....: )
   .....: 
Out[156]: 
          Participated  Passed  Employed      Grade
ExamYear                                           
2007                 3       2         3  74.000000
2008                 3       3         0  68.500000
2009                 3       2         2  60.666667
  Plot pandas DataFrame with year over year data To create year and month cross tabulation: 
In [157]: df = pd.DataFrame(
   .....:     {"value": np.random.randn(36)},
   .....:     index=pd.date_range("2011-01-01", freq="M", periods=36),
   .....: )
   .....: 

In [158]: pd.pivot_table(
   .....:     df, index=df.index.month, columns=df.index.year, values="value", aggfunc="sum"
   .....: )
   .....: 
Out[158]: 
        2011      2012      2013
1  -1.039268 -0.968914  2.565646
2  -0.370647 -1.294524  1.431256
3  -1.157892  0.413738  1.340309
4  -1.344312  0.276662 -1.170299
5   0.844885 -0.472035 -0.226169
6   1.075770 -0.013960  0.410835
7  -0.109050 -0.362543  0.813850
8   1.643563 -0.006154  0.132003
9  -1.469388 -0.923061 -0.827317
10  0.357021  0.895717 -0.076467
11 -0.674600  0.805244 -1.187678
12 -1.776904 -1.206412  1.130127
    Apply Rolling apply to organize - Turning embedded lists into a MultiIndex frame 
In [159]: df = pd.DataFrame(
   .....:     data={
   .....:         "A": [[2, 4, 8, 16], [100, 200], [10, 20, 30]],
   .....:         "B": [["a", "b", "c"], ["jj", "kk"], ["ccc"]],
   .....:     },
   .....:     index=["I", "II", "III"],
   .....: )
   .....: 

In [160]: def SeriesFromSubList(aList):
   .....:     return pd.Series(aList)
   .....: 

In [161]: df_orgz = pd.concat(
   .....:     {ind: row.apply(SeriesFromSubList) for ind, row in df.iterrows()}
   .....: )
   .....: 

In [162]: df_orgz
Out[162]: 
         0     1     2     3
I   A    2     4     8  16.0
    B    a     b     c   NaN
II  A  100   200   NaN   NaN
    B   jj    kk   NaN   NaN
III A   10  20.0  30.0   NaN
    B  ccc   NaN   NaN   NaN
  Rolling apply with a DataFrame returning a Series Rolling Apply to multiple columns where function calculates a Series before a Scalar from the Series is returned 
In [163]: df = pd.DataFrame(
   .....:     data=np.random.randn(2000, 2) / 10000,
   .....:     index=pd.date_range("2001-01-01", periods=2000),
   .....:     columns=["A", "B"],
   .....: )
   .....: 

In [164]: df
Out[164]: 
                   A         B
2001-01-01 -0.000144 -0.000141
2001-01-02  0.000161  0.000102
2001-01-03  0.000057  0.000088
2001-01-04 -0.000221  0.000097
2001-01-05 -0.000201 -0.000041
...              ...       ...
2006-06-19  0.000040 -0.000235
2006-06-20 -0.000123 -0.000021
2006-06-21 -0.000113  0.000114
2006-06-22  0.000136  0.000109
2006-06-23  0.000027  0.000030

[2000 rows x 2 columns]

In [165]: def gm(df, const):
   .....:     v = ((((df["A"] + df["B"]) + 1).cumprod()) - 1) * const
   .....:     return v.iloc[-1]
   .....: 

In [166]: s = pd.Series(
   .....:     {
   .....:         df.index[i]: gm(df.iloc[i: min(i + 51, len(df) - 1)], 5)
   .....:         for i in range(len(df) - 50)
   .....:     }
   .....: )
   .....: 

In [167]: s
Out[167]: 
2001-01-01    0.000930
2001-01-02    0.002615
2001-01-03    0.001281
2001-01-04    0.001117
2001-01-05    0.002772
                ...   
2006-04-30    0.003296
2006-05-01    0.002629
2006-05-02    0.002081
2006-05-03    0.004247
2006-05-04    0.003928
Length: 1950, dtype: float64
  Rolling apply with a DataFrame returning a Scalar Rolling Apply to multiple columns where function returns a Scalar (Volume Weighted Average Price) 
In [168]: rng = pd.date_range(start="2014-01-01", periods=100)

In [169]: df = pd.DataFrame(
   .....:     {
   .....:         "Open": np.random.randn(len(rng)),
   .....:         "Close": np.random.randn(len(rng)),
   .....:         "Volume": np.random.randint(100, 2000, len(rng)),
   .....:     },
   .....:     index=rng,
   .....: )
   .....: 

In [170]: df
Out[170]: 
                Open     Close  Volume
2014-01-01 -1.611353 -0.492885    1219
2014-01-02 -3.000951  0.445794    1054
2014-01-03 -0.138359 -0.076081    1381
2014-01-04  0.301568  1.198259    1253
2014-01-05  0.276381 -0.669831    1728
...              ...       ...     ...
2014-04-06 -0.040338  0.937843    1188
2014-04-07  0.359661 -0.285908    1864
2014-04-08  0.060978  1.714814     941
2014-04-09  1.759055 -0.455942    1065
2014-04-10  0.138185 -1.147008    1453

[100 rows x 3 columns]

In [171]: def vwap(bars):
   .....:     return (bars.Close * bars.Volume).sum() / bars.Volume.sum()
   .....: 

In [172]: window = 5

In [173]: s = pd.concat(
   .....:     [
   .....:         (pd.Series(vwap(df.iloc[i: i + window]), index=[df.index[i + window]]))
   .....:         for i in range(len(df) - window)
   .....:     ]
   .....: )
   .....: 

In [174]: s.round(2)
Out[174]: 
2014-01-06    0.02
2014-01-07    0.11
2014-01-08    0.10
2014-01-09    0.07
2014-01-10   -0.29
              ... 
2014-04-06   -0.63
2014-04-07   -0.02
2014-04-08   -0.03
2014-04-09    0.34
2014-04-10    0.29
Length: 95, dtype: float64
     Timeseries Between times Using indexer between time Constructing a datetime range that excludes weekends and includes only certain times Vectorized Lookup Aggregation and plotting time series Turn a matrix with hours in columns and days in rows into a continuous row sequence in the form of a time series. How to rearrange a Python pandas DataFrame? Dealing with duplicates when reindexing a timeseries to a specified frequency Calculate the first day of the month for each entry in a DatetimeIndex 
In [175]: dates = pd.date_range("2000-01-01", periods=5)

In [176]: dates.to_period(freq="M").to_timestamp()
Out[176]: 
DatetimeIndex(['2000-01-01', '2000-01-01', '2000-01-01', '2000-01-01',
               '2000-01-01'],
              dtype='datetime64[ns]', freq=None)
   Resampling The Resample docs. Using Grouper instead of TimeGrouper for time grouping of values Time grouping with some missing values Valid frequency arguments to Grouper Timeseries Grouping using a MultiIndex Using TimeGrouper and another grouping to create subgroups, then apply a custom function GH3791 Resampling with custom periods Resample intraday frame without adding new days Resample minute data Resample with groupby    Merge The Join docs. Concatenate two dataframes with overlapping index (emulate R rbind) 
In [177]: rng = pd.date_range("2000-01-01", periods=6)

In [178]: df1 = pd.DataFrame(np.random.randn(6, 3), index=rng, columns=["A", "B", "C"])

In [179]: df2 = df1.copy()
  Depending on df construction, ignore_index may be needed 
In [180]: df = pd.concat([df1, df2], ignore_index=True)

In [181]: df
Out[181]: 
           A         B         C
0  -0.870117 -0.479265 -0.790855
1   0.144817  1.726395 -0.464535
2  -0.821906  1.597605  0.187307
3  -0.128342 -1.511638 -0.289858
4   0.399194 -1.430030 -0.639760
5   1.115116 -2.012600  1.810662
6  -0.870117 -0.479265 -0.790855
7   0.144817  1.726395 -0.464535
8  -0.821906  1.597605  0.187307
9  -0.128342 -1.511638 -0.289858
10  0.399194 -1.430030 -0.639760
11  1.115116 -2.012600  1.810662
  Self Join of a DataFrame GH2996 
In [182]: df = pd.DataFrame(
   .....:     data={
   .....:         "Area": ["A"] * 5 + ["C"] * 2,
   .....:         "Bins": [110] * 2 + [160] * 3 + [40] * 2,
   .....:         "Test_0": [0, 1, 0, 1, 2, 0, 1],
   .....:         "Data": np.random.randn(7),
   .....:     }
   .....: )
   .....: 

In [183]: df
Out[183]: 
  Area  Bins  Test_0      Data
0    A   110       0 -0.433937
1    A   110       1 -0.160552
2    A   160       0  0.744434
3    A   160       1  1.754213
4    A   160       2  0.000850
5    C    40       0  0.342243
6    C    40       1  1.070599

In [184]: df["Test_1"] = df["Test_0"] - 1

In [185]: pd.merge(
   .....:     df,
   .....:     df,
   .....:     left_on=["Bins", "Area", "Test_0"],
   .....:     right_on=["Bins", "Area", "Test_1"],
   .....:     suffixes=("_L", "_R"),
   .....: )
   .....: 
Out[185]: 
  Area  Bins  Test_0_L    Data_L  Test_1_L  Test_0_R    Data_R  Test_1_R
0    A   110         0 -0.433937        -1         1 -0.160552         0
1    A   160         0  0.744434        -1         1  1.754213         0
2    A   160         1  1.754213         0         2  0.000850         1
3    C    40         0  0.342243        -1         1  1.070599         0
  How to set the index and join KDB like asof join Join with a criteria based on the values Using searchsorted to merge based on values inside a range   Plotting The Plotting docs. Make Matplotlib look like R Setting x-axis major and minor labels Plotting multiple charts in an IPython Jupyter notebook Creating a multi-line plot Plotting a heatmap Annotate a time-series plot Annotate a time-series plot #2 Generate Embedded plots in excel files using Pandas, Vincent and xlsxwriter Boxplot for each quartile of a stratifying variable 
In [186]: df = pd.DataFrame(
   .....:     {
   .....:         "stratifying_var": np.random.uniform(0, 100, 20),
   .....:         "price": np.random.normal(100, 5, 20),
   .....:     }
   .....: )
   .....: 

In [187]: df["quartiles"] = pd.qcut(
   .....:     df["stratifying_var"], 4, labels=["0-25%", "25-50%", "50-75%", "75-100%"]
   .....: )
   .....: 

In [188]: df.boxplot(column="price", by="quartiles")
Out[188]: <AxesSubplot:title={'center':'price'}, xlabel='quartiles'>
     Data in/out Performance comparison of SQL vs HDF5  CSV The CSV docs read_csv in action appending to a csv Reading a csv chunk-by-chunk Reading only certain rows of a csv chunk-by-chunk Reading the first few lines of a frame Reading a file that is compressed but not by gzip/bz2 (the native compressed formats which read_csv understands). This example shows a WinZipped file, but is a general application of opening the file within a context manager and using that handle to read. See here Inferring dtypes from a file Dealing with bad lines GH2886 Write a multi-row index CSV without writing duplicates  Reading multiple files to create a single DataFrame The best way to combine multiple files into a single DataFrame is to read the individual frames one by one, put all of the individual frames into a list, and then combine the frames in the list using pd.concat(): 
In [189]: for i in range(3):
   .....:     data = pd.DataFrame(np.random.randn(10, 4))
   .....:     data.to_csv("file_{}.csv".format(i))
   .....: 

In [190]: files = ["file_0.csv", "file_1.csv", "file_2.csv"]

In [191]: result = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)
  You can use the same approach to read all files matching a pattern. Here is an example using glob: 
In [192]: import glob

In [193]: import os

In [194]: files = glob.glob("file_*.csv")

In [195]: result = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)
  Finally, this strategy will work with the other pd.read_*(...) functions described in the io docs.   Parsing date components in multi-columns Parsing date components in multi-columns is faster with a format 
In [196]: i = pd.date_range("20000101", periods=10000)

In [197]: df = pd.DataFrame({"year": i.year, "month": i.month, "day": i.day})

In [198]: df.head()
Out[198]: 
   year  month  day
0  2000      1    1
1  2000      1    2
2  2000      1    3
3  2000      1    4
4  2000      1    5

In [199]: %timeit pd.to_datetime(df.year * 10000 + df.month * 100 + df.day, format='%Y%m%d')
   .....: ds = df.apply(lambda x: "%04d%02d%02d" % (x["year"], x["month"], x["day"]), axis=1)
   .....: ds.head()
   .....: %timeit pd.to_datetime(ds)
   .....: 
8.7 ms +- 765 us per loop (mean +- std. dev. of 7 runs, 100 loops each)
2.1 ms +- 419 us per loop (mean +- std. dev. of 7 runs, 100 loops each)
    Skip row between header and data 
In [200]: data = """;;;;
   .....:  ;;;;
   .....:  ;;;;
   .....:  ;;;;
   .....:  ;;;;
   .....:  ;;;;
   .....: ;;;;
   .....:  ;;;;
   .....:  ;;;;
   .....: ;;;;
   .....: date;Param1;Param2;Param4;Param5
   .....:     ;m²;°C;m²;m
   .....: ;;;;
   .....: 01.01.1990 00:00;1;1;2;3
   .....: 01.01.1990 01:00;5;3;4;5
   .....: 01.01.1990 02:00;9;5;6;7
   .....: 01.01.1990 03:00;13;7;8;9
   .....: 01.01.1990 04:00;17;9;10;11
   .....: 01.01.1990 05:00;21;11;12;13
   .....: """
   .....: 
   Option 1: pass rows explicitly to skip rows 
In [201]: from io import StringIO

In [202]: pd.read_csv(
   .....:     StringIO(data),
   .....:     sep=";",
   .....:     skiprows=[11, 12],
   .....:     index_col=0,
   .....:     parse_dates=True,
   .....:     header=10,
   .....: )
   .....: 
Out[202]: 
                     Param1  Param2  Param4  Param5
date                                               
1990-01-01 00:00:00       1       1       2       3
1990-01-01 01:00:00       5       3       4       5
1990-01-01 02:00:00       9       5       6       7
1990-01-01 03:00:00      13       7       8       9
1990-01-01 04:00:00      17       9      10      11
1990-01-01 05:00:00      21      11      12      13
    Option 2: read column names and then data 
In [203]: pd.read_csv(StringIO(data), sep=";", header=10, nrows=10).columns
Out[203]: Index(['date', 'Param1', 'Param2', 'Param4', 'Param5'], dtype='object')

In [204]: columns = pd.read_csv(StringIO(data), sep=";", header=10, nrows=10).columns

In [205]: pd.read_csv(
   .....:     StringIO(data), sep=";", index_col=0, header=12, parse_dates=True, names=columns
   .....: )
   .....: 
Out[205]: 
                     Param1  Param2  Param4  Param5
date                                               
1990-01-01 00:00:00       1       1       2       3
1990-01-01 01:00:00       5       3       4       5
1990-01-01 02:00:00       9       5       6       7
1990-01-01 03:00:00      13       7       8       9
1990-01-01 04:00:00      17       9      10      11
1990-01-01 05:00:00      21      11      12      13
      SQL The SQL docs Reading from databases with SQL   Excel The Excel docs Reading from a filelike handle Modifying formatting in XlsxWriter output Loading only visible sheets GH19842#issuecomment-892150745   HTML Reading HTML tables from a server that cannot handle the default request header   HDFStore The HDFStores docs Simple queries with a Timestamp Index Managing heterogeneous data using a linked multiple table hierarchy GH3032 Merging on-disk tables with millions of rows Avoiding inconsistencies when writing to a store from multiple processes/threads De-duplicating a large store by chunks, essentially a recursive reduction operation. Shows a function for taking in data from csv file and creating a store by chunks, with date parsing as well. See here Creating a store chunk-by-chunk from a csv file Appending to a store, while creating a unique index Large Data work flows Reading in a sequence of files, then providing a global unique index to a store while appending Groupby on a HDFStore with low group density Groupby on a HDFStore with high group density Hierarchical queries on a HDFStore Counting with a HDFStore Troubleshoot HDFStore exceptions Setting min_itemsize with strings Using ptrepack to create a completely-sorted-index on a store Storing Attributes to a group node 
In [206]: df = pd.DataFrame(np.random.randn(8, 3))

In [207]: store = pd.HDFStore("test.h5")

In [208]: store.put("df", df)

# you can store an arbitrary Python object via pickle
In [209]: store.get_storer("df").attrs.my_attribute = {"A": 10}

In [210]: store.get_storer("df").attrs.my_attribute
Out[210]: {'A': 10}
  You can create or load a HDFStore in-memory by passing the driver parameter to PyTables. Changes are only written to disk when the HDFStore is closed. 
In [211]: store = pd.HDFStore("test.h5", "w", driver="H5FD_CORE")

In [212]: df = pd.DataFrame(np.random.randn(8, 3))

In [213]: store["test"] = df

# only after closing the store, data is written to disk:
In [214]: store.close()
    Binary files pandas readily accepts NumPy record arrays, if you need to read in a binary file consisting of an array of C structs. For example, given this C program in a file called main.c compiled with gcc main.c -std=gnu99 on a 64-bit machine, 
#include <stdio.h>
#include <stdint.h>

typedef struct _Data
{
    int32_t count;
    double avg;
    float scale;
} Data;

int main(int argc, const char *argv[])
{
    size_t n = 10;
    Data d[n];

    for (int i = 0; i < n; ++i)
    {
        d[i].count = i;
        d[i].avg = i + 1.0;
        d[i].scale = (float) i + 2.0f;
    }

    FILE *file = fopen("binary.dat", "wb");
    fwrite(&d, sizeof(Data), n, file);
    fclose(file);

    return 0;
}
  the following Python code will read the binary file 'binary.dat' into a pandas DataFrame, where each element of the struct corresponds to a column in the frame: 
names = "count", "avg", "scale"

# note that the offsets are larger than the size of the type because of
# struct padding
offsets = 0, 8, 16
formats = "i4", "f8", "f4"
dt = np.dtype({"names": names, "offsets": offsets, "formats": formats}, align=True)
df = pd.DataFrame(np.fromfile("binary.dat", dt))
   Note The offsets of the structure elements may be different depending on the architecture of the machine on which the file was created. Using a raw binary file format like this for general data storage is not recommended, as it is not cross platform. We recommended either HDF5 or parquet, both of which are supported by pandas’ IO facilities.     Computation Numerical integration (sample-based) of a time series  Correlation Often it’s useful to obtain the lower (or upper) triangular form of a correlation matrix calculated from DataFrame.corr(). This can be achieved by passing a boolean mask to where as follows: 
In [215]: df = pd.DataFrame(np.random.random(size=(100, 5)))

In [216]: corr_mat = df.corr()

In [217]: mask = np.tril(np.ones_like(corr_mat, dtype=np.bool_), k=-1)

In [218]: corr_mat.where(mask)
Out[218]: 
          0         1         2        3   4
0       NaN       NaN       NaN      NaN NaN
1 -0.079861       NaN       NaN      NaN NaN
2 -0.236573  0.183801       NaN      NaN NaN
3 -0.013795 -0.051975  0.037235      NaN NaN
4 -0.031974  0.118342 -0.073499 -0.02063 NaN
  The method argument within DataFrame.corr can accept a callable in addition to the named correlation types. Here we compute the distance correlation matrix for a DataFrame object. 
In [219]: def distcorr(x, y):
   .....:     n = len(x)
   .....:     a = np.zeros(shape=(n, n))
   .....:     b = np.zeros(shape=(n, n))
   .....:     for i in range(n):
   .....:         for j in range(i + 1, n):
   .....:             a[i, j] = abs(x[i] - x[j])
   .....:             b[i, j] = abs(y[i] - y[j])
   .....:     a += a.T
   .....:     b += b.T
   .....:     a_bar = np.vstack([np.nanmean(a, axis=0)] * n)
   .....:     b_bar = np.vstack([np.nanmean(b, axis=0)] * n)
   .....:     A = a - a_bar - a_bar.T + np.full(shape=(n, n), fill_value=a_bar.mean())
   .....:     B = b - b_bar - b_bar.T + np.full(shape=(n, n), fill_value=b_bar.mean())
   .....:     cov_ab = np.sqrt(np.nansum(A * B)) / n
   .....:     std_a = np.sqrt(np.sqrt(np.nansum(A ** 2)) / n)
   .....:     std_b = np.sqrt(np.sqrt(np.nansum(B ** 2)) / n)
   .....:     return cov_ab / std_a / std_b
   .....: 

In [220]: df = pd.DataFrame(np.random.normal(size=(100, 3)))

In [221]: df.corr(method=distcorr)
Out[221]: 
          0         1         2
0  1.000000  0.197613  0.216328
1  0.197613  1.000000  0.208749
2  0.216328  0.208749  1.000000
     Timedeltas The Timedeltas docs. Using timedeltas 
In [222]: import datetime

In [223]: s = pd.Series(pd.date_range("2012-1-1", periods=3, freq="D"))

In [224]: s - s.max()
Out[224]: 
0   -2 days
1   -1 days
2    0 days
dtype: timedelta64[ns]

In [225]: s.max() - s
Out[225]: 
0   2 days
1   1 days
2   0 days
dtype: timedelta64[ns]

In [226]: s - datetime.datetime(2011, 1, 1, 3, 5)
Out[226]: 
0   364 days 20:55:00
1   365 days 20:55:00
2   366 days 20:55:00
dtype: timedelta64[ns]

In [227]: s + datetime.timedelta(minutes=5)
Out[227]: 
0   2012-01-01 00:05:00
1   2012-01-02 00:05:00
2   2012-01-03 00:05:00
dtype: datetime64[ns]

In [228]: datetime.datetime(2011, 1, 1, 3, 5) - s
Out[228]: 
0   -365 days +03:05:00
1   -366 days +03:05:00
2   -367 days +03:05:00
dtype: timedelta64[ns]

In [229]: datetime.timedelta(minutes=5) + s
Out[229]: 
0   2012-01-01 00:05:00
1   2012-01-02 00:05:00
2   2012-01-03 00:05:00
dtype: datetime64[ns]
  Adding and subtracting deltas and dates 
In [230]: deltas = pd.Series([datetime.timedelta(days=i) for i in range(3)])

In [231]: df = pd.DataFrame({"A": s, "B": deltas})

In [232]: df
Out[232]: 
           A      B
0 2012-01-01 0 days
1 2012-01-02 1 days
2 2012-01-03 2 days

In [233]: df["New Dates"] = df["A"] + df["B"]

In [234]: df["Delta"] = df["A"] - df["New Dates"]

In [235]: df
Out[235]: 
           A      B  New Dates   Delta
0 2012-01-01 0 days 2012-01-01  0 days
1 2012-01-02 1 days 2012-01-03 -1 days
2 2012-01-03 2 days 2012-01-05 -2 days

In [236]: df.dtypes
Out[236]: 
A             datetime64[ns]
B            timedelta64[ns]
New Dates     datetime64[ns]
Delta        timedelta64[ns]
dtype: object
  Another example Values can be set to NaT using np.nan, similar to datetime 
In [237]: y = s - s.shift()

In [238]: y
Out[238]: 
0      NaT
1   1 days
2   1 days
dtype: timedelta64[ns]

In [239]: y[1] = np.nan

In [240]: y
Out[240]: 
0      NaT
1      NaT
2   1 days
dtype: timedelta64[ns]
    Creating example data To create a dataframe from every combination of some given values, like R’s expand.grid() function, we can create a dict where the keys are column names and the values are lists of the data values: 
In [241]: def expand_grid(data_dict):
   .....:     rows = itertools.product(*data_dict.values())
   .....:     return pd.DataFrame.from_records(rows, columns=data_dict.keys())
   .....: 

In [242]: df = expand_grid(
   .....:     {"height": [60, 70], "weight": [100, 140, 180], "sex": ["Male", "Female"]}
   .....: )
   .....: 

In [243]: df
Out[243]: 
    height  weight     sex
0       60     100    Male
1       60     100  Female
2       60     140    Male
3       60     140  Female
4       60     180    Male
5       60     180  Female
6       70     100    Male
7       70     100  Female
8       70     140    Male
9       70     140  Female
10      70     180    Male
11      70     180  Female
pandas.DataFrame.bool   DataFrame.bool()[source]
 
Return the bool of a single element Series or DataFrame. This must be a boolean scalar value, either True or False. It will raise a ValueError if the Series or DataFrame does not have exactly 1 element, or that element is not boolean (integer values 0 and 1 will also raise an exception).  Returns 
 bool

The value in the Series or DataFrame.      See also  Series.astype

Change the data type of a Series, including to boolean.  DataFrame.astype

Change the data type of a DataFrame, including to boolean.  numpy.bool_

NumPy boolean data type, used by pandas for boolean values.    Examples The method will only work for single element objects with a boolean value: 
>>> pd.Series([True]).bool()
True
>>> pd.Series([False]).bool()
False
  
>>> pd.DataFrame({'col': [True]}).bool()
True
>>> pd.DataFrame({'col': [False]}).bool()
False
numpy.matrix.nonzero method   matrix.nonzero()
 
Return the indices of the elements that are non-zero. Refer to numpy.nonzero for full documentation.  See also  numpy.nonzero

equivalent function
def f_21800169(df):
    """from dataframe `df` get list of indexes of rows where column 'BoolCol' values match True
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
pandas.DataFrame.lookup   DataFrame.lookup(row_labels, col_labels)[source]
 
Label-based “fancy indexing” function for DataFrame. Given equal-length arrays of row and column labels, return an array of the values corresponding to each (row, col) pair.  Deprecated since version 1.2.0: DataFrame.lookup is deprecated, use DataFrame.melt and DataFrame.loc instead. For further details see Looking up values by index/column labels.   Parameters 
 
row_labels:sequence


The row labels to use for lookup.  
col_labels:sequence


The column labels to use for lookup.    Returns 
 numpy.ndarray

The found values.
pandas.DataFrame.bool   DataFrame.bool()[source]
 
Return the bool of a single element Series or DataFrame. This must be a boolean scalar value, either True or False. It will raise a ValueError if the Series or DataFrame does not have exactly 1 element, or that element is not boolean (integer values 0 and 1 will also raise an exception).  Returns 
 bool

The value in the Series or DataFrame.      See also  Series.astype

Change the data type of a Series, including to boolean.  DataFrame.astype

Change the data type of a DataFrame, including to boolean.  numpy.bool_

NumPy boolean data type, used by pandas for boolean values.    Examples The method will only work for single element objects with a boolean value: 
>>> pd.Series([True]).bool()
True
>>> pd.Series([False]).bool()
False
  
>>> pd.DataFrame({'col': [True]}).bool()
True
>>> pd.DataFrame({'col': [False]}).bool()
False
pandas.Index.asof_locs   Index.asof_locs(where, mask)[source]
 
Return the locations (indices) of labels in the index. As in the asof function, if the label (a particular entry in where) is not in the index, the latest index label up to the passed label is chosen and its index returned. If all of the labels in the index are later than a label in where, -1 is returned. mask is used to ignore NA values in the index during calculation.  Parameters 
 
where:Index


An Index consisting of an array of timestamps.  
mask:np.ndarray[bool]


Array of booleans denoting where values in the original data are not NA.    Returns 
 np.ndarray[np.intp]

An array of locations (indices) of the labels from the Index which correspond to the return values of the asof function for every element in where.
Cookbook This is a repository for short and sweet examples and links for useful pandas recipes. We encourage users to add to this documentation. Adding interesting links and/or inline examples to this section is a great First Pull Request. Simplified, condensed, new-user friendly, in-line examples have been inserted where possible to augment the Stack-Overflow and GitHub links. Many of the links contain expanded information, above what the in-line examples offer. pandas (pd) and NumPy (np) are the only two abbreviated imported modules. The rest are kept explicitly imported for newer users.  Idioms These are some neat pandas idioms if-then/if-then-else on one column, and assignment to another one or more columns: 
In [1]: df = pd.DataFrame(
   ...:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ...: )
   ...: 

In [2]: df
Out[2]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50
   if-then… An if-then on one column 
In [3]: df.loc[df.AAA >= 5, "BBB"] = -1

In [4]: df
Out[4]: 
   AAA  BBB  CCC
0    4   10  100
1    5   -1   50
2    6   -1  -30
3    7   -1  -50
  An if-then with assignment to 2 columns: 
In [5]: df.loc[df.AAA >= 5, ["BBB", "CCC"]] = 555

In [6]: df
Out[6]: 
   AAA  BBB  CCC
0    4   10  100
1    5  555  555
2    6  555  555
3    7  555  555
  Add another line with different logic, to do the -else 
In [7]: df.loc[df.AAA < 5, ["BBB", "CCC"]] = 2000

In [8]: df
Out[8]: 
   AAA   BBB   CCC
0    4  2000  2000
1    5   555   555
2    6   555   555
3    7   555   555
  Or use pandas where after you’ve set up a mask 
In [9]: df_mask = pd.DataFrame(
   ...:     {"AAA": [True] * 4, "BBB": [False] * 4, "CCC": [True, False] * 2}
   ...: )
   ...: 

In [10]: df.where(df_mask, -1000)
Out[10]: 
   AAA   BBB   CCC
0    4 -1000  2000
1    5 -1000 -1000
2    6 -1000   555
3    7 -1000 -1000
  if-then-else using NumPy’s where() 
In [11]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ....: )
   ....: 

In [12]: df
Out[12]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50

In [13]: df["logic"] = np.where(df["AAA"] > 5, "high", "low")

In [14]: df
Out[14]: 
   AAA  BBB  CCC logic
0    4   10  100   low
1    5   20   50   low
2    6   30  -30  high
3    7   40  -50  high
    Splitting Split a frame with a boolean criterion 
In [15]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ....: )
   ....: 

In [16]: df
Out[16]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50

In [17]: df[df.AAA <= 5]
Out[17]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50

In [18]: df[df.AAA > 5]
Out[18]: 
   AAA  BBB  CCC
2    6   30  -30
3    7   40  -50
    Building criteria Select with multi-column criteria 
In [19]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ....: )
   ....: 

In [20]: df
Out[20]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50
  …and (without assignment returns a Series) 
In [21]: df.loc[(df["BBB"] < 25) & (df["CCC"] >= -40), "AAA"]
Out[21]: 
0    4
1    5
Name: AAA, dtype: int64
  …or (without assignment returns a Series) 
In [22]: df.loc[(df["BBB"] > 25) | (df["CCC"] >= -40), "AAA"]
Out[22]: 
0    4
1    5
2    6
3    7
Name: AAA, dtype: int64
  …or (with assignment modifies the DataFrame.) 
In [23]: df.loc[(df["BBB"] > 25) | (df["CCC"] >= 75), "AAA"] = 0.1

In [24]: df
Out[24]: 
   AAA  BBB  CCC
0  0.1   10  100
1  5.0   20   50
2  0.1   30  -30
3  0.1   40  -50
  Select rows with data closest to certain value using argsort 
In [25]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ....: )
   ....: 

In [26]: df
Out[26]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50

In [27]: aValue = 43.0

In [28]: df.loc[(df.CCC - aValue).abs().argsort()]
Out[28]: 
   AAA  BBB  CCC
1    5   20   50
0    4   10  100
2    6   30  -30
3    7   40  -50
  Dynamically reduce a list of criteria using a binary operators 
In [29]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ....: )
   ....: 

In [30]: df
Out[30]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50

In [31]: Crit1 = df.AAA <= 5.5

In [32]: Crit2 = df.BBB == 10.0

In [33]: Crit3 = df.CCC > -40.0
  One could hard code: 
In [34]: AllCrit = Crit1 & Crit2 & Crit3
  …Or it can be done with a list of dynamically built criteria 
In [35]: import functools

In [36]: CritList = [Crit1, Crit2, Crit3]

In [37]: AllCrit = functools.reduce(lambda x, y: x & y, CritList)

In [38]: df[AllCrit]
Out[38]: 
   AAA  BBB  CCC
0    4   10  100
     Selection  Dataframes The indexing docs. Using both row labels and value conditionals 
In [39]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ....: )
   ....: 

In [40]: df
Out[40]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50

In [41]: df[(df.AAA <= 6) & (df.index.isin([0, 2, 4]))]
Out[41]: 
   AAA  BBB  CCC
0    4   10  100
2    6   30  -30
  Use loc for label-oriented slicing and iloc positional slicing GH2904 
In [42]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]},
   ....:     index=["foo", "bar", "boo", "kar"],
   ....: )
   ....: 
  There are 2 explicit slicing methods, with a third general case  Positional-oriented (Python slicing style : exclusive of end) Label-oriented (Non-Python slicing style : inclusive of end) General (Either slicing style : depends on if the slice contains labels or positions)  
In [43]: df.loc["bar":"kar"]  # Label
Out[43]: 
     AAA  BBB  CCC
bar    5   20   50
boo    6   30  -30
kar    7   40  -50

# Generic
In [44]: df[0:3]
Out[44]: 
     AAA  BBB  CCC
foo    4   10  100
bar    5   20   50
boo    6   30  -30

In [45]: df["bar":"kar"]
Out[45]: 
     AAA  BBB  CCC
bar    5   20   50
boo    6   30  -30
kar    7   40  -50
  Ambiguity arises when an index consists of integers with a non-zero start or non-unit increment. 
In [46]: data = {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}

In [47]: df2 = pd.DataFrame(data=data, index=[1, 2, 3, 4])  # Note index starts at 1.

In [48]: df2.iloc[1:3]  # Position-oriented
Out[48]: 
   AAA  BBB  CCC
2    5   20   50
3    6   30  -30

In [49]: df2.loc[1:3]  # Label-oriented
Out[49]: 
   AAA  BBB  CCC
1    4   10  100
2    5   20   50
3    6   30  -30
  Using inverse operator (~) to take the complement of a mask 
In [50]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ....: )
   ....: 

In [51]: df
Out[51]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50

In [52]: df[~((df.AAA <= 6) & (df.index.isin([0, 2, 4])))]
Out[52]: 
   AAA  BBB  CCC
1    5   20   50
3    7   40  -50
    New columns Efficiently and dynamically creating new columns using applymap 
In [53]: df = pd.DataFrame({"AAA": [1, 2, 1, 3], "BBB": [1, 1, 2, 2], "CCC": [2, 1, 3, 1]})

In [54]: df
Out[54]: 
   AAA  BBB  CCC
0    1    1    2
1    2    1    1
2    1    2    3
3    3    2    1

In [55]: source_cols = df.columns  # Or some subset would work too

In [56]: new_cols = [str(x) + "_cat" for x in source_cols]

In [57]: categories = {1: "Alpha", 2: "Beta", 3: "Charlie"}

In [58]: df[new_cols] = df[source_cols].applymap(categories.get)

In [59]: df
Out[59]: 
   AAA  BBB  CCC  AAA_cat BBB_cat  CCC_cat
0    1    1    2    Alpha   Alpha     Beta
1    2    1    1     Beta   Alpha    Alpha
2    1    2    3    Alpha    Beta  Charlie
3    3    2    1  Charlie    Beta    Alpha
  Keep other columns when using min() with groupby 
In [60]: df = pd.DataFrame(
   ....:     {"AAA": [1, 1, 1, 2, 2, 2, 3, 3], "BBB": [2, 1, 3, 4, 5, 1, 2, 3]}
   ....: )
   ....: 

In [61]: df
Out[61]: 
   AAA  BBB
0    1    2
1    1    1
2    1    3
3    2    4
4    2    5
5    2    1
6    3    2
7    3    3
  Method 1 : idxmin() to get the index of the minimums 
In [62]: df.loc[df.groupby("AAA")["BBB"].idxmin()]
Out[62]: 
   AAA  BBB
1    1    1
5    2    1
6    3    2
  Method 2 : sort then take first of each 
In [63]: df.sort_values(by="BBB").groupby("AAA", as_index=False).first()
Out[63]: 
   AAA  BBB
0    1    1
1    2    1
2    3    2
  Notice the same results, with the exception of the index.    Multiindexing The multindexing docs. Creating a MultiIndex from a labeled frame 
In [64]: df = pd.DataFrame(
   ....:     {
   ....:         "row": [0, 1, 2],
   ....:         "One_X": [1.1, 1.1, 1.1],
   ....:         "One_Y": [1.2, 1.2, 1.2],
   ....:         "Two_X": [1.11, 1.11, 1.11],
   ....:         "Two_Y": [1.22, 1.22, 1.22],
   ....:     }
   ....: )
   ....: 

In [65]: df
Out[65]: 
   row  One_X  One_Y  Two_X  Two_Y
0    0    1.1    1.2   1.11   1.22
1    1    1.1    1.2   1.11   1.22
2    2    1.1    1.2   1.11   1.22

# As Labelled Index
In [66]: df = df.set_index("row")

In [67]: df
Out[67]: 
     One_X  One_Y  Two_X  Two_Y
row                            
0      1.1    1.2   1.11   1.22
1      1.1    1.2   1.11   1.22
2      1.1    1.2   1.11   1.22

# With Hierarchical Columns
In [68]: df.columns = pd.MultiIndex.from_tuples([tuple(c.split("_")) for c in df.columns])

In [69]: df
Out[69]: 
     One        Two      
       X    Y     X     Y
row                      
0    1.1  1.2  1.11  1.22
1    1.1  1.2  1.11  1.22
2    1.1  1.2  1.11  1.22

# Now stack & Reset
In [70]: df = df.stack(0).reset_index(1)

In [71]: df
Out[71]: 
    level_1     X     Y
row                    
0       One  1.10  1.20
0       Two  1.11  1.22
1       One  1.10  1.20
1       Two  1.11  1.22
2       One  1.10  1.20
2       Two  1.11  1.22

# And fix the labels (Notice the label 'level_1' got added automatically)
In [72]: df.columns = ["Sample", "All_X", "All_Y"]

In [73]: df
Out[73]: 
    Sample  All_X  All_Y
row                     
0      One   1.10   1.20
0      Two   1.11   1.22
1      One   1.10   1.20
1      Two   1.11   1.22
2      One   1.10   1.20
2      Two   1.11   1.22
   Arithmetic Performing arithmetic with a MultiIndex that needs broadcasting 
In [74]: cols = pd.MultiIndex.from_tuples(
   ....:     [(x, y) for x in ["A", "B", "C"] for y in ["O", "I"]]
   ....: )
   ....: 

In [75]: df = pd.DataFrame(np.random.randn(2, 6), index=["n", "m"], columns=cols)

In [76]: df
Out[76]: 
          A                   B                   C          
          O         I         O         I         O         I
n  0.469112 -0.282863 -1.509059 -1.135632  1.212112 -0.173215
m  0.119209 -1.044236 -0.861849 -2.104569 -0.494929  1.071804

In [77]: df = df.div(df["C"], level=1)

In [78]: df
Out[78]: 
          A                   B              C     
          O         I         O         I    O    I
n  0.387021  1.633022 -1.244983  6.556214  1.0  1.0
m -0.240860 -0.974279  1.741358 -1.963577  1.0  1.0
    Slicing Slicing a MultiIndex with xs 
In [79]: coords = [("AA", "one"), ("AA", "six"), ("BB", "one"), ("BB", "two"), ("BB", "six")]

In [80]: index = pd.MultiIndex.from_tuples(coords)

In [81]: df = pd.DataFrame([11, 22, 33, 44, 55], index, ["MyData"])

In [82]: df
Out[82]: 
        MyData
AA one      11
   six      22
BB one      33
   two      44
   six      55
  To take the cross section of the 1st level and 1st axis the index: 
# Note : level and axis are optional, and default to zero
In [83]: df.xs("BB", level=0, axis=0)
Out[83]: 
     MyData
one      33
two      44
six      55
  …and now the 2nd level of the 1st axis. 
In [84]: df.xs("six", level=1, axis=0)
Out[84]: 
    MyData
AA      22
BB      55
  Slicing a MultiIndex with xs, method #2 
In [85]: import itertools

In [86]: index = list(itertools.product(["Ada", "Quinn", "Violet"], ["Comp", "Math", "Sci"]))

In [87]: headr = list(itertools.product(["Exams", "Labs"], ["I", "II"]))

In [88]: indx = pd.MultiIndex.from_tuples(index, names=["Student", "Course"])

In [89]: cols = pd.MultiIndex.from_tuples(headr)  # Notice these are un-named

In [90]: data = [[70 + x + y + (x * y) % 3 for x in range(4)] for y in range(9)]

In [91]: df = pd.DataFrame(data, indx, cols)

In [92]: df
Out[92]: 
               Exams     Labs    
                   I  II    I  II
Student Course                   
Ada     Comp      70  71   72  73
        Math      71  73   75  74
        Sci       72  75   75  75
Quinn   Comp      73  74   75  76
        Math      74  76   78  77
        Sci       75  78   78  78
Violet  Comp      76  77   78  79
        Math      77  79   81  80
        Sci       78  81   81  81

In [93]: All = slice(None)

In [94]: df.loc["Violet"]
Out[94]: 
       Exams     Labs    
           I  II    I  II
Course                   
Comp      76  77   78  79
Math      77  79   81  80
Sci       78  81   81  81

In [95]: df.loc[(All, "Math"), All]
Out[95]: 
               Exams     Labs    
                   I  II    I  II
Student Course                   
Ada     Math      71  73   75  74
Quinn   Math      74  76   78  77
Violet  Math      77  79   81  80

In [96]: df.loc[(slice("Ada", "Quinn"), "Math"), All]
Out[96]: 
               Exams     Labs    
                   I  II    I  II
Student Course                   
Ada     Math      71  73   75  74
Quinn   Math      74  76   78  77

In [97]: df.loc[(All, "Math"), ("Exams")]
Out[97]: 
                 I  II
Student Course        
Ada     Math    71  73
Quinn   Math    74  76
Violet  Math    77  79

In [98]: df.loc[(All, "Math"), (All, "II")]
Out[98]: 
               Exams Labs
                  II   II
Student Course           
Ada     Math      73   74
Quinn   Math      76   77
Violet  Math      79   80
  Setting portions of a MultiIndex with xs   Sorting Sort by specific column or an ordered list of columns, with a MultiIndex 
In [99]: df.sort_values(by=("Labs", "II"), ascending=False)
Out[99]: 
               Exams     Labs    
                   I  II    I  II
Student Course                   
Violet  Sci       78  81   81  81
        Math      77  79   81  80
        Comp      76  77   78  79
Quinn   Sci       75  78   78  78
        Math      74  76   78  77
        Comp      73  74   75  76
Ada     Sci       72  75   75  75
        Math      71  73   75  74
        Comp      70  71   72  73
  Partial selection, the need for sortedness GH2995   Levels Prepending a level to a multiindex Flatten Hierarchical columns    Missing data The missing data docs. Fill forward a reversed timeseries 
In [100]: df = pd.DataFrame(
   .....:     np.random.randn(6, 1),
   .....:     index=pd.date_range("2013-08-01", periods=6, freq="B"),
   .....:     columns=list("A"),
   .....: )
   .....: 

In [101]: df.loc[df.index[3], "A"] = np.nan

In [102]: df
Out[102]: 
                   A
2013-08-01  0.721555
2013-08-02 -0.706771
2013-08-05 -1.039575
2013-08-06       NaN
2013-08-07 -0.424972
2013-08-08  0.567020

In [103]: df.reindex(df.index[::-1]).ffill()
Out[103]: 
                   A
2013-08-08  0.567020
2013-08-07 -0.424972
2013-08-06 -0.424972
2013-08-05 -1.039575
2013-08-02 -0.706771
2013-08-01  0.721555
  cumsum reset at NaN values  Replace Using replace with backrefs    Grouping The grouping docs. Basic grouping with apply Unlike agg, apply’s callable is passed a sub-DataFrame which gives you access to all the columns 
In [104]: df = pd.DataFrame(
   .....:     {
   .....:         "animal": "cat dog cat fish dog cat cat".split(),
   .....:         "size": list("SSMMMLL"),
   .....:         "weight": [8, 10, 11, 1, 20, 12, 12],
   .....:         "adult": [False] * 5 + [True] * 2,
   .....:     }
   .....: )
   .....: 

In [105]: df
Out[105]: 
  animal size  weight  adult
0    cat    S       8  False
1    dog    S      10  False
2    cat    M      11  False
3   fish    M       1  False
4    dog    M      20  False
5    cat    L      12   True
6    cat    L      12   True

# List the size of the animals with the highest weight.
In [106]: df.groupby("animal").apply(lambda subf: subf["size"][subf["weight"].idxmax()])
Out[106]: 
animal
cat     L
dog     M
fish    M
dtype: object
  Using get_group 
In [107]: gb = df.groupby(["animal"])

In [108]: gb.get_group("cat")
Out[108]: 
  animal size  weight  adult
0    cat    S       8  False
2    cat    M      11  False
5    cat    L      12   True
6    cat    L      12   True
  Apply to different items in a group 
In [109]: def GrowUp(x):
   .....:     avg_weight = sum(x[x["size"] == "S"].weight * 1.5)
   .....:     avg_weight += sum(x[x["size"] == "M"].weight * 1.25)
   .....:     avg_weight += sum(x[x["size"] == "L"].weight)
   .....:     avg_weight /= len(x)
   .....:     return pd.Series(["L", avg_weight, True], index=["size", "weight", "adult"])
   .....: 

In [110]: expected_df = gb.apply(GrowUp)

In [111]: expected_df
Out[111]: 
       size   weight  adult
animal                     
cat       L  12.4375   True
dog       L  20.0000   True
fish      L   1.2500   True
  Expanding apply 
In [112]: S = pd.Series([i / 100.0 for i in range(1, 11)])

In [113]: def cum_ret(x, y):
   .....:     return x * (1 + y)
   .....: 

In [114]: def red(x):
   .....:     return functools.reduce(cum_ret, x, 1.0)
   .....: 

In [115]: S.expanding().apply(red, raw=True)
Out[115]: 
0    1.010000
1    1.030200
2    1.061106
3    1.103550
4    1.158728
5    1.228251
6    1.314229
7    1.419367
8    1.547110
9    1.701821
dtype: float64
  Replacing some values with mean of the rest of a group 
In [116]: df = pd.DataFrame({"A": [1, 1, 2, 2], "B": [1, -1, 1, 2]})

In [117]: gb = df.groupby("A")

In [118]: def replace(g):
   .....:     mask = g < 0
   .....:     return g.where(mask, g[~mask].mean())
   .....: 

In [119]: gb.transform(replace)
Out[119]: 
     B
0  1.0
1 -1.0
2  1.5
3  1.5
  Sort groups by aggregated data 
In [120]: df = pd.DataFrame(
   .....:     {
   .....:         "code": ["foo", "bar", "baz"] * 2,
   .....:         "data": [0.16, -0.21, 0.33, 0.45, -0.59, 0.62],
   .....:         "flag": [False, True] * 3,
   .....:     }
   .....: )
   .....: 

In [121]: code_groups = df.groupby("code")

In [122]: agg_n_sort_order = code_groups[["data"]].transform(sum).sort_values(by="data")

In [123]: sorted_df = df.loc[agg_n_sort_order.index]

In [124]: sorted_df
Out[124]: 
  code  data   flag
1  bar -0.21   True
4  bar -0.59  False
0  foo  0.16  False
3  foo  0.45   True
2  baz  0.33  False
5  baz  0.62   True
  Create multiple aggregated columns 
In [125]: rng = pd.date_range(start="2014-10-07", periods=10, freq="2min")

In [126]: ts = pd.Series(data=list(range(10)), index=rng)

In [127]: def MyCust(x):
   .....:     if len(x) > 2:
   .....:         return x[1] * 1.234
   .....:     return pd.NaT
   .....: 

In [128]: mhc = {"Mean": np.mean, "Max": np.max, "Custom": MyCust}

In [129]: ts.resample("5min").apply(mhc)
Out[129]: 
                     Mean  Max Custom
2014-10-07 00:00:00   1.0    2  1.234
2014-10-07 00:05:00   3.5    4    NaT
2014-10-07 00:10:00   6.0    7  7.404
2014-10-07 00:15:00   8.5    9    NaT

In [130]: ts
Out[130]: 
2014-10-07 00:00:00    0
2014-10-07 00:02:00    1
2014-10-07 00:04:00    2
2014-10-07 00:06:00    3
2014-10-07 00:08:00    4
2014-10-07 00:10:00    5
2014-10-07 00:12:00    6
2014-10-07 00:14:00    7
2014-10-07 00:16:00    8
2014-10-07 00:18:00    9
Freq: 2T, dtype: int64
  Create a value counts column and reassign back to the DataFrame 
In [131]: df = pd.DataFrame(
   .....:     {"Color": "Red Red Red Blue".split(), "Value": [100, 150, 50, 50]}
   .....: )
   .....: 

In [132]: df
Out[132]: 
  Color  Value
0   Red    100
1   Red    150
2   Red     50
3  Blue     50

In [133]: df["Counts"] = df.groupby(["Color"]).transform(len)

In [134]: df
Out[134]: 
  Color  Value  Counts
0   Red    100       3
1   Red    150       3
2   Red     50       3
3  Blue     50       1
  Shift groups of the values in a column based on the index 
In [135]: df = pd.DataFrame(
   .....:     {"line_race": [10, 10, 8, 10, 10, 8], "beyer": [99, 102, 103, 103, 88, 100]},
   .....:     index=[
   .....:         "Last Gunfighter",
   .....:         "Last Gunfighter",
   .....:         "Last Gunfighter",
   .....:         "Paynter",
   .....:         "Paynter",
   .....:         "Paynter",
   .....:     ],
   .....: )
   .....: 

In [136]: df
Out[136]: 
                 line_race  beyer
Last Gunfighter         10     99
Last Gunfighter         10    102
Last Gunfighter          8    103
Paynter                 10    103
Paynter                 10     88
Paynter                  8    100

In [137]: df["beyer_shifted"] = df.groupby(level=0)["beyer"].shift(1)

In [138]: df
Out[138]: 
                 line_race  beyer  beyer_shifted
Last Gunfighter         10     99            NaN
Last Gunfighter         10    102           99.0
Last Gunfighter          8    103          102.0
Paynter                 10    103            NaN
Paynter                 10     88          103.0
Paynter                  8    100           88.0
  Select row with maximum value from each group 
In [139]: df = pd.DataFrame(
   .....:     {
   .....:         "host": ["other", "other", "that", "this", "this"],
   .....:         "service": ["mail", "web", "mail", "mail", "web"],
   .....:         "no": [1, 2, 1, 2, 1],
   .....:     }
   .....: ).set_index(["host", "service"])
   .....: 

In [140]: mask = df.groupby(level=0).agg("idxmax")

In [141]: df_count = df.loc[mask["no"]].reset_index()

In [142]: df_count
Out[142]: 
    host service  no
0  other     web   2
1   that    mail   1
2   this    mail   2
  Grouping like Python’s itertools.groupby 
In [143]: df = pd.DataFrame([0, 1, 0, 1, 1, 1, 0, 1, 1], columns=["A"])

In [144]: df["A"].groupby((df["A"] != df["A"].shift()).cumsum()).groups
Out[144]: {1: [0], 2: [1], 3: [2], 4: [3, 4, 5], 5: [6], 6: [7, 8]}

In [145]: df["A"].groupby((df["A"] != df["A"].shift()).cumsum()).cumsum()
Out[145]: 
0    0
1    1
2    0
3    1
4    2
5    3
6    0
7    1
8    2
Name: A, dtype: int64
   Expanding data Alignment and to-date Rolling Computation window based on values instead of counts Rolling Mean by Time Interval   Splitting Splitting a frame Create a list of dataframes, split using a delineation based on logic included in rows. 
In [146]: df = pd.DataFrame(
   .....:     data={
   .....:         "Case": ["A", "A", "A", "B", "A", "A", "B", "A", "A"],
   .....:         "Data": np.random.randn(9),
   .....:     }
   .....: )
   .....: 

In [147]: dfs = list(
   .....:     zip(
   .....:         *df.groupby(
   .....:             (1 * (df["Case"] == "B"))
   .....:             .cumsum()
   .....:             .rolling(window=3, min_periods=1)
   .....:             .median()
   .....:         )
   .....:     )
   .....: )[-1]
   .....: 

In [148]: dfs[0]
Out[148]: 
  Case      Data
0    A  0.276232
1    A -1.087401
2    A -0.673690
3    B  0.113648

In [149]: dfs[1]
Out[149]: 
  Case      Data
4    A -1.478427
5    A  0.524988
6    B  0.404705

In [150]: dfs[2]
Out[150]: 
  Case      Data
7    A  0.577046
8    A -1.715002
    Pivot The Pivot docs. Partial sums and subtotals 
In [151]: df = pd.DataFrame(
   .....:     data={
   .....:         "Province": ["ON", "QC", "BC", "AL", "AL", "MN", "ON"],
   .....:         "City": [
   .....:             "Toronto",
   .....:             "Montreal",
   .....:             "Vancouver",
   .....:             "Calgary",
   .....:             "Edmonton",
   .....:             "Winnipeg",
   .....:             "Windsor",
   .....:         ],
   .....:         "Sales": [13, 6, 16, 8, 4, 3, 1],
   .....:     }
   .....: )
   .....: 

In [152]: table = pd.pivot_table(
   .....:     df,
   .....:     values=["Sales"],
   .....:     index=["Province"],
   .....:     columns=["City"],
   .....:     aggfunc=np.sum,
   .....:     margins=True,
   .....: )
   .....: 

In [153]: table.stack("City")
Out[153]: 
                    Sales
Province City            
AL       All         12.0
         Calgary      8.0
         Edmonton     4.0
BC       All         16.0
         Vancouver   16.0
...                   ...
All      Montreal     6.0
         Toronto     13.0
         Vancouver   16.0
         Windsor      1.0
         Winnipeg     3.0

[20 rows x 1 columns]
  Frequency table like plyr in R 
In [154]: grades = [48, 99, 75, 80, 42, 80, 72, 68, 36, 78]

In [155]: df = pd.DataFrame(
   .....:     {
   .....:         "ID": ["x%d" % r for r in range(10)],
   .....:         "Gender": ["F", "M", "F", "M", "F", "M", "F", "M", "M", "M"],
   .....:         "ExamYear": [
   .....:             "2007",
   .....:             "2007",
   .....:             "2007",
   .....:             "2008",
   .....:             "2008",
   .....:             "2008",
   .....:             "2008",
   .....:             "2009",
   .....:             "2009",
   .....:             "2009",
   .....:         ],
   .....:         "Class": [
   .....:             "algebra",
   .....:             "stats",
   .....:             "bio",
   .....:             "algebra",
   .....:             "algebra",
   .....:             "stats",
   .....:             "stats",
   .....:             "algebra",
   .....:             "bio",
   .....:             "bio",
   .....:         ],
   .....:         "Participated": [
   .....:             "yes",
   .....:             "yes",
   .....:             "yes",
   .....:             "yes",
   .....:             "no",
   .....:             "yes",
   .....:             "yes",
   .....:             "yes",
   .....:             "yes",
   .....:             "yes",
   .....:         ],
   .....:         "Passed": ["yes" if x > 50 else "no" for x in grades],
   .....:         "Employed": [
   .....:             True,
   .....:             True,
   .....:             True,
   .....:             False,
   .....:             False,
   .....:             False,
   .....:             False,
   .....:             True,
   .....:             True,
   .....:             False,
   .....:         ],
   .....:         "Grade": grades,
   .....:     }
   .....: )
   .....: 

In [156]: df.groupby("ExamYear").agg(
   .....:     {
   .....:         "Participated": lambda x: x.value_counts()["yes"],
   .....:         "Passed": lambda x: sum(x == "yes"),
   .....:         "Employed": lambda x: sum(x),
   .....:         "Grade": lambda x: sum(x) / len(x),
   .....:     }
   .....: )
   .....: 
Out[156]: 
          Participated  Passed  Employed      Grade
ExamYear                                           
2007                 3       2         3  74.000000
2008                 3       3         0  68.500000
2009                 3       2         2  60.666667
  Plot pandas DataFrame with year over year data To create year and month cross tabulation: 
In [157]: df = pd.DataFrame(
   .....:     {"value": np.random.randn(36)},
   .....:     index=pd.date_range("2011-01-01", freq="M", periods=36),
   .....: )
   .....: 

In [158]: pd.pivot_table(
   .....:     df, index=df.index.month, columns=df.index.year, values="value", aggfunc="sum"
   .....: )
   .....: 
Out[158]: 
        2011      2012      2013
1  -1.039268 -0.968914  2.565646
2  -0.370647 -1.294524  1.431256
3  -1.157892  0.413738  1.340309
4  -1.344312  0.276662 -1.170299
5   0.844885 -0.472035 -0.226169
6   1.075770 -0.013960  0.410835
7  -0.109050 -0.362543  0.813850
8   1.643563 -0.006154  0.132003
9  -1.469388 -0.923061 -0.827317
10  0.357021  0.895717 -0.076467
11 -0.674600  0.805244 -1.187678
12 -1.776904 -1.206412  1.130127
    Apply Rolling apply to organize - Turning embedded lists into a MultiIndex frame 
In [159]: df = pd.DataFrame(
   .....:     data={
   .....:         "A": [[2, 4, 8, 16], [100, 200], [10, 20, 30]],
   .....:         "B": [["a", "b", "c"], ["jj", "kk"], ["ccc"]],
   .....:     },
   .....:     index=["I", "II", "III"],
   .....: )
   .....: 

In [160]: def SeriesFromSubList(aList):
   .....:     return pd.Series(aList)
   .....: 

In [161]: df_orgz = pd.concat(
   .....:     {ind: row.apply(SeriesFromSubList) for ind, row in df.iterrows()}
   .....: )
   .....: 

In [162]: df_orgz
Out[162]: 
         0     1     2     3
I   A    2     4     8  16.0
    B    a     b     c   NaN
II  A  100   200   NaN   NaN
    B   jj    kk   NaN   NaN
III A   10  20.0  30.0   NaN
    B  ccc   NaN   NaN   NaN
  Rolling apply with a DataFrame returning a Series Rolling Apply to multiple columns where function calculates a Series before a Scalar from the Series is returned 
In [163]: df = pd.DataFrame(
   .....:     data=np.random.randn(2000, 2) / 10000,
   .....:     index=pd.date_range("2001-01-01", periods=2000),
   .....:     columns=["A", "B"],
   .....: )
   .....: 

In [164]: df
Out[164]: 
                   A         B
2001-01-01 -0.000144 -0.000141
2001-01-02  0.000161  0.000102
2001-01-03  0.000057  0.000088
2001-01-04 -0.000221  0.000097
2001-01-05 -0.000201 -0.000041
...              ...       ...
2006-06-19  0.000040 -0.000235
2006-06-20 -0.000123 -0.000021
2006-06-21 -0.000113  0.000114
2006-06-22  0.000136  0.000109
2006-06-23  0.000027  0.000030

[2000 rows x 2 columns]

In [165]: def gm(df, const):
   .....:     v = ((((df["A"] + df["B"]) + 1).cumprod()) - 1) * const
   .....:     return v.iloc[-1]
   .....: 

In [166]: s = pd.Series(
   .....:     {
   .....:         df.index[i]: gm(df.iloc[i: min(i + 51, len(df) - 1)], 5)
   .....:         for i in range(len(df) - 50)
   .....:     }
   .....: )
   .....: 

In [167]: s
Out[167]: 
2001-01-01    0.000930
2001-01-02    0.002615
2001-01-03    0.001281
2001-01-04    0.001117
2001-01-05    0.002772
                ...   
2006-04-30    0.003296
2006-05-01    0.002629
2006-05-02    0.002081
2006-05-03    0.004247
2006-05-04    0.003928
Length: 1950, dtype: float64
  Rolling apply with a DataFrame returning a Scalar Rolling Apply to multiple columns where function returns a Scalar (Volume Weighted Average Price) 
In [168]: rng = pd.date_range(start="2014-01-01", periods=100)

In [169]: df = pd.DataFrame(
   .....:     {
   .....:         "Open": np.random.randn(len(rng)),
   .....:         "Close": np.random.randn(len(rng)),
   .....:         "Volume": np.random.randint(100, 2000, len(rng)),
   .....:     },
   .....:     index=rng,
   .....: )
   .....: 

In [170]: df
Out[170]: 
                Open     Close  Volume
2014-01-01 -1.611353 -0.492885    1219
2014-01-02 -3.000951  0.445794    1054
2014-01-03 -0.138359 -0.076081    1381
2014-01-04  0.301568  1.198259    1253
2014-01-05  0.276381 -0.669831    1728
...              ...       ...     ...
2014-04-06 -0.040338  0.937843    1188
2014-04-07  0.359661 -0.285908    1864
2014-04-08  0.060978  1.714814     941
2014-04-09  1.759055 -0.455942    1065
2014-04-10  0.138185 -1.147008    1453

[100 rows x 3 columns]

In [171]: def vwap(bars):
   .....:     return (bars.Close * bars.Volume).sum() / bars.Volume.sum()
   .....: 

In [172]: window = 5

In [173]: s = pd.concat(
   .....:     [
   .....:         (pd.Series(vwap(df.iloc[i: i + window]), index=[df.index[i + window]]))
   .....:         for i in range(len(df) - window)
   .....:     ]
   .....: )
   .....: 

In [174]: s.round(2)
Out[174]: 
2014-01-06    0.02
2014-01-07    0.11
2014-01-08    0.10
2014-01-09    0.07
2014-01-10   -0.29
              ... 
2014-04-06   -0.63
2014-04-07   -0.02
2014-04-08   -0.03
2014-04-09    0.34
2014-04-10    0.29
Length: 95, dtype: float64
     Timeseries Between times Using indexer between time Constructing a datetime range that excludes weekends and includes only certain times Vectorized Lookup Aggregation and plotting time series Turn a matrix with hours in columns and days in rows into a continuous row sequence in the form of a time series. How to rearrange a Python pandas DataFrame? Dealing with duplicates when reindexing a timeseries to a specified frequency Calculate the first day of the month for each entry in a DatetimeIndex 
In [175]: dates = pd.date_range("2000-01-01", periods=5)

In [176]: dates.to_period(freq="M").to_timestamp()
Out[176]: 
DatetimeIndex(['2000-01-01', '2000-01-01', '2000-01-01', '2000-01-01',
               '2000-01-01'],
              dtype='datetime64[ns]', freq=None)
   Resampling The Resample docs. Using Grouper instead of TimeGrouper for time grouping of values Time grouping with some missing values Valid frequency arguments to Grouper Timeseries Grouping using a MultiIndex Using TimeGrouper and another grouping to create subgroups, then apply a custom function GH3791 Resampling with custom periods Resample intraday frame without adding new days Resample minute data Resample with groupby    Merge The Join docs. Concatenate two dataframes with overlapping index (emulate R rbind) 
In [177]: rng = pd.date_range("2000-01-01", periods=6)

In [178]: df1 = pd.DataFrame(np.random.randn(6, 3), index=rng, columns=["A", "B", "C"])

In [179]: df2 = df1.copy()
  Depending on df construction, ignore_index may be needed 
In [180]: df = pd.concat([df1, df2], ignore_index=True)

In [181]: df
Out[181]: 
           A         B         C
0  -0.870117 -0.479265 -0.790855
1   0.144817  1.726395 -0.464535
2  -0.821906  1.597605  0.187307
3  -0.128342 -1.511638 -0.289858
4   0.399194 -1.430030 -0.639760
5   1.115116 -2.012600  1.810662
6  -0.870117 -0.479265 -0.790855
7   0.144817  1.726395 -0.464535
8  -0.821906  1.597605  0.187307
9  -0.128342 -1.511638 -0.289858
10  0.399194 -1.430030 -0.639760
11  1.115116 -2.012600  1.810662
  Self Join of a DataFrame GH2996 
In [182]: df = pd.DataFrame(
   .....:     data={
   .....:         "Area": ["A"] * 5 + ["C"] * 2,
   .....:         "Bins": [110] * 2 + [160] * 3 + [40] * 2,
   .....:         "Test_0": [0, 1, 0, 1, 2, 0, 1],
   .....:         "Data": np.random.randn(7),
   .....:     }
   .....: )
   .....: 

In [183]: df
Out[183]: 
  Area  Bins  Test_0      Data
0    A   110       0 -0.433937
1    A   110       1 -0.160552
2    A   160       0  0.744434
3    A   160       1  1.754213
4    A   160       2  0.000850
5    C    40       0  0.342243
6    C    40       1  1.070599

In [184]: df["Test_1"] = df["Test_0"] - 1

In [185]: pd.merge(
   .....:     df,
   .....:     df,
   .....:     left_on=["Bins", "Area", "Test_0"],
   .....:     right_on=["Bins", "Area", "Test_1"],
   .....:     suffixes=("_L", "_R"),
   .....: )
   .....: 
Out[185]: 
  Area  Bins  Test_0_L    Data_L  Test_1_L  Test_0_R    Data_R  Test_1_R
0    A   110         0 -0.433937        -1         1 -0.160552         0
1    A   160         0  0.744434        -1         1  1.754213         0
2    A   160         1  1.754213         0         2  0.000850         1
3    C    40         0  0.342243        -1         1  1.070599         0
  How to set the index and join KDB like asof join Join with a criteria based on the values Using searchsorted to merge based on values inside a range   Plotting The Plotting docs. Make Matplotlib look like R Setting x-axis major and minor labels Plotting multiple charts in an IPython Jupyter notebook Creating a multi-line plot Plotting a heatmap Annotate a time-series plot Annotate a time-series plot #2 Generate Embedded plots in excel files using Pandas, Vincent and xlsxwriter Boxplot for each quartile of a stratifying variable 
In [186]: df = pd.DataFrame(
   .....:     {
   .....:         "stratifying_var": np.random.uniform(0, 100, 20),
   .....:         "price": np.random.normal(100, 5, 20),
   .....:     }
   .....: )
   .....: 

In [187]: df["quartiles"] = pd.qcut(
   .....:     df["stratifying_var"], 4, labels=["0-25%", "25-50%", "50-75%", "75-100%"]
   .....: )
   .....: 

In [188]: df.boxplot(column="price", by="quartiles")
Out[188]: <AxesSubplot:title={'center':'price'}, xlabel='quartiles'>
     Data in/out Performance comparison of SQL vs HDF5  CSV The CSV docs read_csv in action appending to a csv Reading a csv chunk-by-chunk Reading only certain rows of a csv chunk-by-chunk Reading the first few lines of a frame Reading a file that is compressed but not by gzip/bz2 (the native compressed formats which read_csv understands). This example shows a WinZipped file, but is a general application of opening the file within a context manager and using that handle to read. See here Inferring dtypes from a file Dealing with bad lines GH2886 Write a multi-row index CSV without writing duplicates  Reading multiple files to create a single DataFrame The best way to combine multiple files into a single DataFrame is to read the individual frames one by one, put all of the individual frames into a list, and then combine the frames in the list using pd.concat(): 
In [189]: for i in range(3):
   .....:     data = pd.DataFrame(np.random.randn(10, 4))
   .....:     data.to_csv("file_{}.csv".format(i))
   .....: 

In [190]: files = ["file_0.csv", "file_1.csv", "file_2.csv"]

In [191]: result = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)
  You can use the same approach to read all files matching a pattern. Here is an example using glob: 
In [192]: import glob

In [193]: import os

In [194]: files = glob.glob("file_*.csv")

In [195]: result = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)
  Finally, this strategy will work with the other pd.read_*(...) functions described in the io docs.   Parsing date components in multi-columns Parsing date components in multi-columns is faster with a format 
In [196]: i = pd.date_range("20000101", periods=10000)

In [197]: df = pd.DataFrame({"year": i.year, "month": i.month, "day": i.day})

In [198]: df.head()
Out[198]: 
   year  month  day
0  2000      1    1
1  2000      1    2
2  2000      1    3
3  2000      1    4
4  2000      1    5

In [199]: %timeit pd.to_datetime(df.year * 10000 + df.month * 100 + df.day, format='%Y%m%d')
   .....: ds = df.apply(lambda x: "%04d%02d%02d" % (x["year"], x["month"], x["day"]), axis=1)
   .....: ds.head()
   .....: %timeit pd.to_datetime(ds)
   .....: 
8.7 ms +- 765 us per loop (mean +- std. dev. of 7 runs, 100 loops each)
2.1 ms +- 419 us per loop (mean +- std. dev. of 7 runs, 100 loops each)
    Skip row between header and data 
In [200]: data = """;;;;
   .....:  ;;;;
   .....:  ;;;;
   .....:  ;;;;
   .....:  ;;;;
   .....:  ;;;;
   .....: ;;;;
   .....:  ;;;;
   .....:  ;;;;
   .....: ;;;;
   .....: date;Param1;Param2;Param4;Param5
   .....:     ;m²;°C;m²;m
   .....: ;;;;
   .....: 01.01.1990 00:00;1;1;2;3
   .....: 01.01.1990 01:00;5;3;4;5
   .....: 01.01.1990 02:00;9;5;6;7
   .....: 01.01.1990 03:00;13;7;8;9
   .....: 01.01.1990 04:00;17;9;10;11
   .....: 01.01.1990 05:00;21;11;12;13
   .....: """
   .....: 
   Option 1: pass rows explicitly to skip rows 
In [201]: from io import StringIO

In [202]: pd.read_csv(
   .....:     StringIO(data),
   .....:     sep=";",
   .....:     skiprows=[11, 12],
   .....:     index_col=0,
   .....:     parse_dates=True,
   .....:     header=10,
   .....: )
   .....: 
Out[202]: 
                     Param1  Param2  Param4  Param5
date                                               
1990-01-01 00:00:00       1       1       2       3
1990-01-01 01:00:00       5       3       4       5
1990-01-01 02:00:00       9       5       6       7
1990-01-01 03:00:00      13       7       8       9
1990-01-01 04:00:00      17       9      10      11
1990-01-01 05:00:00      21      11      12      13
    Option 2: read column names and then data 
In [203]: pd.read_csv(StringIO(data), sep=";", header=10, nrows=10).columns
Out[203]: Index(['date', 'Param1', 'Param2', 'Param4', 'Param5'], dtype='object')

In [204]: columns = pd.read_csv(StringIO(data), sep=";", header=10, nrows=10).columns

In [205]: pd.read_csv(
   .....:     StringIO(data), sep=";", index_col=0, header=12, parse_dates=True, names=columns
   .....: )
   .....: 
Out[205]: 
                     Param1  Param2  Param4  Param5
date                                               
1990-01-01 00:00:00       1       1       2       3
1990-01-01 01:00:00       5       3       4       5
1990-01-01 02:00:00       9       5       6       7
1990-01-01 03:00:00      13       7       8       9
1990-01-01 04:00:00      17       9      10      11
1990-01-01 05:00:00      21      11      12      13
      SQL The SQL docs Reading from databases with SQL   Excel The Excel docs Reading from a filelike handle Modifying formatting in XlsxWriter output Loading only visible sheets GH19842#issuecomment-892150745   HTML Reading HTML tables from a server that cannot handle the default request header   HDFStore The HDFStores docs Simple queries with a Timestamp Index Managing heterogeneous data using a linked multiple table hierarchy GH3032 Merging on-disk tables with millions of rows Avoiding inconsistencies when writing to a store from multiple processes/threads De-duplicating a large store by chunks, essentially a recursive reduction operation. Shows a function for taking in data from csv file and creating a store by chunks, with date parsing as well. See here Creating a store chunk-by-chunk from a csv file Appending to a store, while creating a unique index Large Data work flows Reading in a sequence of files, then providing a global unique index to a store while appending Groupby on a HDFStore with low group density Groupby on a HDFStore with high group density Hierarchical queries on a HDFStore Counting with a HDFStore Troubleshoot HDFStore exceptions Setting min_itemsize with strings Using ptrepack to create a completely-sorted-index on a store Storing Attributes to a group node 
In [206]: df = pd.DataFrame(np.random.randn(8, 3))

In [207]: store = pd.HDFStore("test.h5")

In [208]: store.put("df", df)

# you can store an arbitrary Python object via pickle
In [209]: store.get_storer("df").attrs.my_attribute = {"A": 10}

In [210]: store.get_storer("df").attrs.my_attribute
Out[210]: {'A': 10}
  You can create or load a HDFStore in-memory by passing the driver parameter to PyTables. Changes are only written to disk when the HDFStore is closed. 
In [211]: store = pd.HDFStore("test.h5", "w", driver="H5FD_CORE")

In [212]: df = pd.DataFrame(np.random.randn(8, 3))

In [213]: store["test"] = df

# only after closing the store, data is written to disk:
In [214]: store.close()
    Binary files pandas readily accepts NumPy record arrays, if you need to read in a binary file consisting of an array of C structs. For example, given this C program in a file called main.c compiled with gcc main.c -std=gnu99 on a 64-bit machine, 
#include <stdio.h>
#include <stdint.h>

typedef struct _Data
{
    int32_t count;
    double avg;
    float scale;
} Data;

int main(int argc, const char *argv[])
{
    size_t n = 10;
    Data d[n];

    for (int i = 0; i < n; ++i)
    {
        d[i].count = i;
        d[i].avg = i + 1.0;
        d[i].scale = (float) i + 2.0f;
    }

    FILE *file = fopen("binary.dat", "wb");
    fwrite(&d, sizeof(Data), n, file);
    fclose(file);

    return 0;
}
  the following Python code will read the binary file 'binary.dat' into a pandas DataFrame, where each element of the struct corresponds to a column in the frame: 
names = "count", "avg", "scale"

# note that the offsets are larger than the size of the type because of
# struct padding
offsets = 0, 8, 16
formats = "i4", "f8", "f4"
dt = np.dtype({"names": names, "offsets": offsets, "formats": formats}, align=True)
df = pd.DataFrame(np.fromfile("binary.dat", dt))
   Note The offsets of the structure elements may be different depending on the architecture of the machine on which the file was created. Using a raw binary file format like this for general data storage is not recommended, as it is not cross platform. We recommended either HDF5 or parquet, both of which are supported by pandas’ IO facilities.     Computation Numerical integration (sample-based) of a time series  Correlation Often it’s useful to obtain the lower (or upper) triangular form of a correlation matrix calculated from DataFrame.corr(). This can be achieved by passing a boolean mask to where as follows: 
In [215]: df = pd.DataFrame(np.random.random(size=(100, 5)))

In [216]: corr_mat = df.corr()

In [217]: mask = np.tril(np.ones_like(corr_mat, dtype=np.bool_), k=-1)

In [218]: corr_mat.where(mask)
Out[218]: 
          0         1         2        3   4
0       NaN       NaN       NaN      NaN NaN
1 -0.079861       NaN       NaN      NaN NaN
2 -0.236573  0.183801       NaN      NaN NaN
3 -0.013795 -0.051975  0.037235      NaN NaN
4 -0.031974  0.118342 -0.073499 -0.02063 NaN
  The method argument within DataFrame.corr can accept a callable in addition to the named correlation types. Here we compute the distance correlation matrix for a DataFrame object. 
In [219]: def distcorr(x, y):
   .....:     n = len(x)
   .....:     a = np.zeros(shape=(n, n))
   .....:     b = np.zeros(shape=(n, n))
   .....:     for i in range(n):
   .....:         for j in range(i + 1, n):
   .....:             a[i, j] = abs(x[i] - x[j])
   .....:             b[i, j] = abs(y[i] - y[j])
   .....:     a += a.T
   .....:     b += b.T
   .....:     a_bar = np.vstack([np.nanmean(a, axis=0)] * n)
   .....:     b_bar = np.vstack([np.nanmean(b, axis=0)] * n)
   .....:     A = a - a_bar - a_bar.T + np.full(shape=(n, n), fill_value=a_bar.mean())
   .....:     B = b - b_bar - b_bar.T + np.full(shape=(n, n), fill_value=b_bar.mean())
   .....:     cov_ab = np.sqrt(np.nansum(A * B)) / n
   .....:     std_a = np.sqrt(np.sqrt(np.nansum(A ** 2)) / n)
   .....:     std_b = np.sqrt(np.sqrt(np.nansum(B ** 2)) / n)
   .....:     return cov_ab / std_a / std_b
   .....: 

In [220]: df = pd.DataFrame(np.random.normal(size=(100, 3)))

In [221]: df.corr(method=distcorr)
Out[221]: 
          0         1         2
0  1.000000  0.197613  0.216328
1  0.197613  1.000000  0.208749
2  0.216328  0.208749  1.000000
     Timedeltas The Timedeltas docs. Using timedeltas 
In [222]: import datetime

In [223]: s = pd.Series(pd.date_range("2012-1-1", periods=3, freq="D"))

In [224]: s - s.max()
Out[224]: 
0   -2 days
1   -1 days
2    0 days
dtype: timedelta64[ns]

In [225]: s.max() - s
Out[225]: 
0   2 days
1   1 days
2   0 days
dtype: timedelta64[ns]

In [226]: s - datetime.datetime(2011, 1, 1, 3, 5)
Out[226]: 
0   364 days 20:55:00
1   365 days 20:55:00
2   366 days 20:55:00
dtype: timedelta64[ns]

In [227]: s + datetime.timedelta(minutes=5)
Out[227]: 
0   2012-01-01 00:05:00
1   2012-01-02 00:05:00
2   2012-01-03 00:05:00
dtype: datetime64[ns]

In [228]: datetime.datetime(2011, 1, 1, 3, 5) - s
Out[228]: 
0   -365 days +03:05:00
1   -366 days +03:05:00
2   -367 days +03:05:00
dtype: timedelta64[ns]

In [229]: datetime.timedelta(minutes=5) + s
Out[229]: 
0   2012-01-01 00:05:00
1   2012-01-02 00:05:00
2   2012-01-03 00:05:00
dtype: datetime64[ns]
  Adding and subtracting deltas and dates 
In [230]: deltas = pd.Series([datetime.timedelta(days=i) for i in range(3)])

In [231]: df = pd.DataFrame({"A": s, "B": deltas})

In [232]: df
Out[232]: 
           A      B
0 2012-01-01 0 days
1 2012-01-02 1 days
2 2012-01-03 2 days

In [233]: df["New Dates"] = df["A"] + df["B"]

In [234]: df["Delta"] = df["A"] - df["New Dates"]

In [235]: df
Out[235]: 
           A      B  New Dates   Delta
0 2012-01-01 0 days 2012-01-01  0 days
1 2012-01-02 1 days 2012-01-03 -1 days
2 2012-01-03 2 days 2012-01-05 -2 days

In [236]: df.dtypes
Out[236]: 
A             datetime64[ns]
B            timedelta64[ns]
New Dates     datetime64[ns]
Delta        timedelta64[ns]
dtype: object
  Another example Values can be set to NaT using np.nan, similar to datetime 
In [237]: y = s - s.shift()

In [238]: y
Out[238]: 
0      NaT
1   1 days
2   1 days
dtype: timedelta64[ns]

In [239]: y[1] = np.nan

In [240]: y
Out[240]: 
0      NaT
1      NaT
2   1 days
dtype: timedelta64[ns]
    Creating example data To create a dataframe from every combination of some given values, like R’s expand.grid() function, we can create a dict where the keys are column names and the values are lists of the data values: 
In [241]: def expand_grid(data_dict):
   .....:     rows = itertools.product(*data_dict.values())
   .....:     return pd.DataFrame.from_records(rows, columns=data_dict.keys())
   .....: 

In [242]: df = expand_grid(
   .....:     {"height": [60, 70], "weight": [100, 140, 180], "sex": ["Male", "Female"]}
   .....: )
   .....: 

In [243]: df
Out[243]: 
    height  weight     sex
0       60     100    Male
1       60     100  Female
2       60     140    Male
3       60     140  Female
4       60     180    Male
5       60     180  Female
6       70     100    Male
7       70     100  Female
8       70     140    Male
9       70     140  Female
10      70     180    Male
11      70     180  Female
pandas.BooleanDtype   classpandas.BooleanDtype[source]
 
Extension dtype for boolean data.  New in version 1.0.0.   Warning BooleanDtype is considered experimental. The implementation and parts of the API may change without warning.  Examples 
>>> pd.BooleanDtype()
BooleanDtype
  Attributes       
None     Methods       
None
def f_21800169(df):
    """get index of rows in dataframe `df` which column 'BoolCol' matches value True
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
test.support.SAVEDCWD  
Set to os.getcwd().
numpy.random.RandomState.wald method   random.RandomState.wald(mean, scale, size=None)
 
Draw samples from a Wald, or inverse Gaussian, distribution. As the scale approaches infinity, the distribution becomes more like a Gaussian. Some references claim that the Wald is an inverse Gaussian with mean equal to 1, but this is by no means universal. The inverse Gaussian distribution was first studied in relationship to Brownian motion. In 1956 M.C.K. Tweedie used the name inverse Gaussian because there is an inverse relationship between the time to cover a unit distance and distance covered in unit time.  Note New code should use the wald method of a default_rng() instance instead; please see the Quick Start.   Parameters 
 
meanfloat or array_like of floats


Distribution mean, must be > 0.  
scalefloat or array_like of floats


Scale parameter, must be > 0.  
sizeint or tuple of ints, optional


Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if mean and scale are both scalars. Otherwise, np.broadcast(mean, scale).size samples are drawn.    Returns 
 
outndarray or scalar


Drawn samples from the parameterized Wald distribution.      See also  Generator.wald

which should be used for new code.    Notes The probability density function for the Wald distribution is  \[P(x;mean,scale) = \sqrt{\frac{scale}{2\pi x^3}}e^ \frac{-scale(x-mean)^2}{2\cdotp mean^2x}\] As noted above the inverse Gaussian distribution first arise from attempts to model Brownian motion. It is also a competitor to the Weibull for use in reliability modeling and modeling stock returns and interest rate processes. References  1 
Brighton Webs Ltd., Wald Distribution, https://web.archive.org/web/20090423014010/http://www.brighton-webs.co.uk:80/distributions/wald.asp  2 
Chhikara, Raj S., and Folks, J. Leroy, “The Inverse Gaussian Distribution: Theory : Methodology, and Applications”, CRC Press, 1988.  3 
Wikipedia, “Inverse Gaussian distribution” https://en.wikipedia.org/wiki/Inverse_Gaussian_distribution   Examples Draw values from the distribution and plot the histogram: >>> import matplotlib.pyplot as plt
>>> h = plt.hist(np.random.wald(3, 2, 100000), bins=200, density=True)
>>> plt.show()
numpy.random.wald   random.wald(mean, scale, size=None)
 
Draw samples from a Wald, or inverse Gaussian, distribution. As the scale approaches infinity, the distribution becomes more like a Gaussian. Some references claim that the Wald is an inverse Gaussian with mean equal to 1, but this is by no means universal. The inverse Gaussian distribution was first studied in relationship to Brownian motion. In 1956 M.C.K. Tweedie used the name inverse Gaussian because there is an inverse relationship between the time to cover a unit distance and distance covered in unit time.  Note New code should use the wald method of a default_rng() instance instead; please see the Quick Start.   Parameters 
 
meanfloat or array_like of floats


Distribution mean, must be > 0.  
scalefloat or array_like of floats


Scale parameter, must be > 0.  
sizeint or tuple of ints, optional


Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if mean and scale are both scalars. Otherwise, np.broadcast(mean, scale).size samples are drawn.    Returns 
 
outndarray or scalar


Drawn samples from the parameterized Wald distribution.      See also  Generator.wald

which should be used for new code.    Notes The probability density function for the Wald distribution is  \[P(x;mean,scale) = \sqrt{\frac{scale}{2\pi x^3}}e^ \frac{-scale(x-mean)^2}{2\cdotp mean^2x}\] As noted above the inverse Gaussian distribution first arise from attempts to model Brownian motion. It is also a competitor to the Weibull for use in reliability modeling and modeling stock returns and interest rate processes. References  1 
Brighton Webs Ltd., Wald Distribution, https://web.archive.org/web/20090423014010/http://www.brighton-webs.co.uk:80/distributions/wald.asp  2 
Chhikara, Raj S., and Folks, J. Leroy, “The Inverse Gaussian Distribution: Theory : Methodology, and Applications”, CRC Press, 1988.  3 
Wikipedia, “Inverse Gaussian distribution” https://en.wikipedia.org/wiki/Inverse_Gaussian_distribution   Examples Draw values from the distribution and plot the histogram: >>> import matplotlib.pyplot as plt
>>> h = plt.hist(np.random.wald(3, 2, 100000), bins=200, density=True)
>>> plt.show()
numpy.random.Generator.wald method   random.Generator.wald(mean, scale, size=None)
 
Draw samples from a Wald, or inverse Gaussian, distribution. As the scale approaches infinity, the distribution becomes more like a Gaussian. Some references claim that the Wald is an inverse Gaussian with mean equal to 1, but this is by no means universal. The inverse Gaussian distribution was first studied in relationship to Brownian motion. In 1956 M.C.K. Tweedie used the name inverse Gaussian because there is an inverse relationship between the time to cover a unit distance and distance covered in unit time.  Parameters 
 
meanfloat or array_like of floats


Distribution mean, must be > 0.  
scalefloat or array_like of floats


Scale parameter, must be > 0.  
sizeint or tuple of ints, optional


Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if mean and scale are both scalars. Otherwise, np.broadcast(mean, scale).size samples are drawn.    Returns 
 
outndarray or scalar


Drawn samples from the parameterized Wald distribution.     Notes The probability density function for the Wald distribution is  \[P(x;mean,scale) = \sqrt{\frac{scale}{2\pi x^3}}e^ \frac{-scale(x-mean)^2}{2\cdotp mean^2x}\] As noted above the inverse Gaussian distribution first arise from attempts to model Brownian motion. It is also a competitor to the Weibull for use in reliability modeling and modeling stock returns and interest rate processes. References  1 
Brighton Webs Ltd., Wald Distribution, https://web.archive.org/web/20090423014010/http://www.brighton-webs.co.uk:80/distributions/wald.asp  2 
Chhikara, Raj S., and Folks, J. Leroy, “The Inverse Gaussian Distribution: Theory : Methodology, and Applications”, CRC Press, 1988.  3 
Wikipedia, “Inverse Gaussian distribution” https://en.wikipedia.org/wiki/Inverse_Gaussian_distribution   Examples Draw values from the distribution and plot the histogram: >>> import matplotlib.pyplot as plt
>>> h = plt.hist(np.random.default_rng().wald(3, 2, 100000), bins=200, density=True)
>>> plt.show()
signal.SIGCLD  
Alias to SIGCHLD.
def f_299446(owd):
    """change working directory to the directory `owd`
    """
     
 --------------------
Please refer to the following documentation to generate the code:
stars.main() 
 run a simple starfield example stars.main() -> None  A simple starfield example. You can change the center of perspective by leftclicking the mouse on the screen.
sqlite3 — DB-API 2.0 interface for SQLite databases Source code: Lib/sqlite3/ SQLite is a C library that provides a lightweight disk-based database that doesn’t require a separate server process and allows accessing the database using a nonstandard variant of the SQL query language. Some applications can use SQLite for internal data storage. It’s also possible to prototype an application using SQLite and then port the code to a larger database such as PostgreSQL or Oracle. The sqlite3 module was written by Gerhard Häring. It provides a SQL interface compliant with the DB-API 2.0 specification described by PEP 249. To use the module, you must first create a Connection object that represents the database. Here the data will be stored in the example.db file: import sqlite3
con = sqlite3.connect('example.db')
 You can also supply the special name :memory: to create a database in RAM. Once you have a Connection, you can create a Cursor object and call its execute() method to perform SQL commands: cur = con.cursor()

# Create table
cur.execute('''CREATE TABLE stocks
               (date text, trans text, symbol text, qty real, price real)''')

# Insert a row of data
cur.execute("INSERT INTO stocks VALUES ('2006-01-05','BUY','RHAT',100,35.14)")

# Save (commit) the changes
con.commit()

# We can also close the connection if we are done with it.
# Just be sure any changes have been committed or they will be lost.
con.close()
 The data you’ve saved is persistent and is available in subsequent sessions: import sqlite3
con = sqlite3.connect('example.db')
cur = con.cursor()
 To retrieve data after executing a SELECT statement, you can either treat the cursor as an iterator, call the cursor’s fetchone() method to retrieve a single matching row, or call fetchall() to get a list of the matching rows. This example uses the iterator form: >>> for row in cur.execute('SELECT * FROM stocks ORDER BY price'):
        print(row)

('2006-01-05', 'BUY', 'RHAT', 100, 35.14)
('2006-03-28', 'BUY', 'IBM', 1000, 45.0)
('2006-04-06', 'SELL', 'IBM', 500, 53.0)
('2006-04-05', 'BUY', 'MSFT', 1000, 72.0)
 Usually your SQL operations will need to use values from Python variables. You shouldn’t assemble your query using Python’s string operations because doing so is insecure; it makes your program vulnerable to an SQL injection attack (see the xkcd webcomic for a humorous example of what can go wrong): # Never do this -- insecure!
symbol = 'RHAT'
cur.execute("SELECT * FROM stocks WHERE symbol = '%s'" % symbol)
 Instead, use the DB-API’s parameter substitution. Put a placeholder wherever you want to use a value, and then provide a tuple of values as the second argument to the cursor’s execute() method. An SQL statement may use one of two kinds of placeholders: question marks (qmark style) or named placeholders (named style). For the qmark style, parameters must be a sequence. For the named style, it can be either a sequence or dict instance. The length of the sequence must match the number of placeholders, or a ProgrammingError is raised. If a dict is given, it must contain keys for all named parameters. Any extra items are ignored. Here’s an example of both styles: import sqlite3

con = sqlite3.connect(":memory:")
cur = con.cursor()
cur.execute("create table lang (lang_name, lang_age)")

# This is the qmark style:
cur.execute("insert into lang values (?, ?)", ("C", 49))

# The qmark style used with executemany():
lang_list = [
    ("Fortran", 64),
    ("Python", 30),
    ("Go", 11),
]
cur.executemany("insert into lang values (?, ?)", lang_list)

# And this is the named style:
cur.execute("select * from lang where lang_name=:name and lang_age=:age",
            {"name": "C", "age": 49})
print(cur.fetchall())

con.close()
  See also  https://www.sqlite.org

The SQLite web page; the documentation describes the syntax and the available data types for the supported SQL dialect.  https://www.w3schools.com/sql/

Tutorial, reference and examples for learning SQL syntax.  
PEP 249 - Database API Specification 2.0

PEP written by Marc-André Lemburg.    Module functions and constants  
sqlite3.version  
The version number of this module, as a string. This is not the version of the SQLite library. 
  
sqlite3.version_info  
The version number of this module, as a tuple of integers. This is not the version of the SQLite library. 
  
sqlite3.sqlite_version  
The version number of the run-time SQLite library, as a string. 
  
sqlite3.sqlite_version_info  
The version number of the run-time SQLite library, as a tuple of integers. 
  
sqlite3.PARSE_DECLTYPES  
This constant is meant to be used with the detect_types parameter of the connect() function. Setting it makes the sqlite3 module parse the declared type for each column it returns. It will parse out the first word of the declared type, i. e. for “integer primary key”, it will parse out “integer”, or for “number(10)” it will parse out “number”. Then for that column, it will look into the converters dictionary and use the converter function registered for that type there. 
  
sqlite3.PARSE_COLNAMES  
This constant is meant to be used with the detect_types parameter of the connect() function. Setting this makes the SQLite interface parse the column name for each column it returns. It will look for a string formed [mytype] in there, and then decide that ‘mytype’ is the type of the column. It will try to find an entry of ‘mytype’ in the converters dictionary and then use the converter function found there to return the value. The column name found in Cursor.description does not include the type, i. e. if you use something like 'as "Expiration date [datetime]"' in your SQL, then we will parse out everything until the first '[' for the column name and strip the preceeding space: the column name would simply be “Expiration date”. 
  
sqlite3.connect(database[, timeout, detect_types, isolation_level, check_same_thread, factory, cached_statements, uri])  
Opens a connection to the SQLite database file database. By default returns a Connection object, unless a custom factory is given. database is a path-like object giving the pathname (absolute or relative to the current working directory) of the database file to be opened. You can use ":memory:" to open a database connection to a database that resides in RAM instead of on disk. When a database is accessed by multiple connections, and one of the processes modifies the database, the SQLite database is locked until that transaction is committed. The timeout parameter specifies how long the connection should wait for the lock to go away until raising an exception. The default for the timeout parameter is 5.0 (five seconds). For the isolation_level parameter, please see the isolation_level property of Connection objects. SQLite natively supports only the types TEXT, INTEGER, REAL, BLOB and NULL. If you want to use other types you must add support for them yourself. The detect_types parameter and the using custom converters registered with the module-level register_converter() function allow you to easily do that. detect_types defaults to 0 (i. e. off, no type detection), you can set it to any combination of PARSE_DECLTYPES and PARSE_COLNAMES to turn type detection on. Due to SQLite behaviour, types can’t be detected for generated fields (for example max(data)), even when detect_types parameter is set. In such case, the returned type is str. By default, check_same_thread is True and only the creating thread may use the connection. If set False, the returned connection may be shared across multiple threads. When using multiple threads with the same connection writing operations should be serialized by the user to avoid data corruption. By default, the sqlite3 module uses its Connection class for the connect call. You can, however, subclass the Connection class and make connect() use your class instead by providing your class for the factory parameter. Consult the section SQLite and Python types of this manual for details. The sqlite3 module internally uses a statement cache to avoid SQL parsing overhead. If you want to explicitly set the number of statements that are cached for the connection, you can set the cached_statements parameter. The currently implemented default is to cache 100 statements. If uri is true, database is interpreted as a URI. This allows you to specify options. For example, to open a database in read-only mode you can use: db = sqlite3.connect('file:path/to/database?mode=ro', uri=True)
 More information about this feature, including a list of recognized options, can be found in the SQLite URI documentation. Raises an auditing event sqlite3.connect with argument database.  Changed in version 3.4: Added the uri parameter.   Changed in version 3.7: database can now also be a path-like object, not only a string.  
  
sqlite3.register_converter(typename, callable)  
Registers a callable to convert a bytestring from the database into a custom Python type. The callable will be invoked for all database values that are of the type typename. Confer the parameter detect_types of the connect() function for how the type detection works. Note that typename and the name of the type in your query are matched in case-insensitive manner. 
  
sqlite3.register_adapter(type, callable)  
Registers a callable to convert the custom Python type type into one of SQLite’s supported types. The callable callable accepts as single parameter the Python value, and must return a value of the following types: int, float, str or bytes. 
  
sqlite3.complete_statement(sql)  
Returns True if the string sql contains one or more complete SQL statements terminated by semicolons. It does not verify that the SQL is syntactically correct, only that there are no unclosed string literals and the statement is terminated by a semicolon. This can be used to build a shell for SQLite, as in the following example: # A minimal SQLite shell for experiments

import sqlite3

con = sqlite3.connect(":memory:")
con.isolation_level = None
cur = con.cursor()

buffer = ""

print("Enter your SQL commands to execute in sqlite3.")
print("Enter a blank line to exit.")

while True:
    line = input()
    if line == "":
        break
    buffer += line
    if sqlite3.complete_statement(buffer):
        try:
            buffer = buffer.strip()
            cur.execute(buffer)

            if buffer.lstrip().upper().startswith("SELECT"):
                print(cur.fetchall())
        except sqlite3.Error as e:
            print("An error occurred:", e.args[0])
        buffer = ""

con.close()
 
  
sqlite3.enable_callback_tracebacks(flag)  
By default you will not get any tracebacks in user-defined functions, aggregates, converters, authorizer callbacks etc. If you want to debug them, you can call this function with flag set to True. Afterwards, you will get tracebacks from callbacks on sys.stderr. Use False to disable the feature again. 
 Connection Objects  
class sqlite3.Connection  
A SQLite database connection has the following attributes and methods:  
isolation_level  
Get or set the current default isolation level. None for autocommit mode or one of “DEFERRED”, “IMMEDIATE” or “EXCLUSIVE”. See section Controlling Transactions for a more detailed explanation. 
  
in_transaction  
True if a transaction is active (there are uncommitted changes), False otherwise. Read-only attribute.  New in version 3.2.  
  
cursor(factory=Cursor)  
The cursor method accepts a single optional parameter factory. If supplied, this must be a callable returning an instance of Cursor or its subclasses. 
  
commit()  
This method commits the current transaction. If you don’t call this method, anything you did since the last call to commit() is not visible from other database connections. If you wonder why you don’t see the data you’ve written to the database, please check you didn’t forget to call this method. 
  
rollback()  
This method rolls back any changes to the database since the last call to commit(). 
  
close()  
This closes the database connection. Note that this does not automatically call commit(). If you just close your database connection without calling commit() first, your changes will be lost! 
  
execute(sql[, parameters])  
This is a nonstandard shortcut that creates a cursor object by calling the cursor() method, calls the cursor’s execute() method with the parameters given, and returns the cursor. 
  
executemany(sql[, parameters])  
This is a nonstandard shortcut that creates a cursor object by calling the cursor() method, calls the cursor’s executemany() method with the parameters given, and returns the cursor. 
  
executescript(sql_script)  
This is a nonstandard shortcut that creates a cursor object by calling the cursor() method, calls the cursor’s executescript() method with the given sql_script, and returns the cursor. 
  
create_function(name, num_params, func, *, deterministic=False)  
Creates a user-defined function that you can later use from within SQL statements under the function name name. num_params is the number of parameters the function accepts (if num_params is -1, the function may take any number of arguments), and func is a Python callable that is called as the SQL function. If deterministic is true, the created function is marked as deterministic, which allows SQLite to perform additional optimizations. This flag is supported by SQLite 3.8.3 or higher, NotSupportedError will be raised if used with older versions. The function can return any of the types supported by SQLite: bytes, str, int, float and None.  Changed in version 3.8: The deterministic parameter was added.  Example: import sqlite3
import hashlib

def md5sum(t):
    return hashlib.md5(t).hexdigest()

con = sqlite3.connect(":memory:")
con.create_function("md5", 1, md5sum)
cur = con.cursor()
cur.execute("select md5(?)", (b"foo",))
print(cur.fetchone()[0])

con.close()
 
  
create_aggregate(name, num_params, aggregate_class)  
Creates a user-defined aggregate function. The aggregate class must implement a step method, which accepts the number of parameters num_params (if num_params is -1, the function may take any number of arguments), and a finalize method which will return the final result of the aggregate. The finalize method can return any of the types supported by SQLite: bytes, str, int, float and None. Example: import sqlite3

class MySum:
    def __init__(self):
        self.count = 0

    def step(self, value):
        self.count += value

    def finalize(self):
        return self.count

con = sqlite3.connect(":memory:")
con.create_aggregate("mysum", 1, MySum)
cur = con.cursor()
cur.execute("create table test(i)")
cur.execute("insert into test(i) values (1)")
cur.execute("insert into test(i) values (2)")
cur.execute("select mysum(i) from test")
print(cur.fetchone()[0])

con.close()
 
  
create_collation(name, callable)  
Creates a collation with the specified name and callable. The callable will be passed two string arguments. It should return -1 if the first is ordered lower than the second, 0 if they are ordered equal and 1 if the first is ordered higher than the second. Note that this controls sorting (ORDER BY in SQL) so your comparisons don’t affect other SQL operations. Note that the callable will get its parameters as Python bytestrings, which will normally be encoded in UTF-8. The following example shows a custom collation that sorts “the wrong way”: import sqlite3

def collate_reverse(string1, string2):
    if string1 == string2:
        return 0
    elif string1 < string2:
        return 1
    else:
        return -1

con = sqlite3.connect(":memory:")
con.create_collation("reverse", collate_reverse)

cur = con.cursor()
cur.execute("create table test(x)")
cur.executemany("insert into test(x) values (?)", [("a",), ("b",)])
cur.execute("select x from test order by x collate reverse")
for row in cur:
    print(row)
con.close()
 To remove a collation, call create_collation with None as callable: con.create_collation("reverse", None)
 
  
interrupt()  
You can call this method from a different thread to abort any queries that might be executing on the connection. The query will then abort and the caller will get an exception. 
  
set_authorizer(authorizer_callback)  
This routine registers a callback. The callback is invoked for each attempt to access a column of a table in the database. The callback should return SQLITE_OK if access is allowed, SQLITE_DENY if the entire SQL statement should be aborted with an error and SQLITE_IGNORE if the column should be treated as a NULL value. These constants are available in the sqlite3 module. The first argument to the callback signifies what kind of operation is to be authorized. The second and third argument will be arguments or None depending on the first argument. The 4th argument is the name of the database (“main”, “temp”, etc.) if applicable. The 5th argument is the name of the inner-most trigger or view that is responsible for the access attempt or None if this access attempt is directly from input SQL code. Please consult the SQLite documentation about the possible values for the first argument and the meaning of the second and third argument depending on the first one. All necessary constants are available in the sqlite3 module. 
  
set_progress_handler(handler, n)  
This routine registers a callback. The callback is invoked for every n instructions of the SQLite virtual machine. This is useful if you want to get called from SQLite during long-running operations, for example to update a GUI. If you want to clear any previously installed progress handler, call the method with None for handler. Returning a non-zero value from the handler function will terminate the currently executing query and cause it to raise an OperationalError exception. 
  
set_trace_callback(trace_callback)  
Registers trace_callback to be called for each SQL statement that is actually executed by the SQLite backend. The only argument passed to the callback is the statement (as string) that is being executed. The return value of the callback is ignored. Note that the backend does not only run statements passed to the Cursor.execute() methods. Other sources include the transaction management of the Python module and the execution of triggers defined in the current database. Passing None as trace_callback will disable the trace callback.  New in version 3.3.  
  
enable_load_extension(enabled)  
This routine allows/disallows the SQLite engine to load SQLite extensions from shared libraries. SQLite extensions can define new functions, aggregates or whole new virtual table implementations. One well-known extension is the fulltext-search extension distributed with SQLite. Loadable extensions are disabled by default. See 1.  New in version 3.2.  import sqlite3

con = sqlite3.connect(":memory:")

# enable extension loading
con.enable_load_extension(True)

# Load the fulltext search extension
con.execute("select load_extension('./fts3.so')")

# alternatively you can load the extension using an API call:
# con.load_extension("./fts3.so")

# disable extension loading again
con.enable_load_extension(False)

# example from SQLite wiki
con.execute("create virtual table recipe using fts3(name, ingredients)")
con.executescript("""
    insert into recipe (name, ingredients) values ('broccoli stew', 'broccoli peppers cheese tomatoes');
    insert into recipe (name, ingredients) values ('pumpkin stew', 'pumpkin onions garlic celery');
    insert into recipe (name, ingredients) values ('broccoli pie', 'broccoli cheese onions flour');
    insert into recipe (name, ingredients) values ('pumpkin pie', 'pumpkin sugar flour butter');
    """)
for row in con.execute("select rowid, name, ingredients from recipe where name match 'pie'"):
    print(row)

con.close()
 
  
load_extension(path)  
This routine loads a SQLite extension from a shared library. You have to enable extension loading with enable_load_extension() before you can use this routine. Loadable extensions are disabled by default. See 1.  New in version 3.2.  
  
row_factory  
You can change this attribute to a callable that accepts the cursor and the original row as a tuple and will return the real result row. This way, you can implement more advanced ways of returning results, such as returning an object that can also access columns by name. Example: import sqlite3

def dict_factory(cursor, row):
    d = {}
    for idx, col in enumerate(cursor.description):
        d[col[0]] = row[idx]
    return d

con = sqlite3.connect(":memory:")
con.row_factory = dict_factory
cur = con.cursor()
cur.execute("select 1 as a")
print(cur.fetchone()["a"])

con.close()
 If returning a tuple doesn’t suffice and you want name-based access to columns, you should consider setting row_factory to the highly-optimized sqlite3.Row type. Row provides both index-based and case-insensitive name-based access to columns with almost no memory overhead. It will probably be better than your own custom dictionary-based approach or even a db_row based solution. 
  
text_factory  
Using this attribute you can control what objects are returned for the TEXT data type. By default, this attribute is set to str and the sqlite3 module will return Unicode objects for TEXT. If you want to return bytestrings instead, you can set it to bytes. You can also set it to any other callable that accepts a single bytestring parameter and returns the resulting object. See the following example code for illustration: import sqlite3

con = sqlite3.connect(":memory:")
cur = con.cursor()

AUSTRIA = "\xd6sterreich"

# by default, rows are returned as Unicode
cur.execute("select ?", (AUSTRIA,))
row = cur.fetchone()
assert row[0] == AUSTRIA

# but we can make sqlite3 always return bytestrings ...
con.text_factory = bytes
cur.execute("select ?", (AUSTRIA,))
row = cur.fetchone()
assert type(row[0]) is bytes
# the bytestrings will be encoded in UTF-8, unless you stored garbage in the
# database ...
assert row[0] == AUSTRIA.encode("utf-8")

# we can also implement a custom text_factory ...
# here we implement one that appends "foo" to all strings
con.text_factory = lambda x: x.decode("utf-8") + "foo"
cur.execute("select ?", ("bar",))
row = cur.fetchone()
assert row[0] == "barfoo"

con.close()
 
  
total_changes  
Returns the total number of database rows that have been modified, inserted, or deleted since the database connection was opened. 
  
iterdump()  
Returns an iterator to dump the database in an SQL text format. Useful when saving an in-memory database for later restoration. This function provides the same capabilities as the .dump command in the sqlite3 shell. Example: # Convert file existing_db.db to SQL dump file dump.sql
import sqlite3

con = sqlite3.connect('existing_db.db')
with open('dump.sql', 'w') as f:
    for line in con.iterdump():
        f.write('%s\n' % line)
con.close()
 
  
backup(target, *, pages=-1, progress=None, name="main", sleep=0.250)  
This method makes a backup of a SQLite database even while it’s being accessed by other clients, or concurrently by the same connection. The copy will be written into the mandatory argument target, that must be another Connection instance. By default, or when pages is either 0 or a negative integer, the entire database is copied in a single step; otherwise the method performs a loop copying up to pages pages at a time. If progress is specified, it must either be None or a callable object that will be executed at each iteration with three integer arguments, respectively the status of the last iteration, the remaining number of pages still to be copied and the total number of pages. The name argument specifies the database name that will be copied: it must be a string containing either "main", the default, to indicate the main database, "temp" to indicate the temporary database or the name specified after the AS keyword in an ATTACH DATABASE statement for an attached database. The sleep argument specifies the number of seconds to sleep by between successive attempts to backup remaining pages, can be specified either as an integer or a floating point value. Example 1, copy an existing database into another: import sqlite3

def progress(status, remaining, total):
    print(f'Copied {total-remaining} of {total} pages...')

con = sqlite3.connect('existing_db.db')
bck = sqlite3.connect('backup.db')
with bck:
    con.backup(bck, pages=1, progress=progress)
bck.close()
con.close()
 Example 2, copy an existing database into a transient copy: import sqlite3

source = sqlite3.connect('existing_db.db')
dest = sqlite3.connect(':memory:')
source.backup(dest)
 Availability: SQLite 3.6.11 or higher  New in version 3.7.  
 
 Cursor Objects  
class sqlite3.Cursor  
A Cursor instance has the following attributes and methods.  
execute(sql[, parameters])  
Executes an SQL statement. Values may be bound to the statement using placeholders. execute() will only execute a single SQL statement. If you try to execute more than one statement with it, it will raise a Warning. Use executescript() if you want to execute multiple SQL statements with one call. 
  
executemany(sql, seq_of_parameters)  
Executes a parameterized SQL command against all parameter sequences or mappings found in the sequence seq_of_parameters. The sqlite3 module also allows using an iterator yielding parameters instead of a sequence. import sqlite3

class IterChars:
    def __init__(self):
        self.count = ord('a')

    def __iter__(self):
        return self

    def __next__(self):
        if self.count > ord('z'):
            raise StopIteration
        self.count += 1
        return (chr(self.count - 1),) # this is a 1-tuple

con = sqlite3.connect(":memory:")
cur = con.cursor()
cur.execute("create table characters(c)")

theIter = IterChars()
cur.executemany("insert into characters(c) values (?)", theIter)

cur.execute("select c from characters")
print(cur.fetchall())

con.close()
 Here’s a shorter example using a generator: import sqlite3
import string

def char_generator():
    for c in string.ascii_lowercase:
        yield (c,)

con = sqlite3.connect(":memory:")
cur = con.cursor()
cur.execute("create table characters(c)")

cur.executemany("insert into characters(c) values (?)", char_generator())

cur.execute("select c from characters")
print(cur.fetchall())

con.close()
 
  
executescript(sql_script)  
This is a nonstandard convenience method for executing multiple SQL statements at once. It issues a COMMIT statement first, then executes the SQL script it gets as a parameter. sql_script can be an instance of str. Example: import sqlite3

con = sqlite3.connect(":memory:")
cur = con.cursor()
cur.executescript("""
    create table person(
        firstname,
        lastname,
        age
    );

    create table book(
        title,
        author,
        published
    );

    insert into book(title, author, published)
    values (
        'Dirk Gently''s Holistic Detective Agency',
        'Douglas Adams',
        1987
    );
    """)
con.close()
 
  
fetchone()  
Fetches the next row of a query result set, returning a single sequence, or None when no more data is available. 
  
fetchmany(size=cursor.arraysize)  
Fetches the next set of rows of a query result, returning a list. An empty list is returned when no more rows are available. The number of rows to fetch per call is specified by the size parameter. If it is not given, the cursor’s arraysize determines the number of rows to be fetched. The method should try to fetch as many rows as indicated by the size parameter. If this is not possible due to the specified number of rows not being available, fewer rows may be returned. Note there are performance considerations involved with the size parameter. For optimal performance, it is usually best to use the arraysize attribute. If the size parameter is used, then it is best for it to retain the same value from one fetchmany() call to the next. 
  
fetchall()  
Fetches all (remaining) rows of a query result, returning a list. Note that the cursor’s arraysize attribute can affect the performance of this operation. An empty list is returned when no rows are available. 
  
close()  
Close the cursor now (rather than whenever __del__ is called). The cursor will be unusable from this point forward; a ProgrammingError exception will be raised if any operation is attempted with the cursor. 
  
rowcount  
Although the Cursor class of the sqlite3 module implements this attribute, the database engine’s own support for the determination of “rows affected”/”rows selected” is quirky. For executemany() statements, the number of modifications are summed up into rowcount. As required by the Python DB API Spec, the rowcount attribute “is -1 in case no executeXX() has been performed on the cursor or the rowcount of the last operation is not determinable by the interface”. This includes SELECT statements because we cannot determine the number of rows a query produced until all rows were fetched. With SQLite versions before 3.6.5, rowcount is set to 0 if you make a DELETE FROM table without any condition. 
  
lastrowid  
This read-only attribute provides the rowid of the last modified row. It is only set if you issued an INSERT or a REPLACE statement using the execute() method. For operations other than INSERT or REPLACE or when executemany() is called, lastrowid is set to None. If the INSERT or REPLACE statement failed to insert the previous successful rowid is returned.  Changed in version 3.6: Added support for the REPLACE statement.  
  
arraysize  
Read/write attribute that controls the number of rows returned by fetchmany(). The default value is 1 which means a single row would be fetched per call. 
  
description  
This read-only attribute provides the column names of the last query. To remain compatible with the Python DB API, it returns a 7-tuple for each column where the last six items of each tuple are None. It is set for SELECT statements without any matching rows as well. 
  
connection  
This read-only attribute provides the SQLite database Connection used by the Cursor object. A Cursor object created by calling con.cursor() will have a connection attribute that refers to con: >>> con = sqlite3.connect(":memory:")
>>> cur = con.cursor()
>>> cur.connection == con
True
 
 
 Row Objects  
class sqlite3.Row  
A Row instance serves as a highly optimized row_factory for Connection objects. It tries to mimic a tuple in most of its features. It supports mapping access by column name and index, iteration, representation, equality testing and len(). If two Row objects have exactly the same columns and their members are equal, they compare equal.  
keys()  
This method returns a list of column names. Immediately after a query, it is the first member of each tuple in Cursor.description. 
  Changed in version 3.5: Added support of slicing.  
 Let’s assume we initialize a table as in the example given above: con = sqlite3.connect(":memory:")
cur = con.cursor()
cur.execute('''create table stocks
(date text, trans text, symbol text,
 qty real, price real)''')
cur.execute("""insert into stocks
            values ('2006-01-05','BUY','RHAT',100,35.14)""")
con.commit()
cur.close()
 Now we plug Row in: >>> con.row_factory = sqlite3.Row
>>> cur = con.cursor()
>>> cur.execute('select * from stocks')
<sqlite3.Cursor object at 0x7f4e7dd8fa80>
>>> r = cur.fetchone()
>>> type(r)
<class 'sqlite3.Row'>
>>> tuple(r)
('2006-01-05', 'BUY', 'RHAT', 100.0, 35.14)
>>> len(r)
5
>>> r[2]
'RHAT'
>>> r.keys()
['date', 'trans', 'symbol', 'qty', 'price']
>>> r['qty']
100.0
>>> for member in r:
...     print(member)
...
2006-01-05
BUY
RHAT
100.0
35.14
 Exceptions  
exception sqlite3.Warning  
A subclass of Exception. 
  
exception sqlite3.Error  
The base class of the other exceptions in this module. It is a subclass of Exception. 
  
exception sqlite3.DatabaseError  
Exception raised for errors that are related to the database. 
  
exception sqlite3.IntegrityError  
Exception raised when the relational integrity of the database is affected, e.g. a foreign key check fails. It is a subclass of DatabaseError. 
  
exception sqlite3.ProgrammingError  
Exception raised for programming errors, e.g. table not found or already exists, syntax error in the SQL statement, wrong number of parameters specified, etc. It is a subclass of DatabaseError. 
  
exception sqlite3.OperationalError  
Exception raised for errors that are related to the database’s operation and not necessarily under the control of the programmer, e.g. an unexpected disconnect occurs, the data source name is not found, a transaction could not be processed, etc. It is a subclass of DatabaseError. 
  
exception sqlite3.NotSupportedError  
Exception raised in case a method or database API was used which is not supported by the database, e.g. calling the rollback() method on a connection that does not support transaction or has transactions turned off. It is a subclass of DatabaseError. 
 SQLite and Python types Introduction SQLite natively supports the following types: NULL, INTEGER, REAL, TEXT, BLOB. The following Python types can thus be sent to SQLite without any problem:   
Python type SQLite type   
None NULL  
int INTEGER  
float REAL  
str TEXT  
bytes BLOB   This is how SQLite types are converted to Python types by default:   
SQLite type Python type   
NULL None  
INTEGER int  
REAL float  
TEXT depends on text_factory, str by default  
BLOB bytes   The type system of the sqlite3 module is extensible in two ways: you can store additional Python types in a SQLite database via object adaptation, and you can let the sqlite3 module convert SQLite types to different Python types via converters. Using adapters to store additional Python types in SQLite databases As described before, SQLite supports only a limited set of types natively. To use other Python types with SQLite, you must adapt them to one of the sqlite3 module’s supported types for SQLite: one of NoneType, int, float, str, bytes. There are two ways to enable the sqlite3 module to adapt a custom Python type to one of the supported ones. Letting your object adapt itself This is a good approach if you write the class yourself. Let’s suppose you have a class like this: class Point:
    def __init__(self, x, y):
        self.x, self.y = x, y
 Now you want to store the point in a single SQLite column. First you’ll have to choose one of the supported types to be used for representing the point. Let’s just use str and separate the coordinates using a semicolon. Then you need to give your class a method __conform__(self, protocol) which must return the converted value. The parameter protocol will be PrepareProtocol. import sqlite3

class Point:
    def __init__(self, x, y):
        self.x, self.y = x, y

    def __conform__(self, protocol):
        if protocol is sqlite3.PrepareProtocol:
            return "%f;%f" % (self.x, self.y)

con = sqlite3.connect(":memory:")
cur = con.cursor()

p = Point(4.0, -3.2)
cur.execute("select ?", (p,))
print(cur.fetchone()[0])

con.close()
 Registering an adapter callable The other possibility is to create a function that converts the type to the string representation and register the function with register_adapter(). import sqlite3

class Point:
    def __init__(self, x, y):
        self.x, self.y = x, y

def adapt_point(point):
    return "%f;%f" % (point.x, point.y)

sqlite3.register_adapter(Point, adapt_point)

con = sqlite3.connect(":memory:")
cur = con.cursor()

p = Point(4.0, -3.2)
cur.execute("select ?", (p,))
print(cur.fetchone()[0])

con.close()
 The sqlite3 module has two default adapters for Python’s built-in datetime.date and datetime.datetime types. Now let’s suppose we want to store datetime.datetime objects not in ISO representation, but as a Unix timestamp. import sqlite3
import datetime
import time

def adapt_datetime(ts):
    return time.mktime(ts.timetuple())

sqlite3.register_adapter(datetime.datetime, adapt_datetime)

con = sqlite3.connect(":memory:")
cur = con.cursor()

now = datetime.datetime.now()
cur.execute("select ?", (now,))
print(cur.fetchone()[0])

con.close()
 Converting SQLite values to custom Python types Writing an adapter lets you send custom Python types to SQLite. But to make it really useful we need to make the Python to SQLite to Python roundtrip work. Enter converters. Let’s go back to the Point class. We stored the x and y coordinates separated via semicolons as strings in SQLite. First, we’ll define a converter function that accepts the string as a parameter and constructs a Point object from it.  Note Converter functions always get called with a bytes object, no matter under which data type you sent the value to SQLite.  def convert_point(s):
    x, y = map(float, s.split(b";"))
    return Point(x, y)
 Now you need to make the sqlite3 module know that what you select from the database is actually a point. There are two ways of doing this:  Implicitly via the declared type Explicitly via the column name  Both ways are described in section Module functions and constants, in the entries for the constants PARSE_DECLTYPES and PARSE_COLNAMES. The following example illustrates both approaches. import sqlite3

class Point:
    def __init__(self, x, y):
        self.x, self.y = x, y

    def __repr__(self):
        return "(%f;%f)" % (self.x, self.y)

def adapt_point(point):
    return ("%f;%f" % (point.x, point.y)).encode('ascii')

def convert_point(s):
    x, y = list(map(float, s.split(b";")))
    return Point(x, y)

# Register the adapter
sqlite3.register_adapter(Point, adapt_point)

# Register the converter
sqlite3.register_converter("point", convert_point)

p = Point(4.0, -3.2)

#########################
# 1) Using declared types
con = sqlite3.connect(":memory:", detect_types=sqlite3.PARSE_DECLTYPES)
cur = con.cursor()
cur.execute("create table test(p point)")

cur.execute("insert into test(p) values (?)", (p,))
cur.execute("select p from test")
print("with declared types:", cur.fetchone()[0])
cur.close()
con.close()

#######################
# 1) Using column names
con = sqlite3.connect(":memory:", detect_types=sqlite3.PARSE_COLNAMES)
cur = con.cursor()
cur.execute("create table test(p)")

cur.execute("insert into test(p) values (?)", (p,))
cur.execute('select p as "p [point]" from test')
print("with column names:", cur.fetchone()[0])
cur.close()
con.close()
 Default adapters and converters There are default adapters for the date and datetime types in the datetime module. They will be sent as ISO dates/ISO timestamps to SQLite. The default converters are registered under the name “date” for datetime.date and under the name “timestamp” for datetime.datetime. This way, you can use date/timestamps from Python without any additional fiddling in most cases. The format of the adapters is also compatible with the experimental SQLite date/time functions. The following example demonstrates this. import sqlite3
import datetime

con = sqlite3.connect(":memory:", detect_types=sqlite3.PARSE_DECLTYPES|sqlite3.PARSE_COLNAMES)
cur = con.cursor()
cur.execute("create table test(d date, ts timestamp)")

today = datetime.date.today()
now = datetime.datetime.now()

cur.execute("insert into test(d, ts) values (?, ?)", (today, now))
cur.execute("select d, ts from test")
row = cur.fetchone()
print(today, "=>", row[0], type(row[0]))
print(now, "=>", row[1], type(row[1]))

cur.execute('select current_date as "d [date]", current_timestamp as "ts [timestamp]"')
row = cur.fetchone()
print("current_date", row[0], type(row[0]))
print("current_timestamp", row[1], type(row[1]))

con.close()
 If a timestamp stored in SQLite has a fractional part longer than 6 numbers, its value will be truncated to microsecond precision by the timestamp converter. Controlling Transactions The underlying sqlite3 library operates in autocommit mode by default, but the Python sqlite3 module by default does not. autocommit mode means that statements that modify the database take effect immediately. A BEGIN or SAVEPOINT statement disables autocommit mode, and a COMMIT, a ROLLBACK, or a RELEASE that ends the outermost transaction, turns autocommit mode back on. The Python sqlite3 module by default issues a BEGIN statement implicitly before a Data Modification Language (DML) statement (i.e. INSERT/UPDATE/DELETE/REPLACE). You can control which kind of BEGIN statements sqlite3 implicitly executes via the isolation_level parameter to the connect() call, or via the isolation_level property of connections. If you specify no isolation_level, a plain BEGIN is used, which is equivalent to specifying DEFERRED. Other possible values are IMMEDIATE and EXCLUSIVE. You can disable the sqlite3 module’s implicit transaction management by setting isolation_level to None. This will leave the underlying sqlite3 library operating in autocommit mode. You can then completely control the transaction state by explicitly issuing BEGIN, ROLLBACK, SAVEPOINT, and RELEASE statements in your code.  Changed in version 3.6: sqlite3 used to implicitly commit an open transaction before DDL statements. This is no longer the case.  Using sqlite3 efficiently Using shortcut methods Using the nonstandard execute(), executemany() and executescript() methods of the Connection object, your code can be written more concisely because you don’t have to create the (often superfluous) Cursor objects explicitly. Instead, the Cursor objects are created implicitly and these shortcut methods return the cursor objects. This way, you can execute a SELECT statement and iterate over it directly using only a single call on the Connection object. import sqlite3

persons = [
    ("Hugo", "Boss"),
    ("Calvin", "Klein")
    ]

con = sqlite3.connect(":memory:")

# Create the table
con.execute("create table person(firstname, lastname)")

# Fill the table
con.executemany("insert into person(firstname, lastname) values (?, ?)", persons)

# Print the table contents
for row in con.execute("select firstname, lastname from person"):
    print(row)

print("I just deleted", con.execute("delete from person").rowcount, "rows")

# close is not a shortcut method and it's not called automatically,
# so the connection object should be closed manually
con.close()
 Accessing columns by name instead of by index One useful feature of the sqlite3 module is the built-in sqlite3.Row class designed to be used as a row factory. Rows wrapped with this class can be accessed both by index (like tuples) and case-insensitively by name: import sqlite3

con = sqlite3.connect(":memory:")
con.row_factory = sqlite3.Row

cur = con.cursor()
cur.execute("select 'John' as name, 42 as age")
for row in cur:
    assert row[0] == row["name"]
    assert row["name"] == row["nAmE"]
    assert row[1] == row["age"]
    assert row[1] == row["AgE"]

con.close()
 Using the connection as a context manager Connection objects can be used as context managers that automatically commit or rollback transactions. In the event of an exception, the transaction is rolled back; otherwise, the transaction is committed: import sqlite3

con = sqlite3.connect(":memory:")
con.execute("create table person (id integer primary key, firstname varchar unique)")

# Successful, con.commit() is called automatically afterwards
with con:
    con.execute("insert into person(firstname) values (?)", ("Joe",))

# con.rollback() is called after the with block finishes with an exception, the
# exception is still raised and must be caught
try:
    with con:
        con.execute("insert into person(firstname) values (?)", ("Joe",))
except sqlite3.IntegrityError:
    print("couldn't add Joe twice")

# Connection object used as context manager only commits or rollbacks transactions,
# so the connection object should be closed manually
con.close()
 Footnotes  
1(1,2)  
The sqlite3 module is not built with loadable extension support by default, because some platforms (notably Mac OS X) have SQLite libraries which are compiled without this feature. To get loadable extension support, you must pass --enable-loadable-sqlite-extensions to configure.
msilib.add_data(database, table, records)  
Add all records to the table named table in database. The table argument must be one of the predefined tables in the MSI schema, e.g. 'Feature', 'File', 'Component', 'Dialog', 'Control', etc. records should be a list of tuples, each one containing all fields of a record according to the schema of the table. For optional fields, None can be passed. Field values can be ints, strings, or instances of the Binary class.
test_cookie_worked()  
Returns either True or False, depending on whether the user’s browser accepted the test cookie. Due to the way cookies work, you’ll have to call set_test_cookie() on a previous, separate page request. See Setting test cookies below for more information.
set_test_cookie()  
Sets a test cookie to determine whether the user’s browser supports cookies. Due to the way cookies work, you won’t be able to test this until the user’s next page request. See Setting test cookies below for more information.
def f_14695134(c, testfield):
    """insert data from a string `testfield` to sqlite db `c`
    """
     
 --------------------

def f_24242433():
    """decode string "\\x89\\n" into a normal string
    """
    return  
 --------------------

def f_24242433(raw_string):
    """convert a raw string `raw_string` into a normal string
    """
    return  
 --------------------

def f_24242433(raw_byte_string):
    """convert a raw string `raw_byte_string` into a normal string
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
str.splitlines([keepends])  
Return a list of the lines in the string, breaking at line boundaries. Line breaks are not included in the resulting list unless keepends is given and true. This method splits on the following line boundaries. In particular, the boundaries are a superset of universal newlines.   
Representation Description   
\n Line Feed  
\r Carriage Return  
\r\n Carriage Return + Line Feed  
\v or \x0b Line Tabulation  
\f or \x0c Form Feed  
\x1c File Separator  
\x1d Group Separator  
\x1e Record Separator  
\x85 Next Line (C1 Control Code)  
\u2028 Line Separator  
\u2029 Paragraph Separator    Changed in version 3.2: \v and \f added to list of line boundaries.  For example: >>> 'ab c\n\nde fg\rkl\r\n'.splitlines()
['ab c', '', 'de fg', 'kl']
>>> 'ab c\n\nde fg\rkl\r\n'.splitlines(keepends=True)
['ab c\n', '\n', 'de fg\r', 'kl\r\n']
 Unlike split() when a delimiter string sep is given, this method returns an empty list for the empty string, and a terminal line break does not result in an extra line: >>> "".splitlines()
[]
>>> "One line\n".splitlines()
['One line']
 For comparison, split('\n') gives: >>> ''.split('\n')
['']
>>> 'Two lines\n'.split('\n')
['Two lines', '']
Pattern.split(string, maxsplit=0)  
Identical to the split() function, using the compiled pattern.
pandas.Series.str.repeat   Series.str.repeat(repeats)[source]
 
Duplicate each string in the Series or Index.  Parameters 
 
repeats:int or sequence of int


Same value for all (int) or different value per (sequence).    Returns 
 Series or Index of object

Series or Index of repeated string objects specified by input parameter repeats.     Examples 
>>> s = pd.Series(['a', 'b', 'c'])
>>> s
0    a
1    b
2    c
dtype: object
  Single int repeats string in Series 
>>> s.str.repeat(repeats=2)
0    aa
1    bb
2    cc
dtype: object
  Sequence of int repeats corresponding string in Series 
>>> s.str.repeat(repeats=[1, 2, 3])
0      a
1     bb
2    ccc
dtype: object
bytes.splitlines(keepends=False)  
bytearray.splitlines(keepends=False)  
Return a list of the lines in the binary sequence, breaking at ASCII line boundaries. This method uses the universal newlines approach to splitting lines. Line breaks are not included in the resulting list unless keepends is given and true. For example: >>> b'ab c\n\nde fg\rkl\r\n'.splitlines()
[b'ab c', b'', b'de fg', b'kl']
>>> b'ab c\n\nde fg\rkl\r\n'.splitlines(keepends=True)
[b'ab c\n', b'\n', b'de fg\r', b'kl\r\n']
 Unlike split() when a delimiter string sep is given, this method returns an empty list for the empty string, and a terminal line break does not result in an extra line: >>> b"".split(b'\n'), b"Two lines\n".split(b'\n')
([b''], [b'Two lines', b''])
>>> b"".splitlines(), b"One line\n".splitlines()
([], [b'One line'])
numpy.char.splitlines   char.splitlines(a, keepends=None)[source]
 
For each element in a, return a list of the lines in the element, breaking at line boundaries. Calls str.splitlines element-wise.  Parameters 
 
aarray_like of str or unicode


keependsbool, optional


Line breaks are not included in the resulting list unless keepends is given and true.    Returns 
 
outndarray


Array of list objects      See also  str.splitlines
def f_22882922(s):
    """split a string `s` with into all strings of repeated characters
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
matplotlib.pyplot.plot   matplotlib.pyplot.plot(*args, scalex=True, scaley=True, data=None, **kwargs)[source]
 
Plot y versus x as lines and/or markers. Call signatures: plot([x], y, [fmt], *, data=None, **kwargs)
plot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)
 The coordinates of the points or line nodes are given by x, y. The optional parameter fmt is a convenient way for defining basic formatting like color, marker and linestyle. It's a shortcut string notation described in the Notes section below. >>> plot(x, y)        # plot x and y using default line style and color
>>> plot(x, y, 'bo')  # plot x and y using blue circle markers
>>> plot(y)           # plot y using x as index array 0..N-1
>>> plot(y, 'r+')     # ditto, but with red plusses
 You can use Line2D properties as keyword arguments for more control on the appearance. Line properties and fmt can be mixed. The following two calls yield identical results: >>> plot(x, y, 'go--', linewidth=2, markersize=12)
>>> plot(x, y, color='green', marker='o', linestyle='dashed',
...      linewidth=2, markersize=12)
 When conflicting with fmt, keyword arguments take precedence. Plotting labelled data There's a convenient way for plotting objects with labelled data (i.e. data that can be accessed by index obj['y']). Instead of giving the data in x and y, you can provide the object in the data parameter and just give the labels for x and y: >>> plot('xlabel', 'ylabel', data=obj)
 All indexable objects are supported. This could e.g. be a dict, a pandas.DataFrame or a structured numpy array. Plotting multiple sets of data There are various ways to plot multiple sets of data.  
The most straight forward way is just to call plot multiple times. Example: >>> plot(x1, y1, 'bo')
>>> plot(x2, y2, 'go')
  
If x and/or y are 2D arrays a separate data set will be drawn for every column. If both x and y are 2D, they must have the same shape. If only one of them is 2D with shape (N, m) the other must have length N and will be used for every data set m. Example: >>> x = [1, 2, 3]
>>> y = np.array([[1, 2], [3, 4], [5, 6]])
>>> plot(x, y)
 is equivalent to: >>> for col in range(y.shape[1]):
...     plot(x, y[:, col])
  
The third way is to specify multiple sets of [x], y, [fmt] groups: >>> plot(x1, y1, 'g^', x2, y2, 'g-')
 In this case, any additional keyword argument applies to all datasets. Also this syntax cannot be combined with the data parameter.   By default, each line is assigned a different style specified by a 'style cycle'. The fmt and line property parameters are only necessary if you want explicit deviations from these defaults. Alternatively, you can also change the style cycle using rcParams["axes.prop_cycle"] (default: cycler('color', ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])).  Parameters 
 
x, yarray-like or scalar


The horizontal / vertical coordinates of the data points. x values are optional and default to range(len(y)). Commonly, these parameters are 1D arrays. They can also be scalars, or two-dimensional (in that case, the columns represent separate data sets). These arguments cannot be passed as keywords.  
fmtstr, optional


A format string, e.g. 'ro' for red circles. See the Notes section for a full description of the format strings. Format strings are just an abbreviation for quickly setting basic line properties. All of these and more can also be controlled by keyword arguments. This argument cannot be passed as keyword.  
dataindexable object, optional


An object with labelled data. If given, provide the label names to plot in x and y.  Note Technically there's a slight ambiguity in calls where the second label is a valid fmt. plot('n', 'o', data=obj) could be plt(x, y) or plt(y, fmt). In such cases, the former interpretation is chosen, but a warning is issued. You may suppress the warning by adding an empty format string plot('n', 'o', '', data=obj).     Returns 
 list of Line2D


A list of lines representing the plotted data.    Other Parameters 
 
scalex, scaleybool, default: True


These parameters determine if the view limits are adapted to the data limits. The values are passed on to autoscale_view.  
**kwargsLine2D properties, optional


kwargs are used to specify properties like a line label (for auto legends), linewidth, antialiasing, marker face color. Example: >>> plot([1, 2, 3], [1, 2, 3], 'go-', label='line 1', linewidth=2)
>>> plot([1, 2, 3], [1, 4, 9], 'rs', label='line 2')
 If you specify multiple lines with one plot call, the kwargs apply to all those lines. In case the label object is iterable, each element is used as labels for each set of data. Here is a list of available Line2D properties:   
Property Description   
agg_filter a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array  
alpha scalar or None  
animated bool  
antialiased or aa bool  
clip_box Bbox  
clip_on bool  
clip_path Patch or (Path, Transform) or None  
color or c color  
dash_capstyle CapStyle or {'butt', 'projecting', 'round'}  
dash_joinstyle JoinStyle or {'miter', 'round', 'bevel'}  
dashes sequence of floats (on/off ink in points) or (None, None)  
data (2, N) array or two 1D arrays  
drawstyle or ds {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'  
figure Figure  
fillstyle {'full', 'left', 'right', 'bottom', 'top', 'none'}  
gid str  
in_layout bool  
label object  
linestyle or ls {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}  
linewidth or lw float  
marker marker style string, Path or MarkerStyle  
markeredgecolor or mec color  
markeredgewidth or mew float  
markerfacecolor or mfc color  
markerfacecoloralt or mfcalt color  
markersize or ms float  
markevery None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]  
path_effects AbstractPathEffect  
picker float or callable[[Artist, Event], tuple[bool, dict]]  
pickradius float  
rasterized bool  
sketch_params (scale: float, length: float, randomness: float)  
snap bool or None  
solid_capstyle CapStyle or {'butt', 'projecting', 'round'}  
solid_joinstyle JoinStyle or {'miter', 'round', 'bevel'}  
transform unknown  
url str  
visible bool  
xdata 1D array  
ydata 1D array  
zorder float        See also  scatter

XY scatter plot with markers of varying size and/or color ( sometimes also called bubble chart).    Notes Format Strings A format string consists of a part for color, marker and line: fmt = '[marker][line][color]'
 Each of them is optional. If not provided, the value from the style cycle is used. Exception: If line is given, but no marker, the data will be a line without markers. Other combinations such as [color][marker][line] are also supported, but note that their parsing may be ambiguous. Markers   
character description   
'.' point marker  
',' pixel marker  
'o' circle marker  
'v' triangle_down marker  
'^' triangle_up marker  
'<' triangle_left marker  
'>' triangle_right marker  
'1' tri_down marker  
'2' tri_up marker  
'3' tri_left marker  
'4' tri_right marker  
'8' octagon marker  
's' square marker  
'p' pentagon marker  
'P' plus (filled) marker  
'*' star marker  
'h' hexagon1 marker  
'H' hexagon2 marker  
'+' plus marker  
'x' x marker  
'X' x (filled) marker  
'D' diamond marker  
'd' thin_diamond marker  
'|' vline marker  
'_' hline marker   Line Styles   
character description   
'-' solid line style  
'--' dashed line style  
'-.' dash-dot line style  
':' dotted line style   Example format strings: 'b'    # blue markers with default shape
'or'   # red circles
'-g'   # green solid line
'--'   # dashed line with default color
'^k:'  # black triangle_up markers connected by a dotted line
 Colors The supported color abbreviations are the single letter codes   
character color   
'b' blue  
'g' green  
'r' red  
'c' cyan  
'm' magenta  
'y' yellow  
'k' black  
'w' white   and the 'CN' colors that index into the default property cycle. If the color is the only part of the format string, you can additionally use any matplotlib.colors spec, e.g. full names ('green') or hex strings ('#008000'). 
  Examples using matplotlib.pyplot.plot
 
   Plotting masked and NaN values   

   Scatter Masked   

   Stairs Demo   

   Step Demo   

   Custom Figure subclasses   

   Managing multiple figures in pyplot   

   Shared Axis   

   Multiple subplots   

   Controlling style of text and labels using a dictionary   

   Title positioning   

   Infinite lines   

   plot() format string   

   Pyplot Mathtext   

   Pyplot Simple   

   Pyplot Three   

   Pyplot Two Subplots   

   Dolphins   

   Solarized Light stylesheet   

   Frame grabbing   

   Coords Report   

   Customize Rc   

   Findobj Demo   

   Multipage PDF   

   Print Stdout   

   Set and get properties   

   transforms.offset_copy   

   Zorder Demo   

   Custom scale   

   Placing date ticks using recurrence rules   

   Rotating custom tick labels   

   Tool Manager   

   Buttons   

   Slider   

   Snapping Sliders to Discrete Values   

   Basic Usage   

   Pyplot tutorial   

   Customizing Matplotlib with style sheets and rcParams   

   Path effects guide
matplotlib.pyplot.scatter   matplotlib.pyplot.scatter(x, y, s=None, c=None, marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=None, linewidths=None, *, edgecolors=None, plotnonfinite=False, data=None, **kwargs)[source]
 
A scatter plot of y vs. x with varying marker size and/or color.  Parameters 
 
x, yfloat or array-like, shape (n, )


The data positions.  
sfloat or array-like, shape (n, ), optional


The marker size in points**2. Default is rcParams['lines.markersize'] ** 2.  
carray-like or list of colors or color, optional


The marker colors. Possible values:  A scalar or sequence of n numbers to be mapped to colors using cmap and norm. A 2D array in which the rows are RGB or RGBA. A sequence of colors of length n. A single color format string.  Note that c should not be a single numeric RGB or RGBA sequence because that is indistinguishable from an array of values to be colormapped. If you want to specify the same RGB or RGBA value for all points, use a 2D array with a single row. Otherwise, value- matching will have precedence in case of a size matching with x and y. If you wish to specify a single color for all points prefer the color keyword argument. Defaults to None. In that case the marker color is determined by the value of color, facecolor or facecolors. In case those are not specified or None, the marker color is determined by the next color of the Axes' current "shape and fill" color cycle. This cycle defaults to rcParams["axes.prop_cycle"] (default: cycler('color', ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])).  
markerMarkerStyle, default: rcParams["scatter.marker"] (default: 'o')


The marker style. marker can be either an instance of the class or the text shorthand for a particular marker. See matplotlib.markers for more information about marker styles.  
cmapstr or Colormap, default: rcParams["image.cmap"] (default: 'viridis')


A Colormap instance or registered colormap name. cmap is only used if c is an array of floats.  
normNormalize, default: None


If c is an array of floats, norm is used to scale the color data, c, in the range 0 to 1, in order to map into the colormap cmap. If None, use the default colors.Normalize.  
vmin, vmaxfloat, default: None


vmin and vmax are used in conjunction with the default norm to map the color array c to the colormap cmap. If None, the respective min and max of the color array is used. It is an error to use vmin/vmax when norm is given.  
alphafloat, default: None


The alpha blending value, between 0 (transparent) and 1 (opaque).  
linewidthsfloat or array-like, default: rcParams["lines.linewidth"] (default: 1.5)


The linewidth of the marker edges. Note: The default edgecolors is 'face'. You may want to change this as well.  
edgecolors{'face', 'none', None} or color or sequence of color, default: rcParams["scatter.edgecolors"] (default: 'face')


The edge color of the marker. Possible values:  'face': The edge color will always be the same as the face color. 'none': No patch boundary will be drawn. A color or sequence of colors.  For non-filled markers, edgecolors is ignored. Instead, the color is determined like with 'face', i.e. from c, colors, or facecolors.  
plotnonfinitebool, default: False


Whether to plot points with nonfinite c (i.e. inf, -inf or nan). If True the points are drawn with the bad colormap color (see Colormap.set_bad).    Returns 
 PathCollection
  Other Parameters 
 
dataindexable object, optional


If given, the following parameters also accept a string s, which is interpreted as data[s] (unless this raises an exception): x, y, s, linewidths, edgecolors, c, facecolor, facecolors, color  
**kwargsCollection properties

    See also  plot

To plot scatter plots when markers are identical in size and color.    Notes  The plot function will be faster for scatterplots where markers don't vary in size or color. Any or all of x, y, s, and c may be masked arrays, in which case all masks will be combined and only unmasked points will be plotted. Fundamentally, scatter works with 1D arrays; x, y, s, and c may be input as N-D arrays, but within scatter they will be flattened. The exception is c, which will be flattened only if its size matches the size of x and y.  
  Examples using matplotlib.pyplot.scatter
 
   Scatter Masked   

   Scatter Symbol   

   Scatter plot   

   Hyperlinks   

   Pyplot tutorial
matplotlib.axes.Axes.plot   Axes.plot(*args, scalex=True, scaley=True, data=None, **kwargs)[source]
 
Plot y versus x as lines and/or markers. Call signatures: plot([x], y, [fmt], *, data=None, **kwargs)
plot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)
 The coordinates of the points or line nodes are given by x, y. The optional parameter fmt is a convenient way for defining basic formatting like color, marker and linestyle. It's a shortcut string notation described in the Notes section below. >>> plot(x, y)        # plot x and y using default line style and color
>>> plot(x, y, 'bo')  # plot x and y using blue circle markers
>>> plot(y)           # plot y using x as index array 0..N-1
>>> plot(y, 'r+')     # ditto, but with red plusses
 You can use Line2D properties as keyword arguments for more control on the appearance. Line properties and fmt can be mixed. The following two calls yield identical results: >>> plot(x, y, 'go--', linewidth=2, markersize=12)
>>> plot(x, y, color='green', marker='o', linestyle='dashed',
...      linewidth=2, markersize=12)
 When conflicting with fmt, keyword arguments take precedence. Plotting labelled data There's a convenient way for plotting objects with labelled data (i.e. data that can be accessed by index obj['y']). Instead of giving the data in x and y, you can provide the object in the data parameter and just give the labels for x and y: >>> plot('xlabel', 'ylabel', data=obj)
 All indexable objects are supported. This could e.g. be a dict, a pandas.DataFrame or a structured numpy array. Plotting multiple sets of data There are various ways to plot multiple sets of data.  
The most straight forward way is just to call plot multiple times. Example: >>> plot(x1, y1, 'bo')
>>> plot(x2, y2, 'go')
  
If x and/or y are 2D arrays a separate data set will be drawn for every column. If both x and y are 2D, they must have the same shape. If only one of them is 2D with shape (N, m) the other must have length N and will be used for every data set m. Example: >>> x = [1, 2, 3]
>>> y = np.array([[1, 2], [3, 4], [5, 6]])
>>> plot(x, y)
 is equivalent to: >>> for col in range(y.shape[1]):
...     plot(x, y[:, col])
  
The third way is to specify multiple sets of [x], y, [fmt] groups: >>> plot(x1, y1, 'g^', x2, y2, 'g-')
 In this case, any additional keyword argument applies to all datasets. Also this syntax cannot be combined with the data parameter.   By default, each line is assigned a different style specified by a 'style cycle'. The fmt and line property parameters are only necessary if you want explicit deviations from these defaults. Alternatively, you can also change the style cycle using rcParams["axes.prop_cycle"] (default: cycler('color', ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])).  Parameters 
 
x, yarray-like or scalar


The horizontal / vertical coordinates of the data points. x values are optional and default to range(len(y)). Commonly, these parameters are 1D arrays. They can also be scalars, or two-dimensional (in that case, the columns represent separate data sets). These arguments cannot be passed as keywords.  
fmtstr, optional


A format string, e.g. 'ro' for red circles. See the Notes section for a full description of the format strings. Format strings are just an abbreviation for quickly setting basic line properties. All of these and more can also be controlled by keyword arguments. This argument cannot be passed as keyword.  
dataindexable object, optional


An object with labelled data. If given, provide the label names to plot in x and y.  Note Technically there's a slight ambiguity in calls where the second label is a valid fmt. plot('n', 'o', data=obj) could be plt(x, y) or plt(y, fmt). In such cases, the former interpretation is chosen, but a warning is issued. You may suppress the warning by adding an empty format string plot('n', 'o', '', data=obj).     Returns 
 list of Line2D


A list of lines representing the plotted data.    Other Parameters 
 
scalex, scaleybool, default: True


These parameters determine if the view limits are adapted to the data limits. The values are passed on to autoscale_view.  
**kwargsLine2D properties, optional


kwargs are used to specify properties like a line label (for auto legends), linewidth, antialiasing, marker face color. Example: >>> plot([1, 2, 3], [1, 2, 3], 'go-', label='line 1', linewidth=2)
>>> plot([1, 2, 3], [1, 4, 9], 'rs', label='line 2')
 If you specify multiple lines with one plot call, the kwargs apply to all those lines. In case the label object is iterable, each element is used as labels for each set of data. Here is a list of available Line2D properties:   
Property Description   
agg_filter a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array  
alpha scalar or None  
animated bool  
antialiased or aa bool  
clip_box Bbox  
clip_on bool  
clip_path Patch or (Path, Transform) or None  
color or c color  
dash_capstyle CapStyle or {'butt', 'projecting', 'round'}  
dash_joinstyle JoinStyle or {'miter', 'round', 'bevel'}  
dashes sequence of floats (on/off ink in points) or (None, None)  
data (2, N) array or two 1D arrays  
drawstyle or ds {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'  
figure Figure  
fillstyle {'full', 'left', 'right', 'bottom', 'top', 'none'}  
gid str  
in_layout bool  
label object  
linestyle or ls {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}  
linewidth or lw float  
marker marker style string, Path or MarkerStyle  
markeredgecolor or mec color  
markeredgewidth or mew float  
markerfacecolor or mfc color  
markerfacecoloralt or mfcalt color  
markersize or ms float  
markevery None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]  
path_effects AbstractPathEffect  
picker float or callable[[Artist, Event], tuple[bool, dict]]  
pickradius float  
rasterized bool  
sketch_params (scale: float, length: float, randomness: float)  
snap bool or None  
solid_capstyle CapStyle or {'butt', 'projecting', 'round'}  
solid_joinstyle JoinStyle or {'miter', 'round', 'bevel'}  
transform unknown  
url str  
visible bool  
xdata 1D array  
ydata 1D array  
zorder float        See also  scatter

XY scatter plot with markers of varying size and/or color ( sometimes also called bubble chart).    Notes Format Strings A format string consists of a part for color, marker and line: fmt = '[marker][line][color]'
 Each of them is optional. If not provided, the value from the style cycle is used. Exception: If line is given, but no marker, the data will be a line without markers. Other combinations such as [color][marker][line] are also supported, but note that their parsing may be ambiguous. Markers   
character description   
'.' point marker  
',' pixel marker  
'o' circle marker  
'v' triangle_down marker  
'^' triangle_up marker  
'<' triangle_left marker  
'>' triangle_right marker  
'1' tri_down marker  
'2' tri_up marker  
'3' tri_left marker  
'4' tri_right marker  
'8' octagon marker  
's' square marker  
'p' pentagon marker  
'P' plus (filled) marker  
'*' star marker  
'h' hexagon1 marker  
'H' hexagon2 marker  
'+' plus marker  
'x' x marker  
'X' x (filled) marker  
'D' diamond marker  
'd' thin_diamond marker  
'|' vline marker  
'_' hline marker   Line Styles   
character description   
'-' solid line style  
'--' dashed line style  
'-.' dash-dot line style  
':' dotted line style   Example format strings: 'b'    # blue markers with default shape
'or'   # red circles
'-g'   # green solid line
'--'   # dashed line with default color
'^k:'  # black triangle_up markers connected by a dotted line
 Colors The supported color abbreviations are the single letter codes   
character color   
'b' blue  
'g' green  
'r' red  
'c' cyan  
'm' magenta  
'y' yellow  
'k' black  
'w' white   and the 'CN' colors that index into the default property cycle. If the color is the only part of the format string, you can additionally use any matplotlib.colors spec, e.g. full names ('green') or hex strings ('#008000'). 
  Examples using matplotlib.axes.Axes.plot
 
   Plotting categorical variables   

   CSD Demo   

   Curve with error band   

   EventCollection Demo   

   Fill Between and Alpha   

   Filling the area between lines   

   Fill Betweenx Demo   

   Customizing dashed line styles   

   Lines with a ticked patheffect   

   Marker reference   

   Markevery Demo   

   prop_cycle property markevery in rcParams   

   Psd Demo   

   Simple Plot   

   Using span_where   

   Creating a timeline with lines, dates, and text   

   hlines and vlines   

   Contour Corner Mask   

   Contour plot of irregularly spaced data   

   pcolormesh grids and shading   

   Streamplot   

   Spectrogram Demo   

   Watermark image   

   Aligning Labels   

   Axes box aspect   

   Axes Demo   

   Controlling view limits using margins and sticky_edges   

   Axes Props   

   axhspan Demo   

   Broken Axis   

   Resizing axes with constrained layout   

   Resizing axes with tight layout   

   Figure labels: suptitle, supxlabel, supylabel   

   Invert Axes   

   Secondary Axis   

   Sharing axis limits and views   

   Figure subfigures   

   Multiple subplots   

   Creating multiple subplots using plt.subplots   

   Plots with different scales   

   Boxplots   

   Using histograms to plot a cumulative distribution   

   Some features of the histogram (hist) function   

   Polar plot   

   Polar Legend   

   Using accented text in matplotlib   

   Scale invariant angle label   

   Annotating Plots   

   Composing Custom Legends   

   Date tick labels   

   Custom tick formatter for time series   

   AnnotationBbox demo   

   Labeling ticks using engineering notation   

   Annotation arrow style reference   

   Legend using pre-defined labels   

   Legend Demo   

   Mathtext   

   Math fontfamily   

   Multiline   

   Rendering math equations using TeX   

   Text Rotation Relative To Line   

   Title positioning   

   Text watermark   

   Annotate Transform   

   Annotating a plot   

   Annotation Polar   

   Programmatically controlling subplot adjustment   

   Dollar Ticks   

   Simple axes labels   

   Text Commands   

   Color Demo   

   Color by y-value   

   PathPatch object   

   Bezier Curve   

   Dark background style sheet   

   FiveThirtyEight style sheet   

   ggplot style sheet   

   Axes with a fixed physical size   

   Parasite Simple   

   Simple Axisline4   

   Axis line styles   

   Parasite Axes demo   

   Parasite axis demo   

   Custom spines with axisartist   

   Simple Axisline   

   Anatomy of a figure   

   Bachelor's degrees by gender   

   Integral as the area under a curve   

   XKCD   

   Decay   

   The Bayes update   

   The double pendulum problem   

   Animated 3D random walk   

   Animated line plot   

   MATPLOTLIB UNCHAINED   

   Mouse move and click events   

   Data Browser   

   Keypress event   

   Legend Picking   

   Looking Glass   

   Path Editor   

   Pick Event Demo2   

   Resampling Data   

   Timers   

   Frontpage histogram example   

   Frontpage plot example   

   Changing colors of lines intersecting a box   

   Cross hair cursor   

   Custom projection   

   Patheffect Demo   

   Pythonic Matplotlib   

   SVG Filter Line   

   TickedStroke patheffect   

   Zorder Demo   

   Plot 2D data on 3D plot   

   3D box surface plot   

   Parametric Curve   

   Lorenz Attractor   

   2D and 3D Axes in same Figure   

   Loglog Aspect   

   Scales   

   Symlog Demo   

   Anscombe's quartet   

   Radar chart (aka spider or star chart)   

   Centered spines with arrows   

   Multiple Yaxis With Spines   

   Spine Placement   

   Spines   

   Custom spine bounds   

   Centering labels between ticks   

   Formatting date ticks using ConciseDateFormatter   

   Date Demo Convert   

   Date Index Formatter   

   Date Precision and Epochs   

   Major and minor ticks   

   The default tick formatter   

   Set default y-axis tick labels on the right   

   Setting tick labels from a list of values   

   Set default x-axis tick labels on the top   

   Evans test   

   CanvasAgg demo   

   Annotate Explain   

   Connect Simple01   

   Connection styles for annotations   

   Nested GridSpecs   

   Pgf Fonts   

   Pgf Texsystem   

   Simple Annotate01   

   Simple Legend01   

   Simple Legend02   

   Annotated Cursor   

   Check Buttons   

   Cursor   

   Multicursor   

   Radio Buttons   

   Rectangle and ellipse selectors   

   Span Selector   

   Textbox   

   Basic Usage   

   Artist tutorial   

   Legend guide   

   Styling with cycler   

   Constrained Layout Guide   

   Tight Layout guide   

   Arranging multiple Axes in a Figure   

   Autoscaling   

   Faster rendering by using blitting   

   Path Tutorial   

   Transformations Tutorial   

   Specifying Colors   

   Text in Matplotlib Plots   

   plot(x, y)   

   fill_between(x, y1, y2)   

   tricontour(x, y, z)   

   tricontourf(x, y, z)   

   tripcolor(x, y, z)
matplotlib.axes.Axes.scatter   Axes.scatter(x, y, s=None, c=None, marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=None, linewidths=None, *, edgecolors=None, plotnonfinite=False, data=None, **kwargs)[source]
 
A scatter plot of y vs. x with varying marker size and/or color.  Parameters 
 
x, yfloat or array-like, shape (n, )


The data positions.  
sfloat or array-like, shape (n, ), optional


The marker size in points**2. Default is rcParams['lines.markersize'] ** 2.  
carray-like or list of colors or color, optional


The marker colors. Possible values:  A scalar or sequence of n numbers to be mapped to colors using cmap and norm. A 2D array in which the rows are RGB or RGBA. A sequence of colors of length n. A single color format string.  Note that c should not be a single numeric RGB or RGBA sequence because that is indistinguishable from an array of values to be colormapped. If you want to specify the same RGB or RGBA value for all points, use a 2D array with a single row. Otherwise, value- matching will have precedence in case of a size matching with x and y. If you wish to specify a single color for all points prefer the color keyword argument. Defaults to None. In that case the marker color is determined by the value of color, facecolor or facecolors. In case those are not specified or None, the marker color is determined by the next color of the Axes' current "shape and fill" color cycle. This cycle defaults to rcParams["axes.prop_cycle"] (default: cycler('color', ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])).  
markerMarkerStyle, default: rcParams["scatter.marker"] (default: 'o')


The marker style. marker can be either an instance of the class or the text shorthand for a particular marker. See matplotlib.markers for more information about marker styles.  
cmapstr or Colormap, default: rcParams["image.cmap"] (default: 'viridis')


A Colormap instance or registered colormap name. cmap is only used if c is an array of floats.  
normNormalize, default: None


If c is an array of floats, norm is used to scale the color data, c, in the range 0 to 1, in order to map into the colormap cmap. If None, use the default colors.Normalize.  
vmin, vmaxfloat, default: None


vmin and vmax are used in conjunction with the default norm to map the color array c to the colormap cmap. If None, the respective min and max of the color array is used. It is an error to use vmin/vmax when norm is given.  
alphafloat, default: None


The alpha blending value, between 0 (transparent) and 1 (opaque).  
linewidthsfloat or array-like, default: rcParams["lines.linewidth"] (default: 1.5)


The linewidth of the marker edges. Note: The default edgecolors is 'face'. You may want to change this as well.  
edgecolors{'face', 'none', None} or color or sequence of color, default: rcParams["scatter.edgecolors"] (default: 'face')


The edge color of the marker. Possible values:  'face': The edge color will always be the same as the face color. 'none': No patch boundary will be drawn. A color or sequence of colors.  For non-filled markers, edgecolors is ignored. Instead, the color is determined like with 'face', i.e. from c, colors, or facecolors.  
plotnonfinitebool, default: False


Whether to plot points with nonfinite c (i.e. inf, -inf or nan). If True the points are drawn with the bad colormap color (see Colormap.set_bad).    Returns 
 PathCollection
  Other Parameters 
 
dataindexable object, optional


If given, the following parameters also accept a string s, which is interpreted as data[s] (unless this raises an exception): x, y, s, linewidths, edgecolors, c, facecolor, facecolors, color  
**kwargsCollection properties

    See also  plot

To plot scatter plots when markers are identical in size and color.    Notes  The plot function will be faster for scatterplots where markers don't vary in size or color. Any or all of x, y, s, and c may be masked arrays, in which case all masks will be combined and only unmasked points will be plotted. Fundamentally, scatter works with 1D arrays; x, y, s, and c may be input as N-D arrays, but within scatter they will be flattened. The exception is c, which will be flattened only if its size matches the size of x and y.  
  Examples using matplotlib.axes.Axes.scatter
 
   Scatter Custom Symbol   

   Scatter Demo2   

   Scatter plot with histograms   

   Scatter plot with pie chart markers   

   Scatter plots with a legend   

   Advanced quiver and quiverkey functions   

   Axes box aspect   

   Axis Label Position   

   Plot a confidence ellipse of a two-dimensional dataset   

   Violin plot customization   

   Scatter plot on polar axis   

   Legend Demo   

   Scatter Histogram (Locatable Axes)   

   mpl_toolkits.axisartist.floating_axes features   

   Rain simulation   

   Zoom Window   

   Plotting with keywords   

   Zorder Demo   

   Plot 2D data on 3D plot   

   3D scatterplot   

   Automatically setting tick positions   

   Unit handling   

   Annotate Text Arrow   

   Polygon Selector   

   Basic Usage   

   Choosing Colormaps in Matplotlib   

   scatter(x, y)
pandas.DataFrame.plot.scatter   DataFrame.plot.scatter(x, y, s=None, c=None, **kwargs)[source]
 
Create a scatter plot with varying marker point size and color. The coordinates of each point are defined by two dataframe columns and filled circles are used to represent each point. This kind of plot is useful to see complex correlations between two variables. Points could be for instance natural 2D coordinates like longitude and latitude in a map or, in general, any pair of metrics that can be plotted against each other.  Parameters 
 
x:int or str


The column name or column position to be used as horizontal coordinates for each point.  
y:int or str


The column name or column position to be used as vertical coordinates for each point.  
s:str, scalar or array-like, optional


The size of each point. Possible values are:  A string with the name of the column to be used for marker’s size. A single scalar so all points have the same size. 
A sequence of scalars, which will be used for each point’s size recursively. For instance, when passing [2,14] all points size will be either 2 or 14, alternatively.  Changed in version 1.1.0.     
c:str, int or array-like, optional


The color of each point. Possible values are:  A single color string referred to by name, RGB or RGBA code, for instance ‘red’ or ‘#a98d19’. A sequence of color strings referred to by name, RGB or RGBA code, which will be used for each point’s color recursively. For instance [‘green’,’yellow’] all points will be filled in green or yellow, alternatively. A column name or position whose values will be used to color the marker points according to a colormap.   **kwargs

Keyword arguments to pass on to DataFrame.plot().    Returns 
 
matplotlib.axes.Axes or numpy.ndarray of them
    See also  matplotlib.pyplot.scatter

Scatter plot using multiple input data formats.    Examples Let’s see how to draw a scatter plot using coordinates from the values in a DataFrame’s columns. 
>>> df = pd.DataFrame([[5.1, 3.5, 0], [4.9, 3.0, 0], [7.0, 3.2, 1],
...                    [6.4, 3.2, 1], [5.9, 3.0, 2]],
...                   columns=['length', 'width', 'species'])
>>> ax1 = df.plot.scatter(x='length',
...                       y='width',
...                       c='DarkBlue')
     And now with the color determined by a column as well. 
>>> ax2 = df.plot.scatter(x='length',
...                       y='width',
...                       c='species',
...                       colormap='viridis')
def f_4143502():
    """scatter a plot with x, y position of `np.random.randn(100)` and face color equal to none
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
get_datalim(transData)[source]
classmatplotlib.legend_handler.HandlerCircleCollection(yoffsets=None, sizes=None, **kwargs)[source]
 
Handler for CircleCollections.  Parameters 
 
numpointsint


Number of points to show in legend entry.  
yoffsetsarray of floats


Length numpoints list of y offsets for each point in legend entry.  **kwargs

Keyword arguments forwarded to HandlerNpoints.       create_collection(orig_handle, sizes, offsets, transOffset)[source]
autoscale_None()[source]
 
Autoscale the scalar limits on the norm instance using the current array, changing only limits that are None
classmatplotlib.collections.CircleCollection(sizes, **kwargs)[source]
 
Bases: matplotlib.collections._CollectionWithSizes A collection of circles, drawn using splines.  Parameters 
 
sizesfloat or array-like


The area of each circle in points^2.  **kwargs

Forwarded to Collection.       add_callback(func)[source]
 
Add a callback function that will be called whenever one of the Artist's properties changes.  Parameters 
 
funccallable


The callback function. It must have the signature: def func(artist: Artist) -> Any
 where artist is the calling Artist. Return values may exist but are ignored.    Returns 
 int

The observer id associated with the callback. This id can be used for removing the callback with remove_callback later.      See also  remove_callback
  
   autoscale()[source]
 
Autoscale the scalar limits on the norm instance using the current array 
   autoscale_None()[source]
 
Autoscale the scalar limits on the norm instance using the current array, changing only limits that are None 
   propertyaxes
 
The Axes instance the artist resides in, or None. 
   propertycallbacksSM[source]

   changed()[source]
 
Call this whenever the mappable is changed to notify all the callbackSM listeners to the 'changed' signal. 
   colorbar
 
The last colorbar associated with this ScalarMappable. May be None. 
   contains(mouseevent)[source]
 
Test whether the mouse event occurred in the collection. Returns bool, dict(ind=itemlist), where every item in itemlist contains the event. 
   convert_xunits(x)[source]
 
Convert x using the unit type of the xaxis. If the artist is not in contained in an Axes or if the xaxis does not have units, x itself is returned. 
   convert_yunits(y)[source]
 
Convert y using the unit type of the yaxis. If the artist is not in contained in an Axes or if the yaxis does not have units, y itself is returned. 
   draw(renderer)[source]
 
Draw the Artist (and its children) using the given renderer. This has no effect if the artist is not visible (Artist.get_visible returns False).  Parameters 
 
rendererRendererBase subclass.

   Notes This method is overridden in the Artist subclasses. 
   findobj(match=None, include_self=True)[source]
 
Find artist objects. Recursively find all Artist instances contained in the artist.  Parameters 
 match

A filter criterion for the matches. This can be  
None: Return all objects contained in artist. A function with signature def match(artist: Artist) -> bool. The result will only contain artists for which the function returns True. A class instance: e.g., Line2D. The result will only contain artists of this class or its subclasses (isinstance check).   
include_selfbool


Include self in the list to be checked for a match.    Returns 
 list of Artist

   
   format_cursor_data(data)[source]
 
Return a string representation of data.  Note This method is intended to be overridden by artist subclasses. As an end-user of Matplotlib you will most likely not call this method yourself.  The default implementation converts ints and floats and arrays of ints and floats into a comma-separated string enclosed in square brackets, unless the artist has an associated colorbar, in which case scalar values are formatted using the colorbar's formatter.  See also  get_cursor_data
  
   get_agg_filter()[source]
 
Return filter function to be used for agg filter. 
   get_alpha()[source]
 
Return the alpha value used for blending - not supported on all backends. 
   get_animated()[source]
 
Return whether the artist is animated. 
   get_array()[source]
 
Return the array of values, that are mapped to colors. The base class ScalarMappable does not make any assumptions on the dimensionality and shape of the array. 
   get_capstyle()[source]

   get_children()[source]
 
Return a list of the child Artists of this Artist. 
   get_clim()[source]
 
Return the values (min, max) that are mapped to the colormap limits. 
   get_clip_box()[source]
 
Return the clipbox. 
   get_clip_on()[source]
 
Return whether the artist uses clipping. 
   get_clip_path()[source]
 
Return the clip path. 
   get_cmap()[source]
 
Return the Colormap instance. 
   get_cursor_data(event)[source]
 
Return the cursor data for a given event.  Note This method is intended to be overridden by artist subclasses. As an end-user of Matplotlib you will most likely not call this method yourself.  Cursor data can be used by Artists to provide additional context information for a given event. The default implementation just returns None. Subclasses can override the method and return arbitrary data. However, when doing so, they must ensure that format_cursor_data can convert the data to a string representation. The only current use case is displaying the z-value of an AxesImage in the status bar of a plot window, while moving the mouse.  Parameters 
 
eventmatplotlib.backend_bases.MouseEvent

    See also  format_cursor_data
  
   get_dashes()[source]
 
Alias for get_linestyle. 
   get_datalim(transData)[source]

   get_ec()[source]
 
Alias for get_edgecolor. 
   get_edgecolor()[source]

   get_edgecolors()[source]
 
Alias for get_edgecolor. 
   get_facecolor()[source]

   get_facecolors()[source]
 
Alias for get_facecolor. 
   get_fc()[source]
 
Alias for get_facecolor. 
   get_figure()[source]
 
Return the Figure instance the artist belongs to. 
   get_fill()[source]
 
Return whether face is colored. 
   get_gid()[source]
 
Return the group id. 
   get_hatch()[source]
 
Return the current hatching pattern. 
   get_in_layout()[source]
 
Return boolean flag, True if artist is included in layout calculations. E.g. Constrained Layout Guide, Figure.tight_layout(), and fig.savefig(fname, bbox_inches='tight'). 
   get_joinstyle()[source]

   get_label()[source]
 
Return the label used for this artist in the legend. 
   get_linestyle()[source]

   get_linestyles()[source]
 
Alias for get_linestyle. 
   get_linewidth()[source]

   get_linewidths()[source]
 
Alias for get_linewidth. 
   get_ls()[source]
 
Alias for get_linestyle. 
   get_lw()[source]
 
Alias for get_linewidth. 
   get_offset_transform()[source]
 
Return the Transform instance used by this artist offset. 
   get_offsets()[source]
 
Return the offsets for the collection. 
   get_path_effects()[source]

   get_paths()[source]

   get_picker()[source]
 
Return the picking behavior of the artist. The possible values are described in set_picker.  See also  
set_picker, pickable, pick

  
   get_pickradius()[source]

   get_rasterized()[source]
 
Return whether the artist is to be rasterized. 
   get_sizes()[source]
 
Return the sizes ('areas') of the elements in the collection.  Returns 
 array

The 'area' of each element.     
   get_sketch_params()[source]
 
Return the sketch parameters for the artist.  Returns 
 tuple or None

A 3-tuple with the following elements:  
scale: The amplitude of the wiggle perpendicular to the source line. 
length: The length of the wiggle along the line. 
randomness: The scale factor by which the length is shrunken or expanded.  Returns None if no sketch parameters were set.     
   get_snap()[source]
 
Return the snap setting. See set_snap for details. 
   get_tightbbox(renderer)[source]
 
Like Artist.get_window_extent, but includes any clipping.  Parameters 
 
rendererRendererBase subclass


renderer that will be used to draw the figures (i.e. fig.canvas.get_renderer())    Returns 
 Bbox

The enclosing bounding box (in figure pixel coordinates).     
   get_transform()[source]
 
Return the Transform instance used by this artist. 
   get_transformed_clip_path_and_affine()[source]
 
Return the clip path with the non-affine part of its transformation applied, and the remaining affine part of its transformation. 
   get_transforms()[source]

   get_url()[source]
 
Return the url. 
   get_urls()[source]
 
Return a list of URLs, one for each element of the collection. The list contains None for elements without a URL. See Hyperlinks for an example. 
   get_visible()[source]
 
Return the visibility. 
   get_window_extent(renderer)[source]
 
Get the artist's bounding box in display space. The bounding box' width and height are nonnegative. Subclasses should override for inclusion in the bounding box "tight" calculation. Default is to return an empty bounding box at 0, 0. Be careful when using this function, the results will not update if the artist window extent of the artist changes. The extent can change due to any changes in the transform stack, such as changing the axes limits, the figure size, or the canvas used (as is done when saving a figure). This can lead to unexpected behavior where interactive figures will look fine on the screen, but will save incorrectly. 
   get_zorder()[source]
 
Return the artist's zorder. 
   have_units()[source]
 
Return whether units are set on any axis. 
   is_transform_set()[source]
 
Return whether the Artist has an explicitly set transform. This is True after set_transform has been called. 
   propertymouseover
 
If this property is set to True, the artist will be queried for custom context information when the mouse cursor moves over it. See also get_cursor_data(), ToolCursorPosition and NavigationToolbar2. 
   propertynorm

   pchanged()[source]
 
Call all of the registered callbacks. This function is triggered internally when a property is changed.  See also  add_callback
remove_callback
  
   pick(mouseevent)[source]
 
Process a pick event. Each child artist will fire a pick event if mouseevent is over the artist and the artist has picker set.  See also  
set_picker, get_picker, pickable

  
   pickable()[source]
 
Return whether the artist is pickable.  See also  
set_picker, get_picker, pick

  
   properties()[source]
 
Return a dictionary of all the properties of the artist. 
   remove()[source]
 
Remove the artist from the figure if possible. The effect will not be visible until the figure is redrawn, e.g., with FigureCanvasBase.draw_idle. Call relim to update the axes limits if desired. Note: relim will not see collections even if the collection was added to the axes with autolim = True. Note: there is no support for removing the artist's legend entry. 
   remove_callback(oid)[source]
 
Remove a callback based on its observer id.  See also  add_callback
  
   set(*, agg_filter=<UNSET>, alpha=<UNSET>, animated=<UNSET>, antialiased=<UNSET>, array=<UNSET>, capstyle=<UNSET>, clim=<UNSET>, clip_box=<UNSET>, clip_on=<UNSET>, clip_path=<UNSET>, cmap=<UNSET>, color=<UNSET>, edgecolor=<UNSET>, facecolor=<UNSET>, gid=<UNSET>, hatch=<UNSET>, in_layout=<UNSET>, joinstyle=<UNSET>, label=<UNSET>, linestyle=<UNSET>, linewidth=<UNSET>, norm=<UNSET>, offset_transform=<UNSET>, offsets=<UNSET>, path_effects=<UNSET>, picker=<UNSET>, pickradius=<UNSET>, rasterized=<UNSET>, sizes=<UNSET>, sketch_params=<UNSET>, snap=<UNSET>, transform=<UNSET>, url=<UNSET>, urls=<UNSET>, visible=<UNSET>, zorder=<UNSET>)[source]
 
Set multiple properties at once. Supported properties are   
Property Description   
agg_filter a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array  
alpha array-like or scalar or None  
animated bool  
antialiased or aa or antialiaseds bool or list of bools  
array array-like or None  
capstyle CapStyle or {'butt', 'projecting', 'round'}  
clim (vmin: float, vmax: float)  
clip_box Bbox  
clip_on bool  
clip_path Patch or (Path, Transform) or None  
cmap Colormap or str or None  
color color or list of rgba tuples  
edgecolor or ec or edgecolors color or list of colors or 'face'  
facecolor or facecolors or fc color or list of colors  
figure Figure  
gid str  
hatch {'/', '\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}  
in_layout bool  
joinstyle JoinStyle or {'miter', 'round', 'bevel'}  
label object  
linestyle or dashes or linestyles or ls str or tuple or list thereof  
linewidth or linewidths or lw float or list of floats  
norm Normalize or None  
offset_transform Transform  
offsets (N, 2) or (2,) array-like  
path_effects AbstractPathEffect  
picker None or bool or float or callable  
pickradius float  
rasterized bool  
sizes ndarray or None  
sketch_params (scale: float, length: float, randomness: float)  
snap bool or None  
transform Transform  
url str  
urls list of str or None  
visible bool  
zorder float   
   set_aa(aa)[source]
 
Alias for set_antialiased. 
   set_agg_filter(filter_func)[source]
 
Set the agg filter.  Parameters 
 
filter_funccallable


A filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array.     
   set_alpha(alpha)[source]
 
Set the alpha value used for blending - not supported on all backends.  Parameters 
 
alphaarray-like or scalar or None


All values must be within the 0-1 range, inclusive. Masked values and nans are not supported.     
   set_animated(b)[source]
 
Set whether the artist is intended to be used in an animation. If True, the artist is excluded from regular drawing of the figure. You have to call Figure.draw_artist / Axes.draw_artist explicitly on the artist. This appoach is used to speed up animations using blitting. See also matplotlib.animation and Faster rendering by using blitting.  Parameters 
 
bbool

   
   set_antialiased(aa)[source]
 
Set the antialiasing state for rendering.  Parameters 
 
aabool or list of bools

   
   set_antialiaseds(aa)[source]
 
Alias for set_antialiased. 
   set_array(A)[source]
 
Set the value array from array-like A.  Parameters 
 
Aarray-like or None


The values that are mapped to colors. The base class ScalarMappable does not make any assumptions on the dimensionality and shape of the value array A.     
   set_capstyle(cs)[source]
 
Set the CapStyle for the collection (for all its elements).  Parameters 
 
csCapStyle or {'butt', 'projecting', 'round'}

   
   set_clim(vmin=None, vmax=None)[source]
 
Set the norm limits for image scaling.  Parameters 
 
vmin, vmaxfloat


The limits. The limits may also be passed as a tuple (vmin, vmax) as a single positional argument.     
   set_clip_box(clipbox)[source]
 
Set the artist's clip Bbox.  Parameters 
 
clipboxBbox

   
   set_clip_on(b)[source]
 
Set whether the artist uses clipping. When False artists will be visible outside of the axes which can lead to unexpected results.  Parameters 
 
bbool

   
   set_clip_path(path, transform=None)[source]
 
Set the artist's clip path.  Parameters 
 
pathPatch or Path or TransformedPath or None


The clip path. If given a Path, transform must be provided as well. If None, a previously set clip path is removed.  
transformTransform, optional


Only used if path is a Path, in which case the given Path is converted to a TransformedPath using transform.     Notes For efficiency, if path is a Rectangle this method will set the clipping box to the corresponding rectangle and set the clipping path to None. For technical reasons (support of set), a tuple (path, transform) is also accepted as a single positional parameter. 
   set_cmap(cmap)[source]
 
Set the colormap for luminance data.  Parameters 
 
cmapColormap or str or None

   
   set_color(c)[source]
 
Set both the edgecolor and the facecolor.  Parameters 
 
ccolor or list of rgba tuples

    See also  
Collection.set_facecolor, Collection.set_edgecolor


For setting the edge or face color individually.    
   set_dashes(ls)[source]
 
Alias for set_linestyle. 
   set_ec(c)[source]
 
Alias for set_edgecolor. 
   set_edgecolor(c)[source]
 
Set the edgecolor(s) of the collection.  Parameters 
 
ccolor or list of colors or 'face'


The collection edgecolor(s). If a sequence, the patches cycle through it. If 'face', match the facecolor.     
   set_edgecolors(c)[source]
 
Alias for set_edgecolor. 
   set_facecolor(c)[source]
 
Set the facecolor(s) of the collection. c can be a color (all patches have same color), or a sequence of colors; if it is a sequence the patches will cycle through the sequence. If c is 'none', the patch will not be filled.  Parameters 
 
ccolor or list of colors

   
   set_facecolors(c)[source]
 
Alias for set_facecolor. 
   set_fc(c)[source]
 
Alias for set_facecolor. 
   set_figure(fig)[source]
 
Set the Figure instance the artist belongs to.  Parameters 
 
figFigure

   
   set_gid(gid)[source]
 
Set the (group) id for the artist.  Parameters 
 
gidstr

   
   set_hatch(hatch)[source]
 
Set the hatching pattern hatch can be one of: /   - diagonal hatching
\   - back diagonal
|   - vertical
-   - horizontal
+   - crossed
x   - crossed diagonal
o   - small circle
O   - large circle
.   - dots
*   - stars
 Letters can be combined, in which case all the specified hatchings are done. If same letter repeats, it increases the density of hatching of that pattern. Hatching is supported in the PostScript, PDF, SVG and Agg backends only. Unlike other properties such as linewidth and colors, hatching can only be specified for the collection as a whole, not separately for each member.  Parameters 
 
hatch{'/', '\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}

   
   set_in_layout(in_layout)[source]
 
Set if artist is to be included in layout calculations, E.g. Constrained Layout Guide, Figure.tight_layout(), and fig.savefig(fname, bbox_inches='tight').  Parameters 
 
in_layoutbool

   
   set_joinstyle(js)[source]
 
Set the JoinStyle for the collection (for all its elements).  Parameters 
 
jsJoinStyle or {'miter', 'round', 'bevel'}

   
   set_label(s)[source]
 
Set a label that will be displayed in the legend.  Parameters 
 
sobject


s will be converted to a string by calling str.     
   set_linestyle(ls)[source]
 
Set the linestyle(s) for the collection.   
linestyle description   
'-' or 'solid' solid line  
'--' or 'dashed' dashed line  
'-.' or 'dashdot' dash-dotted line  
':' or 'dotted' dotted line   Alternatively a dash tuple of the following form can be provided: (offset, onoffseq),
 where onoffseq is an even length tuple of on and off ink in points.  Parameters 
 
lsstr or tuple or list thereof


Valid values for individual linestyles include {'-', '--', '-.', ':', '', (offset, on-off-seq)}. See Line2D.set_linestyle for a complete description.     
   set_linestyles(ls)[source]
 
Alias for set_linestyle. 
   set_linewidth(lw)[source]
 
Set the linewidth(s) for the collection. lw can be a scalar or a sequence; if it is a sequence the patches will cycle through the sequence  Parameters 
 
lwfloat or list of floats

   
   set_linewidths(lw)[source]
 
Alias for set_linewidth. 
   set_ls(ls)[source]
 
Alias for set_linestyle. 
   set_lw(lw)[source]
 
Alias for set_linewidth. 
   set_norm(norm)[source]
 
Set the normalization instance.  Parameters 
 
normNormalize or None

   Notes If there are any colorbars using the mappable for this norm, setting the norm of the mappable will reset the norm, locator, and formatters on the colorbar to default. 
   set_offset_transform(transOffset)[source]
 
Set the artist offset transform.  Parameters 
 
transOffsetTransform

   
   set_offsets(offsets)[source]
 
Set the offsets for the collection.  Parameters 
 
offsets(N, 2) or (2,) array-like

   
   set_path_effects(path_effects)[source]
 
Set the path effects.  Parameters 
 
path_effectsAbstractPathEffect

   
   set_paths()[source]

   set_picker(picker)[source]
 
Define the picking behavior of the artist.  Parameters 
 
pickerNone or bool or float or callable


This can be one of the following:  
None: Picking is disabled for this artist (default). A boolean: If True then picking will be enabled and the artist will fire a pick event if the mouse event is over the artist. A float: If picker is a number it is interpreted as an epsilon tolerance in points and the artist will fire off an event if its data is within epsilon of the mouse event. For some artists like lines and patch collections, the artist may provide additional data to the pick event that is generated, e.g., the indices of the data within epsilon of the pick event 
A function: If picker is callable, it is a user supplied function which determines whether the artist is hit by the mouse event: hit, props = picker(artist, mouseevent)
 to determine the hit test. if the mouse event is over the artist, return hit=True and props is a dictionary of properties you want added to the PickEvent attributes.       
   set_pickradius(pr)[source]
 
Set the pick radius used for containment tests.  Parameters 
 
prfloat


Pick radius, in points.     
   set_rasterized(rasterized)[source]
 
Force rasterized (bitmap) drawing for vector graphics output. Rasterized drawing is not supported by all artists. If you try to enable this on an artist that does not support it, the command has no effect and a warning will be issued. This setting is ignored for pixel-based output. See also Rasterization for vector graphics.  Parameters 
 
rasterizedbool

   
   set_sizes(sizes, dpi=72.0)[source]
 
Set the sizes of each member of the collection.  Parameters 
 
sizesndarray or None


The size to set for each element of the collection. The value is the 'area' of the element.  
dpifloat, default: 72


The dpi of the canvas.     
   set_sketch_params(scale=None, length=None, randomness=None)[source]
 
Set the sketch parameters.  Parameters 
 
scalefloat, optional


The amplitude of the wiggle perpendicular to the source line, in pixels. If scale is None, or not provided, no sketch filter will be provided.  
lengthfloat, optional


The length of the wiggle along the line, in pixels (default 128.0)  
randomnessfloat, optional


The scale factor by which the length is shrunken or expanded (default 16.0) The PGF backend uses this argument as an RNG seed and not as described above. Using the same seed yields the same random shape.     
   set_snap(snap)[source]
 
Set the snapping behavior. Snapping aligns positions with the pixel grid, which results in clearer images. For example, if a black line of 1px width was defined at a position in between two pixels, the resulting image would contain the interpolated value of that line in the pixel grid, which would be a grey value on both adjacent pixel positions. In contrast, snapping will move the line to the nearest integer pixel value, so that the resulting image will really contain a 1px wide black line. Snapping is currently only supported by the Agg and MacOSX backends.  Parameters 
 
snapbool or None


Possible values:  
True: Snap vertices to the nearest pixel center. 
False: Do not modify vertex positions. 
None: (auto) If the path contains only rectilinear line segments, round to the nearest pixel center.      
   set_transform(t)[source]
 
Set the artist transform.  Parameters 
 
tTransform

   
   set_url(url)[source]
 
Set the url for the artist.  Parameters 
 
urlstr

   
   set_urls(urls)[source]
 
 Parameters 
 
urlslist of str or None

   Notes URLs are currently only implemented by the SVG backend. They are ignored by all other backends. 
   set_visible(b)[source]
 
Set the artist's visibility.  Parameters 
 
bbool

   
   set_zorder(level)[source]
 
Set the zorder for the artist. Artists with lower zorder values are drawn first.  Parameters 
 
levelfloat

   
   propertystale
 
Whether the artist is 'stale' and needs to be re-drawn for the output to match the internal state of the artist. 
   propertysticky_edges
 
x and y sticky edge lists for autoscaling. When performing autoscaling, if a data limit coincides with a value in the corresponding sticky_edges list, then no margin will be added--the view limit "sticks" to the edge. A typical use case is histograms, where one usually expects no margin on the bottom edge (0) of the histogram. Moreover, margin expansion "bumps" against sticky edges and cannot cross them. For example, if the upper data limit is 1.0, the upper view limit computed by simple margin application is 1.2, but there is a sticky edge at 1.1, then the actual upper view limit will be 1.1. This attribute cannot be assigned to; however, the x and y lists can be modified in place as needed. Examples >>> artist.sticky_edges.x[:] = (xmin, xmax)
>>> artist.sticky_edges.y[:] = (ymin, ymax)
 
   to_rgba(x, alpha=None, bytes=False, norm=True)[source]
 
Return a normalized rgba array corresponding to x. In the normal case, x is a 1D or 2D sequence of scalars, and the corresponding ndarray of rgba values will be returned, based on the norm and colormap set for this ScalarMappable. There is one special case, for handling images that are already rgb or rgba, such as might have been read from an image file. If x is an ndarray with 3 dimensions, and the last dimension is either 3 or 4, then it will be treated as an rgb or rgba array, and no mapping will be done. The array can be uint8, or it can be floating point with values in the 0-1 range; otherwise a ValueError will be raised. If it is a masked array, the mask will be ignored. If the last dimension is 3, the alpha kwarg (defaulting to 1) will be used to fill in the transparency. If the last dimension is 4, the alpha kwarg is ignored; it does not replace the pre-existing alpha. A ValueError will be raised if the third dimension is other than 3 or 4. In either case, if bytes is False (default), the rgba array will be floats in the 0-1 range; if it is True, the returned rgba array will be uint8 in the 0 to 255 range. If norm is False, no normalization of the input data is performed, and it is assumed to be in the range (0-1). 
   update(props)[source]
 
Update this artist's properties from the dict props.  Parameters 
 
propsdict

   
   update_from(other)[source]
 
Copy properties from other to self. 
   update_scalarmappable()[source]
 
Update colors from the scalar mappable array, if any. Assign colors to edges and faces based on the array and/or colors that were directly set, as appropriate. 
   zorder=0
matplotlib.axes.Axes.spy   Axes.spy(Z, precision=0, marker=None, markersize=None, aspect='equal', origin='upper', **kwargs)[source]
 
Plot the sparsity pattern of a 2D array. This visualizes the non-zero values of the array. Two plotting styles are available: image and marker. Both are available for full arrays, but only the marker style works for scipy.sparse.spmatrix instances. Image style If marker and markersize are None, imshow is used. Any extra remaining keyword arguments are passed to this method. Marker style If Z is a scipy.sparse.spmatrix or marker or markersize are None, a Line2D object will be returned with the value of marker determining the marker type, and any remaining keyword arguments passed to plot.  Parameters 
 
Z(M, N) array-like


The array to be plotted.  
precisionfloat or 'present', default: 0


If precision is 0, any non-zero value will be plotted. Otherwise, values of \(|Z| > precision\) will be plotted. For scipy.sparse.spmatrix instances, you can also pass 'present'. In this case any value present in the array will be plotted, even if it is identically zero.  
aspect{'equal', 'auto', None} or float, default: 'equal'


The aspect ratio of the Axes. This parameter is particularly relevant for images since it determines whether data pixels are square. This parameter is a shortcut for explicitly calling Axes.set_aspect. See there for further details.  'equal': Ensures an aspect ratio of 1. Pixels will be square. 'auto': The Axes is kept fixed and the aspect is adjusted so that the data fit in the Axes. In general, this will result in non-square pixels. 
None: Use rcParams["image.aspect"] (default: 'equal').   
origin{'upper', 'lower'}, default: rcParams["image.origin"] (default: 'upper')


Place the [0, 0] index of the array in the upper left or lower left corner of the Axes. The convention 'upper' is typically used for matrices and images.    Returns 
 
AxesImage or Line2D


The return type depends on the plotting style (see above).    Other Parameters 
 **kwargs

The supported additional parameters depend on the plotting style. For the image style, you can pass the following additional parameters of imshow:  cmap alpha url any Artist properties (passed on to the AxesImage)  For the marker style, you can pass any Line2D property except for linestyle:   
Property Description   
agg_filter a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array  
alpha scalar or None  
animated bool  
antialiased or aa bool  
clip_box Bbox  
clip_on bool  
clip_path Patch or (Path, Transform) or None  
color or c color  
dash_capstyle CapStyle or {'butt', 'projecting', 'round'}  
dash_joinstyle JoinStyle or {'miter', 'round', 'bevel'}  
dashes sequence of floats (on/off ink in points) or (None, None)  
data (2, N) array or two 1D arrays  
drawstyle or ds {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'  
figure Figure  
fillstyle {'full', 'left', 'right', 'bottom', 'top', 'none'}  
gid str  
in_layout bool  
label object  
linestyle or ls {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}  
linewidth or lw float  
marker marker style string, Path or MarkerStyle  
markeredgecolor or mec color  
markeredgewidth or mew float  
markerfacecolor or mfc color  
markerfacecoloralt or mfcalt color  
markersize or ms float  
markevery None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]  
path_effects AbstractPathEffect  
picker float or callable[[Artist, Event], tuple[bool, dict]]  
pickradius float  
rasterized bool  
sketch_params (scale: float, length: float, randomness: float)  
snap bool or None  
solid_capstyle CapStyle or {'butt', 'projecting', 'round'}  
solid_joinstyle JoinStyle or {'miter', 'round', 'bevel'}  
transform unknown  
url str  
visible bool  
xdata 1D array  
ydata 1D array  
zorder float       
  Examples using matplotlib.axes.Axes.spy
 
   Spy Demos
def f_4143502():
    """do a scatter plot with empty circles
    """
    return  
 --------------------

def f_32063985(soup):
    """remove a div from `soup` with a id `main-content` using beautifulsoup
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
class torch.distributions.chi2.Chi2(df, validate_args=None) [source]
 
Bases: torch.distributions.gamma.Gamma Creates a Chi2 distribution parameterized by shape parameter df. This is exactly equivalent to Gamma(alpha=0.5*df, beta=0.5) Example: >>> m = Chi2(torch.tensor([1.0]))
>>> m.sample()  # Chi2 distributed with shape df=1
tensor([ 0.1046])
  Parameters 
df (float or Tensor) – shape parameter of the distribution    
arg_constraints = {'df': GreaterThan(lower_bound=0.0)} 
  
property df 
  
expand(batch_shape, _instance=None) [source]
numpy.random.RandomState.chisquare method   random.RandomState.chisquare(df, size=None)
 
Draw samples from a chi-square distribution. When df independent random variables, each with standard normal distributions (mean 0, variance 1), are squared and summed, the resulting distribution is chi-square (see Notes). This distribution is often used in hypothesis testing.  Note New code should use the chisquare method of a default_rng() instance instead; please see the Quick Start.   Parameters 
 
dffloat or array_like of floats


Number of degrees of freedom, must be > 0.  
sizeint or tuple of ints, optional


Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if df is a scalar. Otherwise, np.array(df).size samples are drawn.    Returns 
 
outndarray or scalar


Drawn samples from the parameterized chi-square distribution.    Raises 
 ValueError

When df <= 0 or when an inappropriate size (e.g. size=-1) is given.      See also  Generator.chisquare

which should be used for new code.    Notes The variable obtained by summing the squares of df independent, standard normally distributed random variables:  \[Q = \sum_{i=0}^{\mathtt{df}} X^2_i\] is chi-square distributed, denoted  \[Q \sim \chi^2_k.\] The probability density function of the chi-squared distribution is  \[p(x) = \frac{(1/2)^{k/2}}{\Gamma(k/2)} x^{k/2 - 1} e^{-x/2},\] where \(\Gamma\) is the gamma function,  \[\Gamma(x) = \int_0^{-\infty} t^{x - 1} e^{-t} dt.\] References  1 
NIST “Engineering Statistics Handbook” https://www.itl.nist.gov/div898/handbook/eda/section3/eda3666.htm   Examples >>> np.random.chisquare(2,4)
array([ 1.89920014,  9.00867716,  3.13710533,  5.62318272]) # random
numpy.random.chisquare   random.chisquare(df, size=None)
 
Draw samples from a chi-square distribution. When df independent random variables, each with standard normal distributions (mean 0, variance 1), are squared and summed, the resulting distribution is chi-square (see Notes). This distribution is often used in hypothesis testing.  Note New code should use the chisquare method of a default_rng() instance instead; please see the Quick Start.   Parameters 
 
dffloat or array_like of floats


Number of degrees of freedom, must be > 0.  
sizeint or tuple of ints, optional


Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if df is a scalar. Otherwise, np.array(df).size samples are drawn.    Returns 
 
outndarray or scalar


Drawn samples from the parameterized chi-square distribution.    Raises 
 ValueError

When df <= 0 or when an inappropriate size (e.g. size=-1) is given.      See also  Generator.chisquare

which should be used for new code.    Notes The variable obtained by summing the squares of df independent, standard normally distributed random variables:  \[Q = \sum_{i=0}^{\mathtt{df}} X^2_i\] is chi-square distributed, denoted  \[Q \sim \chi^2_k.\] The probability density function of the chi-squared distribution is  \[p(x) = \frac{(1/2)^{k/2}}{\Gamma(k/2)} x^{k/2 - 1} e^{-x/2},\] where \(\Gamma\) is the gamma function,  \[\Gamma(x) = \int_0^{-\infty} t^{x - 1} e^{-t} dt.\] References  1 
NIST “Engineering Statistics Handbook” https://www.itl.nist.gov/div898/handbook/eda/section3/eda3666.htm   Examples >>> np.random.chisquare(2,4)
array([ 1.89920014,  9.00867716,  3.13710533,  5.62318272]) # random
pandas.DataFrame.filter   DataFrame.filter(items=None, like=None, regex=None, axis=None)[source]
 
Subset the dataframe rows or columns according to the specified index labels. Note that this routine does not filter a dataframe on its contents. The filter is applied to the labels of the index.  Parameters 
 
items:list-like


Keep labels from axis which are in items.  
like:str


Keep labels from axis for which “like in label == True”.  
regex:str (regular expression)


Keep labels from axis for which re.search(regex, label) == True.  
axis:{0 or ‘index’, 1 or ‘columns’, None}, default None


The axis to filter on, expressed either as an index (int) or axis name (str). By default this is the info axis, ‘index’ for Series, ‘columns’ for DataFrame.    Returns 
 same type as input object
    See also  DataFrame.loc

Access a group of rows and columns by label(s) or a boolean array.    Notes The items, like, and regex parameters are enforced to be mutually exclusive. axis defaults to the info axis that is used when indexing with []. Examples 
>>> df = pd.DataFrame(np.array(([1, 2, 3], [4, 5, 6])),
...                   index=['mouse', 'rabbit'],
...                   columns=['one', 'two', 'three'])
>>> df
        one  two  three
mouse     1    2      3
rabbit    4    5      6
  
>>> # select columns by name
>>> df.filter(items=['one', 'three'])
         one  three
mouse     1      3
rabbit    4      6
  
>>> # select columns by regular expression
>>> df.filter(regex='e$', axis=1)
         one  three
mouse     1      3
rabbit    4      6
  
>>> # select rows containing 'bbi'
>>> df.filter(like='bbi', axis=0)
         one  two  three
rabbit    4    5      6
numpy.random.Generator.chisquare method   random.Generator.chisquare(df, size=None)
 
Draw samples from a chi-square distribution. When df independent random variables, each with standard normal distributions (mean 0, variance 1), are squared and summed, the resulting distribution is chi-square (see Notes). This distribution is often used in hypothesis testing.  Parameters 
 
dffloat or array_like of floats


Number of degrees of freedom, must be > 0.  
sizeint or tuple of ints, optional


Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if df is a scalar. Otherwise, np.array(df).size samples are drawn.    Returns 
 
outndarray or scalar


Drawn samples from the parameterized chi-square distribution.    Raises 
 ValueError

When df <= 0 or when an inappropriate size (e.g. size=-1) is given.     Notes The variable obtained by summing the squares of df independent, standard normally distributed random variables:  \[Q = \sum_{i=0}^{\mathtt{df}} X^2_i\] is chi-square distributed, denoted  \[Q \sim \chi^2_k.\] The probability density function of the chi-squared distribution is  \[p(x) = \frac{(1/2)^{k/2}}{\Gamma(k/2)} x^{k/2 - 1} e^{-x/2},\] where \(\Gamma\) is the gamma function,  \[\Gamma(x) = \int_0^{-\infty} t^{x - 1} e^{-t} dt.\] References  1 
NIST “Engineering Statistics Handbook” https://www.itl.nist.gov/div898/handbook/eda/section3/eda3666.htm   Examples >>> np.random.default_rng().chisquare(2,4)
array([ 1.89920014,  9.00867716,  3.13710533,  5.62318272]) # random
def f_27975069(df):
    """filter rows of datafram `df` containing key word `ball` in column `ids`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
pandas.Series.keys   Series.keys()[source]
 
Return alias for index.  Returns 
 Index

Index of the Series.
pandas.TimedeltaIndex.to_frame   TimedeltaIndex.to_frame(index=True, name=NoDefault.no_default)[source]
 
Create a DataFrame with a column containing the Index.  Parameters 
 
index:bool, default True


Set the index of the returned DataFrame as the original Index.  
name:object, default None


The passed name should substitute for the index name (if it has one).    Returns 
 DataFrame

DataFrame containing the original Index data.      See also  Index.to_series

Convert an Index to a Series.  Series.to_frame

Convert Series to DataFrame.    Examples 
>>> idx = pd.Index(['Ant', 'Bear', 'Cow'], name='animal')
>>> idx.to_frame()
       animal
animal
Ant       Ant
Bear     Bear
Cow       Cow
  By default, the original Index is reused. To enforce a new Index: 
>>> idx.to_frame(index=False)
    animal
0   Ant
1  Bear
2   Cow
  To override the name of the resulting column, specify name: 
>>> idx.to_frame(index=False, name='zoo')
    zoo
0   Ant
1  Bear
2   Cow
pandas.Index.to_frame   Index.to_frame(index=True, name=NoDefault.no_default)[source]
 
Create a DataFrame with a column containing the Index.  Parameters 
 
index:bool, default True


Set the index of the returned DataFrame as the original Index.  
name:object, default None


The passed name should substitute for the index name (if it has one).    Returns 
 DataFrame

DataFrame containing the original Index data.      See also  Index.to_series

Convert an Index to a Series.  Series.to_frame

Convert Series to DataFrame.    Examples 
>>> idx = pd.Index(['Ant', 'Bear', 'Cow'], name='animal')
>>> idx.to_frame()
       animal
animal
Ant       Ant
Bear     Bear
Cow       Cow
  By default, the original Index is reused. To enforce a new Index: 
>>> idx.to_frame(index=False)
    animal
0   Ant
1  Bear
2   Cow
  To override the name of the resulting column, specify name: 
>>> idx.to_frame(index=False, name='zoo')
    zoo
0   Ant
1  Bear
2   Cow
pandas.DatetimeIndex.to_frame   DatetimeIndex.to_frame(index=True, name=NoDefault.no_default)[source]
 
Create a DataFrame with a column containing the Index.  Parameters 
 
index:bool, default True


Set the index of the returned DataFrame as the original Index.  
name:object, default None


The passed name should substitute for the index name (if it has one).    Returns 
 DataFrame

DataFrame containing the original Index data.      See also  Index.to_series

Convert an Index to a Series.  Series.to_frame

Convert Series to DataFrame.    Examples 
>>> idx = pd.Index(['Ant', 'Bear', 'Cow'], name='animal')
>>> idx.to_frame()
       animal
animal
Ant       Ant
Bear     Bear
Cow       Cow
  By default, the original Index is reused. To enforce a new Index: 
>>> idx.to_frame(index=False)
    animal
0   Ant
1  Bear
2   Cow
  To override the name of the resulting column, specify name: 
>>> idx.to_frame(index=False, name='zoo')
    zoo
0   Ant
1  Bear
2   Cow
pandas.DataFrame.index   DataFrame.index
 
The index (row labels) of the DataFrame.
def f_20461165(df):
    """convert index at level 0 into a column in dataframe `df`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
pandas.MultiIndex.from_frame   classmethodMultiIndex.from_frame(df, sortorder=None, names=None)[source]
 
Make a MultiIndex from a DataFrame.  Parameters 
 
df:DataFrame


DataFrame to be converted to MultiIndex.  
sortorder:int, optional


Level of sortedness (must be lexicographically sorted by that level).  
names:list-like, optional


If no names are provided, use the column names, or tuple of column names if the columns is a MultiIndex. If a sequence, overwrite names with the given sequence.    Returns 
 MultiIndex

The MultiIndex representation of the given DataFrame.      See also  MultiIndex.from_arrays

Convert list of arrays to MultiIndex.  MultiIndex.from_tuples

Convert list of tuples to MultiIndex.  MultiIndex.from_product

Make a MultiIndex from cartesian product of iterables.    Examples 
>>> df = pd.DataFrame([['HI', 'Temp'], ['HI', 'Precip'],
...                    ['NJ', 'Temp'], ['NJ', 'Precip']],
...                   columns=['a', 'b'])
>>> df
      a       b
0    HI    Temp
1    HI  Precip
2    NJ    Temp
3    NJ  Precip
  
>>> pd.MultiIndex.from_frame(df)
MultiIndex([('HI',   'Temp'),
            ('HI', 'Precip'),
            ('NJ',   'Temp'),
            ('NJ', 'Precip')],
           names=['a', 'b'])
  Using explicit names, instead of the column names 
>>> pd.MultiIndex.from_frame(df, names=['state', 'observation'])
MultiIndex([('HI',   'Temp'),
            ('HI', 'Precip'),
            ('NJ',   'Temp'),
            ('NJ', 'Precip')],
           names=['state', 'observation'])
pandas.DataFrame.reindex_like   DataFrame.reindex_like(other, method=None, copy=True, limit=None, tolerance=None)[source]
 
Return an object with matching indices as other object. Conform the object to the same index on all axes. Optional filling logic, placing NaN in locations having no value in the previous index. A new object is produced unless the new index is equivalent to the current one and copy=False.  Parameters 
 
other:Object of the same data type


Its row and column indices are used to define the new indices of this object.  
method:{None, ‘backfill’/’bfill’, ‘pad’/’ffill’, ‘nearest’}


Method to use for filling holes in reindexed DataFrame. Please note: this is only applicable to DataFrames/Series with a monotonically increasing/decreasing index.  None (default): don’t fill gaps pad / ffill: propagate last valid observation forward to next valid backfill / bfill: use next valid observation to fill gap nearest: use nearest valid observations to fill gap.   
copy:bool, default True


Return a new object, even if the passed indexes are the same.  
limit:int, default None


Maximum number of consecutive labels to fill for inexact matches.  
tolerance:optional


Maximum distance between original and new labels for inexact matches. The values of the index at the matching locations must satisfy the equation abs(index[indexer] - target) <= tolerance. Tolerance may be a scalar value, which applies the same tolerance to all values, or list-like, which applies variable tolerance per element. List-like includes list, tuple, array, Series, and must be the same size as the index and its dtype must exactly match the index’s type.    Returns 
 Series or DataFrame

Same type as caller, but with changed indices on each axis.      See also  DataFrame.set_index

Set row labels.  DataFrame.reset_index

Remove row labels or move them to new columns.  DataFrame.reindex

Change to new indices or expand indices.    Notes Same as calling .reindex(index=other.index, columns=other.columns,...). Examples 
>>> df1 = pd.DataFrame([[24.3, 75.7, 'high'],
...                     [31, 87.8, 'high'],
...                     [22, 71.6, 'medium'],
...                     [35, 95, 'medium']],
...                    columns=['temp_celsius', 'temp_fahrenheit',
...                             'windspeed'],
...                    index=pd.date_range(start='2014-02-12',
...                                        end='2014-02-15', freq='D'))
  
>>> df1
            temp_celsius  temp_fahrenheit windspeed
2014-02-12          24.3             75.7      high
2014-02-13          31.0             87.8      high
2014-02-14          22.0             71.6    medium
2014-02-15          35.0             95.0    medium
  
>>> df2 = pd.DataFrame([[28, 'low'],
...                     [30, 'low'],
...                     [35.1, 'medium']],
...                    columns=['temp_celsius', 'windspeed'],
...                    index=pd.DatetimeIndex(['2014-02-12', '2014-02-13',
...                                            '2014-02-15']))
  
>>> df2
            temp_celsius windspeed
2014-02-12          28.0       low
2014-02-13          30.0       low
2014-02-15          35.1    medium
  
>>> df2.reindex_like(df1)
            temp_celsius  temp_fahrenheit windspeed
2014-02-12          28.0              NaN       low
2014-02-13          30.0              NaN       low
2014-02-14           NaN              NaN       NaN
2014-02-15          35.1              NaN    medium
pandas.DataFrame.reindex   DataFrame.reindex(labels=None, index=None, columns=None, axis=None, method=None, copy=True, level=None, fill_value=nan, limit=None, tolerance=None)[source]
 
Conform Series/DataFrame to new index with optional filling logic. Places NA/NaN in locations having no value in the previous index. A new object is produced unless the new index is equivalent to the current one and copy=False.  Parameters 
 
keywords for axes:array-like, optional


New labels / index to conform to, should be specified using keywords. Preferably an Index object to avoid duplicating data.  
method:{None, ‘backfill’/’bfill’, ‘pad’/’ffill’, ‘nearest’}


Method to use for filling holes in reindexed DataFrame. Please note: this is only applicable to DataFrames/Series with a monotonically increasing/decreasing index.  None (default): don’t fill gaps pad / ffill: Propagate last valid observation forward to next valid. backfill / bfill: Use next valid observation to fill gap. nearest: Use nearest valid observations to fill gap.   
copy:bool, default True


Return a new object, even if the passed indexes are the same.  
level:int or name


Broadcast across a level, matching Index values on the passed MultiIndex level.  
fill_value:scalar, default np.NaN


Value to use for missing values. Defaults to NaN, but can be any “compatible” value.  
limit:int, default None


Maximum number of consecutive elements to forward or backward fill.  
tolerance:optional


Maximum distance between original and new labels for inexact matches. The values of the index at the matching locations most satisfy the equation abs(index[indexer] - target) <= tolerance. Tolerance may be a scalar value, which applies the same tolerance to all values, or list-like, which applies variable tolerance per element. List-like includes list, tuple, array, Series, and must be the same size as the index and its dtype must exactly match the index’s type.    Returns 
 Series/DataFrame with changed index.
    See also  DataFrame.set_index

Set row labels.  DataFrame.reset_index

Remove row labels or move them to new columns.  DataFrame.reindex_like

Change to same indices as other DataFrame.    Examples DataFrame.reindex supports two calling conventions  (index=index_labels, columns=column_labels, ...) (labels, axis={'index', 'columns'}, ...)  We highly recommend using keyword arguments to clarify your intent. Create a dataframe with some fictional data. 
>>> index = ['Firefox', 'Chrome', 'Safari', 'IE10', 'Konqueror']
>>> df = pd.DataFrame({'http_status': [200, 200, 404, 404, 301],
...                   'response_time': [0.04, 0.02, 0.07, 0.08, 1.0]},
...                   index=index)
>>> df
           http_status  response_time
Firefox            200           0.04
Chrome             200           0.02
Safari             404           0.07
IE10               404           0.08
Konqueror          301           1.00
  Create a new index and reindex the dataframe. By default values in the new index that do not have corresponding records in the dataframe are assigned NaN. 
>>> new_index = ['Safari', 'Iceweasel', 'Comodo Dragon', 'IE10',
...              'Chrome']
>>> df.reindex(new_index)
               http_status  response_time
Safari               404.0           0.07
Iceweasel              NaN            NaN
Comodo Dragon          NaN            NaN
IE10                 404.0           0.08
Chrome               200.0           0.02
  We can fill in the missing values by passing a value to the keyword fill_value. Because the index is not monotonically increasing or decreasing, we cannot use arguments to the keyword method to fill the NaN values. 
>>> df.reindex(new_index, fill_value=0)
               http_status  response_time
Safari                 404           0.07
Iceweasel                0           0.00
Comodo Dragon            0           0.00
IE10                   404           0.08
Chrome                 200           0.02
  
>>> df.reindex(new_index, fill_value='missing')
              http_status response_time
Safari                404          0.07
Iceweasel         missing       missing
Comodo Dragon     missing       missing
IE10                  404          0.08
Chrome                200          0.02
  We can also reindex the columns. 
>>> df.reindex(columns=['http_status', 'user_agent'])
           http_status  user_agent
Firefox            200         NaN
Chrome             200         NaN
Safari             404         NaN
IE10               404         NaN
Konqueror          301         NaN
  Or we can use “axis-style” keyword arguments 
>>> df.reindex(['http_status', 'user_agent'], axis="columns")
           http_status  user_agent
Firefox            200         NaN
Chrome             200         NaN
Safari             404         NaN
IE10               404         NaN
Konqueror          301         NaN
  To further illustrate the filling functionality in reindex, we will create a dataframe with a monotonically increasing index (for example, a sequence of dates). 
>>> date_index = pd.date_range('1/1/2010', periods=6, freq='D')
>>> df2 = pd.DataFrame({"prices": [100, 101, np.nan, 100, 89, 88]},
...                    index=date_index)
>>> df2
            prices
2010-01-01   100.0
2010-01-02   101.0
2010-01-03     NaN
2010-01-04   100.0
2010-01-05    89.0
2010-01-06    88.0
  Suppose we decide to expand the dataframe to cover a wider date range. 
>>> date_index2 = pd.date_range('12/29/2009', periods=10, freq='D')
>>> df2.reindex(date_index2)
            prices
2009-12-29     NaN
2009-12-30     NaN
2009-12-31     NaN
2010-01-01   100.0
2010-01-02   101.0
2010-01-03     NaN
2010-01-04   100.0
2010-01-05    89.0
2010-01-06    88.0
2010-01-07     NaN
  The index entries that did not have a value in the original data frame (for example, ‘2009-12-29’) are by default filled with NaN. If desired, we can fill in the missing values using one of several options. For example, to back-propagate the last valid value to fill the NaN values, pass bfill as an argument to the method keyword. 
>>> df2.reindex(date_index2, method='bfill')
            prices
2009-12-29   100.0
2009-12-30   100.0
2009-12-31   100.0
2010-01-01   100.0
2010-01-02   101.0
2010-01-03     NaN
2010-01-04   100.0
2010-01-05    89.0
2010-01-06    88.0
2010-01-07     NaN
  Please note that the NaN value present in the original dataframe (at index value 2010-01-03) will not be filled by any of the value propagation schemes. This is because filling while reindexing does not look at dataframe values, but only compares the original and desired indexes. If you do want to fill in the NaN values present in the original dataframe, use the fillna() method. See the user guide for more.
pandas.Series.reindex_like   Series.reindex_like(other, method=None, copy=True, limit=None, tolerance=None)[source]
 
Return an object with matching indices as other object. Conform the object to the same index on all axes. Optional filling logic, placing NaN in locations having no value in the previous index. A new object is produced unless the new index is equivalent to the current one and copy=False.  Parameters 
 
other:Object of the same data type


Its row and column indices are used to define the new indices of this object.  
method:{None, ‘backfill’/’bfill’, ‘pad’/’ffill’, ‘nearest’}


Method to use for filling holes in reindexed DataFrame. Please note: this is only applicable to DataFrames/Series with a monotonically increasing/decreasing index.  None (default): don’t fill gaps pad / ffill: propagate last valid observation forward to next valid backfill / bfill: use next valid observation to fill gap nearest: use nearest valid observations to fill gap.   
copy:bool, default True


Return a new object, even if the passed indexes are the same.  
limit:int, default None


Maximum number of consecutive labels to fill for inexact matches.  
tolerance:optional


Maximum distance between original and new labels for inexact matches. The values of the index at the matching locations must satisfy the equation abs(index[indexer] - target) <= tolerance. Tolerance may be a scalar value, which applies the same tolerance to all values, or list-like, which applies variable tolerance per element. List-like includes list, tuple, array, Series, and must be the same size as the index and its dtype must exactly match the index’s type.    Returns 
 Series or DataFrame

Same type as caller, but with changed indices on each axis.      See also  DataFrame.set_index

Set row labels.  DataFrame.reset_index

Remove row labels or move them to new columns.  DataFrame.reindex

Change to new indices or expand indices.    Notes Same as calling .reindex(index=other.index, columns=other.columns,...). Examples 
>>> df1 = pd.DataFrame([[24.3, 75.7, 'high'],
...                     [31, 87.8, 'high'],
...                     [22, 71.6, 'medium'],
...                     [35, 95, 'medium']],
...                    columns=['temp_celsius', 'temp_fahrenheit',
...                             'windspeed'],
...                    index=pd.date_range(start='2014-02-12',
...                                        end='2014-02-15', freq='D'))
  
>>> df1
            temp_celsius  temp_fahrenheit windspeed
2014-02-12          24.3             75.7      high
2014-02-13          31.0             87.8      high
2014-02-14          22.0             71.6    medium
2014-02-15          35.0             95.0    medium
  
>>> df2 = pd.DataFrame([[28, 'low'],
...                     [30, 'low'],
...                     [35.1, 'medium']],
...                    columns=['temp_celsius', 'windspeed'],
...                    index=pd.DatetimeIndex(['2014-02-12', '2014-02-13',
...                                            '2014-02-15']))
  
>>> df2
            temp_celsius windspeed
2014-02-12          28.0       low
2014-02-13          30.0       low
2014-02-15          35.1    medium
  
>>> df2.reindex_like(df1)
            temp_celsius  temp_fahrenheit windspeed
2014-02-12          28.0              NaN       low
2014-02-13          30.0              NaN       low
2014-02-14           NaN              NaN       NaN
2014-02-15          35.1              NaN    medium
pandas.Series.reindex   Series.reindex(*args, **kwargs)[source]
 
Conform Series to new index with optional filling logic. Places NA/NaN in locations having no value in the previous index. A new object is produced unless the new index is equivalent to the current one and copy=False.  Parameters 
 
index:array-like, optional


New labels / index to conform to, should be specified using keywords. Preferably an Index object to avoid duplicating data.  
method:{None, ‘backfill’/’bfill’, ‘pad’/’ffill’, ‘nearest’}


Method to use for filling holes in reindexed DataFrame. Please note: this is only applicable to DataFrames/Series with a monotonically increasing/decreasing index.  None (default): don’t fill gaps pad / ffill: Propagate last valid observation forward to next valid. backfill / bfill: Use next valid observation to fill gap. nearest: Use nearest valid observations to fill gap.   
copy:bool, default True


Return a new object, even if the passed indexes are the same.  
level:int or name


Broadcast across a level, matching Index values on the passed MultiIndex level.  
fill_value:scalar, default np.NaN


Value to use for missing values. Defaults to NaN, but can be any “compatible” value.  
limit:int, default None


Maximum number of consecutive elements to forward or backward fill.  
tolerance:optional


Maximum distance between original and new labels for inexact matches. The values of the index at the matching locations most satisfy the equation abs(index[indexer] - target) <= tolerance. Tolerance may be a scalar value, which applies the same tolerance to all values, or list-like, which applies variable tolerance per element. List-like includes list, tuple, array, Series, and must be the same size as the index and its dtype must exactly match the index’s type.    Returns 
 Series with changed index.
    See also  DataFrame.set_index

Set row labels.  DataFrame.reset_index

Remove row labels or move them to new columns.  DataFrame.reindex_like

Change to same indices as other DataFrame.    Examples DataFrame.reindex supports two calling conventions  (index=index_labels, columns=column_labels, ...) (labels, axis={'index', 'columns'}, ...)  We highly recommend using keyword arguments to clarify your intent. Create a dataframe with some fictional data. 
>>> index = ['Firefox', 'Chrome', 'Safari', 'IE10', 'Konqueror']
>>> df = pd.DataFrame({'http_status': [200, 200, 404, 404, 301],
...                   'response_time': [0.04, 0.02, 0.07, 0.08, 1.0]},
...                   index=index)
>>> df
           http_status  response_time
Firefox            200           0.04
Chrome             200           0.02
Safari             404           0.07
IE10               404           0.08
Konqueror          301           1.00
  Create a new index and reindex the dataframe. By default values in the new index that do not have corresponding records in the dataframe are assigned NaN. 
>>> new_index = ['Safari', 'Iceweasel', 'Comodo Dragon', 'IE10',
...              'Chrome']
>>> df.reindex(new_index)
               http_status  response_time
Safari               404.0           0.07
Iceweasel              NaN            NaN
Comodo Dragon          NaN            NaN
IE10                 404.0           0.08
Chrome               200.0           0.02
  We can fill in the missing values by passing a value to the keyword fill_value. Because the index is not monotonically increasing or decreasing, we cannot use arguments to the keyword method to fill the NaN values. 
>>> df.reindex(new_index, fill_value=0)
               http_status  response_time
Safari                 404           0.07
Iceweasel                0           0.00
Comodo Dragon            0           0.00
IE10                   404           0.08
Chrome                 200           0.02
  
>>> df.reindex(new_index, fill_value='missing')
              http_status response_time
Safari                404          0.07
Iceweasel         missing       missing
Comodo Dragon     missing       missing
IE10                  404          0.08
Chrome                200          0.02
  We can also reindex the columns. 
>>> df.reindex(columns=['http_status', 'user_agent'])
           http_status  user_agent
Firefox            200         NaN
Chrome             200         NaN
Safari             404         NaN
IE10               404         NaN
Konqueror          301         NaN
  Or we can use “axis-style” keyword arguments 
>>> df.reindex(['http_status', 'user_agent'], axis="columns")
           http_status  user_agent
Firefox            200         NaN
Chrome             200         NaN
Safari             404         NaN
IE10               404         NaN
Konqueror          301         NaN
  To further illustrate the filling functionality in reindex, we will create a dataframe with a monotonically increasing index (for example, a sequence of dates). 
>>> date_index = pd.date_range('1/1/2010', periods=6, freq='D')
>>> df2 = pd.DataFrame({"prices": [100, 101, np.nan, 100, 89, 88]},
...                    index=date_index)
>>> df2
            prices
2010-01-01   100.0
2010-01-02   101.0
2010-01-03     NaN
2010-01-04   100.0
2010-01-05    89.0
2010-01-06    88.0
  Suppose we decide to expand the dataframe to cover a wider date range. 
>>> date_index2 = pd.date_range('12/29/2009', periods=10, freq='D')
>>> df2.reindex(date_index2)
            prices
2009-12-29     NaN
2009-12-30     NaN
2009-12-31     NaN
2010-01-01   100.0
2010-01-02   101.0
2010-01-03     NaN
2010-01-04   100.0
2010-01-05    89.0
2010-01-06    88.0
2010-01-07     NaN
  The index entries that did not have a value in the original data frame (for example, ‘2009-12-29’) are by default filled with NaN. If desired, we can fill in the missing values using one of several options. For example, to back-propagate the last valid value to fill the NaN values, pass bfill as an argument to the method keyword. 
>>> df2.reindex(date_index2, method='bfill')
            prices
2009-12-29   100.0
2009-12-30   100.0
2009-12-31   100.0
2010-01-01   100.0
2010-01-02   101.0
2010-01-03     NaN
2010-01-04   100.0
2010-01-05    89.0
2010-01-06    88.0
2010-01-07     NaN
  Please note that the NaN value present in the original dataframe (at index value 2010-01-03) will not be filled by any of the value propagation schemes. This is because filling while reindexing does not look at dataframe values, but only compares the original and desired indexes. If you do want to fill in the NaN values present in the original dataframe, use the fillna() method. See the user guide for more.
def f_20461165(df):
    """Add indexes in a data frame `df` to a column `index1`
    """
     
 --------------------
Please refer to the following documentation to generate the code:
pandas.DataFrame.index   DataFrame.index
 
The index (row labels) of the DataFrame.
pandas.MultiIndex.from_frame   classmethodMultiIndex.from_frame(df, sortorder=None, names=None)[source]
 
Make a MultiIndex from a DataFrame.  Parameters 
 
df:DataFrame


DataFrame to be converted to MultiIndex.  
sortorder:int, optional


Level of sortedness (must be lexicographically sorted by that level).  
names:list-like, optional


If no names are provided, use the column names, or tuple of column names if the columns is a MultiIndex. If a sequence, overwrite names with the given sequence.    Returns 
 MultiIndex

The MultiIndex representation of the given DataFrame.      See also  MultiIndex.from_arrays

Convert list of arrays to MultiIndex.  MultiIndex.from_tuples

Convert list of tuples to MultiIndex.  MultiIndex.from_product

Make a MultiIndex from cartesian product of iterables.    Examples 
>>> df = pd.DataFrame([['HI', 'Temp'], ['HI', 'Precip'],
...                    ['NJ', 'Temp'], ['NJ', 'Precip']],
...                   columns=['a', 'b'])
>>> df
      a       b
0    HI    Temp
1    HI  Precip
2    NJ    Temp
3    NJ  Precip
  
>>> pd.MultiIndex.from_frame(df)
MultiIndex([('HI',   'Temp'),
            ('HI', 'Precip'),
            ('NJ',   'Temp'),
            ('NJ', 'Precip')],
           names=['a', 'b'])
  Using explicit names, instead of the column names 
>>> pd.MultiIndex.from_frame(df, names=['state', 'observation'])
MultiIndex([('HI',   'Temp'),
            ('HI', 'Precip'),
            ('NJ',   'Temp'),
            ('NJ', 'Precip')],
           names=['state', 'observation'])
pandas.Index.to_frame   Index.to_frame(index=True, name=NoDefault.no_default)[source]
 
Create a DataFrame with a column containing the Index.  Parameters 
 
index:bool, default True


Set the index of the returned DataFrame as the original Index.  
name:object, default None


The passed name should substitute for the index name (if it has one).    Returns 
 DataFrame

DataFrame containing the original Index data.      See also  Index.to_series

Convert an Index to a Series.  Series.to_frame

Convert Series to DataFrame.    Examples 
>>> idx = pd.Index(['Ant', 'Bear', 'Cow'], name='animal')
>>> idx.to_frame()
       animal
animal
Ant       Ant
Bear     Bear
Cow       Cow
  By default, the original Index is reused. To enforce a new Index: 
>>> idx.to_frame(index=False)
    animal
0   Ant
1  Bear
2   Cow
  To override the name of the resulting column, specify name: 
>>> idx.to_frame(index=False, name='zoo')
    zoo
0   Ant
1  Bear
2   Cow
pandas.DatetimeIndex.to_frame   DatetimeIndex.to_frame(index=True, name=NoDefault.no_default)[source]
 
Create a DataFrame with a column containing the Index.  Parameters 
 
index:bool, default True


Set the index of the returned DataFrame as the original Index.  
name:object, default None


The passed name should substitute for the index name (if it has one).    Returns 
 DataFrame

DataFrame containing the original Index data.      See also  Index.to_series

Convert an Index to a Series.  Series.to_frame

Convert Series to DataFrame.    Examples 
>>> idx = pd.Index(['Ant', 'Bear', 'Cow'], name='animal')
>>> idx.to_frame()
       animal
animal
Ant       Ant
Bear     Bear
Cow       Cow
  By default, the original Index is reused. To enforce a new Index: 
>>> idx.to_frame(index=False)
    animal
0   Ant
1  Bear
2   Cow
  To override the name of the resulting column, specify name: 
>>> idx.to_frame(index=False, name='zoo')
    zoo
0   Ant
1  Bear
2   Cow
pandas.Series.keys   Series.keys()[source]
 
Return alias for index.  Returns 
 Index

Index of the Series.
def f_20461165(df):
    """convert pandas index in a dataframe `df` to columns
    """
    return  
 --------------------

def f_4685571(b):
    """Get reverse of list items from list 'b' using extended slicing
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
numpy.lib.recfunctions.rec_join(key, r1, r2, jointype='inner', r1postfix='1', r2postfix='2', defaults=None)[source]
 
Join arrays r1 and r2 on keys. Alternative to join_by, that always returns a np.recarray.  See also  join_by

equivalent function
zip(*iterables)  
Make an iterator that aggregates elements from each of the iterables. Returns an iterator of tuples, where the i-th tuple contains the i-th element from each of the argument sequences or iterables. The iterator stops when the shortest input iterable is exhausted. With a single iterable argument, it returns an iterator of 1-tuples. With no arguments, it returns an empty iterator. Equivalent to: def zip(*iterables):
    # zip('ABCD', 'xy') --> Ax By
    sentinel = object()
    iterators = [iter(it) for it in iterables]
    while iterators:
        result = []
        for it in iterators:
            elem = next(it, sentinel)
            if elem is sentinel:
                return
            result.append(elem)
        yield tuple(result)
 The left-to-right evaluation order of the iterables is guaranteed. This makes possible an idiom for clustering a data series into n-length groups using zip(*[iter(s)]*n). This repeats the same iterator n times so that each output tuple has the result of n calls to the iterator. This has the effect of dividing the input into n-length chunks. zip() should only be used with unequal length inputs when you don’t care about trailing, unmatched values from the longer iterables. If those values are important, use itertools.zip_longest() instead. zip() in conjunction with the * operator can be used to unzip a list: >>> x = [1, 2, 3]
>>> y = [4, 5, 6]
>>> zipped = zip(x, y)
>>> list(zipped)
[(1, 4), (2, 5), (3, 6)]
>>> x2, y2 = zip(*zip(x, y))
>>> x == list(x2) and y == list(y2)
True
itertools.zip_longest(*iterables, fillvalue=None)  
Make an iterator that aggregates elements from each of the iterables. If the iterables are of uneven length, missing values are filled-in with fillvalue. Iteration continues until the longest iterable is exhausted. Roughly equivalent to: def zip_longest(*args, fillvalue=None):
    # zip_longest('ABCD', 'xy', fillvalue='-') --> Ax By C- D-
    iterators = [iter(it) for it in args]
    num_active = len(iterators)
    if not num_active:
        return
    while True:
        values = []
        for i, it in enumerate(iterators):
            try:
                value = next(it)
            except StopIteration:
                num_active -= 1
                if not num_active:
                    return
                iterators[i] = repeat(fillvalue)
                value = fillvalue
            values.append(value)
        yield tuple(values)
 If one of the iterables is potentially infinite, then the zip_longest() function should be wrapped with something that limits the number of calls (for example islice() or takewhile()). If not specified, fillvalue defaults to None.
numpy.column_stack   numpy.column_stack(tup)[source]
 
Stack 1-D arrays as columns into a 2-D array. Take a sequence of 1-D arrays and stack them as columns to make a single 2-D array. 2-D arrays are stacked as-is, just like with hstack. 1-D arrays are turned into 2-D columns first.  Parameters 
 
tupsequence of 1-D or 2-D arrays.


Arrays to stack. All of them must have the same first dimension.    Returns 
 
stacked2-D array


The array formed by stacking the given arrays.      See also  
stack, hstack, vstack, concatenate

  Examples >>> a = np.array((1,2,3))
>>> b = np.array((2,3,4))
>>> np.column_stack((a,b))
array([[1, 2],
       [2, 3],
       [3, 4]])
join(a, *args)[source]
 
Join given arguments into the same set. Accepts one or more arguments.
def f_17960441(a, b):
    """join each element in array `a` with element at the same index in array `b` as a tuple
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
numpy.column_stack   numpy.column_stack(tup)[source]
 
Stack 1-D arrays as columns into a 2-D array. Take a sequence of 1-D arrays and stack them as columns to make a single 2-D array. 2-D arrays are stacked as-is, just like with hstack. 1-D arrays are turned into 2-D columns first.  Parameters 
 
tupsequence of 1-D or 2-D arrays.


Arrays to stack. All of them must have the same first dimension.    Returns 
 
stacked2-D array


The array formed by stacking the given arrays.      See also  
stack, hstack, vstack, concatenate

  Examples >>> a = np.array((1,2,3))
>>> b = np.array((2,3,4))
>>> np.column_stack((a,b))
array([[1, 2],
       [2, 3],
       [3, 4]])
numpy.ma.column_stack   ma.column_stack(*args, **kwargs) = <numpy.ma.extras._fromnxfunction_seq object>
 
Stack 1-D arrays as columns into a 2-D array. Take a sequence of 1-D arrays and stack them as columns to make a single 2-D array. 2-D arrays are stacked as-is, just like with hstack. 1-D arrays are turned into 2-D columns first.  Parameters 
 
tupsequence of 1-D or 2-D arrays.


Arrays to stack. All of them must have the same first dimension.    Returns 
 
stacked2-D array


The array formed by stacking the given arrays.      See also  
stack, hstack, vstack, concatenate

  Notes The function is applied to both the _data and the _mask, if any. Examples >>> a = np.array((1,2,3))
>>> b = np.array((2,3,4))
>>> np.column_stack((a,b))
array([[1, 2],
       [2, 3],
       [3, 4]])
numpy.atleast_2d   numpy.atleast_2d(*arys)[source]
 
View inputs as arrays with at least two dimensions.  Parameters 
 
arys1, arys2, …array_like


One or more array-like sequences. Non-array inputs are converted to arrays. Arrays that already have two or more dimensions are preserved.    Returns 
 
res, res2, …ndarray


An array, or list of arrays, each with a.ndim >= 2. Copies are avoided where possible, and views with two or more dimensions are returned.      See also  
atleast_1d, atleast_3d

  Examples >>> np.atleast_2d(3.0)
array([[3.]])
 >>> x = np.arange(3.0)
>>> np.atleast_2d(x)
array([[0., 1., 2.]])
>>> np.atleast_2d(x).base is x
True
 >>> np.atleast_2d(1, [1, 2], [[1, 2]])
[array([[1]]), array([[1, 2]]), array([[1, 2]])]
numpy.c_   numpy.c_ = <numpy.lib.index_tricks.CClass object>
 
Translates slice objects to concatenation along the second axis. This is short-hand for np.r_['-1,2,0', index expression], which is useful because of its common occurrence. In particular, arrays will be stacked along their last axis after being upgraded to at least 2-D with 1’s post-pended to the shape (column vectors made out of 1-D arrays).  See also  column_stack

Stack 1-D arrays as columns into a 2-D array.  r_

For more detailed documentation.    Examples >>> np.c_[np.array([1,2,3]), np.array([4,5,6])]
array([[1, 4],
       [2, 5],
       [3, 6]])
>>> np.c_[np.array([[1,2,3]]), 0, 0, np.array([[4,5,6]])]
array([[1, 2, 3, ..., 4, 5, 6]])
numpy.hstack   numpy.hstack(tup)[source]
 
Stack arrays in sequence horizontally (column wise). This is equivalent to concatenation along the second axis, except for 1-D arrays where it concatenates along the first axis. Rebuilds arrays divided by hsplit. This function makes most sense for arrays with up to 3 dimensions. For instance, for pixel-data with a height (first axis), width (second axis), and r/g/b channels (third axis). The functions concatenate, stack and block provide more general stacking and concatenation operations.  Parameters 
 
tupsequence of ndarrays


The arrays must have the same shape along all but the second axis, except 1-D arrays which can be any length.    Returns 
 
stackedndarray


The array formed by stacking the given arrays.      See also  concatenate

Join a sequence of arrays along an existing axis.  stack

Join a sequence of arrays along a new axis.  block

Assemble an nd-array from nested lists of blocks.  vstack

Stack arrays in sequence vertically (row wise).  dstack

Stack arrays in sequence depth wise (along third axis).  column_stack

Stack 1-D arrays as columns into a 2-D array.  hsplit

Split an array into multiple sub-arrays horizontally (column-wise).    Examples >>> a = np.array((1,2,3))
>>> b = np.array((4,5,6))
>>> np.hstack((a,b))
array([1, 2, 3, 4, 5, 6])
>>> a = np.array([[1],[2],[3]])
>>> b = np.array([[4],[5],[6]])
>>> np.hstack((a,b))
array([[1, 4],
       [2, 5],
       [3, 6]])
def f_17960441(a, b):
    """zip two 2-d arrays `a` and `b`
    """
    return  
 --------------------

def f_438684(list_of_ints):
    """convert list `list_of_ints` into a comma separated string
    """
    return  
 --------------------

def f_8519922(url, DATA, HEADERS_DICT, username, password):
    """Send a post request with raw data `DATA` and basic authentication with `username` and `password`
    """
    return  
 --------------------

def f_26443308():
    """Find last occurrence of character '}' in string "abcd}def}"
    """
    return  
 --------------------

def f_22365172():
    """Iterate ove list `[1, 2, 3]` using list comprehension
    """
    return  
 --------------------

def f_12300912(d):
    """extract all the values with keys 'x' and 'y' from a list of dictionaries `d` to list of tuples
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
numpy.polynomial.hermite_e.hermeline   polynomial.hermite_e.hermeline(off, scl)[source]
 
Hermite series whose graph is a straight line.  Parameters 
 
off, sclscalars


The specified line is given by off + scl*x.    Returns 
 
yndarray


This module’s representation of the Hermite series for off + scl*x.      See also  numpy.polynomial.polynomial.polyline
numpy.polynomial.chebyshev.chebline
numpy.polynomial.legendre.legline
numpy.polynomial.laguerre.lagline
numpy.polynomial.hermite.hermline
  Examples >>> from numpy.polynomial.hermite_e import hermeline
>>> from numpy.polynomial.hermite_e import hermeline, hermeval
>>> hermeval(0,hermeline(3, 2))
3.0
>>> hermeval(1,hermeline(3, 2))
5.0
tf.keras.initializers.HeNormal He normal initializer. Inherits From: VarianceScaling, Initializer  View aliases  Main aliases 
tf.initializers.HeNormal, tf.initializers.he_normal, tf.keras.initializers.he_normal  
tf.keras.initializers.HeNormal(
    seed=None
)
 Also available via the shortcut function tf.keras.initializers.he_normal. It draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 / fan_in) where fan_in is the number of input units in the weight tensor. Examples: 
# Standalone usage:
initializer = tf.keras.initializers.HeNormal()
values = initializer(shape=(2, 2))
 
# Usage in a Keras layer:
initializer = tf.keras.initializers.HeNormal()
layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)

 


 Arguments
  seed   A Python integer. An initializer created with a given seed will always produce the same random tensor for a given shape and dtype.    References: He et al., 2015 (pdf) Methods from_config View source 
@classmethod
from_config(
    config
)
 Instantiates an initializer from a configuration dictionary. Example: initializer = RandomUniform(-1, 1)
config = initializer.get_config()
initializer = RandomUniform.from_config(config)

 


 Args
  config   A Python dictionary. It will typically be the output of get_config.   
 


 Returns   An Initializer instance.  
 get_config View source 
get_config()
 Returns the configuration of the initializer as a JSON-serializable dict.
 


 Returns   A JSON-serializable Python dict.  
 __call__ View source 
__call__(
    shape, dtype=None, **kwargs
)
 Returns a tensor object initialized as specified by the initializer.
 


 Args
  shape   Shape of the tensor.  
  dtype   Optional dtype of the tensor. Only floating point types are supported. If not specified, tf.keras.backend.floatx() is used, which default to float32 unless you configured it otherwise (via tf.keras.backend.set_floatx(float_dtype))  
  **kwargs   Additional keyword arguments.
tf.keras.initializers.HeUniform He uniform variance scaling initializer. Inherits From: VarianceScaling, Initializer  View aliases  Main aliases 
tf.initializers.HeUniform, tf.initializers.he_uniform, tf.keras.initializers.he_uniform  
tf.keras.initializers.HeUniform(
    seed=None
)
 Also available via the shortcut function tf.keras.initializers.he_uniform. Draws samples from a uniform distribution within [-limit, limit], where limit = sqrt(6 / fan_in) (fan_in is the number of input units in the weight tensor). Examples: 
# Standalone usage:
initializer = tf.keras.initializers.HeUniform()
values = initializer(shape=(2, 2))
 
# Usage in a Keras layer:
initializer = tf.keras.initializers.HeUniform()
layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)

 


 Arguments
  seed   A Python integer. An initializer created with a given seed will always produce the same random tensor for a given shape and dtype.    References: He et al., 2015 (pdf) Methods from_config View source 
@classmethod
from_config(
    config
)
 Instantiates an initializer from a configuration dictionary. Example: initializer = RandomUniform(-1, 1)
config = initializer.get_config()
initializer = RandomUniform.from_config(config)

 


 Args
  config   A Python dictionary. It will typically be the output of get_config.   
 


 Returns   An Initializer instance.  
 get_config View source 
get_config()
 Returns the configuration of the initializer as a JSON-serializable dict.
 


 Returns   A JSON-serializable Python dict.  
 __call__ View source 
__call__(
    shape, dtype=None, **kwargs
)
 Returns a tensor object initialized as specified by the initializer.
 


 Args
  shape   Shape of the tensor.  
  dtype   Optional dtype of the tensor. Only floating point types are supported. If not specified, tf.keras.backend.floatx() is used, which default to float32 unless you configured it otherwise (via tf.keras.backend.set_floatx(float_dtype))  
  **kwargs   Additional keyword arguments.
numpy.polynomial.hermite.hermline   polynomial.hermite.hermline(off, scl)[source]
 
Hermite series whose graph is a straight line.  Parameters 
 
off, sclscalars


The specified line is given by off + scl*x.    Returns 
 
yndarray


This module’s representation of the Hermite series for off + scl*x.      See also  numpy.polynomial.polynomial.polyline
numpy.polynomial.chebyshev.chebline
numpy.polynomial.legendre.legline
numpy.polynomial.laguerre.lagline
numpy.polynomial.hermite_e.hermeline
  Examples >>> from numpy.polynomial.hermite import hermline, hermval
>>> hermval(0,hermline(3, 2))
3.0
>>> hermval(1,hermline(3, 2))
5.0
exception smtplib.SMTPHeloError  
The server refused our HELO message.
def f_678236():
    """get the filename without the extension from file 'hemanth.txt'
    """
    return  
 --------------------

def f_7895449():
    """create a list containing flattened list `[['A', 'B', 'C'], ['D', 'E', 'F'], ['G', 'H', 'I']]`
    """
    return  
 --------------------

def f_31617845(df):
    """select rows in a dataframe `df` column 'closing_price' between two values 99 and 101
    """
     
 --------------------
Please refer to the following documentation to generate the code:
curses.nl()  
Enter newline mode. This mode translates the return key into newline on input, and translates newline into return and line-feed on output. Newline mode is initially on.
pandas.DataFrame.to_html   DataFrame.to_html(buf=None, columns=None, col_space=None, header=True, index=True, na_rep='NaN', formatters=None, float_format=None, sparsify=None, index_names=True, justify=None, max_rows=None, max_cols=None, show_dimensions=False, decimal='.', bold_rows=True, classes=None, escape=True, notebook=False, border=None, table_id=None, render_links=False, encoding=None)[source]
 
Render a DataFrame as an HTML table.  Parameters 
 
buf:str, Path or StringIO-like, optional, default None


Buffer to write to. If None, the output is returned as a string.  
columns:sequence, optional, default None


The subset of columns to write. Writes all columns by default.  
col_space:str or int, list or dict of int or str, optional


The minimum width of each column in CSS length units. An int is assumed to be px units.  New in version 0.25.0: Ability to use str.   
header:bool, optional


Whether to print column labels, default True.  
index:bool, optional, default True


Whether to print index (row) labels.  
na_rep:str, optional, default ‘NaN’


String representation of NaN to use.  
formatters:list, tuple or dict of one-param. functions, optional


Formatter functions to apply to columns’ elements by position or name. The result of each function must be a unicode string. List/tuple must be of length equal to the number of columns.  
float_format:one-parameter function, optional, default None


Formatter function to apply to columns’ elements if they are floats. This function must return a unicode string and will be applied only to the non-NaN elements, with NaN being handled by na_rep.  Changed in version 1.2.0.   
sparsify:bool, optional, default True


Set to False for a DataFrame with a hierarchical index to print every multiindex key at each row.  
index_names:bool, optional, default True


Prints the names of the indexes.  
justify:str, default None


How to justify the column labels. If None uses the option from the print configuration (controlled by set_option), ‘right’ out of the box. Valid values are  left right center justify justify-all start end inherit match-parent initial unset.   
max_rows:int, optional


Maximum number of rows to display in the console.  
max_cols:int, optional


Maximum number of columns to display in the console.  
show_dimensions:bool, default False


Display DataFrame dimensions (number of rows by number of columns).  
decimal:str, default ‘.’


Character recognized as decimal separator, e.g. ‘,’ in Europe.  
bold_rows:bool, default True


Make the row labels bold in the output.  
classes:str or list or tuple, default None


CSS class(es) to apply to the resulting html table.  
escape:bool, default True


Convert the characters <, >, and & to HTML-safe sequences.  
notebook:{True, False}, default False


Whether the generated HTML is for IPython Notebook.  
border:int


A border=border attribute is included in the opening <table> tag. Default pd.options.display.html.border.  
table_id:str, optional


A css id is included in the opening <table> tag if specified.  
render_links:bool, default False


Convert URLs to HTML links.  
encoding:str, default “utf-8”


Set character encoding.  New in version 1.0.     Returns 
 str or None

If buf is None, returns the result as a string. Otherwise returns None.      See also  to_string

Convert DataFrame to a string.
pandas.io.formats.style.Styler   classpandas.io.formats.style.Styler(data, precision=None, table_styles=None, uuid=None, caption=None, table_attributes=None, cell_ids=True, na_rep=None, uuid_len=5, decimal=None, thousands=None, escape=None, formatter=None)[source]
 
Helps style a DataFrame or Series according to the data with HTML and CSS.  Parameters 
 
data:Series or DataFrame


Data to be styled - either a Series or DataFrame.  
precision:int, optional


Precision to round floats to. If not given defaults to pandas.options.styler.format.precision.  Changed in version 1.4.0.   
table_styles:list-like, default None


List of {selector: (attr, value)} dicts; see Notes.  
uuid:str, default None


A unique identifier to avoid CSS collisions; generated automatically.  
caption:str, tuple, default None


String caption to attach to the table. Tuple only used for LaTeX dual captions.  
table_attributes:str, default None


Items that show up in the opening <table> tag in addition to automatic (by default) id.  
cell_ids:bool, default True


If True, each cell will have an id attribute in their HTML tag. The id takes the form T_<uuid>_row<num_row>_col<num_col> where <uuid> is the unique identifier, <num_row> is the row number and <num_col> is the column number.  
na_rep:str, optional


Representation for missing values. If na_rep is None, no special formatting is applied, and falls back to pandas.options.styler.format.na_rep.  New in version 1.0.0.   
uuid_len:int, default 5


If uuid is not specified, the length of the uuid to randomly generate expressed in hex characters, in range [0, 32].  New in version 1.2.0.   
decimal:str, optional


Character used as decimal separator for floats, complex and integers. If not given uses pandas.options.styler.format.decimal.  New in version 1.3.0.   
thousands:str, optional, default None


Character used as thousands separator for floats, complex and integers. If not given uses pandas.options.styler.format.thousands.  New in version 1.3.0.   
escape:str, optional


Use ‘html’ to replace the characters &, <, >, ', and " in cell display string with HTML-safe sequences. Use ‘latex’ to replace the characters &, %, $, #, _, {, }, ~, ^, and \ in the cell display string with LaTeX-safe sequences. If not given uses pandas.options.styler.format.escape.  New in version 1.3.0.   
formatter:str, callable, dict, optional


Object to define how values are displayed. See Styler.format. If not given uses pandas.options.styler.format.formatter.  New in version 1.4.0.       See also  DataFrame.style

Return a Styler object containing methods for building a styled HTML representation for the DataFrame.    Notes Most styling will be done by passing style functions into Styler.apply or Styler.applymap. Style functions should return values with strings containing CSS 'attr: value' that will be applied to the indicated cells. If using in the Jupyter notebook, Styler has defined a _repr_html_ to automatically render itself. Otherwise call Styler.to_html to get the generated HTML. CSS classes are attached to the generated HTML  Index and Column names include index_name and level<k> where k is its level in a MultiIndex 
Index label cells include  row_heading row<n> where n is the numeric position of the row level<k> where k is the level in a MultiIndex   Column label cells include * col_heading * col<n> where n is the numeric position of the column * level<k> where k is the level in a MultiIndex Blank cells include blank Data cells include data Trimmed cells include col_trim or row_trim.  Any, or all, or these classes can be renamed by using the css_class_names argument in Styler.set_table_classes, giving a value such as {“row”: “MY_ROW_CLASS”, “col_trim”: “”, “row_trim”: “”}. Attributes       
env (Jinja2 jinja2.Environment)  
template_html (Jinja2 Template)  
template_html_table (Jinja2 Template)  
template_html_style (Jinja2 Template)  
template_latex (Jinja2 Template)  
loader (Jinja2 Loader)    Methods       
apply(func[, axis, subset]) Apply a CSS-styling function column-wise, row-wise, or table-wise.  
apply_index(func[, axis, level]) Apply a CSS-styling function to the index or column headers, level-wise.  
applymap(func[, subset]) Apply a CSS-styling function elementwise.  
applymap_index(func[, axis, level]) Apply a CSS-styling function to the index or column headers, elementwise.  
background_gradient([cmap, low, high, axis, ...]) Color the background in a gradient style.  
bar([subset, axis, color, cmap, width, ...]) Draw bar chart in the cell backgrounds.  
clear() Reset the Styler, removing any previously applied styles.  
export() Export the styles applied to the current Styler.  
format([formatter, subset, na_rep, ...]) Format the text display value of cells.  
format_index([formatter, axis, level, ...]) Format the text display value of index labels or column headers.  
from_custom_template(searchpath[, ...]) Factory function for creating a subclass of Styler.  
hide([subset, axis, level, names]) Hide the entire index / column headers, or specific rows / columns from display.  
hide_columns([subset, level, names]) Hide the column headers or specific keys in the columns from rendering.  
hide_index([subset, level, names]) (DEPRECATED) Hide the entire index, or specific keys in the index from rendering.  
highlight_between([subset, color, axis, ...]) Highlight a defined range with a style.  
highlight_max([subset, color, axis, props]) Highlight the maximum with a style.  
highlight_min([subset, color, axis, props]) Highlight the minimum with a style.  
highlight_null([null_color, subset, props]) Highlight missing values with a style.  
highlight_quantile([subset, color, axis, ...]) Highlight values defined by a quantile with a style.  
pipe(func, *args, **kwargs) Apply func(self, *args, **kwargs), and return the result.  
render([sparse_index, sparse_columns]) (DEPRECATED) Render the Styler including all applied styles to HTML.  
set_caption(caption) Set the text added to a <caption> HTML element.  
set_na_rep(na_rep) (DEPRECATED) Set the missing data representation on a Styler.  
set_precision(precision) (DEPRECATED) Set the precision used to display values.  
set_properties([subset]) Set defined CSS-properties to each <td> HTML element within the given subset.  
set_sticky([axis, pixel_size, levels]) Add CSS to permanently display the index or column headers in a scrolling frame.  
set_table_attributes(attributes) Set the table attributes added to the <table> HTML element.  
set_table_styles([table_styles, axis, ...]) Set the table styles included within the <style> HTML element.  
set_td_classes(classes) Set the DataFrame of strings added to the class attribute of <td> HTML elements.  
set_tooltips(ttips[, props, css_class]) Set the DataFrame of strings on Styler generating :hover tooltips.  
set_uuid(uuid) Set the uuid applied to id attributes of HTML elements.  
text_gradient([cmap, low, high, axis, ...]) Color the text in a gradient style.  
to_excel(excel_writer[, sheet_name, na_rep, ...]) Write Styler to an Excel sheet.  
to_html([buf, table_uuid, table_attributes, ...]) Write Styler to a file, buffer or string in HTML-CSS format.  
to_latex([buf, column_format, position, ...]) Write Styler to a file, buffer or string in LaTeX format.  
use(styles) Set the styles on the current Styler.  
where(cond, value[, other, subset]) (DEPRECATED) Apply CSS-styles based on a conditional function elementwise.
token.NL  
Token value used to indicate a non-terminating newline. The NEWLINE token indicates the end of a logical line of Python code; NL tokens are generated when a logical line of code is continued over multiple physical lines.
window.insertln()  
Insert a blank line under the cursor. All following lines are moved down by one line.
def f_25698710(df):
    """replace all occurences of newlines `\n` with `<br>` in dataframe `df`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
curses.nl()  
Enter newline mode. This mode translates the return key into newline on input, and translates newline into return and line-feed on output. Newline mode is initially on.
pandas.DataFrame.to_html   DataFrame.to_html(buf=None, columns=None, col_space=None, header=True, index=True, na_rep='NaN', formatters=None, float_format=None, sparsify=None, index_names=True, justify=None, max_rows=None, max_cols=None, show_dimensions=False, decimal='.', bold_rows=True, classes=None, escape=True, notebook=False, border=None, table_id=None, render_links=False, encoding=None)[source]
 
Render a DataFrame as an HTML table.  Parameters 
 
buf:str, Path or StringIO-like, optional, default None


Buffer to write to. If None, the output is returned as a string.  
columns:sequence, optional, default None


The subset of columns to write. Writes all columns by default.  
col_space:str or int, list or dict of int or str, optional


The minimum width of each column in CSS length units. An int is assumed to be px units.  New in version 0.25.0: Ability to use str.   
header:bool, optional


Whether to print column labels, default True.  
index:bool, optional, default True


Whether to print index (row) labels.  
na_rep:str, optional, default ‘NaN’


String representation of NaN to use.  
formatters:list, tuple or dict of one-param. functions, optional


Formatter functions to apply to columns’ elements by position or name. The result of each function must be a unicode string. List/tuple must be of length equal to the number of columns.  
float_format:one-parameter function, optional, default None


Formatter function to apply to columns’ elements if they are floats. This function must return a unicode string and will be applied only to the non-NaN elements, with NaN being handled by na_rep.  Changed in version 1.2.0.   
sparsify:bool, optional, default True


Set to False for a DataFrame with a hierarchical index to print every multiindex key at each row.  
index_names:bool, optional, default True


Prints the names of the indexes.  
justify:str, default None


How to justify the column labels. If None uses the option from the print configuration (controlled by set_option), ‘right’ out of the box. Valid values are  left right center justify justify-all start end inherit match-parent initial unset.   
max_rows:int, optional


Maximum number of rows to display in the console.  
max_cols:int, optional


Maximum number of columns to display in the console.  
show_dimensions:bool, default False


Display DataFrame dimensions (number of rows by number of columns).  
decimal:str, default ‘.’


Character recognized as decimal separator, e.g. ‘,’ in Europe.  
bold_rows:bool, default True


Make the row labels bold in the output.  
classes:str or list or tuple, default None


CSS class(es) to apply to the resulting html table.  
escape:bool, default True


Convert the characters <, >, and & to HTML-safe sequences.  
notebook:{True, False}, default False


Whether the generated HTML is for IPython Notebook.  
border:int


A border=border attribute is included in the opening <table> tag. Default pd.options.display.html.border.  
table_id:str, optional


A css id is included in the opening <table> tag if specified.  
render_links:bool, default False


Convert URLs to HTML links.  
encoding:str, default “utf-8”


Set character encoding.  New in version 1.0.     Returns 
 str or None

If buf is None, returns the result as a string. Otherwise returns None.      See also  to_string

Convert DataFrame to a string.
Dialect.lineterminator  
The string used to terminate lines produced by the writer. It defaults to '\r\n'.  Note The reader is hard-coded to recognise either '\r' or '\n' as end-of-line, and ignores lineterminator. This behavior may change in the future.
token.NL  
Token value used to indicate a non-terminating newline. The NEWLINE token indicates the end of a logical line of Python code; NL tokens are generated when a logical line of code is continued over multiple physical lines.
pandas.DataFrame.replace   DataFrame.replace(to_replace=None, value=NoDefault.no_default, inplace=False, limit=None, regex=False, method=NoDefault.no_default)[source]
 
Replace values given in to_replace with value. Values of the DataFrame are replaced with other values dynamically. This differs from updating with .loc or .iloc, which require you to specify a location to update with some value.  Parameters 
 
to_replace:str, regex, list, dict, Series, int, float, or None


How to find the values that will be replaced.  
numeric, str or regex:  
 numeric: numeric values equal to to_replace will be replaced with value str: string exactly matching to_replace will be replaced with value regex: regexs matching to_replace will be replaced with value  
  
list of str, regex, or numeric:  
 First, if to_replace and value are both lists, they must be the same length. Second, if regex=True then all of the strings in both lists will be interpreted as regexs otherwise they will match directly. This doesn’t matter much for value since there are only a few possible substitution regexes you can use. str, regex and numeric rules apply as above.  
  
dict:  
 Dicts can be used to specify different replacement values for different existing values. For example, {'a': 'b', 'y': 'z'} replaces the value ‘a’ with ‘b’ and ‘y’ with ‘z’. To use a dict in this way the value parameter should be None. For a DataFrame a dict can specify that different values should be replaced in different columns. For example, {'a': 1, 'b': 'z'} looks for the value 1 in column ‘a’ and the value ‘z’ in column ‘b’ and replaces these values with whatever is specified in value. The value parameter should not be None in this case. You can treat this as a special case of passing two lists except that you are specifying the column to search in. For a DataFrame nested dictionaries, e.g., {'a': {'b': np.nan}}, are read as follows: look in column ‘a’ for the value ‘b’ and replace it with NaN. The value parameter should be None to use a nested dict in this way. You can nest regular expressions as well. Note that column names (the top-level dictionary keys in a nested dictionary) cannot be regular expressions.  
  
None:  
 This means that the regex argument must be a string, compiled regular expression, or list, dict, ndarray or Series of such elements. If value is also None then this must be a nested dictionary or Series.  
   See the examples section for examples of each of these.  
value:scalar, dict, list, str, regex, default None


Value to replace any values matching to_replace with. For a DataFrame a dict of values can be used to specify which value to use for each column (columns not in the dict will not be filled). Regular expressions, strings and lists or dicts of such objects are also allowed.  
inplace:bool, default False


If True, performs operation inplace and returns None.  
limit:int, default None


Maximum size gap to forward or backward fill.  
regex:bool or same types as to_replace, default False


Whether to interpret to_replace and/or value as regular expressions. If this is True then to_replace must be a string. Alternatively, this could be a regular expression or a list, dict, or array of regular expressions in which case to_replace must be None.  
method:{‘pad’, ‘ffill’, ‘bfill’, None}


The method to use when for replacement, when to_replace is a scalar, list or tuple and value is None.  Changed in version 0.23.0: Added to DataFrame.     Returns 
 DataFrame

Object after replacement.    Raises 
 AssertionError

 If regex is not a bool and to_replace is not None.   TypeError

 If to_replace is not a scalar, array-like, dict, or None If to_replace is a dict and value is not a list, dict, ndarray, or Series If to_replace is None and regex is not compilable into a regular expression or is a list, dict, ndarray, or Series. When replacing multiple bool or datetime64 objects and the arguments to to_replace does not match the type of the value being replaced   ValueError

 If a list or an ndarray is passed to to_replace and value but they are not the same length.       See also  DataFrame.fillna

Fill NA values.  DataFrame.where

Replace values based on boolean condition.  Series.str.replace

Simple string replacement.    Notes  Regex substitution is performed under the hood with re.sub. The rules for substitution for re.sub are the same. Regular expressions will only substitute on strings, meaning you cannot provide, for example, a regular expression matching floating point numbers and expect the columns in your frame that have a numeric dtype to be matched. However, if those floating point numbers are strings, then you can do this. This method has a lot of options. You are encouraged to experiment and play with this method to gain intuition about how it works. When dict is used as the to_replace value, it is like key(s) in the dict are the to_replace part and value(s) in the dict are the value parameter.  Examples Scalar `to_replace` and `value` 
>>> s = pd.Series([1, 2, 3, 4, 5])
>>> s.replace(1, 5)
0    5
1    2
2    3
3    4
4    5
dtype: int64
  
>>> df = pd.DataFrame({'A': [0, 1, 2, 3, 4],
...                    'B': [5, 6, 7, 8, 9],
...                    'C': ['a', 'b', 'c', 'd', 'e']})
>>> df.replace(0, 5)
    A  B  C
0  5  5  a
1  1  6  b
2  2  7  c
3  3  8  d
4  4  9  e
  List-like `to_replace` 
>>> df.replace([0, 1, 2, 3], 4)
    A  B  C
0  4  5  a
1  4  6  b
2  4  7  c
3  4  8  d
4  4  9  e
  
>>> df.replace([0, 1, 2, 3], [4, 3, 2, 1])
    A  B  C
0  4  5  a
1  3  6  b
2  2  7  c
3  1  8  d
4  4  9  e
  
>>> s.replace([1, 2], method='bfill')
0    3
1    3
2    3
3    4
4    5
dtype: int64
  dict-like `to_replace` 
>>> df.replace({0: 10, 1: 100})
        A  B  C
0   10  5  a
1  100  6  b
2    2  7  c
3    3  8  d
4    4  9  e
  
>>> df.replace({'A': 0, 'B': 5}, 100)
        A    B  C
0  100  100  a
1    1    6  b
2    2    7  c
3    3    8  d
4    4    9  e
  
>>> df.replace({'A': {0: 100, 4: 400}})
        A  B  C
0  100  5  a
1    1  6  b
2    2  7  c
3    3  8  d
4  400  9  e
  Regular expression `to_replace` 
>>> df = pd.DataFrame({'A': ['bat', 'foo', 'bait'],
...                    'B': ['abc', 'bar', 'xyz']})
>>> df.replace(to_replace=r'^ba.$', value='new', regex=True)
        A    B
0   new  abc
1   foo  new
2  bait  xyz
  
>>> df.replace({'A': r'^ba.$'}, {'A': 'new'}, regex=True)
        A    B
0   new  abc
1   foo  bar
2  bait  xyz
  
>>> df.replace(regex=r'^ba.$', value='new')
        A    B
0   new  abc
1   foo  new
2  bait  xyz
  
>>> df.replace(regex={r'^ba.$': 'new', 'foo': 'xyz'})
        A    B
0   new  abc
1   xyz  new
2  bait  xyz
  
>>> df.replace(regex=[r'^ba.$', 'foo'], value='new')
        A    B
0   new  abc
1   new  new
2  bait  xyz
  Compare the behavior of s.replace({'a': None}) and s.replace('a', None) to understand the peculiarities of the to_replace parameter: 
>>> s = pd.Series([10, 'a', 'a', 'b', 'a'])
  When one uses a dict as the to_replace value, it is like the value(s) in the dict are equal to the value parameter. s.replace({'a': None}) is equivalent to s.replace(to_replace={'a': None}, value=None, method=None): 
>>> s.replace({'a': None})
0      10
1    None
2    None
3       b
4    None
dtype: object
  When value is not explicitly passed and to_replace is a scalar, list or tuple, replace uses the method parameter (default ‘pad’) to do the replacement. So this is why the ‘a’ values are being replaced by 10 in rows 1 and 2 and ‘b’ in row 4 in this case. 
>>> s.replace('a')
0    10
1    10
2    10
3     b
4     b
dtype: object
  On the other hand, if None is explicitly passed for value, it will be respected: 
>>> s.replace('a', None)
0      10
1    None
2    None
3       b
4    None
dtype: object
   
 Changed in version 1.4.0: Previously the explicit None was silently ignored.
def f_25698710(df):
    """replace all occurrences of a string `\n` by string `<br>` in a pandas data frame `df`
    """
    return  
 --------------------

def f_41923858(word):
    """create a list containing each two adjacent letters in string `word` as its elements
    """
    return  
 --------------------

def f_41923858(word):
    """Get a list of pairs from a string `word` using lambda function
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
urllib.parse.unwrap(url)  
Extract the url from a wrapped URL (that is, a string formatted as <URL:scheme://host/path>, <scheme://host/path>, URL:scheme://host/path or scheme://host/path). If url is not a wrapped URL, it is returned without changes.
urllib.parse.unquote_plus(string, encoding='utf-8', errors='replace')  
Like unquote(), but also replace plus signs with spaces, as required for unquoting HTML form values. string must be a str. Example: unquote_plus('/El+Ni%C3%B1o/') yields '/El Niño/'.
werkzeug.urls.url_fix(s, charset='utf-8')  
Sometimes you get an URL by a user that just isn’t a real URL because it contains unsafe characters like ‘ ‘ and so on. This function can fix some of the problems in a similar way browsers handle data entered by the user: >>> url_fix('http://de.wikipedia.org/wiki/Elf (Begriffskl\xe4rung)')
'http://de.wikipedia.org/wiki/Elf%20(Begriffskl%C3%A4rung)'
  Parameters 
 
s (str) – the string with the URL to fix. 
charset (str) – The target charset for the URL if the url was given as a string.   Return type 
str
urllib.parse.urlsplit(urlstring, scheme='', allow_fragments=True)  
This is similar to urlparse(), but does not split the params from the URL. This should generally be used instead of urlparse() if the more recent URL syntax allowing parameters to be applied to each segment of the path portion of the URL (see RFC 2396) is wanted. A separate function is needed to separate the path segments and parameters. This function returns a 5-item named tuple: (addressing scheme, network location, path, query, fragment identifier).
 The return value is a named tuple, its items can be accessed by index or as named attributes:   
Attribute Index Value Value if not present   
scheme 0 URL scheme specifier scheme parameter  
netloc 1 Network location part empty string  
path 2 Hierarchical path empty string  
query 3 Query component empty string  
fragment 4 Fragment identifier empty string  
username  User name None  
password  Password None  
hostname  Host name (lower case) None  
port  Port number as integer, if present None   Reading the port attribute will raise a ValueError if an invalid port is specified in the URL. See section Structured Parse Results for more information on the result object. Unmatched square brackets in the netloc attribute will raise a ValueError. Characters in the netloc attribute that decompose under NFKC normalization (as used by the IDNA encoding) into any of /, ?, #, @, or : will raise a ValueError. If the URL is decomposed before parsing, no error will be raised.  Changed in version 3.6: Out-of-range port numbers now raise ValueError, instead of returning None.   Changed in version 3.8: Characters that affect netloc parsing under NFKC normalization will now raise ValueError.
urllib.parse.SplitResult.geturl()  
Return the re-combined version of the original URL as a string. This may differ from the original URL in that the scheme may be normalized to lower case and empty components may be dropped. Specifically, empty parameters, queries, and fragment identifiers will be removed. For urldefrag() results, only empty fragment identifiers will be removed. For urlsplit() and urlparse() results, all noted changes will be made to the URL returned by this method. The result of this method remains unchanged if passed back through the original parsing function: >>> from urllib.parse import urlsplit
>>> url = 'HTTP://www.Python.org/doc/#'
>>> r1 = urlsplit(url)
>>> r1.geturl()
'http://www.Python.org/doc/'
>>> r2 = urlsplit(r1.geturl())
>>> r2.geturl()
'http://www.Python.org/doc/'
def f_9760588(myString):
    """extract a url from a string `myString`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
urllib.parse.unwrap(url)  
Extract the url from a wrapped URL (that is, a string formatted as <URL:scheme://host/path>, <scheme://host/path>, URL:scheme://host/path or scheme://host/path). If url is not a wrapped URL, it is returned without changes.
urllib.parse.unquote_plus(string, encoding='utf-8', errors='replace')  
Like unquote(), but also replace plus signs with spaces, as required for unquoting HTML form values. string must be a str. Example: unquote_plus('/El+Ni%C3%B1o/') yields '/El Niño/'.
werkzeug.urls.url_fix(s, charset='utf-8')  
Sometimes you get an URL by a user that just isn’t a real URL because it contains unsafe characters like ‘ ‘ and so on. This function can fix some of the problems in a similar way browsers handle data entered by the user: >>> url_fix('http://de.wikipedia.org/wiki/Elf (Begriffskl\xe4rung)')
'http://de.wikipedia.org/wiki/Elf%20(Begriffskl%C3%A4rung)'
  Parameters 
 
s (str) – the string with the URL to fix. 
charset (str) – The target charset for the URL if the url was given as a string.   Return type 
str
urllib.parse.urlsplit(urlstring, scheme='', allow_fragments=True)  
This is similar to urlparse(), but does not split the params from the URL. This should generally be used instead of urlparse() if the more recent URL syntax allowing parameters to be applied to each segment of the path portion of the URL (see RFC 2396) is wanted. A separate function is needed to separate the path segments and parameters. This function returns a 5-item named tuple: (addressing scheme, network location, path, query, fragment identifier).
 The return value is a named tuple, its items can be accessed by index or as named attributes:   
Attribute Index Value Value if not present   
scheme 0 URL scheme specifier scheme parameter  
netloc 1 Network location part empty string  
path 2 Hierarchical path empty string  
query 3 Query component empty string  
fragment 4 Fragment identifier empty string  
username  User name None  
password  Password None  
hostname  Host name (lower case) None  
port  Port number as integer, if present None   Reading the port attribute will raise a ValueError if an invalid port is specified in the URL. See section Structured Parse Results for more information on the result object. Unmatched square brackets in the netloc attribute will raise a ValueError. Characters in the netloc attribute that decompose under NFKC normalization (as used by the IDNA encoding) into any of /, ?, #, @, or : will raise a ValueError. If the URL is decomposed before parsing, no error will be raised.  Changed in version 3.6: Out-of-range port numbers now raise ValueError, instead of returning None.   Changed in version 3.8: Characters that affect netloc parsing under NFKC normalization will now raise ValueError.
urllib.parse.SplitResult.geturl()  
Return the re-combined version of the original URL as a string. This may differ from the original URL in that the scheme may be normalized to lower case and empty components may be dropped. Specifically, empty parameters, queries, and fragment identifiers will be removed. For urldefrag() results, only empty fragment identifiers will be removed. For urlsplit() and urlparse() results, all noted changes will be made to the URL returned by this method. The result of this method remains unchanged if passed back through the original parsing function: >>> from urllib.parse import urlsplit
>>> url = 'HTTP://www.Python.org/doc/#'
>>> r1 = urlsplit(url)
>>> r1.geturl()
'http://www.Python.org/doc/'
>>> r2 = urlsplit(r1.geturl())
>>> r2.geturl()
'http://www.Python.org/doc/'
def f_9760588(myString):
    """extract a url from a string `myString`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
IMAP4.myrights(mailbox)  
Show my ACLs for a mailbox (i.e. the rights that I have on mailbox).
locale.YESEXPR  
Get a regular expression that can be used with the regex function to recognize a positive response to a yes/no question.  Note The expression is in the syntax suitable for the regex() function from the C library, which might differ from the syntax used in re.
Pattern.split(string, maxsplit=0)  
Identical to the split() function, using the compiled pattern.
transform(X) [source]
 
Reduce X to the selected features.  Parameters 
 
Xarray of shape [n_samples, n_features] 

The input samples.    Returns 
 
X_rarray of shape [n_samples, n_selected_features] 

The input samples with only the selected features.
re.purge()  
Clear the regular expression cache.
def f_5843518(mystring):
    """remove all special characters, punctuation and spaces from a string `mystring` using regex
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
pandas.bdate_range   pandas.bdate_range(start=None, end=None, periods=None, freq='B', tz=None, normalize=True, name=None, weekmask=None, holidays=None, closed=NoDefault.no_default, inclusive=None, **kwargs)[source]
 
Return a fixed frequency DatetimeIndex, with business day as the default frequency.  Parameters 
 
start:str or datetime-like, default None


Left bound for generating dates.  
end:str or datetime-like, default None


Right bound for generating dates.  
periods:int, default None


Number of periods to generate.  
freq:str or DateOffset, default ‘B’ (business daily)


Frequency strings can have multiples, e.g. ‘5H’.  
tz:str or None


Time zone name for returning localized DatetimeIndex, for example Asia/Beijing.  
normalize:bool, default False


Normalize start/end dates to midnight before generating date range.  
name:str, default None


Name of the resulting DatetimeIndex.  
weekmask:str or None, default None


Weekmask of valid business days, passed to numpy.busdaycalendar, only used when custom frequency strings are passed. The default value None is equivalent to ‘Mon Tue Wed Thu Fri’.  
holidays:list-like or None, default None


Dates to exclude from the set of valid business days, passed to numpy.busdaycalendar, only used when custom frequency strings are passed.  
closed:str, default None


Make the interval closed with respect to the given frequency to the ‘left’, ‘right’, or both sides (None).  Deprecated since version 1.4.0: Argument closed has been deprecated to standardize boundary inputs. Use inclusive instead, to set each bound as closed or open.   
inclusive:{“both”, “neither”, “left”, “right”}, default “both”


Include boundaries; Whether to set each bound as closed or open.  New in version 1.4.0.   **kwargs

For compatibility. Has no effect on the result.    Returns 
 DatetimeIndex
   Notes Of the four parameters: start, end, periods, and freq, exactly three must be specified. Specifying freq is a requirement for bdate_range. Use date_range if specifying freq is not desired. To learn more about the frequency strings, please see this link. Examples Note how the two weekend days are skipped in the result. 
>>> pd.bdate_range(start='1/1/2018', end='1/08/2018')
DatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04',
           '2018-01-05', '2018-01-08'],
          dtype='datetime64[ns]', freq='B')
pandas.tseries.offsets.SemiMonthEnd.name   SemiMonthEnd.name
pandas.DatetimeIndex   classpandas.DatetimeIndex(data=None, freq=NoDefault.no_default, tz=None, normalize=False, closed=None, ambiguous='raise', dayfirst=False, yearfirst=False, dtype=None, copy=False, name=None)[source]
 
Immutable ndarray-like of datetime64 data. Represented internally as int64, and which can be boxed to Timestamp objects that are subclasses of datetime and carry metadata.  Parameters 
 
data:array-like (1-dimensional), optional


Optional datetime-like data to construct index with.  
freq:str or pandas offset object, optional


One of pandas date offset strings or corresponding objects. The string ‘infer’ can be passed in order to set the frequency of the index as the inferred frequency upon creation.  
tz:pytz.timezone or dateutil.tz.tzfile or datetime.tzinfo or str


Set the Timezone of the data.  
normalize:bool, default False


Normalize start/end dates to midnight before generating date range.  
closed:{‘left’, ‘right’}, optional


Set whether to include start and end that are on the boundary. The default includes boundary points on either end.  
ambiguous:‘infer’, bool-ndarray, ‘NaT’, default ‘raise’


When clocks moved backward due to DST, ambiguous times may arise. For example in Central European Time (UTC+01), when going from 03:00 DST to 02:00 non-DST, 02:30:00 local time occurs both at 00:30:00 UTC and at 01:30:00 UTC. In such a situation, the ambiguous parameter dictates how ambiguous times should be handled.  ‘infer’ will attempt to infer fall dst-transition hours based on order bool-ndarray where True signifies a DST time, False signifies a non-DST time (note that this flag is only applicable for ambiguous times) ‘NaT’ will return NaT where there are ambiguous times ‘raise’ will raise an AmbiguousTimeError if there are ambiguous times.   
dayfirst:bool, default False


If True, parse dates in data with the day first order.  
yearfirst:bool, default False


If True parse dates in data with the year first order.  
dtype:numpy.dtype or DatetimeTZDtype or str, default None


Note that the only NumPy dtype allowed is ‘datetime64[ns]’.  
copy:bool, default False


Make a copy of input ndarray.  
name:label, default None


Name to be stored in the index.      See also  Index

The base pandas Index type.  TimedeltaIndex

Index of timedelta64 data.  PeriodIndex

Index of Period data.  to_datetime

Convert argument to datetime.  date_range

Create a fixed-frequency DatetimeIndex.    Notes To learn more about the frequency strings, please see this link. Attributes       
year The year of the datetime.  
month The month as January=1, December=12.  
day The day of the datetime.  
hour The hours of the datetime.  
minute The minutes of the datetime.  
second The seconds of the datetime.  
microsecond The microseconds of the datetime.  
nanosecond The nanoseconds of the datetime.  
date Returns numpy array of python datetime.date objects.  
time Returns numpy array of datetime.time objects.  
timetz Returns numpy array of datetime.time objects with timezone information.  
dayofyear The ordinal day of the year.  
day_of_year The ordinal day of the year.  
weekofyear (DEPRECATED) The week ordinal of the year.  
week (DEPRECATED) The week ordinal of the year.  
dayofweek The day of the week with Monday=0, Sunday=6.  
day_of_week The day of the week with Monday=0, Sunday=6.  
weekday The day of the week with Monday=0, Sunday=6.  
quarter The quarter of the date.  
tz Return the timezone.  
freq Return the frequency object if it is set, otherwise None.  
freqstr Return the frequency object as a string if its set, otherwise None.  
is_month_start Indicates whether the date is the first day of the month.  
is_month_end Indicates whether the date is the last day of the month.  
is_quarter_start Indicator for whether the date is the first day of a quarter.  
is_quarter_end Indicator for whether the date is the last day of a quarter.  
is_year_start Indicate whether the date is the first day of a year.  
is_year_end Indicate whether the date is the last day of the year.  
is_leap_year Boolean indicator if the date belongs to a leap year.  
inferred_freq Tries to return a string representing a frequency guess, generated by infer_freq.    Methods       
normalize(*args, **kwargs) Convert times to midnight.  
strftime(*args, **kwargs) Convert to Index using specified date_format.  
snap([freq]) Snap time stamps to nearest occurring frequency.  
tz_convert(tz) Convert tz-aware Datetime Array/Index from one time zone to another.  
tz_localize(tz[, ambiguous, nonexistent]) Localize tz-naive Datetime Array/Index to tz-aware Datetime Array/Index.  
round(*args, **kwargs) Perform round operation on the data to the specified freq.  
floor(*args, **kwargs) Perform floor operation on the data to the specified freq.  
ceil(*args, **kwargs) Perform ceil operation on the data to the specified freq.  
to_period(*args, **kwargs) Cast to PeriodArray/Index at a particular frequency.  
to_perioddelta(freq) Calculate TimedeltaArray of difference between index values and index converted to PeriodArray at specified freq.  
to_pydatetime(*args, **kwargs) Return Datetime Array/Index as object ndarray of datetime.datetime objects.  
to_series([keep_tz, index, name]) Create a Series with both index and values equal to the index keys useful with map for returning an indexer based on an index.  
to_frame([index, name]) Create a DataFrame with a column containing the Index.  
month_name(*args, **kwargs) Return the month names of the DateTimeIndex with specified locale.  
day_name(*args, **kwargs) Return the day names of the DateTimeIndex with specified locale.  
mean(*args, **kwargs) Return the mean value of the Array.  
std(*args, **kwargs) Return sample standard deviation over requested axis.
pandas.tseries.offsets.SemiMonthEnd.n   SemiMonthEnd.n
pandas.tseries.offsets.CustomBusinessMonthEnd.freqstr   CustomBusinessMonthEnd.freqstr
def f_36674519():
    """create a DatetimeIndex containing 13 periods of the second friday of each month starting from date '2016-01-01'
    """
    return  
 --------------------

def f_508657():
    """Create multidimensional array `matrix` with 3 rows and 2 columns in python
    """
     
 --------------------

def f_1007481(mystring):
    """replace spaces with underscore in string `mystring`
    """
    return  
 --------------------

def f_1249786(my_string):
    """split string `my_string` on white spaces
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
str.removesuffix(suffix, /)  
If the string ends with the suffix string and that suffix is not empty, return string[:-len(suffix)]. Otherwise, return a copy of the original string: >>> 'MiscTests'.removesuffix('Tests')
'Misc'
>>> 'TmpDirMixin'.removesuffix('Tests')
'TmpDirMixin'
  New in version 3.9.
bytes.removesuffix(suffix, /)  
bytearray.removesuffix(suffix, /)  
If the binary data ends with the suffix string and that suffix is not empty, return bytes[:-len(suffix)]. Otherwise, return a copy of the original binary data: >>> b'MiscTests'.removesuffix(b'Tests')
b'Misc'
>>> b'TmpDirMixin'.removesuffix(b'Tests')
b'TmpDirMixin'
 The suffix may be any bytes-like object.  Note The bytearray version of this method does not operate in place - it always produces a new object, even if no changes were made.   New in version 3.9.
bytes.removesuffix(suffix, /)  
bytearray.removesuffix(suffix, /)  
If the binary data ends with the suffix string and that suffix is not empty, return bytes[:-len(suffix)]. Otherwise, return a copy of the original binary data: >>> b'MiscTests'.removesuffix(b'Tests')
b'Misc'
>>> b'TmpDirMixin'.removesuffix(b'Tests')
b'TmpDirMixin'
 The suffix may be any bytes-like object.  Note The bytearray version of this method does not operate in place - it always produces a new object, even if no changes were made.   New in version 3.9.
clean_username(username)  
Performs any cleaning on the username (e.g. stripping LDAP DN information) prior to using it to get or create a user object. Returns the cleaned username.
username  
The username portion of the address, with all quoting removed.
def f_4444923(filename):
    """get filename without extension from file `filename`
    """
    return  
 --------------------

def f_13728486(l):
    """get a list containing the sum of each element `i` in list `l` plus the previous elements
    """
    return  
 --------------------

def f_9743134():
    """split a string `Docs/src/Scripts/temp` by `/` keeping `/` in the result
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
sklearn.utils.shuffle(*arrays, random_state=None, n_samples=None) [source]
 
Shuffle arrays or sparse matrices in a consistent way. This is a convenience alias to resample(*arrays, replace=False) to do random permutations of the collections.  Parameters 
 
*arrayssequence of indexable data-structures 

Indexable data-structures can be arrays, lists, dataframes or scipy sparse matrices with consistent first dimension.  
random_stateint, RandomState instance or None, default=None 

Determines random number generation for shuffling the data. Pass an int for reproducible results across multiple function calls. See Glossary.  
n_samplesint, default=None 

Number of samples to generate. If left to None this is automatically set to the first dimension of the arrays. It should not be larger than the length of arrays.    Returns 
 
shuffled_arrayssequence of indexable data-structures 

Sequence of shuffled copies of the collections. The original arrays are not impacted.      See also  
resample

  Examples It is possible to mix sparse and dense arrays in the same run: >>> X = np.array([[1., 0.], [2., 1.], [0., 0.]])
>>> y = np.array([0, 1, 2])

>>> from scipy.sparse import coo_matrix
>>> X_sparse = coo_matrix(X)

>>> from sklearn.utils import shuffle
>>> X, X_sparse, y = shuffle(X, X_sparse, y, random_state=0)
>>> X
array([[0., 0.],
       [2., 1.],
       [1., 0.]])

>>> X_sparse
<3x2 sparse matrix of type '<... 'numpy.float64'>'
    with 3 stored elements in Compressed Sparse Row format>

>>> X_sparse.toarray()
array([[0., 0.],
       [2., 1.],
       [1., 0.]])

>>> y
array([2, 1, 0])

>>> shuffle(y, n_samples=2, random_state=0)
array([0, 1])
sklearn.utils.shuffle  
sklearn.utils.shuffle(*arrays, random_state=None, n_samples=None) [source]
 
Shuffle arrays or sparse matrices in a consistent way. This is a convenience alias to resample(*arrays, replace=False) to do random permutations of the collections.  Parameters 
 
*arrayssequence of indexable data-structures 

Indexable data-structures can be arrays, lists, dataframes or scipy sparse matrices with consistent first dimension.  
random_stateint, RandomState instance or None, default=None 

Determines random number generation for shuffling the data. Pass an int for reproducible results across multiple function calls. See Glossary.  
n_samplesint, default=None 

Number of samples to generate. If left to None this is automatically set to the first dimension of the arrays. It should not be larger than the length of arrays.    Returns 
 
shuffled_arrayssequence of indexable data-structures 

Sequence of shuffled copies of the collections. The original arrays are not impacted.      See also  
resample

  Examples It is possible to mix sparse and dense arrays in the same run: >>> X = np.array([[1., 0.], [2., 1.], [0., 0.]])
>>> y = np.array([0, 1, 2])

>>> from scipy.sparse import coo_matrix
>>> X_sparse = coo_matrix(X)

>>> from sklearn.utils import shuffle
>>> X, X_sparse, y = shuffle(X, X_sparse, y, random_state=0)
>>> X
array([[0., 0.],
       [2., 1.],
       [1., 0.]])

>>> X_sparse
<3x2 sparse matrix of type '<... 'numpy.float64'>'
    with 3 stored elements in Compressed Sparse Row format>

>>> X_sparse.toarray()
array([[0., 0.],
       [2., 1.],
       [1., 0.]])

>>> y
array([2, 1, 0])

>>> shuffle(y, n_samples=2, random_state=0)
array([0, 1])
 
 Examples using sklearn.utils.shuffle
 
  Color Quantization using K-Means  

  Empirical evaluation of the impact of k-means initialization  

  Combine predictors using stacking  

  Model Complexity Influence  

  Prediction Latency  

  Early stopping of Stochastic Gradient Descent  

  Approximate nearest neighbors in TSNE  

  Effect of varying threshold for self-training
random.shuffle(x[, random])  
Shuffle the sequence x in place. The optional argument random is a 0-argument function returning a random float in [0.0, 1.0); by default, this is the function random(). To shuffle an immutable sequence and return a new shuffled list, use sample(x, k=len(x)) instead. Note that even for small len(x), the total number of permutations of x can quickly grow larger than the period of most random number generators. This implies that most permutations of a long sequence can never be generated. For example, a sequence of length 2080 is the largest that can fit within the period of the Mersenne Twister random number generator.  Deprecated since version 3.9, will be removed in version 3.11: The optional parameter random.
itertools.permutations(iterable, r=None)  
Return successive r length permutations of elements in the iterable. If r is not specified or is None, then r defaults to the length of the iterable and all possible full-length permutations are generated. The permutation tuples are emitted in lexicographic ordering according to the order of the input iterable. So, if the input iterable is sorted, the combination tuples will be produced in sorted order. Elements are treated as unique based on their position, not on their value. So if the input elements are unique, there will be no repeat values in each permutation. Roughly equivalent to: def permutations(iterable, r=None):
    # permutations('ABCD', 2) --> AB AC AD BA BC BD CA CB CD DA DB DC
    # permutations(range(3)) --> 012 021 102 120 201 210
    pool = tuple(iterable)
    n = len(pool)
    r = n if r is None else r
    if r > n:
        return
    indices = list(range(n))
    cycles = list(range(n, n-r, -1))
    yield tuple(pool[i] for i in indices[:r])
    while n:
        for i in reversed(range(r)):
            cycles[i] -= 1
            if cycles[i] == 0:
                indices[i:] = indices[i+1:] + indices[i:i+1]
                cycles[i] = n - i
            else:
                j = cycles[i]
                indices[i], indices[-j] = indices[-j], indices[i]
                yield tuple(pool[i] for i in indices[:r])
                break
        else:
            return
 The code for permutations() can be also expressed as a subsequence of product(), filtered to exclude entries with repeated elements (those from the same position in the input pool): def permutations(iterable, r=None):
    pool = tuple(iterable)
    n = len(pool)
    r = n if r is None else r
    for indices in product(range(n), repeat=r):
        if len(set(indices)) == r:
            yield tuple(pool[i] for i in indices)
 The number of items returned is n! / (n-r)! when 0 <= r <= n or zero when r > n.
numpy.random.RandomState.shuffle method   random.RandomState.shuffle(x)
 
Modify a sequence in-place by shuffling its contents. This function only shuffles the array along the first axis of a multi-dimensional array. The order of sub-arrays is changed but their contents remains the same.  Note New code should use the shuffle method of a default_rng() instance instead; please see the Quick Start.   Parameters 
 
xndarray or MutableSequence


The array, list or mutable sequence to be shuffled.    Returns 
 None
    See also  Generator.shuffle

which should be used for new code.    Examples >>> arr = np.arange(10)
>>> np.random.shuffle(arr)
>>> arr
[1 7 5 2 9 4 3 6 0 8] # random
 Multi-dimensional arrays are only shuffled along the first axis: >>> arr = np.arange(9).reshape((3, 3))
>>> np.random.shuffle(arr)
>>> arr
array([[3, 4, 5], # random
       [6, 7, 8],
       [0, 1, 2]])
def f_20546419(r):
    """shuffle columns of an numpy array 'r'
    """
    return  
 --------------------

def f_32675861(df):
    """copy all values in a column 'B' to a new column 'D' in a pandas data frame 'df'
    """
     
 --------------------

def f_14227561(data):
    """find a value within nested json 'data' where the key inside another key 'B' is unknown.
    """
    return  
 --------------------

def f_14858916(string, predicate):
    """check characters of string `string` are true predication of function `predicate`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
test.support.fd_count()  
Count the number of open file descriptors.
pygame.cdrom.get_count() 
 number of cd drives on the system get_count() -> count  Return the number of cd drives on the system. When you create CD objects you need to pass an integer id that must be lower than this count. The count will be 0 if there are no drives on the system.
numpy.distutils.misc_util.get_num_build_jobs()[source]
 
Get number of parallel build jobs set by the –parallel command line argument of setup.py If the command did not receive a setting the environment variable NPY_NUM_BUILD_JOBS is checked. If that is unset, return the number of processors on the system, with a maximum of 8 (to prevent overloading the system if there a lot of CPUs).  Returns 
 
outint


number of parallel jobs that can be run
driver_count
fnmatch.fnmatch(filename, pattern)  
Test whether the filename string matches the pattern string, returning True or False. Both parameters are case-normalized using os.path.normcase(). fnmatchcase() can be used to perform a case-sensitive comparison, regardless of whether that’s standard for the operating system. This example will print all file names in the current directory with the extension .txt: import fnmatch
import os

for file in os.listdir('.'):
    if fnmatch.fnmatch(file, '*.txt'):
        print(file)
def f_574236():
    """determine number of files on a drive with python
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
fetchone()  
Fetches the next row of a query result set, returning a single sequence, or None when no more data is available.
executescript(sql_script)  
This is a nonstandard shortcut that creates a cursor object by calling the cursor() method, calls the cursor’s executescript() method with the given sql_script, and returns the cursor.
fetchall()  
Fetches all (remaining) rows of a query result, returning a list. Note that the cursor’s arraysize attribute can affect the performance of this operation. An empty list is returned when no rows are available.
class sqlite3.Cursor  
A Cursor instance has the following attributes and methods.  
execute(sql[, parameters])  
Executes an SQL statement. Values may be bound to the statement using placeholders. execute() will only execute a single SQL statement. If you try to execute more than one statement with it, it will raise a Warning. Use executescript() if you want to execute multiple SQL statements with one call. 
  
executemany(sql, seq_of_parameters)  
Executes a parameterized SQL command against all parameter sequences or mappings found in the sequence seq_of_parameters. The sqlite3 module also allows using an iterator yielding parameters instead of a sequence. import sqlite3

class IterChars:
    def __init__(self):
        self.count = ord('a')

    def __iter__(self):
        return self

    def __next__(self):
        if self.count > ord('z'):
            raise StopIteration
        self.count += 1
        return (chr(self.count - 1),) # this is a 1-tuple

con = sqlite3.connect(":memory:")
cur = con.cursor()
cur.execute("create table characters(c)")

theIter = IterChars()
cur.executemany("insert into characters(c) values (?)", theIter)

cur.execute("select c from characters")
print(cur.fetchall())

con.close()
 Here’s a shorter example using a generator: import sqlite3
import string

def char_generator():
    for c in string.ascii_lowercase:
        yield (c,)

con = sqlite3.connect(":memory:")
cur = con.cursor()
cur.execute("create table characters(c)")

cur.executemany("insert into characters(c) values (?)", char_generator())

cur.execute("select c from characters")
print(cur.fetchall())

con.close()
 
  
executescript(sql_script)  
This is a nonstandard convenience method for executing multiple SQL statements at once. It issues a COMMIT statement first, then executes the SQL script it gets as a parameter. sql_script can be an instance of str. Example: import sqlite3

con = sqlite3.connect(":memory:")
cur = con.cursor()
cur.executescript("""
    create table person(
        firstname,
        lastname,
        age
    );

    create table book(
        title,
        author,
        published
    );

    insert into book(title, author, published)
    values (
        'Dirk Gently''s Holistic Detective Agency',
        'Douglas Adams',
        1987
    );
    """)
con.close()
 
  
fetchone()  
Fetches the next row of a query result set, returning a single sequence, or None when no more data is available. 
  
fetchmany(size=cursor.arraysize)  
Fetches the next set of rows of a query result, returning a list. An empty list is returned when no more rows are available. The number of rows to fetch per call is specified by the size parameter. If it is not given, the cursor’s arraysize determines the number of rows to be fetched. The method should try to fetch as many rows as indicated by the size parameter. If this is not possible due to the specified number of rows not being available, fewer rows may be returned. Note there are performance considerations involved with the size parameter. For optimal performance, it is usually best to use the arraysize attribute. If the size parameter is used, then it is best for it to retain the same value from one fetchmany() call to the next. 
  
fetchall()  
Fetches all (remaining) rows of a query result, returning a list. Note that the cursor’s arraysize attribute can affect the performance of this operation. An empty list is returned when no rows are available. 
  
close()  
Close the cursor now (rather than whenever __del__ is called). The cursor will be unusable from this point forward; a ProgrammingError exception will be raised if any operation is attempted with the cursor. 
  
rowcount  
Although the Cursor class of the sqlite3 module implements this attribute, the database engine’s own support for the determination of “rows affected”/”rows selected” is quirky. For executemany() statements, the number of modifications are summed up into rowcount. As required by the Python DB API Spec, the rowcount attribute “is -1 in case no executeXX() has been performed on the cursor or the rowcount of the last operation is not determinable by the interface”. This includes SELECT statements because we cannot determine the number of rows a query produced until all rows were fetched. With SQLite versions before 3.6.5, rowcount is set to 0 if you make a DELETE FROM table without any condition. 
  
lastrowid  
This read-only attribute provides the rowid of the last modified row. It is only set if you issued an INSERT or a REPLACE statement using the execute() method. For operations other than INSERT or REPLACE or when executemany() is called, lastrowid is set to None. If the INSERT or REPLACE statement failed to insert the previous successful rowid is returned.  Changed in version 3.6: Added support for the REPLACE statement.  
  
arraysize  
Read/write attribute that controls the number of rows returned by fetchmany(). The default value is 1 which means a single row would be fetched per call. 
  
description  
This read-only attribute provides the column names of the last query. To remain compatible with the Python DB API, it returns a 7-tuple for each column where the last six items of each tuple are None. It is set for SELECT statements without any matching rows as well. 
  
connection  
This read-only attribute provides the SQLite database Connection used by the Cursor object. A Cursor object created by calling con.cursor() will have a connection attribute that refers to con: >>> con = sqlite3.connect(":memory:")
>>> cur = con.cursor()
>>> cur.connection == con
True
execute(sql[, parameters])  
This is a nonstandard shortcut that creates a cursor object by calling the cursor() method, calls the cursor’s execute() method with the parameters given, and returns the cursor.
def f_7011291(cursor):
    """how to get a single result from a SQLite query from `cursor`
    """
    return  
 --------------------

def f_6378889(user_input):
    """convert string `user_input` into a list of integers `user_list`
    """
     
 --------------------

def f_6378889(user):
    """Get a list of integers by splitting  a string `user` with comma
    """
    return  
 --------------------

def f_5212870(list):
    """Sorting a Python list `list` by the first item ascending and last item descending
    """
    return  
 --------------------

def f_403421(ut, cmpfun):
    """sort a list of objects `ut`, based on a function `cmpfun` in descending order
    """
     
 --------------------

def f_403421(ut):
    """reverse list `ut` based on the `count` attribute of each object
    """
     
 --------------------

def f_3944876(i):
    """cast an int `i` to a string and concat to string 'ME'
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
class secrets.SystemRandom  
A class for generating random numbers using the highest-quality sources provided by the operating system. See random.SystemRandom for additional details.
class random.SystemRandom([seed])  
Class that uses the os.urandom() function for generating random numbers from sources provided by the operating system. Not available on all systems. Does not rely on software state, and sequences are not reproducible. Accordingly, the seed() method has no effect and is ignored. The getstate() and setstate() methods raise NotImplementedError if called.
InputSource.setSystemId(id)  
Sets the system identifier of this InputSource.
write_sys_ex() 
 writes a timestamped system-exclusive midi message. write_sys_ex(when, msg) -> None  Writes a timestamped system-exclusive midi message.     
Parameters:

 
msg (list[int] or str) -- midi message 
when -- timestamp in milliseconds      Example: midi_output.write_sys_ex(0, '\xF0\x7D\x10\x11\x12\x13\xF7')

# is equivalent to

midi_output.write_sys_ex(pygame.midi.time(),
                         [0xF0, 0x7D, 0x10, 0x11, 0x12, 0x13, 0xF7])
dis.opname  
Sequence of operation names, indexable using the bytecode.
def f_40903174(df):
    """Sorting data in Pandas DataFrame `df` with columns 'System_num' and 'Dis'
    """
    return  
 --------------------

def f_4454298(infile, outfile):
    """prepend the line '#test firstline\n' to the contents of file 'infile' and save as the file 'outfile'
    """
     
 --------------------

def f_19729928(l):
    """sort a list `l` by length of value in tuple
    """
     
 --------------------
Please refer to the following documentation to generate the code:
fmt_d_m_partial='$%s%d^{\\circ}\\,%02d^{\\prime}\\,'
matches(self, d, **kwargs)  
Try to match a single dict with the supplied arguments.
class test.support.Matcher  
 
matches(self, d, **kwargs)  
Try to match a single dict with the supplied arguments. 
  
match_value(self, k, dv, v)  
Try to match a single stored value (dv) with a supplied value (v).
keyword.issoftkeyword(s)  
Return True if s is a Python soft keyword.  New in version 3.9.
fmt_s_partial='%02d^{\\prime\\prime}$'
def f_31371879(s):
    """split string `s` by words that ends with 'd'
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
pandas.api.types.is_re_compilable   pandas.api.types.is_re_compilable(obj)[source]
 
Check if the object can be compiled into a regex pattern instance.  Parameters 
 
obj:The object to check

  Returns 
 
is_regex_compilable:bool


Whether obj can be compiled as a regex pattern.     Examples 
>>> is_re_compilable(".*")
True
>>> is_re_compilable(1)
False
token.LSQB  
Token value for "[".
matplotlib.cbook.is_math_text(s)[source]
 
Return whether the string s contains math expressions. This is done by checking whether s contains an even number of non-escaped dollar signs.
pandas.api.types.is_re   pandas.api.types.is_re(obj)[source]
 
Check if the object is a regex pattern instance.  Parameters 
 
obj:The object to check

  Returns 
 
is_regex:bool


Whether obj is a regex pattern.     Examples 
>>> is_re(re.compile(".*"))
True
>>> is_re("foo")
False
numpy.ma.is_masked   ma.is_masked(x)[source]
 
Determine whether input has masked values. Accepts any object as input, but always returns False unless the input is a MaskedArray containing masked values.  Parameters 
 
xarray_like


Array to check for masked values.    Returns 
 
resultbool


True if x is a MaskedArray with masked values, False otherwise.     Examples >>> import numpy.ma as ma
>>> x = ma.masked_equal([0, 1, 0, 2, 3], 0)
>>> x
masked_array(data=[--, 1, --, 2, 3],
             mask=[ True, False,  True, False, False],
       fill_value=0)
>>> ma.is_masked(x)
True
>>> x = ma.masked_equal([0, 1, 0, 2, 3], 42)
>>> x
masked_array(data=[0, 1, 0, 2, 3],
             mask=False,
       fill_value=42)
>>> ma.is_masked(x)
False
 Always returns False if x isn’t a MaskedArray. >>> x = [False, True, False]
>>> ma.is_masked(x)
False
>>> x = 'a string'
>>> ma.is_masked(x)
False
def f_9012008():
    """return `True` if string `foobarrrr` contains regex `ba[rzd]`
    """
    return  
 --------------------

def f_7961363(t):
    """Removing duplicates in list `t`
    """
    return  
 --------------------

def f_7961363(source_list):
    """Removing duplicates in list `source_list`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
commit(database)  
Generate a CAB file, add it as a stream to the MSI file, put it into the Media table, and remove the generated file from the disk.
lock()  
unlock()  
Three locking mechanisms are used—dot locking and, if available, the flock() and lockf() system calls.
lock()  
unlock()  
Three locking mechanisms are used—dot locking and, if available, the flock() and lockf() system calls.
class msilib.CAB(name)  
The class CAB represents a CAB file. During MSI construction, files will be added simultaneously to the Files table, and to a CAB file. Then, when all files have been added, the CAB file can be written, then added to the MSI file. name is the name of the CAB file in the MSI file.  
append(full, file, logical)  
Add the file with the pathname full to the CAB file, under the name logical. If there is already a file named logical, a new file name is created. Return the index of the file in the CAB file, and the new name of the file inside the CAB file. 
  
commit(database)  
Generate a CAB file, add it as a stream to the MSI file, put it into the Media table, and remove the generated file from the disk.
sys.abiflags  
On POSIX systems where Python was built with the standard configure script, this contains the ABI flags as specified by PEP 3149.  Changed in version 3.8: Default flags became an empty string (m flag for pymalloc has been removed).   New in version 3.2.
def f_7961363():
    """Removing duplicates in list `abracadabra`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
array.tolist()  
Convert the array to an ordinary list with the same items.
numpy.ndarray.tolist method   ndarray.tolist()
 
Return the array as an a.ndim-levels deep nested list of Python scalars. Return a copy of the array data as a (nested) Python list. Data items are converted to the nearest compatible builtin Python type, via the item function. If a.ndim is 0, then since the depth of the nested list is 0, it will not be a list at all, but a simple Python scalar.  Parameters 
 none
  Returns 
 
yobject, or list of object, or list of list of object, or …


The possibly nested list of array elements.     Notes The array may be recreated via a = np.array(a.tolist()), although this may sometimes lose precision. Examples For a 1D array, a.tolist() is almost the same as list(a), except that tolist changes numpy scalars to Python scalars: >>> a = np.uint32([1, 2])
>>> a_list = list(a)
>>> a_list
[1, 2]
>>> type(a_list[0])
<class 'numpy.uint32'>
>>> a_tolist = a.tolist()
>>> a_tolist
[1, 2]
>>> type(a_tolist[0])
<class 'int'>
 Additionally, for a 2D array, tolist applies recursively: >>> a = np.array([[1, 2], [3, 4]])
>>> list(a)
[array([1, 2]), array([3, 4])]
>>> a.tolist()
[[1, 2], [3, 4]]
 The base case for this recursion is a 0D array: >>> a = np.array(1)
>>> list(a)
Traceback (most recent call last):
  ...
TypeError: iteration over a 0-d array
>>> a.tolist()
1
numpy.char.chararray.tolist method   char.chararray.tolist()
 
Return the array as an a.ndim-levels deep nested list of Python scalars. Return a copy of the array data as a (nested) Python list. Data items are converted to the nearest compatible builtin Python type, via the item function. If a.ndim is 0, then since the depth of the nested list is 0, it will not be a list at all, but a simple Python scalar.  Parameters 
 none
  Returns 
 
yobject, or list of object, or list of list of object, or …


The possibly nested list of array elements.     Notes The array may be recreated via a = np.array(a.tolist()), although this may sometimes lose precision. Examples For a 1D array, a.tolist() is almost the same as list(a), except that tolist changes numpy scalars to Python scalars: >>> a = np.uint32([1, 2])
>>> a_list = list(a)
>>> a_list
[1, 2]
>>> type(a_list[0])
<class 'numpy.uint32'>
>>> a_tolist = a.tolist()
>>> a_tolist
[1, 2]
>>> type(a_tolist[0])
<class 'int'>
 Additionally, for a 2D array, tolist applies recursively: >>> a = np.array([[1, 2], [3, 4]])
>>> list(a)
[array([1, 2]), array([3, 4])]
>>> a.tolist()
[[1, 2], [3, 4]]
 The base case for this recursion is a 0D array: >>> a = np.array(1)
>>> list(a)
Traceback (most recent call last):
  ...
TypeError: iteration over a 0-d array
>>> a.tolist()
1
numpy.recarray.tolist method   recarray.tolist()
 
Return the array as an a.ndim-levels deep nested list of Python scalars. Return a copy of the array data as a (nested) Python list. Data items are converted to the nearest compatible builtin Python type, via the item function. If a.ndim is 0, then since the depth of the nested list is 0, it will not be a list at all, but a simple Python scalar.  Parameters 
 none
  Returns 
 
yobject, or list of object, or list of list of object, or …


The possibly nested list of array elements.     Notes The array may be recreated via a = np.array(a.tolist()), although this may sometimes lose precision. Examples For a 1D array, a.tolist() is almost the same as list(a), except that tolist changes numpy scalars to Python scalars: >>> a = np.uint32([1, 2])
>>> a_list = list(a)
>>> a_list
[1, 2]
>>> type(a_list[0])
<class 'numpy.uint32'>
>>> a_tolist = a.tolist()
>>> a_tolist
[1, 2]
>>> type(a_tolist[0])
<class 'int'>
 Additionally, for a 2D array, tolist applies recursively: >>> a = np.array([[1, 2], [3, 4]])
>>> list(a)
[array([1, 2]), array([3, 4])]
>>> a.tolist()
[[1, 2], [3, 4]]
 The base case for this recursion is a 0D array: >>> a = np.array(1)
>>> list(a)
Traceback (most recent call last):
  ...
TypeError: iteration over a 0-d array
>>> a.tolist()
1
pandas.Index.tolist   Index.tolist()[source]
 
Return a list of the values. These are each a scalar type, which is a Python scalar (for str, int, float) or a pandas scalar (for Timestamp/Timedelta/Interval/Period)  Returns 
 list
    See also  numpy.ndarray.tolist

Return the array as an a.ndim-levels deep nested list of Python scalars.
def f_5183533(a):
    """Convert array `a` into a list
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
numpy.matrix.tolist method   matrix.tolist()[source]
 
Return the matrix as a (possibly nested) list. See ndarray.tolist for full documentation.  See also  ndarray.tolist
  Examples >>> x = np.matrix(np.arange(12).reshape((3,4))); x
matrix([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11]])
>>> x.tolist()
[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11]]
numpy.ndarray.tolist method   ndarray.tolist()
 
Return the array as an a.ndim-levels deep nested list of Python scalars. Return a copy of the array data as a (nested) Python list. Data items are converted to the nearest compatible builtin Python type, via the item function. If a.ndim is 0, then since the depth of the nested list is 0, it will not be a list at all, but a simple Python scalar.  Parameters 
 none
  Returns 
 
yobject, or list of object, or list of list of object, or …


The possibly nested list of array elements.     Notes The array may be recreated via a = np.array(a.tolist()), although this may sometimes lose precision. Examples For a 1D array, a.tolist() is almost the same as list(a), except that tolist changes numpy scalars to Python scalars: >>> a = np.uint32([1, 2])
>>> a_list = list(a)
>>> a_list
[1, 2]
>>> type(a_list[0])
<class 'numpy.uint32'>
>>> a_tolist = a.tolist()
>>> a_tolist
[1, 2]
>>> type(a_tolist[0])
<class 'int'>
 Additionally, for a 2D array, tolist applies recursively: >>> a = np.array([[1, 2], [3, 4]])
>>> list(a)
[array([1, 2]), array([3, 4])]
>>> a.tolist()
[[1, 2], [3, 4]]
 The base case for this recursion is a 0D array: >>> a = np.array(1)
>>> list(a)
Traceback (most recent call last):
  ...
TypeError: iteration over a 0-d array
>>> a.tolist()
1
pandas.Index.to_list   Index.to_list()[source]
 
Return a list of the values. These are each a scalar type, which is a Python scalar (for str, int, float) or a pandas scalar (for Timestamp/Timedelta/Interval/Period)  Returns 
 list
    See also  numpy.ndarray.tolist

Return the array as an a.ndim-levels deep nested list of Python scalars.
numpy.matrix.getA1 method   matrix.getA1()[source]
 
Return self as a flattened ndarray. Equivalent to np.asarray(x).ravel()  Parameters 
 None
  Returns 
 
retndarray


self, 1-D, as an ndarray     Examples >>> x = np.matrix(np.arange(12).reshape((3,4))); x
matrix([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11]])
>>> x.getA1()
array([ 0,  1,  2, ...,  9, 10, 11])
pandas.Index.tolist   Index.tolist()[source]
 
Return a list of the values. These are each a scalar type, which is a Python scalar (for str, int, float) or a pandas scalar (for Timestamp/Timedelta/Interval/Period)  Returns 
 list
    See also  numpy.ndarray.tolist

Return the array as an a.ndim-levels deep nested list of Python scalars.
def f_5183533(a):
    """Convert the first row of numpy matrix `a` to a list
    """
    return  
 --------------------

def f_5999747(soup):
    """In `soup`, get the content of the sibling of the `td`  tag with text content `Address:`
    """
    return  
 --------------------

def f_4284648(l):
    """convert elements of each tuple in list `l` into a string  separated by character `@`
    """
    return  
 --------------------

def f_4284648(l):
    """convert each tuple in list `l` to a string with '@' separating the tuples' elements
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
decode(doc) [source]
 
Decode the input into a string of unicode symbols. The decoding strategy depends on the vectorizer parameters.  Parameters 
 
docstr 

The string to decode.    Returns 
 doc: str

A string of unicode symbols.
decode(doc) [source]
 
Decode the input into a string of unicode symbols. The decoding strategy depends on the vectorizer parameters.  Parameters 
 
docstr 

The string to decode.    Returns 
 doc: str

A string of unicode symbols.
decode(doc) [source]
 
Decode the input into a string of unicode symbols. The decoding strategy depends on the vectorizer parameters.  Parameters 
 
docstr 

The string to decode.    Returns 
 doc: str

A string of unicode symbols.
pandas.Series.str.extractall   Series.str.extractall(pat, flags=0)[source]
 
Extract capture groups in the regex pat as columns in DataFrame. For each subject string in the Series, extract groups from all matches of regular expression pat. When each subject string in the Series has exactly one match, extractall(pat).xs(0, level=’match’) is the same as extract(pat).  Parameters 
 
pat:str


Regular expression pattern with capturing groups.  
flags:int, default 0 (no flags)


A re module flag, for example re.IGNORECASE. These allow to modify regular expression matching for things like case, spaces, etc. Multiple flags can be combined with the bitwise OR operator, for example re.IGNORECASE | re.MULTILINE.    Returns 
 DataFrame

A DataFrame with one row for each match, and one column for each group. Its rows have a MultiIndex with first levels that come from the subject Series. The last level is named ‘match’ and indexes the matches in each item of the Series. Any capture group names in regular expression pat will be used for column names; otherwise capture group numbers will be used.      See also  extract

Returns first match only (not all matches).    Examples A pattern with one group will return a DataFrame with one column. Indices with no matches will not appear in the result. 
>>> s = pd.Series(["a1a2", "b1", "c1"], index=["A", "B", "C"])
>>> s.str.extractall(r"[ab](\d)")
        0
match
A 0      1
  1      2
B 0      1
  Capture group names are used for column names of the result. 
>>> s.str.extractall(r"[ab](?P<digit>\d)")
        digit
match
A 0         1
  1         2
B 0         1
  A pattern with two groups will return a DataFrame with two columns. 
>>> s.str.extractall(r"(?P<letter>[ab])(?P<digit>\d)")
        letter digit
match
A 0          a     1
  1          a     2
B 0          b     1
  Optional groups that do not match are NaN in the result. 
>>> s.str.extractall(r"(?P<letter>[ab])?(?P<digit>\d)")
        letter digit
match
A 0          a     1
  1          a     2
B 0          b     1
C 0        NaN     1
skimage.lookfor(what) [source]
 
Do a keyword search on scikit-image docstrings.  Parameters 
 
whatstr 

Words to look for.     Examples >>> import skimage
>>> skimage.lookfor('regular_grid')
Search results for 'regular_grid'
---------------------------------
skimage.lookfor
    Do a keyword search on scikit-image docstrings.
skimage.util.regular_grid
    Find `n_points` regularly spaced along `ar_shape`.
def f_29696641(teststr):
    """Get all matches with regex pattern `\\d+[xX]` in list of string `teststr`
    """
    return  
 --------------------

def f_15315452(df):
    """select values from column 'A' for which corresponding values in column 'B' will be greater than 50, and in column 'C' - equal 900 in dataframe `df`
    """
    return  
 --------------------

def f_4642501(o):
    """Sort dictionary `o` in ascending order based on its keys and items
    """
    return  
 --------------------

def f_4642501(d):
    """get sorted list of keys of dict `d`
    """
    return  
 --------------------

def f_4642501(d):
    """sort dictionaries `d` by keys
    """
    return  
 --------------------

def f_642154():
    """convert string "1" into integer
    """
    return  
 --------------------

def f_642154(T1):
    """convert items in `T1` to integers
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
cgi.test()  
Robust test CGI script, usable as main program. Writes minimal HTTP headers and formats all information provided to the script in HTML form.
user_passes_test(test_func, login_url=None, redirect_field_name='next')  
As a shortcut, you can use the convenient user_passes_test decorator which performs a redirect when the callable returns False: from django.contrib.auth.decorators import user_passes_test

def email_check(user):
    return user.email.endswith('@example.com')

@user_passes_test(email_check)
def my_view(request):
    ...
 user_passes_test() takes a required argument: a callable that takes a User object and returns True if the user is allowed to view the page. Note that user_passes_test() does not automatically check that the User is not anonymous. user_passes_test() takes two optional arguments:  
login_url  Lets you specify the URL that users who don’t pass the test will be redirected to. It may be a login page and defaults to settings.LOGIN_URL if you don’t specify one. 
redirect_field_name  Same as for login_required(). Setting it to None removes it from the URL, which you may want to do if you are redirecting users that don’t pass the test to a non-login page where there’s no “next page”.  For example: @user_passes_test(email_check, login_url='/login/')
def my_view(request):
    ...
test.support.script_helper.spawn_python(*args, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, **kw)  
Run a Python subprocess with the given arguments. kw is extra keyword args to pass to subprocess.Popen(). Returns a subprocess.Popen object.
test.support.script_helper.assert_python_failure(*args, **env_vars)  
Assert that running the interpreter with args and optional environment variables env_vars fails (rc != 0) and return a (return code,
stdout, stderr) tuple. See assert_python_ok() for more options.  Changed in version 3.9: The function no longer strips whitespaces from stderr.
pandas.Series.get   Series.get(key, default=None)[source]
 
Get item from object for given key (ex: DataFrame column). Returns default value if not found.  Parameters 
 
key:object

  Returns 
 
value:same type as items contained in object

   Examples 
>>> df = pd.DataFrame(
...     [
...         [24.3, 75.7, "high"],
...         [31, 87.8, "high"],
...         [22, 71.6, "medium"],
...         [35, 95, "medium"],
...     ],
...     columns=["temp_celsius", "temp_fahrenheit", "windspeed"],
...     index=pd.date_range(start="2014-02-12", end="2014-02-15", freq="D"),
... )
  
>>> df
            temp_celsius  temp_fahrenheit windspeed
2014-02-12          24.3             75.7      high
2014-02-13          31.0             87.8      high
2014-02-14          22.0             71.6    medium
2014-02-15          35.0             95.0    medium
  
>>> df.get(["temp_celsius", "windspeed"])
            temp_celsius windspeed
2014-02-12          24.3      high
2014-02-13          31.0      high
2014-02-14          22.0    medium
2014-02-15          35.0    medium
  If the key isn’t found, the default value will be used. 
>>> df.get(["temp_celsius", "temp_kelvin"], default="default_value")
'default_value'
def f_3777301():
    """call a shell script `./test.sh` using subprocess
    """
     
 --------------------
Please refer to the following documentation to generate the code:
class ast.UAdd  
class ast.USub  
class ast.Not  
class ast.Invert  
Unary operator tokens. Not is the not keyword, Invert is the ~ operator. >>> print(ast.dump(ast.parse('not x', mode='eval'), indent=4))
Expression(
    body=UnaryOp(
        op=Not(),
        operand=Name(id='x', ctx=Load())))
matplotlib.axis.Tick.get_pad   Tick.get_pad()[source]
 
Get the value of the tick label pad in points.
matplotlib.axis.Tick.get_pad_pixels   Tick.get_pad_pixels()[source]
class ast.UAdd  
class ast.USub  
class ast.Not  
class ast.Invert  
Unary operator tokens. Not is the not keyword, Invert is the ~ operator. >>> print(ast.dump(ast.parse('not x', mode='eval'), indent=4))
Expression(
    body=UnaryOp(
        op=Not(),
        operand=Name(id='x', ctx=Load())))
matplotlib.axis.Axis.OFFSETTEXTPAD   Axis.OFFSETTEXTPAD=3
def f_3777301():
    """call a shell script `notepad` using subprocess
    """
     
 --------------------

def f_7946798(l1, l2):
    """combine lists `l1` and `l2`  by alternating their elements
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
base64.b32encode(s)  
Encode the bytes-like object s using Base32 and return the encoded bytes.
binascii.b2a_base64(data, *, newline=True)  
Convert binary data to a line of ASCII characters in base64 coding. The return value is the converted line, including a newline char if newline is true. The output of this function conforms to RFC 3548.  Changed in version 3.6: Added the newline parameter.
base64.b16encode(s)  
Encode the bytes-like object s using Base16 and return the encoded bytes.
werkzeug.http.generate_etag(data)  
Generate an etag for some data.  Changed in version 2.0: Use SHA-1. MD5 may not be available in some environments.   Parameters 
data (bytes) –   Return type 
str
base64.b85encode(b, pad=False)  
Encode the bytes-like object b using base85 (as used in e.g. git-style binary diffs) and return the encoded bytes. If pad is true, the input is padded with b'\0' so its length is a multiple of 4 bytes before encoding.  New in version 3.4.
def f_8908287():
    """encode string 'data to be encoded'
    """
    return  
 --------------------

def f_8908287():
    """encode a string `data to be encoded` to `ascii` encoding
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
Importing data with genfromtxt NumPy provides several functions to create arrays from tabular data. We focus here on the genfromtxt function. In a nutshell, genfromtxt runs two main loops. The first loop converts each line of the file in a sequence of strings. The second loop converts each string to the appropriate data type. This mechanism is slower than a single loop, but gives more flexibility. In particular, genfromtxt is able to take missing data into account, when other faster and simpler functions like loadtxt cannot.  Note When giving examples, we will use the following conventions: >>> import numpy as np
>>> from io import StringIO
   Defining the input The only mandatory argument of genfromtxt is the source of the data. It can be a string, a list of strings, a generator or an open file-like object with a read method, for example, a file or io.StringIO object. If a single string is provided, it is assumed to be the name of a local or remote file. If a list of strings or a generator returning strings is provided, each string is treated as one line in a file. When the URL of a remote file is passed, the file is automatically downloaded to the current directory and opened. Recognized file types are text files and archives. Currently, the function recognizes gzip and bz2 (bzip2) archives. The type of the archive is determined from the extension of the file: if the filename ends with '.gz', a gzip archive is expected; if it ends with 'bz2', a bzip2 archive is assumed.   Splitting the lines into columns  The delimiter argument Once the file is defined and open for reading, genfromtxt splits each non-empty line into a sequence of strings. Empty or commented lines are just skipped. The delimiter keyword is used to define how the splitting should take place. Quite often, a single character marks the separation between columns. For example, comma-separated files (CSV) use a comma (,) or a semicolon (;) as delimiter: >>> data = u"1, 2, 3\n4, 5, 6"
>>> np.genfromtxt(StringIO(data), delimiter=",")
array([[ 1.,  2.,  3.],
       [ 4.,  5.,  6.]])
 Another common separator is "\t", the tabulation character. However, we are not limited to a single character, any string will do. By default, genfromtxt assumes delimiter=None, meaning that the line is split along white spaces (including tabs) and that consecutive white spaces are considered as a single white space. Alternatively, we may be dealing with a fixed-width file, where columns are defined as a given number of characters. In that case, we need to set delimiter to a single integer (if all the columns have the same size) or to a sequence of integers (if columns can have different sizes): >>> data = u"  1  2  3\n  4  5 67\n890123  4"
>>> np.genfromtxt(StringIO(data), delimiter=3)
array([[   1.,    2.,    3.],
       [   4.,    5.,   67.],
       [ 890.,  123.,    4.]])
>>> data = u"123456789\n   4  7 9\n   4567 9"
>>> np.genfromtxt(StringIO(data), delimiter=(4, 3, 2))
array([[ 1234.,   567.,    89.],
       [    4.,     7.,     9.],
       [    4.,   567.,     9.]])
   The autostrip argument By default, when a line is decomposed into a series of strings, the individual entries are not stripped of leading nor trailing white spaces. This behavior can be overwritten by setting the optional argument autostrip to a value of True: >>> data = u"1, abc , 2\n 3, xxx, 4"
>>> # Without autostrip
>>> np.genfromtxt(StringIO(data), delimiter=",", dtype="|U5")
array([['1', ' abc ', ' 2'],
       ['3', ' xxx', ' 4']], dtype='<U5')
>>> # With autostrip
>>> np.genfromtxt(StringIO(data), delimiter=",", dtype="|U5", autostrip=True)
array([['1', 'abc', '2'],
       ['3', 'xxx', '4']], dtype='<U5')
   The comments argument The optional argument comments is used to define a character string that marks the beginning of a comment. By default, genfromtxt assumes comments='#'. The comment marker may occur anywhere on the line. Any character present after the comment marker(s) is simply ignored: >>> data = u"""#
... # Skip me !
... # Skip me too !
... 1, 2
... 3, 4
... 5, 6 #This is the third line of the data
... 7, 8
... # And here comes the last line
... 9, 0
... """
>>> np.genfromtxt(StringIO(data), comments="#", delimiter=",")
array([[1., 2.],
       [3., 4.],
       [5., 6.],
       [7., 8.],
       [9., 0.]])
  New in version 1.7.0: When comments is set to None, no lines are treated as comments.   Note There is one notable exception to this behavior: if the optional argument names=True, the first commented line will be examined for names.     Skipping lines and choosing columns  The skip_header and skip_footer arguments The presence of a header in the file can hinder data processing. In that case, we need to use the skip_header optional argument. The values of this argument must be an integer which corresponds to the number of lines to skip at the beginning of the file, before any other action is performed. Similarly, we can skip the last n lines of the file by using the skip_footer attribute and giving it a value of n: >>> data = u"\n".join(str(i) for i in range(10))
>>> np.genfromtxt(StringIO(data),)
array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.])
>>> np.genfromtxt(StringIO(data),
...               skip_header=3, skip_footer=5)
array([ 3.,  4.])
 By default, skip_header=0 and skip_footer=0, meaning that no lines are skipped.   The usecols argument In some cases, we are not interested in all the columns of the data but only a few of them. We can select which columns to import with the usecols argument. This argument accepts a single integer or a sequence of integers corresponding to the indices of the columns to import. Remember that by convention, the first column has an index of 0. Negative integers behave the same as regular Python negative indexes. For example, if we want to import only the first and the last columns, we can use usecols=(0, -1): >>> data = u"1 2 3\n4 5 6"
>>> np.genfromtxt(StringIO(data), usecols=(0, -1))
array([[ 1.,  3.],
       [ 4.,  6.]])
 If the columns have names, we can also select which columns to import by giving their name to the usecols argument, either as a sequence of strings or a comma-separated string: >>> data = u"1 2 3\n4 5 6"
>>> np.genfromtxt(StringIO(data),
...               names="a, b, c", usecols=("a", "c"))
array([(1.0, 3.0), (4.0, 6.0)],
      dtype=[('a', '<f8'), ('c', '<f8')])
>>> np.genfromtxt(StringIO(data),
...               names="a, b, c", usecols=("a, c"))
    array([(1.0, 3.0), (4.0, 6.0)],
          dtype=[('a', '<f8'), ('c', '<f8')])
    Choosing the data type The main way to control how the sequences of strings we have read from the file are converted to other types is to set the dtype argument. Acceptable values for this argument are:  a single type, such as dtype=float. The output will be 2D with the given dtype, unless a name has been associated with each column with the use of the names argument (see below). Note that dtype=float is the default for genfromtxt. a sequence of types, such as dtype=(int, float, float). a comma-separated string, such as dtype="i4,f8,|U3". a dictionary with two keys 'names' and 'formats'. a sequence of tuples (name, type), such as dtype=[('A', int), ('B', float)]. an existing numpy.dtype object. the special value None. In that case, the type of the columns will be determined from the data itself (see below).  In all the cases but the first one, the output will be a 1D array with a structured dtype. This dtype has as many fields as items in the sequence. The field names are defined with the names keyword. When dtype=None, the type of each column is determined iteratively from its data. We start by checking whether a string can be converted to a boolean (that is, if the string matches true or false in lower cases); then whether it can be converted to an integer, then to a float, then to a complex and eventually to a string. This behavior may be changed by modifying the default mapper of the StringConverter class. The option dtype=None is provided for convenience. However, it is significantly slower than setting the dtype explicitly.   Setting the names  The names argument A natural approach when dealing with tabular data is to allocate a name to each column. A first possibility is to use an explicit structured dtype, as mentioned previously: >>> data = StringIO("1 2 3\n 4 5 6")
>>> np.genfromtxt(data, dtype=[(_, int) for _ in "abc"])
array([(1, 2, 3), (4, 5, 6)],
      dtype=[('a', '<i8'), ('b', '<i8'), ('c', '<i8')])
 Another simpler possibility is to use the names keyword with a sequence of strings or a comma-separated string: >>> data = StringIO("1 2 3\n 4 5 6")
>>> np.genfromtxt(data, names="A, B, C")
array([(1.0, 2.0, 3.0), (4.0, 5.0, 6.0)],
      dtype=[('A', '<f8'), ('B', '<f8'), ('C', '<f8')])
 In the example above, we used the fact that by default, dtype=float. By giving a sequence of names, we are forcing the output to a structured dtype. We may sometimes need to define the column names from the data itself. In that case, we must use the names keyword with a value of True. The names will then be read from the first line (after the skip_header ones), even if the line is commented out: >>> data = StringIO("So it goes\n#a b c\n1 2 3\n 4 5 6")
>>> np.genfromtxt(data, skip_header=1, names=True)
array([(1.0, 2.0, 3.0), (4.0, 5.0, 6.0)],
      dtype=[('a', '<f8'), ('b', '<f8'), ('c', '<f8')])
 The default value of names is None. If we give any other value to the keyword, the new names will overwrite the field names we may have defined with the dtype: >>> data = StringIO("1 2 3\n 4 5 6")
>>> ndtype=[('a',int), ('b', float), ('c', int)]
>>> names = ["A", "B", "C"]
>>> np.genfromtxt(data, names=names, dtype=ndtype)
array([(1, 2.0, 3), (4, 5.0, 6)],
      dtype=[('A', '<i8'), ('B', '<f8'), ('C', '<i8')])
   The defaultfmt argument If names=None but a structured dtype is expected, names are defined with the standard NumPy default of "f%i", yielding names like f0, f1 and so forth: >>> data = StringIO("1 2 3\n 4 5 6")
>>> np.genfromtxt(data, dtype=(int, float, int))
array([(1, 2.0, 3), (4, 5.0, 6)],
      dtype=[('f0', '<i8'), ('f1', '<f8'), ('f2', '<i8')])
 In the same way, if we don’t give enough names to match the length of the dtype, the missing names will be defined with this default template: >>> data = StringIO("1 2 3\n 4 5 6")
>>> np.genfromtxt(data, dtype=(int, float, int), names="a")
array([(1, 2.0, 3), (4, 5.0, 6)],
      dtype=[('a', '<i8'), ('f0', '<f8'), ('f1', '<i8')])
 We can overwrite this default with the defaultfmt argument, that takes any format string: >>> data = StringIO("1 2 3\n 4 5 6")
>>> np.genfromtxt(data, dtype=(int, float, int), defaultfmt="var_%02i")
array([(1, 2.0, 3), (4, 5.0, 6)],
      dtype=[('var_00', '<i8'), ('var_01', '<f8'), ('var_02', '<i8')])
  Note We need to keep in mind that defaultfmt is used only if some names are expected but not defined.    Validating names NumPy arrays with a structured dtype can also be viewed as recarray, where a field can be accessed as if it were an attribute. For that reason, we may need to make sure that the field name doesn’t contain any space or invalid character, or that it does not correspond to the name of a standard attribute (like size or shape), which would confuse the interpreter. genfromtxt accepts three optional arguments that provide a finer control on the names:  deletechars

Gives a string combining all the characters that must be deleted from the name. By default, invalid characters are ~!@#$%^&*()-=+~\|]}[{';:
/?.>,<.  excludelist

Gives a list of the names to exclude, such as return, file, print… If one of the input name is part of this list, an underscore character ('_') will be appended to it.  case_sensitive

Whether the names should be case-sensitive (case_sensitive=True), converted to upper case (case_sensitive=False or case_sensitive='upper') or to lower case (case_sensitive='lower').      Tweaking the conversion  The converters argument Usually, defining a dtype is sufficient to define how the sequence of strings must be converted. However, some additional control may sometimes be required. For example, we may want to make sure that a date in a format YYYY/MM/DD is converted to a datetime object, or that a string like xx% is properly converted to a float between 0 and 1. In such cases, we should define conversion functions with the converters arguments. The value of this argument is typically a dictionary with column indices or column names as keys and a conversion functions as values. These conversion functions can either be actual functions or lambda functions. In any case, they should accept only a string as input and output only a single element of the wanted type. In the following example, the second column is converted from as string representing a percentage to a float between 0 and 1: >>> convertfunc = lambda x: float(x.strip(b"%"))/100.
>>> data = u"1, 2.3%, 45.\n6, 78.9%, 0"
>>> names = ("i", "p", "n")
>>> # General case .....
>>> np.genfromtxt(StringIO(data), delimiter=",", names=names)
array([(1., nan, 45.), (6., nan, 0.)],
      dtype=[('i', '<f8'), ('p', '<f8'), ('n', '<f8')])
 We need to keep in mind that by default, dtype=float. A float is therefore expected for the second column. However, the strings ' 2.3%' and ' 78.9%' cannot be converted to float and we end up having np.nan instead. Let’s now use a converter: >>> # Converted case ...
>>> np.genfromtxt(StringIO(data), delimiter=",", names=names,
...               converters={1: convertfunc})
array([(1.0, 0.023, 45.0), (6.0, 0.78900000000000003, 0.0)],
      dtype=[('i', '<f8'), ('p', '<f8'), ('n', '<f8')])
 The same results can be obtained by using the name of the second column ("p") as key instead of its index (1): >>> # Using a name for the converter ...
>>> np.genfromtxt(StringIO(data), delimiter=",", names=names,
...               converters={"p": convertfunc})
array([(1.0, 0.023, 45.0), (6.0, 0.78900000000000003, 0.0)],
      dtype=[('i', '<f8'), ('p', '<f8'), ('n', '<f8')])
 Converters can also be used to provide a default for missing entries. In the following example, the converter convert transforms a stripped string into the corresponding float or into -999 if the string is empty. We need to explicitly strip the string from white spaces as it is not done by default: >>> data = u"1, , 3\n 4, 5, 6"
>>> convert = lambda x: float(x.strip() or -999)
>>> np.genfromtxt(StringIO(data), delimiter=",",
...               converters={1: convert})
array([[   1., -999.,    3.],
       [   4.,    5.,    6.]])
   Using missing and filling values Some entries may be missing in the dataset we are trying to import. In a previous example, we used a converter to transform an empty string into a float. However, user-defined converters may rapidly become cumbersome to manage. The genfromtxt function provides two other complementary mechanisms: the missing_values argument is used to recognize missing data and a second argument, filling_values, is used to process these missing data.   missing_values By default, any empty string is marked as missing. We can also consider more complex strings, such as "N/A" or "???" to represent missing or invalid data. The missing_values argument accepts three kinds of values:  a string or a comma-separated string

This string will be used as the marker for missing data for all the columns  a sequence of strings

In that case, each item is associated to a column, in order.  a dictionary

Values of the dictionary are strings or sequence of strings. The corresponding keys can be column indices (integers) or column names (strings). In addition, the special key None can be used to define a default applicable to all columns.     filling_values We know how to recognize missing data, but we still need to provide a value for these missing entries. By default, this value is determined from the expected dtype according to this table:   
Expected type Default   
bool False  
int -1  
float np.nan  
complex np.nan+0j  
string '???'   We can get a finer control on the conversion of missing values with the filling_values optional argument. Like missing_values, this argument accepts different kind of values:  a single value

This will be the default for all columns  a sequence of values

Each entry will be the default for the corresponding column  a dictionary

Each key can be a column index or a column name, and the corresponding value should be a single object. We can use the special key None to define a default for all columns.   In the following example, we suppose that the missing values are flagged with "N/A" in the first column and by "???" in the third column. We wish to transform these missing values to 0 if they occur in the first and second column, and to -999 if they occur in the last column: >>> data = u"N/A, 2, 3\n4, ,???"
>>> kwargs = dict(delimiter=",",
...               dtype=int,
...               names="a,b,c",
...               missing_values={0:"N/A", 'b':" ", 2:"???"},
...               filling_values={0:0, 'b':0, 2:-999})
>>> np.genfromtxt(StringIO(data), **kwargs)
array([(0, 2, 3), (4, 0, -999)],
      dtype=[('a', '<i8'), ('b', '<i8'), ('c', '<i8')])
   usemask We may also want to keep track of the occurrence of missing data by constructing a boolean mask, with True entries where data was missing and False otherwise. To do that, we just have to set the optional argument usemask to True (the default is False). The output array will then be a MaskedArray.    Shortcut functions In addition to genfromtxt, the numpy.lib.npyio module provides several convenience functions derived from genfromtxt. These functions work the same way as the original, but they have different default values.  recfromtxt

Returns a standard numpy.recarray (if usemask=False) or a MaskedRecords array (if usemaske=True). The default dtype is dtype=None, meaning that the types of each column will be automatically determined.  recfromcsv

Like recfromtxt, but with a default delimiter=",".
numpy.loadtxt   numpy.loadtxt(fname, dtype=<class 'float'>, comments='#', delimiter=None, converters=None, skiprows=0, usecols=None, unpack=False, ndmin=0, encoding='bytes', max_rows=None, *, like=None)[source]
 
Load data from a text file. Each row in the text file must have the same number of values.  Parameters 
 
fnamefile, str, pathlib.Path, list of str, generator


File, filename, list, or generator to read. If the filename extension is .gz or .bz2, the file is first decompressed. Note that generators must return bytes or strings. The strings in a list or produced by a generator are treated as lines.  
dtypedata-type, optional


Data-type of the resulting array; default: float. If this is a structured data-type, the resulting array will be 1-dimensional, and each row will be interpreted as an element of the array. In this case, the number of columns used must match the number of fields in the data-type.  
commentsstr or sequence of str, optional


The characters or list of characters used to indicate the start of a comment. None implies no comments. For backwards compatibility, byte strings will be decoded as ‘latin1’. The default is ‘#’.  
delimiterstr, optional


The string used to separate values. For backwards compatibility, byte strings will be decoded as ‘latin1’. The default is whitespace.  
convertersdict, optional


A dictionary mapping column number to a function that will parse the column string into the desired value. E.g., if column 0 is a date string: converters = {0: datestr2num}. Converters can also be used to provide a default value for missing data (but see also genfromtxt): converters = {3: lambda s: float(s.strip() or 0)}. Default: None.  
skiprowsint, optional


Skip the first skiprows lines, including comments; default: 0.  
usecolsint or sequence, optional


Which columns to read, with 0 being the first. For example, usecols = (1,4,5) will extract the 2nd, 5th and 6th columns. The default, None, results in all columns being read.  Changed in version 1.11.0: When a single column has to be read it is possible to use an integer instead of a tuple. E.g usecols = 3 reads the fourth column the same way as usecols = (3,) would.   
unpackbool, optional


If True, the returned array is transposed, so that arguments may be unpacked using x, y, z = loadtxt(...). When used with a structured data-type, arrays are returned for each field. Default is False.  
ndminint, optional


The returned array will have at least ndmin dimensions. Otherwise mono-dimensional axes will be squeezed. Legal values: 0 (default), 1 or 2.  New in version 1.6.0.   
encodingstr, optional


Encoding used to decode the inputfile. Does not apply to input streams. The special value ‘bytes’ enables backward compatibility workarounds that ensures you receive byte arrays as results if possible and passes ‘latin1’ encoded strings to converters. Override this value to receive unicode arrays and pass strings as input to converters. If set to None the system default is used. The default value is ‘bytes’.  New in version 1.14.0.   
max_rowsint, optional


Read max_rows lines of content after skiprows lines. The default is to read all the lines.  New in version 1.16.0.   
likearray_like


Reference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as like supports the __array_function__ protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.  New in version 1.20.0.     Returns 
 
outndarray


Data read from the text file.      See also  
load, fromstring, fromregex

genfromtxt

Load data with missing values handled as specified.  scipy.io.loadmat

reads MATLAB data files    Notes This function aims to be a fast reader for simply formatted files. The genfromtxt function provides more sophisticated handling of, e.g., lines with missing values.  New in version 1.10.0.  The strings produced by the Python float.hex method can be used as input for floats. Examples >>> from io import StringIO   # StringIO behaves like a file object
>>> c = StringIO("0 1\n2 3")
>>> np.loadtxt(c)
array([[0., 1.],
       [2., 3.]])
 >>> d = StringIO("M 21 72\nF 35 58")
>>> np.loadtxt(d, dtype={'names': ('gender', 'age', 'weight'),
...                      'formats': ('S1', 'i4', 'f4')})
array([(b'M', 21, 72.), (b'F', 35, 58.)],
      dtype=[('gender', 'S1'), ('age', '<i4'), ('weight', '<f4')])
 >>> c = StringIO("1,0,2\n3,0,4")
>>> x, y = np.loadtxt(c, delimiter=',', usecols=(0, 2), unpack=True)
>>> x
array([1., 3.])
>>> y
array([2., 4.])
 This example shows how converters can be used to convert a field with a trailing minus sign into a negative number. >>> s = StringIO('10.01 31.25-\n19.22 64.31\n17.57- 63.94')
>>> def conv(fld):
...     return -float(fld[:-1]) if fld.endswith(b'-') else float(fld)
...
>>> np.loadtxt(s, converters={0: conv, 1: conv})
array([[ 10.01, -31.25],
       [ 19.22,  64.31],
       [-17.57,  63.94]])
numpy.genfromtxt   numpy.genfromtxt(fname, dtype=<class 'float'>, comments='#', delimiter=None, skip_header=0, skip_footer=0, converters=None, missing_values=None, filling_values=None, usecols=None, names=None, excludelist=None, deletechars=" !#$%&'()*+, -./:;<=>?@[\\]^{|}~", replace_space='_', autostrip=False, case_sensitive=True, defaultfmt='f%i', unpack=None, usemask=False, loose=True, invalid_raise=True, max_rows=None, encoding='bytes', *, like=None)[source]
 
Load data from a text file, with missing values handled as specified. Each line past the first skip_header lines is split at the delimiter character, and characters following the comments character are discarded.  Parameters 
 
fnamefile, str, pathlib.Path, list of str, generator


File, filename, list, or generator to read. If the filename extension is .gz or .bz2, the file is first decompressed. Note that generators must return bytes or strings. The strings in a list or produced by a generator are treated as lines.  
dtypedtype, optional


Data type of the resulting array. If None, the dtypes will be determined by the contents of each column, individually.  
commentsstr, optional


The character used to indicate the start of a comment. All the characters occurring on a line after a comment are discarded.  
delimiterstr, int, or sequence, optional


The string used to separate values. By default, any consecutive whitespaces act as delimiter. An integer or sequence of integers can also be provided as width(s) of each field.  
skiprowsint, optional


skiprows was removed in numpy 1.10. Please use skip_header instead.  
skip_headerint, optional


The number of lines to skip at the beginning of the file.  
skip_footerint, optional


The number of lines to skip at the end of the file.  
convertersvariable, optional


The set of functions that convert the data of a column to a value. The converters can also be used to provide a default value for missing data: converters = {3: lambda s: float(s or 0)}.  
missingvariable, optional


missing was removed in numpy 1.10. Please use missing_values instead.  
missing_valuesvariable, optional


The set of strings corresponding to missing data.  
filling_valuesvariable, optional


The set of values to be used as default when the data are missing.  
usecolssequence, optional


Which columns to read, with 0 being the first. For example, usecols = (1, 4, 5) will extract the 2nd, 5th and 6th columns.  
names{None, True, str, sequence}, optional


If names is True, the field names are read from the first line after the first skip_header lines. This line can optionally be preceded by a comment delimiter. If names is a sequence or a single-string of comma-separated names, the names will be used to define the field names in a structured dtype. If names is None, the names of the dtype fields will be used, if any.  
excludelistsequence, optional


A list of names to exclude. This list is appended to the default list [‘return’,’file’,’print’]. Excluded names are appended with an underscore: for example, file would become file_.  
deletecharsstr, optional


A string combining invalid characters that must be deleted from the names.  
defaultfmtstr, optional


A format used to define default field names, such as “f%i” or “f_%02i”.  
autostripbool, optional


Whether to automatically strip white spaces from the variables.  
replace_spacechar, optional


Character(s) used in replacement of white spaces in the variable names. By default, use a ‘_’.  
case_sensitive{True, False, ‘upper’, ‘lower’}, optional


If True, field names are case sensitive. If False or ‘upper’, field names are converted to upper case. If ‘lower’, field names are converted to lower case.  
unpackbool, optional


If True, the returned array is transposed, so that arguments may be unpacked using x, y, z = genfromtxt(...). When used with a structured data-type, arrays are returned for each field. Default is False.  
usemaskbool, optional


If True, return a masked array. If False, return a regular array.  
loosebool, optional


If True, do not raise errors for invalid values.  
invalid_raisebool, optional


If True, an exception is raised if an inconsistency is detected in the number of columns. If False, a warning is emitted and the offending lines are skipped.  
max_rowsint, optional


The maximum number of rows to read. Must not be used with skip_footer at the same time. If given, the value must be at least 1. Default is to read the entire file.  New in version 1.10.0.   
encodingstr, optional


Encoding used to decode the inputfile. Does not apply when fname is a file object. The special value ‘bytes’ enables backward compatibility workarounds that ensure that you receive byte arrays when possible and passes latin1 encoded strings to converters. Override this value to receive unicode arrays and pass strings as input to converters. If set to None the system default is used. The default value is ‘bytes’.  New in version 1.14.0.   
likearray_like


Reference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as like supports the __array_function__ protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.  New in version 1.20.0.     Returns 
 
outndarray


Data read from the text file. If usemask is True, this is a masked array.      See also  numpy.loadtxt

equivalent function when no data is missing.    Notes  When spaces are used as delimiters, or when no delimiter has been given as input, there should not be any missing data between two fields. When the variables are named (either by a flexible dtype or with names), there must not be any header in the file (else a ValueError exception is raised). Individual values are not stripped of spaces by default. When using a custom converter, make sure the function does remove spaces.  References  1 
NumPy User Guide, section I/O with NumPy.   Examples >>> from io import StringIO
>>> import numpy as np
 Comma delimited file with mixed dtype >>> s = StringIO(u"1,1.3,abcde")
>>> data = np.genfromtxt(s, dtype=[('myint','i8'),('myfloat','f8'),
... ('mystring','S5')], delimiter=",")
>>> data
array((1, 1.3, b'abcde'),
      dtype=[('myint', '<i8'), ('myfloat', '<f8'), ('mystring', 'S5')])
 Using dtype = None >>> _ = s.seek(0) # needed for StringIO example only
>>> data = np.genfromtxt(s, dtype=None,
... names = ['myint','myfloat','mystring'], delimiter=",")
>>> data
array((1, 1.3, b'abcde'),
      dtype=[('myint', '<i8'), ('myfloat', '<f8'), ('mystring', 'S5')])
 Specifying dtype and names >>> _ = s.seek(0)
>>> data = np.genfromtxt(s, dtype="i8,f8,S5",
... names=['myint','myfloat','mystring'], delimiter=",")
>>> data
array((1, 1.3, b'abcde'),
      dtype=[('myint', '<i8'), ('myfloat', '<f8'), ('mystring', 'S5')])
 An example with fixed-width columns >>> s = StringIO(u"11.3abcde")
>>> data = np.genfromtxt(s, dtype=None, names=['intvar','fltvar','strvar'],
...     delimiter=[1,3,5])
>>> data
array((1, 1.3, b'abcde'),
      dtype=[('intvar', '<i8'), ('fltvar', '<f8'), ('strvar', 'S5')])
 An example to show comments >>> f = StringIO('''
... text,# of chars
... hello world,11
... numpy,5''')
>>> np.genfromtxt(f, dtype='S12,S12', delimiter=',')
array([(b'text', b''), (b'hello world', b'11'), (b'numpy', b'5')],
  dtype=[('f0', 'S12'), ('f1', 'S12')])
textwrap.dedent(text)  
Remove any common leading whitespace from every line in text. This can be used to make triple-quoted strings line up with the left edge of the display, while still presenting them in the source code in indented form. Note that tabs and spaces are both treated as whitespace, but they are not equal: the lines "  hello" and "\thello" are considered to have no common leading whitespace. Lines containing only whitespace are ignored in the input and normalized to a single newline character in the output. For example: def test():
    # end first line with \ to avoid the empty line!
    s = '''\
    hello
      world
    '''
    print(repr(s))          # prints '    hello\n      world\n    '
    print(repr(dedent(s)))  # prints 'hello\n  world\n'
tf.raw_ops.TextLineDataset Creates a dataset that emits the lines of one or more text files.  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.raw_ops.TextLineDataset  
tf.raw_ops.TextLineDataset(
    filenames, compression_type, buffer_size, name=None
)

 


 Args
  filenames   A Tensor of type string. A scalar or a vector containing the name(s) of the file(s) to be read.  
  compression_type   A Tensor of type string. A scalar containing either (i) the empty string (no compression), (ii) "ZLIB", or (iii) "GZIP".  
  buffer_size   A Tensor of type int64. A scalar containing the number of bytes to buffer.  
  name   A name for the operation (optional).   
 


 Returns   A Tensor of type variant.
def f_7856296():
    """parse tab-delimited CSV file 'text.txt' into a list
    """
    return  
 --------------------

def f_9035479(my_object, my_str):
    """Get attribute `my_str` of object `my_object`
    """
    return  
 --------------------

def f_5558418(LD):
    """group a list of dicts `LD` into one dict by key
    """
    return  
 --------------------

def f_638048(list_of_pairs):
    """sum the first value in each tuple in a list of tuples `list_of_pairs` in python
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
html.entities.name2codepoint  
A dictionary that maps HTML entity names to the Unicode code points.
html.entities.codepoint2name  
A dictionary that maps Unicode code points to HTML entity names.
NUM_VERTICES_FOR_CODE={0: 1, 1: 1, 2: 1, 3: 2, 4: 3, 79: 1}
 
A dictionary mapping Path codes to the number of vertices that the code expects.
html.entities.html5  
A dictionary that maps HTML5 named character references 1 to the equivalent Unicode character(s), e.g. html5['gt;'] == '>'. Note that the trailing semicolon is included in the name (e.g. 'gt;'), however some of the names are accepted by the standard even without the semicolon: in this case the name is present with and without the ';'. See also html.unescape().  New in version 3.3.
get_glyphs_mathtext(prop, s, glyph_map=None, return_new_glyphs_only=False)[source]
 
Parse mathtext string s and convert it to a (vertices, codes) pair.
def f_14950260():
    """convert unicode string u"{'code1':1,'code2':1}" into dictionary
    """
    return  
 --------------------

def f_11416772(mystring):
    """find all words in a string `mystring` that start with the `$` sign
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
werkzeug.urls.url_fix(s, charset='utf-8')  
Sometimes you get an URL by a user that just isn’t a real URL because it contains unsafe characters like ‘ ‘ and so on. This function can fix some of the problems in a similar way browsers handle data entered by the user: >>> url_fix('http://de.wikipedia.org/wiki/Elf (Begriffskl\xe4rung)')
'http://de.wikipedia.org/wiki/Elf%20(Begriffskl%C3%A4rung)'
  Parameters 
 
s (str) – the string with the URL to fix. 
charset (str) – The target charset for the URL if the url was given as a string.   Return type 
str
urllib.parse.unwrap(url)  
Extract the url from a wrapped URL (that is, a string formatted as <URL:scheme://host/path>, <scheme://host/path>, URL:scheme://host/path or scheme://host/path). If url is not a wrapped URL, it is returned without changes.
urllib.parse.unquote_plus(string, encoding='utf-8', errors='replace')  
Like unquote(), but also replace plus signs with spaces, as required for unquoting HTML form values. string must be a str. Example: unquote_plus('/El+Ni%C3%B1o/') yields '/El Niño/'.
set_urls(urls)[source]
 
 Parameters 
 
urlslist of str or None

   Notes URLs are currently only implemented by the SVG backend. They are ignored by all other backends.
set_urls(urls)[source]
 
 Parameters 
 
urlslist of str or None

   Notes URLs are currently only implemented by the SVG backend. They are ignored by all other backends.
def f_11331982(text):
    """remove any url within string `text`
    """
     
 --------------------
Please refer to the following documentation to generate the code:
numpy.ma.zeros_like   ma.zeros_like(*args, **kwargs) = <numpy.ma.core._convert2ma object>
 
Return an array of zeros with the same shape and type as a given array.  Parameters 
 
aarray_like


The shape and data-type of a define these same attributes of the returned array.  
dtypedata-type, optional


Overrides the data type of the result.  New in version 1.6.0.   
order{‘C’, ‘F’, ‘A’, or ‘K’}, optional


Overrides the memory layout of the result. ‘C’ means C-order, ‘F’ means F-order, ‘A’ means ‘F’ if a is Fortran contiguous, ‘C’ otherwise. ‘K’ means match the layout of a as closely as possible.  New in version 1.6.0.   
subokbool, optional.


If True, then the newly created array will use the sub-class type of a, otherwise it will be a base-class array. Defaults to True.  
shapeint or sequence of ints, optional.


Overrides the shape of the result. If order=’K’ and the number of dimensions is unchanged, will try to keep order, otherwise, order=’C’ is implied.  New in version 1.17.0.     Returns 
 
outMaskedArray


Array of zeros with the same shape and type as a.      See also  empty_like

Return an empty array with shape and type of input.  ones_like

Return an array of ones with shape and type of input.  full_like

Return a new array with shape of input filled with value.  zeros

Return a new array setting values to zero.    Examples >>> x = np.arange(6)
>>> x = x.reshape((2, 3))
>>> x
array([[0, 1, 2],
       [3, 4, 5]])
>>> np.zeros_like(x)
array([[0, 0, 0],
       [0, 0, 0]])
 >>> y = np.arange(3, dtype=float)
>>> y
array([0., 1., 2.])
>>> np.zeros_like(y)
array([0.,  0.,  0.])
numpy.zeros_like   numpy.zeros_like(a, dtype=None, order='K', subok=True, shape=None)[source]
 
Return an array of zeros with the same shape and type as a given array.  Parameters 
 
aarray_like


The shape and data-type of a define these same attributes of the returned array.  
dtypedata-type, optional


Overrides the data type of the result.  New in version 1.6.0.   
order{‘C’, ‘F’, ‘A’, or ‘K’}, optional


Overrides the memory layout of the result. ‘C’ means C-order, ‘F’ means F-order, ‘A’ means ‘F’ if a is Fortran contiguous, ‘C’ otherwise. ‘K’ means match the layout of a as closely as possible.  New in version 1.6.0.   
subokbool, optional.


If True, then the newly created array will use the sub-class type of a, otherwise it will be a base-class array. Defaults to True.  
shapeint or sequence of ints, optional.


Overrides the shape of the result. If order=’K’ and the number of dimensions is unchanged, will try to keep order, otherwise, order=’C’ is implied.  New in version 1.17.0.     Returns 
 
outndarray


Array of zeros with the same shape and type as a.      See also  empty_like

Return an empty array with shape and type of input.  ones_like

Return an array of ones with shape and type of input.  full_like

Return a new array with shape of input filled with value.  zeros

Return a new array setting values to zero.    Examples >>> x = np.arange(6)
>>> x = x.reshape((2, 3))
>>> x
array([[0, 1, 2],
       [3, 4, 5]])
>>> np.zeros_like(x)
array([[0, 0, 0],
       [0, 0, 0]])
 >>> y = np.arange(3, dtype=float)
>>> y
array([0., 1., 2.])
>>> np.zeros_like(y)
array([0.,  0.,  0.])
tf.experimental.numpy.zeros_like TensorFlow variant of NumPy's zeros_like. 
tf.experimental.numpy.zeros_like(
    a, dtype=None
)
 Unsupported arguments: order, subok, shape. See the NumPy documentation for numpy.zeros_like.
tf.sparse.fill_empty_rows     View source on GitHub    Fills empty rows in the input 2-D SparseTensor with a default value.  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.sparse.fill_empty_rows, tf.compat.v1.sparse_fill_empty_rows  
tf.sparse.fill_empty_rows(
    sp_input, default_value, name=None
)
 This op adds entries with the specified default_value at index [row, 0] for any row in the input that does not already have a value. For example, suppose sp_input has shape [5, 6] and non-empty values: [0, 1]: a
[0, 3]: b
[2, 0]: c
[3, 1]: d
 Rows 1 and 4 are empty, so the output will be of shape [5, 6] with values: [0, 1]: a
[0, 3]: b
[1, 0]: default_value
[2, 0]: c
[3, 1]: d
[4, 0]: default_value
 Note that the input may have empty columns at the end, with no effect on this op. The output SparseTensor will be in row-major order and will have the same shape as the input. This op also returns an indicator vector such that empty_row_indicator[i] = True iff row i was an empty row.

 


 Args
  sp_input   A SparseTensor with shape [N, M].  
  default_value   The value to fill for empty rows, with the same type as sp_input.  
  name   A name prefix for the returned tensors (optional)   
 


 Returns
  sp_ordered_output   A SparseTensor with shape [N, M], and with all empty rows filled in with default_value.  
  empty_row_indicator   A bool vector of length N indicating whether each input row was empty.   
 


 Raises
  TypeError   If sp_input is not a SparseTensor.
set_array(A)[source]
 
Set the value array from array-like A.  Parameters 
 
Aarray-like or None


The values that are mapped to colors. The base class ScalarMappable does not make any assumptions on the dimensionality and shape of the value array A.
def f_34945274(A):
    """replace all elements in array `A` that are not present in array `[1, 3, 4]` with zeros
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
numpy.matrix.mean method   matrix.mean(axis=None, dtype=None, out=None)[source]
 
Returns the average of the matrix elements along the given axis. Refer to numpy.mean for full documentation.  See also  numpy.mean
  Notes Same as ndarray.mean except that, where that returns an ndarray, this returns a matrix object. Examples >>> x = np.matrix(np.arange(12).reshape((3, 4)))
>>> x
matrix([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11]])
>>> x.mean()
5.5
>>> x.mean(0)
matrix([[4., 5., 6., 7.]])
>>> x.mean(1)
matrix([[ 1.5],
        [ 5.5],
        [ 9.5]])
numpy.ndarray.mean method   ndarray.mean(axis=None, dtype=None, out=None, keepdims=False, *, where=True)
 
Returns the average of the array elements along given axis. Refer to numpy.mean for full documentation.  See also  numpy.mean

equivalent function
tf.experimental.numpy.mean TensorFlow variant of NumPy's mean. 
tf.experimental.numpy.mean(
    a, axis=None, dtype=None, keepdims=None
)
 Unsupported arguments: out. See the NumPy documentation for numpy.mean.
numpy.recarray.mean method   recarray.mean(axis=None, dtype=None, out=None, keepdims=False, *, where=True)
 
Returns the average of the array elements along given axis. Refer to numpy.mean for full documentation.  See also  numpy.mean

equivalent function
numpy.mean   numpy.mean(a, axis=None, dtype=None, out=None, keepdims=<no value>, *, where=<no value>)[source]
 
Compute the arithmetic mean along the specified axis. Returns the average of the array elements. The average is taken over the flattened array by default, otherwise over the specified axis. float64 intermediate and return values are used for integer inputs.  Parameters 
 
aarray_like


Array containing numbers whose mean is desired. If a is not an array, a conversion is attempted.  
axisNone or int or tuple of ints, optional


Axis or axes along which the means are computed. The default is to compute the mean of the flattened array.  New in version 1.7.0.  If this is a tuple of ints, a mean is performed over multiple axes, instead of a single axis or all the axes as before.  
dtypedata-type, optional


Type to use in computing the mean. For integer inputs, the default is float64; for floating point inputs, it is the same as the input dtype.  
outndarray, optional


Alternate output array in which to place the result. The default is None; if provided, it must have the same shape as the expected output, but the type will be cast if necessary. See Output type determination for more details.  
keepdimsbool, optional


If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array. If the default value is passed, then keepdims will not be passed through to the mean method of sub-classes of ndarray, however any non-default value will be. If the sub-class’ method does not implement keepdims any exceptions will be raised.  
wherearray_like of bool, optional


Elements to include in the mean. See reduce for details.  New in version 1.20.0.     Returns 
 
mndarray, see dtype parameter above


If out=None, returns a new array containing the mean values, otherwise a reference to the output array is returned.      See also  average

Weighted average  
std, var, nanmean, nanstd, nanvar

  Notes The arithmetic mean is the sum of the elements along the axis divided by the number of elements. Note that for floating-point input, the mean is computed using the same precision the input has. Depending on the input data, this can cause the results to be inaccurate, especially for float32 (see example below). Specifying a higher-precision accumulator using the dtype keyword can alleviate this issue. By default, float16 results are computed using float32 intermediates for extra precision. Examples >>> a = np.array([[1, 2], [3, 4]])
>>> np.mean(a)
2.5
>>> np.mean(a, axis=0)
array([2., 3.])
>>> np.mean(a, axis=1)
array([1.5, 3.5])
 In single precision, mean can be inaccurate: >>> a = np.zeros((2, 512*512), dtype=np.float32)
>>> a[0, :] = 1.0
>>> a[1, :] = 0.1
>>> np.mean(a)
0.54999924
 Computing the mean in float64 is more accurate: >>> np.mean(a, dtype=np.float64)
0.55000000074505806 # may vary
 Specifying a where argument: >>> a = np.array([[5, 9, 13], [14, 10, 12], [11, 15, 19]]) >>> np.mean(a) 12.0 >>> np.mean(a, where=[[True], [False], [False]]) 9.0
def f_15819980(a):
    """calculate mean across dimension in a 2d array `a`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
remote(self: torch._C._distributed_rpc.PyRRef, timeout: float = -1.0) → object  
Create a helper proxy to easily launch a remote using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, rref.remote().func_name(*args, **kwargs) is the same as the following: >>> def run(rref, func_name, args, kwargs):
>>>   return getattr(rref.local_value(), func_name)(*args, **kwargs)
>>>
>>> rpc.remote(rref.owner(), run, args=(rref, func_name, args, kwargs))
  Parameters 
timeout (float, optional) – Timeout for rref.remote(). If the creation of this RRef is not successfully completed within the timeout, then the next time there is an attempt to use the RRef (such as to_here), a timeout will be raised. If not provided, the default RPC timeout will be used. Please see rpc.remote() for specific timeout semantics for RRef.    Example::

>>> from torch.distributed import rpc
>>> rref = rpc.remote("worker1", torch.add, args=(torch.zeros(2, 2), 1))
>>> rref.remote().size().to_here()  # returns torch.Size([2, 2])
>>> rref.remote().view(1, 4).to_here()  # returns tensor([[1., 1., 1., 1.]])
class torch.distributed.rpc.RRef [source]
 
 
backward(self: torch._C._distributed_rpc.PyRRef, dist_autograd_ctx_id: int = -1, retain_graph: bool = False) → None   Runs the backward pass using the RRef as the root of the backward pass. If dist_autograd_ctx_id is provided, we perform a distributed backward pass using the provided ctx_id starting from the owner of the RRef. In this case, get_gradients() should be used to retrieve the gradients. If dist_autograd_ctx_id is None, it is assumed that this is a local autograd graph and we only perform a local backward pass. In the local case, the node calling this API has to be the owner of the RRef. The value of the RRef is expected to be a scalar Tensor.  Parameters 
 
dist_autograd_ctx_id (int, optional) – The distributed autograd context id for which we should retrieve the gradients (default: -1). 
retain_graph (bool, optional) – If False, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Usually, you need to set this to True to run backward multiple times (default: False).     Example::

>>> import torch.distributed.autograd as dist_autograd
>>> with dist_autograd.context() as context_id:
>>>     rref.backward(context_id)
   
  
confirmed_by_owner(self: torch._C._distributed_rpc.PyRRef) → bool  
Returns whether this RRef has been confirmed by the owner. OwnerRRef always returns true, while UserRRef only returns true when the owner knowns about this UserRRef. 
  
is_owner(self: torch._C._distributed_rpc.PyRRef) → bool  
Returns whether or not the current node is the owner of this RRef. 
  
local_value(self: torch._C._distributed_rpc.PyRRef) → object  
If the current node is the owner, returns a reference to the local value. Otherwise, throws an exception. 
  
owner(self: torch._C._distributed_rpc.PyRRef) → torch._C._distributed_rpc.WorkerInfo  
Returns worker information of the node that owns this RRef. 
  
owner_name(self: torch._C._distributed_rpc.PyRRef) → str  
Returns worker name of the node that owns this RRef. 
  
remote(self: torch._C._distributed_rpc.PyRRef, timeout: float = -1.0) → object  
Create a helper proxy to easily launch a remote using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, rref.remote().func_name(*args, **kwargs) is the same as the following: >>> def run(rref, func_name, args, kwargs):
>>>   return getattr(rref.local_value(), func_name)(*args, **kwargs)
>>>
>>> rpc.remote(rref.owner(), run, args=(rref, func_name, args, kwargs))
  Parameters 
timeout (float, optional) – Timeout for rref.remote(). If the creation of this RRef is not successfully completed within the timeout, then the next time there is an attempt to use the RRef (such as to_here), a timeout will be raised. If not provided, the default RPC timeout will be used. Please see rpc.remote() for specific timeout semantics for RRef.    Example::

>>> from torch.distributed import rpc
>>> rref = rpc.remote("worker1", torch.add, args=(torch.zeros(2, 2), 1))
>>> rref.remote().size().to_here()  # returns torch.Size([2, 2])
>>> rref.remote().view(1, 4).to_here()  # returns tensor([[1., 1., 1., 1.]])
   
  
rpc_async(self: torch._C._distributed_rpc.PyRRef, timeout: float = -1.0) → object  
Create a helper proxy to easily launch an rpc_async using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, rref.rpc_async().func_name(*args, **kwargs) is the same as the following: >>> def run(rref, func_name, args, kwargs):
>>>   return getattr(rref.local_value(), func_name)(*args, **kwargs)
>>>
>>> rpc.rpc_async(rref.owner(), run, args=(rref, func_name, args, kwargs))
  Parameters 
timeout (float, optional) – Timeout for rref.rpc_async(). If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout will be used.    Example::

>>> from torch.distributed import rpc
>>> rref = rpc.remote("worker1", torch.add, args=(torch.zeros(2, 2), 1))
>>> rref.rpc_async().size().wait()  # returns torch.Size([2, 2])
>>> rref.rpc_async().view(1, 4).wait()  # returns tensor([[1., 1., 1., 1.]])
   
  
rpc_sync(self: torch._C._distributed_rpc.PyRRef, timeout: float = -1.0) → object  
Create a helper proxy to easily launch an rpc_sync using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, rref.rpc_sync().func_name(*args, **kwargs) is the same as the following: >>> def run(rref, func_name, args, kwargs):
>>>   return getattr(rref.local_value(), func_name)(*args, **kwargs)
>>>
>>> rpc.rpc_sync(rref.owner(), run, args=(rref, func_name, args, kwargs))
  Parameters 
timeout (float, optional) – Timeout for rref.rpc_sync(). If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout will be used.    Example::

>>> from torch.distributed import rpc
>>> rref = rpc.remote("worker1", torch.add, args=(torch.zeros(2, 2), 1))
>>> rref.rpc_sync().size()  # returns torch.Size([2, 2])
>>> rref.rpc_sync().view(1, 4)  # returns tensor([[1., 1., 1., 1.]])
   
  
to_here(self: torch._C._distributed_rpc.PyRRef, timeout: float = -1.0) → object  
Blocking call that copies the value of the RRef from the owner to the local node and returns it. If the current node is the owner, returns a reference to the local value.  Parameters 
timeout (float, optional) – Timeout for to_here. If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout (60s) will be used.
rfind(sub[, start[, end]])  
Returns the highest index in the object where the subsequence sub is found, such that sub is contained in the range [start, end]. Optional arguments start and end are interpreted as in slice notation. Returns -1 on failure.  Changed in version 3.5: Writable bytes-like object is now accepted.
str.rfind(sub[, start[, end]])  
Return the highest index in the string where substring sub is found, such that sub is contained within s[start:end]. Optional arguments start and end are interpreted as in slice notation. Return -1 on failure.
to_here(self: torch._C._distributed_rpc.PyRRef, timeout: float = -1.0) → object  
Blocking call that copies the value of the RRef from the owner to the local node and returns it. If the current node is the owner, returns a reference to the local value.  Parameters 
timeout (float, optional) – Timeout for to_here. If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout (60s) will be used.
def f_19894365():
    """running r script '/pathto/MyrScript.r' from python
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
rfind(sub[, start[, end]])  
Returns the highest index in the object where the subsequence sub is found, such that sub is contained in the range [start, end]. Optional arguments start and end are interpreted as in slice notation. Returns -1 on failure.  Changed in version 3.5: Writable bytes-like object is now accepted.
str.rfind(sub[, start[, end]])  
Return the highest index in the string where substring sub is found, such that sub is contained within s[start:end]. Optional arguments start and end are interpreted as in slice notation. Return -1 on failure.
class torch.distributed.rpc.RRef [source]
 
 
backward(self: torch._C._distributed_rpc.PyRRef, dist_autograd_ctx_id: int = -1, retain_graph: bool = False) → None   Runs the backward pass using the RRef as the root of the backward pass. If dist_autograd_ctx_id is provided, we perform a distributed backward pass using the provided ctx_id starting from the owner of the RRef. In this case, get_gradients() should be used to retrieve the gradients. If dist_autograd_ctx_id is None, it is assumed that this is a local autograd graph and we only perform a local backward pass. In the local case, the node calling this API has to be the owner of the RRef. The value of the RRef is expected to be a scalar Tensor.  Parameters 
 
dist_autograd_ctx_id (int, optional) – The distributed autograd context id for which we should retrieve the gradients (default: -1). 
retain_graph (bool, optional) – If False, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Usually, you need to set this to True to run backward multiple times (default: False).     Example::

>>> import torch.distributed.autograd as dist_autograd
>>> with dist_autograd.context() as context_id:
>>>     rref.backward(context_id)
   
  
confirmed_by_owner(self: torch._C._distributed_rpc.PyRRef) → bool  
Returns whether this RRef has been confirmed by the owner. OwnerRRef always returns true, while UserRRef only returns true when the owner knowns about this UserRRef. 
  
is_owner(self: torch._C._distributed_rpc.PyRRef) → bool  
Returns whether or not the current node is the owner of this RRef. 
  
local_value(self: torch._C._distributed_rpc.PyRRef) → object  
If the current node is the owner, returns a reference to the local value. Otherwise, throws an exception. 
  
owner(self: torch._C._distributed_rpc.PyRRef) → torch._C._distributed_rpc.WorkerInfo  
Returns worker information of the node that owns this RRef. 
  
owner_name(self: torch._C._distributed_rpc.PyRRef) → str  
Returns worker name of the node that owns this RRef. 
  
remote(self: torch._C._distributed_rpc.PyRRef, timeout: float = -1.0) → object  
Create a helper proxy to easily launch a remote using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, rref.remote().func_name(*args, **kwargs) is the same as the following: >>> def run(rref, func_name, args, kwargs):
>>>   return getattr(rref.local_value(), func_name)(*args, **kwargs)
>>>
>>> rpc.remote(rref.owner(), run, args=(rref, func_name, args, kwargs))
  Parameters 
timeout (float, optional) – Timeout for rref.remote(). If the creation of this RRef is not successfully completed within the timeout, then the next time there is an attempt to use the RRef (such as to_here), a timeout will be raised. If not provided, the default RPC timeout will be used. Please see rpc.remote() for specific timeout semantics for RRef.    Example::

>>> from torch.distributed import rpc
>>> rref = rpc.remote("worker1", torch.add, args=(torch.zeros(2, 2), 1))
>>> rref.remote().size().to_here()  # returns torch.Size([2, 2])
>>> rref.remote().view(1, 4).to_here()  # returns tensor([[1., 1., 1., 1.]])
   
  
rpc_async(self: torch._C._distributed_rpc.PyRRef, timeout: float = -1.0) → object  
Create a helper proxy to easily launch an rpc_async using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, rref.rpc_async().func_name(*args, **kwargs) is the same as the following: >>> def run(rref, func_name, args, kwargs):
>>>   return getattr(rref.local_value(), func_name)(*args, **kwargs)
>>>
>>> rpc.rpc_async(rref.owner(), run, args=(rref, func_name, args, kwargs))
  Parameters 
timeout (float, optional) – Timeout for rref.rpc_async(). If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout will be used.    Example::

>>> from torch.distributed import rpc
>>> rref = rpc.remote("worker1", torch.add, args=(torch.zeros(2, 2), 1))
>>> rref.rpc_async().size().wait()  # returns torch.Size([2, 2])
>>> rref.rpc_async().view(1, 4).wait()  # returns tensor([[1., 1., 1., 1.]])
   
  
rpc_sync(self: torch._C._distributed_rpc.PyRRef, timeout: float = -1.0) → object  
Create a helper proxy to easily launch an rpc_sync using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, rref.rpc_sync().func_name(*args, **kwargs) is the same as the following: >>> def run(rref, func_name, args, kwargs):
>>>   return getattr(rref.local_value(), func_name)(*args, **kwargs)
>>>
>>> rpc.rpc_sync(rref.owner(), run, args=(rref, func_name, args, kwargs))
  Parameters 
timeout (float, optional) – Timeout for rref.rpc_sync(). If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout will be used.    Example::

>>> from torch.distributed import rpc
>>> rref = rpc.remote("worker1", torch.add, args=(torch.zeros(2, 2), 1))
>>> rref.rpc_sync().size()  # returns torch.Size([2, 2])
>>> rref.rpc_sync().view(1, 4)  # returns tensor([[1., 1., 1., 1.]])
   
  
to_here(self: torch._C._distributed_rpc.PyRRef, timeout: float = -1.0) → object  
Blocking call that copies the value of the RRef from the owner to the local node and returns it. If the current node is the owner, returns a reference to the local value.  Parameters 
timeout (float, optional) – Timeout for to_here. If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout (60s) will be used.
has_rsample = True
has_rsample = True
def f_19894365():
    """run r script '/usr/bin/Rscript --vanilla /pathto/MyrScript.r'
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
class RegrAvgY(y, x, filter=None, default=None)  
Returns the average of the dependent variable (sum(y)/N) as a float, or default if there aren’t any matching rows.
class RegrAvgX(y, x, filter=None, default=None)  
Returns the average of the independent variable (sum(x)/N) as a float, or default if there aren’t any matching rows.
class Avg(expression, output_field=None, distinct=False, filter=None, default=None, **extra)  
Returns the mean value of the given expression, which must be numeric unless you specify a different output_field.  Default alias: <field>__avg
 Return type: float if input is int, otherwise same as input field, or output_field if supplied  Has one optional argument:  
distinct  
If distinct=True, Avg returns the mean value of unique values. This is the SQL equivalent of AVG(DISTINCT <field>). The default value is False.
class RegrSXY(y, x, filter=None, default=None)  
Returns sum(x*y) - sum(x) * sum(y)/N (“sum of products” of independent times dependent variable) as a float, or default if there aren’t any matching rows.
tf.estimator.NanLossDuringTrainingError Unspecified run-time error.  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.estimator.NanLossDuringTrainingError, tf.compat.v1.train.NanLossDuringTrainingError  
tf.estimator.NanLossDuringTrainingError(
    *args, **kwargs
)
def f_33058590(df):
    """replacing nan in the dataframe `df` with row average
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
time.ctime([secs])  
Convert a time expressed in seconds since the epoch to a string of a form: 'Sun Jun 20 23:21:05 1993' representing local time. The day field is two characters long and is space padded if the day is a single digit, e.g.: 'Wed Jun  9 04:26:40 1993'. If secs is not provided or None, the current time as returned by time() is used. ctime(secs) is equivalent to asctime(localtime(secs)). Locale information is not used by ctime().
time.asctime([t])  
Convert a tuple or struct_time representing a time as returned by gmtime() or localtime() to a string of the following form: 'Sun Jun 20 23:21:05 1993'. The day field is two characters long and is space padded if the day is a single digit, e.g.: 'Wed Jun  9 04:26:40 1993'. If t is not provided, the current time as returned by localtime() is used. Locale information is not used by asctime().  Note Unlike the C function of the same name, asctime() does not add a trailing newline.
werkzeug.http.http_date(timestamp=None)  
Format a datetime object or timestamp into an RFC 2822 date string. This is a wrapper for email.utils.format_datetime(). It assumes naive datetime objects are in UTC instead of raising an exception.  Parameters 
timestamp (Optional[Union[datetime.datetime, datetime.date, int, float, time.struct_time]]) – The datetime or timestamp to format. Defaults to the current time.  Return type 
str    Changed in version 2.0: Use email.utils.format_datetime. Accept date objects.
ascii(object)  
As repr(), return a string containing a printable representation of an object, but escape the non-ASCII characters in the string returned by repr() using \x, \u or \U escapes. This generates a string similar to that returned by repr() in Python 2.
st_ctime  
Platform dependent:  the time of most recent metadata change on Unix, the time of creation on Windows, expressed in seconds.
def f_12400256():
    """Convert unix timestamp '1347517370' to formatted string '%Y-%m-%d %H:%M:%S'
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
numpy.argwhere   numpy.argwhere(a)[source]
 
Find the indices of array elements that are non-zero, grouped by element.  Parameters 
 
aarray_like


Input data.    Returns 
 
index_array(N, a.ndim) ndarray


Indices of elements that are non-zero. Indices are grouped by element. This array will have shape (N, a.ndim) where N is the number of non-zero items.      See also  
where, nonzero

  Notes np.argwhere(a) is almost the same as np.transpose(np.nonzero(a)), but produces a result of the correct shape for a 0D array. The output of argwhere is not suitable for indexing arrays. For this purpose use nonzero(a) instead. Examples >>> x = np.arange(6).reshape(2,3)
>>> x
array([[0, 1, 2],
       [3, 4, 5]])
>>> np.argwhere(x>1)
array([[0, 2],
       [1, 0],
       [1, 1],
       [1, 2]])
numpy.ma.ones_like   ma.ones_like(*args, **kwargs) = <numpy.ma.core._convert2ma object>
 
Return an array of ones with the same shape and type as a given array.  Parameters 
 
aarray_like


The shape and data-type of a define these same attributes of the returned array.  
dtypedata-type, optional


Overrides the data type of the result.  New in version 1.6.0.   
order{‘C’, ‘F’, ‘A’, or ‘K’}, optional


Overrides the memory layout of the result. ‘C’ means C-order, ‘F’ means F-order, ‘A’ means ‘F’ if a is Fortran contiguous, ‘C’ otherwise. ‘K’ means match the layout of a as closely as possible.  New in version 1.6.0.   
subokbool, optional.


If True, then the newly created array will use the sub-class type of a, otherwise it will be a base-class array. Defaults to True.  
shapeint or sequence of ints, optional.


Overrides the shape of the result. If order=’K’ and the number of dimensions is unchanged, will try to keep order, otherwise, order=’C’ is implied.  New in version 1.17.0.     Returns 
 
outMaskedArray


Array of ones with the same shape and type as a.      See also  empty_like

Return an empty array with shape and type of input.  zeros_like

Return an array of zeros with shape and type of input.  full_like

Return a new array with shape of input filled with value.  ones

Return a new array setting values to one.    Examples >>> x = np.arange(6)
>>> x = x.reshape((2, 3))
>>> x
array([[0, 1, 2],
       [3, 4, 5]])
>>> np.ones_like(x)
array([[1, 1, 1],
       [1, 1, 1]])
 >>> y = np.arange(3, dtype=float)
>>> y
array([0., 1., 2.])
>>> np.ones_like(y)
array([1.,  1.,  1.])
numpy.ones_like   numpy.ones_like(a, dtype=None, order='K', subok=True, shape=None)[source]
 
Return an array of ones with the same shape and type as a given array.  Parameters 
 
aarray_like


The shape and data-type of a define these same attributes of the returned array.  
dtypedata-type, optional


Overrides the data type of the result.  New in version 1.6.0.   
order{‘C’, ‘F’, ‘A’, or ‘K’}, optional


Overrides the memory layout of the result. ‘C’ means C-order, ‘F’ means F-order, ‘A’ means ‘F’ if a is Fortran contiguous, ‘C’ otherwise. ‘K’ means match the layout of a as closely as possible.  New in version 1.6.0.   
subokbool, optional.


If True, then the newly created array will use the sub-class type of a, otherwise it will be a base-class array. Defaults to True.  
shapeint or sequence of ints, optional.


Overrides the shape of the result. If order=’K’ and the number of dimensions is unchanged, will try to keep order, otherwise, order=’C’ is implied.  New in version 1.17.0.     Returns 
 
outndarray


Array of ones with the same shape and type as a.      See also  empty_like

Return an empty array with shape and type of input.  zeros_like

Return an array of zeros with shape and type of input.  full_like

Return a new array with shape of input filled with value.  ones

Return a new array setting values to one.    Examples >>> x = np.arange(6)
>>> x = x.reshape((2, 3))
>>> x
array([[0, 1, 2],
       [3, 4, 5]])
>>> np.ones_like(x)
array([[1, 1, 1],
       [1, 1, 1]])
 >>> y = np.arange(3, dtype=float)
>>> y
array([0., 1., 2.])
>>> np.ones_like(y)
array([1.,  1.,  1.])
numpy.ma.compress_rows   ma.compress_rows(a)[source]
 
Suppress whole rows of a 2-D array that contain masked values. This is equivalent to np.ma.compress_rowcols(a, 0), see compress_rowcols for details.  See also  compress_rowcols
tf.experimental.numpy.ones_like TensorFlow variant of NumPy's ones_like. 
tf.experimental.numpy.ones_like(
    a, dtype=None
)
 Unsupported arguments: order, subok, shape. See the NumPy documentation for numpy.ones_like.
def f_23359886(a):
    """selecting rows in Numpy ndarray 'a', where the value in the first column is 0 and value in the second column is 1
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
asyncio.run_coroutine_threadsafe(coro, loop)  
Submit a coroutine to the given event loop. Thread-safe. Return a concurrent.futures.Future to wait for the result from another OS thread. This function is meant to be called from a different OS thread than the one where the event loop is running. Example: # Create a coroutine
coro = asyncio.sleep(1, result=3)

# Submit the coroutine to a given loop
future = asyncio.run_coroutine_threadsafe(coro, loop)

# Wait for the result with an optional timeout argument
assert future.result(timeout) == 3
 If an exception is raised in the coroutine, the returned Future will be notified. It can also be used to cancel the task in the event loop: try:
    result = future.result(timeout)
except asyncio.TimeoutError:
    print('The coroutine took too long, cancelling the task...')
    future.cancel()
except Exception as exc:
    print(f'The coroutine raised an exception: {exc!r}')
else:
    print(f'The coroutine returned: {result!r}')
 See the concurrency and multithreading section of the documentation. Unlike other asyncio functions this function requires the loop argument to be passed explicitly.  New in version 3.5.1.
pandas.Series.clip   Series.clip(lower=None, upper=None, axis=None, inplace=False, *args, **kwargs)[source]
 
Trim values at input threshold(s). Assigns values outside boundary to boundary values. Thresholds can be singular values or array like, and in the latter case the clipping is performed element-wise in the specified axis.  Parameters 
 
lower:float or array-like, default None


Minimum threshold value. All values below this threshold will be set to it. A missing threshold (e.g NA) will not clip the value.  
upper:float or array-like, default None


Maximum threshold value. All values above this threshold will be set to it. A missing threshold (e.g NA) will not clip the value.  
axis:int or str axis name, optional


Align object with lower and upper along the given axis.  
inplace:bool, default False


Whether to perform the operation in place on the data.  *args, **kwargs

Additional keywords have no effect but might be accepted for compatibility with numpy.    Returns 
 Series or DataFrame or None

Same type as calling object with the values outside the clip boundaries replaced or None if inplace=True.      See also  Series.clip

Trim values at input threshold in series.  DataFrame.clip

Trim values at input threshold in dataframe.  numpy.clip

Clip (limit) the values in an array.    Examples 
>>> data = {'col_0': [9, -3, 0, -1, 5], 'col_1': [-2, -7, 6, 8, -5]}
>>> df = pd.DataFrame(data)
>>> df
   col_0  col_1
0      9     -2
1     -3     -7
2      0      6
3     -1      8
4      5     -5
  Clips per column using lower and upper thresholds: 
>>> df.clip(-4, 6)
   col_0  col_1
0      6     -2
1     -3     -4
2      0      6
3     -1      6
4      5     -4
  Clips using specific lower and upper thresholds per column element: 
>>> t = pd.Series([2, -4, -1, 6, 3])
>>> t
0    2
1   -4
2   -1
3    6
4    3
dtype: int64
  
>>> df.clip(t, t + 4, axis=0)
   col_0  col_1
0      6      2
1     -3     -4
2      0      3
3      6      8
4      5      3
  Clips using specific lower threshold per column element, with missing values: 
>>> t = pd.Series([2, -4, np.NaN, 6, 3])
>>> t
0    2.0
1   -4.0
2    NaN
3    6.0
4    3.0
dtype: float64
  
>>> df.clip(t, axis=0)
col_0  col_1
0      9      2
1     -3     -4
2      0      6
3      6      8
4      5      3
pandas.DataFrame.clip   DataFrame.clip(lower=None, upper=None, axis=None, inplace=False, *args, **kwargs)[source]
 
Trim values at input threshold(s). Assigns values outside boundary to boundary values. Thresholds can be singular values or array like, and in the latter case the clipping is performed element-wise in the specified axis.  Parameters 
 
lower:float or array-like, default None


Minimum threshold value. All values below this threshold will be set to it. A missing threshold (e.g NA) will not clip the value.  
upper:float or array-like, default None


Maximum threshold value. All values above this threshold will be set to it. A missing threshold (e.g NA) will not clip the value.  
axis:int or str axis name, optional


Align object with lower and upper along the given axis.  
inplace:bool, default False


Whether to perform the operation in place on the data.  *args, **kwargs

Additional keywords have no effect but might be accepted for compatibility with numpy.    Returns 
 Series or DataFrame or None

Same type as calling object with the values outside the clip boundaries replaced or None if inplace=True.      See also  Series.clip

Trim values at input threshold in series.  DataFrame.clip

Trim values at input threshold in dataframe.  numpy.clip

Clip (limit) the values in an array.    Examples 
>>> data = {'col_0': [9, -3, 0, -1, 5], 'col_1': [-2, -7, 6, 8, -5]}
>>> df = pd.DataFrame(data)
>>> df
   col_0  col_1
0      9     -2
1     -3     -7
2      0      6
3     -1      8
4      5     -5
  Clips per column using lower and upper thresholds: 
>>> df.clip(-4, 6)
   col_0  col_1
0      6     -2
1     -3     -4
2      0      6
3     -1      6
4      5     -4
  Clips using specific lower and upper thresholds per column element: 
>>> t = pd.Series([2, -4, -1, 6, 3])
>>> t
0    2
1   -4
2   -1
3    6
4    3
dtype: int64
  
>>> df.clip(t, t + 4, axis=0)
   col_0  col_1
0      6      2
1     -3     -4
2      0      3
3      6      8
4      5      3
  Clips using specific lower threshold per column element, with missing values: 
>>> t = pd.Series([2, -4, np.NaN, 6, 3])
>>> t
0    2.0
1   -4.0
2    NaN
3    6.0
4    3.0
dtype: float64
  
>>> df.clip(t, axis=0)
col_0  col_1
0      9      2
1     -3     -4
2      0      6
3      6      8
4      5      3
tf.image.non_max_suppression_with_scores     View source on GitHub    Greedily selects a subset of bounding boxes in descending order of score.  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.image.non_max_suppression_with_scores  
tf.image.non_max_suppression_with_scores(
    boxes, scores, max_output_size, iou_threshold=0.5,
    score_threshold=float('-inf'), soft_nms_sigma=0.0, name=None
)
 Prunes away boxes that have high intersection-over-union (IOU) overlap with previously selected boxes. Bounding boxes are supplied as [y1, x1, y2, x2], where (y1, x1) and (y2, x2) are the coordinates of any diagonal pair of box corners and the coordinates can be provided as normalized (i.e., lying in the interval [0, 1]) or absolute. Note that this algorithm is agnostic to where the origin is in the coordinate system. Note that this algorithm is invariant to orthogonal transformations and translations of the coordinate system; thus translating or reflections of the coordinate system result in the same boxes being selected by the algorithm. The output of this operation is a set of integers indexing into the input collection of bounding boxes representing the selected boxes. The bounding box coordinates corresponding to the selected indices can then be obtained using the tf.gather operation. For example: selected_indices, selected_scores = tf.image.non_max_suppression_padded(
    boxes, scores, max_output_size, iou_threshold=1.0, score_threshold=0.1,
    soft_nms_sigma=0.5)
selected_boxes = tf.gather(boxes, selected_indices)
 This function generalizes the tf.image.non_max_suppression op by also supporting a Soft-NMS (with Gaussian weighting) mode (c.f. Bodla et al, https://arxiv.org/abs/1704.04503) where boxes reduce the score of other overlapping boxes instead of directly causing them to be pruned. Consequently, in contrast to tf.image.non_max_suppression, tf.image.non_max_suppression_padded returns the new scores of each input box in the second output, selected_scores. To enable this Soft-NMS mode, set the soft_nms_sigma parameter to be larger than 0. When soft_nms_sigma equals 0, the behavior of tf.image.non_max_suppression_padded is identical to that of tf.image.non_max_suppression (except for the extra output) both in function and in running time.
 


 Args
  boxes   A 2-D float Tensor of shape [num_boxes, 4].  
  scores   A 1-D float Tensor of shape [num_boxes] representing a single score corresponding to each box (each row of boxes).  
  max_output_size   A scalar integer Tensor representing the maximum number of boxes to be selected by non-max suppression.  
  iou_threshold   A float representing the threshold for deciding whether boxes overlap too much with respect to IOU.  
  score_threshold   A float representing the threshold for deciding when to remove boxes based on score.  
  soft_nms_sigma   A scalar float representing the Soft NMS sigma parameter; See Bodla et al, https://arxiv.org/abs/1704.04503). When soft_nms_sigma=0.0 (which is default), we fall back to standard (hard) NMS.  
  name   A name for the operation (optional).   
 


 Returns
  selected_indices   A 1-D integer Tensor of shape [M] representing the selected indices from the boxes tensor, where M <= max_output_size.  
  selected_scores   A 1-D float tensor of shape [M] representing the corresponding scores for each selected box, where M <= max_output_size. Scores only differ from corresponding input scores when using Soft NMS (i.e. when soft_nms_sigma>0)
get_rect() 
 Returns a Rect based on the size of the mask get_rect(**kwargs) -> Rect  Returns a new pygame.Rect() object based on the size of this mask. The rect's default position will be (0, 0) and its default width and height will be the same as this mask's. The rect's attributes can be altered via pygame.Rect() attribute keyword arguments/values passed into this method. As an example, a_mask.get_rect(center=(10, 5)) would create a pygame.Rect() based on the mask's size centered at the given position.     
Parameters:

kwargs (dict) -- pygame.Rect() attribute keyword arguments/values that will be applied to the rect  
Returns:
a new pygame.Rect() object based on the size of this mask with any pygame.Rect() attribute keyword arguments/values applied to it  
Return type:
Rect     New in pygame 2.0.0.
def f_4383082(words):
    """separate words delimited by one or more spaces into a list
    """
    return  
 --------------------

def f_14637696(words):
    """length of longest element in list `words`
    """
    return  
 --------------------

def f_3933478(result):
    """get the value associated with unicode key 'from_user' of first dictionary in list `result`
    """
    return  
 --------------------

def f_39112645():
    """Retrieve each line from a file 'File.txt' as a list
    """
    return  
 --------------------

def f_1031851(a):
    """swap keys with values in a dictionary `a`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
Path.touch(mode=0o666, exist_ok=True)  
Create a file at this given path. If mode is given, it is combined with the process’ umask value to determine the file mode and access flags. If the file already exists, the function succeeds if exist_ok is true (and its modification time is updated to the current time), otherwise FileExistsError is raised.
test.support.create_empty_file(filename)  
Create an empty file with filename. If it already exists, truncate it.
os.path.splitext(path)  
Split the pathname path into a pair (root, ext) such that root + ext ==
path, and ext is empty or begins with a period and contains at most one period. Leading periods on the basename are ignored; splitext('.cshrc') returns ('.cshrc', '').  Changed in version 3.6: Accepts a path-like object.
tkinter.filedialog.asksaveasfile(mode="w", **options)  
Create a SaveAs dialog and return a file object opened in write-only mode.
_open(name, mode='rb')
def f_8577137():
    """Open a file `path/to/FILE_NAME.ext` in write mode
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
class RegrCount(y, x, filter=None)  
Returns an int of the number of input rows in which both expressions are not null.  Note The default argument is not supported.
pandas.core.groupby.DataFrameGroupBy.nunique   DataFrameGroupBy.nunique(dropna=True)[source]
 
Return DataFrame with counts of unique elements in each position.  Parameters 
 
dropna:bool, default True


Don’t include NaN in the counts.    Returns 
 nunique: DataFrame
   Examples 
>>> df = pd.DataFrame({'id': ['spam', 'egg', 'egg', 'spam',
...                           'ham', 'ham'],
...                    'value1': [1, 5, 5, 2, 5, 5],
...                    'value2': list('abbaxy')})
>>> df
     id  value1 value2
0  spam       1      a
1   egg       5      b
2   egg       5      b
3  spam       2      a
4   ham       5      x
5   ham       5      y
  
>>> df.groupby('id').nunique()
      value1  value2
id
egg        1       1
ham        1       2
spam       2       1
  Check for rows with the same id but conflicting values: 
>>> df.groupby('id').filter(lambda g: (g.nunique() > 1).any())
     id  value1 value2
0  spam       1      a
3  spam       2      a
4   ham       5      x
5   ham       5      y
get_group_by_cols(alias=None)  
Responsible for returning the list of columns references by this expression. get_group_by_cols() should be called on any nested expressions. F() objects, in particular, hold a reference to a column. The alias parameter will be None unless the expression has been annotated and is used for grouping.
pandas.core.groupby.SeriesGroupBy.nunique   SeriesGroupBy.nunique(dropna=True)[source]
 
Return number of unique elements in the group.  Returns 
 Series

Number of unique values within each group.
pandas.core.resample.Resampler.nunique   Resampler.nunique(_method='nunique')[source]
 
Return number of unique elements in the group.  Returns 
 Series

Number of unique values within each group.
def f_17926273(df):
    """count distinct values in a column 'col3' of a pandas dataframe `df` group by objects in 'col1' and 'col2'
    """
    return  
 --------------------

def f_3735814(dict1):
    """Check if any key in the dictionary `dict1` starts with the string `EMP$$`
    """
    return  
 --------------------

def f_3735814(dict1):
    """create list of values from dictionary `dict1` that have a key that starts with 'EMP$$'
    """
    return  
 --------------------

def f_26097916(sf):
    """convert a pandas series `sf` into a pandas dataframe `df` with columns `email` and `list`
    """
     
 --------------------

def f_4048964(list):
    """concatenate elements of list `list` by tabs `	`
    """
    return  
 --------------------

def f_3182716():
    """print unicode string '\xd0\xbf\xd1\x80\xd0\xb8' with utf-8
    """
    return  
 --------------------

def f_3182716():
    """Encode a latin character in string `Sopet\xc3\xb3n` properly
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
Pattern.subn(repl, string, count=0)  
Identical to the subn() function, using the compiled pattern.
pandas.Series.str.rsplit   Series.str.rsplit(pat=None, n=- 1, expand=False)[source]
 
Split strings around given separator/delimiter. Splits the string in the Series/Index from the end, at the specified delimiter string.  Parameters 
 
pat:str or compiled regex, optional


String or regular expression to split on. If not specified, split on whitespace.  
n:int, default -1 (all)


Limit number of splits in output. None, 0 and -1 will be interpreted as return all splits.  
expand:bool, default False


Expand the split strings into separate columns.  If True, return DataFrame/MultiIndex expanding dimensionality. If False, return Series/Index, containing lists of strings.   
regex:bool, default None


Determines if the passed-in pattern is a regular expression:  If True, assumes the passed-in pattern is a regular expression If False, treats the pattern as a literal string. If None and pat length is 1, treats pat as a literal string. If None and pat length is not 1, treats pat as a regular expression. Cannot be set to False if pat is a compiled regex   New in version 1.4.0.     Returns 
 Series, Index, DataFrame or MultiIndex

Type matches caller unless expand=True (see Notes).    Raises 
 ValueError

 if regex is False and pat is a compiled regex       See also  Series.str.split

Split strings around given separator/delimiter.  Series.str.rsplit

Splits string around given separator/delimiter, starting from the right.  Series.str.join

Join lists contained as elements in the Series/Index with passed delimiter.  str.split

Standard library version for split.  str.rsplit

Standard library version for rsplit.    Notes The handling of the n keyword depends on the number of found splits:  If found splits > n, make first n splits only If found splits <= n, make all splits If for a certain row the number of found splits < n, append None for padding up to n if expand=True  If using expand=True, Series and Index callers return DataFrame and MultiIndex objects, respectively. Use of regex=False with a pat as a compiled regex will raise an error. Examples 
>>> s = pd.Series(
...     [
...         "this is a regular sentence",
...         "https://docs.python.org/3/tutorial/index.html",
...         np.nan
...     ]
... )
>>> s
0                       this is a regular sentence
1    https://docs.python.org/3/tutorial/index.html
2                                              NaN
dtype: object
  In the default setting, the string is split by whitespace. 
>>> s.str.split()
0                   [this, is, a, regular, sentence]
1    [https://docs.python.org/3/tutorial/index.html]
2                                                NaN
dtype: object
  Without the n parameter, the outputs of rsplit and split are identical. 
>>> s.str.rsplit()
0                   [this, is, a, regular, sentence]
1    [https://docs.python.org/3/tutorial/index.html]
2                                                NaN
dtype: object
  The n parameter can be used to limit the number of splits on the delimiter. The outputs of split and rsplit are different. 
>>> s.str.split(n=2)
0                     [this, is, a regular sentence]
1    [https://docs.python.org/3/tutorial/index.html]
2                                                NaN
dtype: object
  
>>> s.str.rsplit(n=2)
0                     [this is a, regular, sentence]
1    [https://docs.python.org/3/tutorial/index.html]
2                                                NaN
dtype: object
  The pat parameter can be used to split by other characters. 
>>> s.str.split(pat="/")
0                         [this is a regular sentence]
1    [https:, , docs.python.org, 3, tutorial, index...
2                                                  NaN
dtype: object
  When using expand=True, the split elements will expand out into separate columns. If NaN is present, it is propagated throughout the columns during the split. 
>>> s.str.split(expand=True)
                                               0     1     2        3         4
0                                           this    is     a  regular  sentence
1  https://docs.python.org/3/tutorial/index.html  None  None     None      None
2                                            NaN   NaN   NaN      NaN       NaN
  For slightly more complex use cases like splitting the html document name from a url, a combination of parameter settings can be used. 
>>> s.str.rsplit("/", n=1, expand=True)
                                    0           1
0          this is a regular sentence        None
1  https://docs.python.org/3/tutorial  index.html
2                                 NaN         NaN
  Remember to escape special characters when explicitly using regular expressions. 
>>> s = pd.Series(["foo and bar plus baz"])
>>> s.str.split(r"and|plus", expand=True)
    0   1   2
0 foo bar baz
  Regular expressions can be used to handle urls or file names. When pat is a string and regex=None (the default), the given pat is compiled as a regex only if len(pat) != 1. 
>>> s = pd.Series(['foojpgbar.jpg'])
>>> s.str.split(r".", expand=True)
           0    1
0  foojpgbar  jpg
  
>>> s.str.split(r"\.jpg", expand=True)
           0 1
0  foojpgbar
  When regex=True, pat is interpreted as a regex 
>>> s.str.split(r"\.jpg", regex=True, expand=True)
           0 1
0  foojpgbar
  A compiled regex can be passed as pat 
>>> import re
>>> s.str.split(re.compile(r"\.jpg"), expand=True)
           0 1
0  foojpgbar
  When regex=False, pat is interpreted as the string itself 
>>> s.str.split(r"\.jpg", regex=False, expand=True)
               0
0  foojpgbar.jpg
pandas.Series.str.split   Series.str.split(pat=None, n=- 1, expand=False, *, regex=None)[source]
 
Split strings around given separator/delimiter. Splits the string in the Series/Index from the beginning, at the specified delimiter string.  Parameters 
 
pat:str or compiled regex, optional


String or regular expression to split on. If not specified, split on whitespace.  
n:int, default -1 (all)


Limit number of splits in output. None, 0 and -1 will be interpreted as return all splits.  
expand:bool, default False


Expand the split strings into separate columns.  If True, return DataFrame/MultiIndex expanding dimensionality. If False, return Series/Index, containing lists of strings.   
regex:bool, default None


Determines if the passed-in pattern is a regular expression:  If True, assumes the passed-in pattern is a regular expression If False, treats the pattern as a literal string. If None and pat length is 1, treats pat as a literal string. If None and pat length is not 1, treats pat as a regular expression. Cannot be set to False if pat is a compiled regex   New in version 1.4.0.     Returns 
 Series, Index, DataFrame or MultiIndex

Type matches caller unless expand=True (see Notes).    Raises 
 ValueError

 if regex is False and pat is a compiled regex       See also  Series.str.split

Split strings around given separator/delimiter.  Series.str.rsplit

Splits string around given separator/delimiter, starting from the right.  Series.str.join

Join lists contained as elements in the Series/Index with passed delimiter.  str.split

Standard library version for split.  str.rsplit

Standard library version for rsplit.    Notes The handling of the n keyword depends on the number of found splits:  If found splits > n, make first n splits only If found splits <= n, make all splits If for a certain row the number of found splits < n, append None for padding up to n if expand=True  If using expand=True, Series and Index callers return DataFrame and MultiIndex objects, respectively. Use of regex=False with a pat as a compiled regex will raise an error. Examples 
>>> s = pd.Series(
...     [
...         "this is a regular sentence",
...         "https://docs.python.org/3/tutorial/index.html",
...         np.nan
...     ]
... )
>>> s
0                       this is a regular sentence
1    https://docs.python.org/3/tutorial/index.html
2                                              NaN
dtype: object
  In the default setting, the string is split by whitespace. 
>>> s.str.split()
0                   [this, is, a, regular, sentence]
1    [https://docs.python.org/3/tutorial/index.html]
2                                                NaN
dtype: object
  Without the n parameter, the outputs of rsplit and split are identical. 
>>> s.str.rsplit()
0                   [this, is, a, regular, sentence]
1    [https://docs.python.org/3/tutorial/index.html]
2                                                NaN
dtype: object
  The n parameter can be used to limit the number of splits on the delimiter. The outputs of split and rsplit are different. 
>>> s.str.split(n=2)
0                     [this, is, a regular sentence]
1    [https://docs.python.org/3/tutorial/index.html]
2                                                NaN
dtype: object
  
>>> s.str.rsplit(n=2)
0                     [this is a, regular, sentence]
1    [https://docs.python.org/3/tutorial/index.html]
2                                                NaN
dtype: object
  The pat parameter can be used to split by other characters. 
>>> s.str.split(pat="/")
0                         [this is a regular sentence]
1    [https:, , docs.python.org, 3, tutorial, index...
2                                                  NaN
dtype: object
  When using expand=True, the split elements will expand out into separate columns. If NaN is present, it is propagated throughout the columns during the split. 
>>> s.str.split(expand=True)
                                               0     1     2        3         4
0                                           this    is     a  regular  sentence
1  https://docs.python.org/3/tutorial/index.html  None  None     None      None
2                                            NaN   NaN   NaN      NaN       NaN
  For slightly more complex use cases like splitting the html document name from a url, a combination of parameter settings can be used. 
>>> s.str.rsplit("/", n=1, expand=True)
                                    0           1
0          this is a regular sentence        None
1  https://docs.python.org/3/tutorial  index.html
2                                 NaN         NaN
  Remember to escape special characters when explicitly using regular expressions. 
>>> s = pd.Series(["foo and bar plus baz"])
>>> s.str.split(r"and|plus", expand=True)
    0   1   2
0 foo bar baz
  Regular expressions can be used to handle urls or file names. When pat is a string and regex=None (the default), the given pat is compiled as a regex only if len(pat) != 1. 
>>> s = pd.Series(['foojpgbar.jpg'])
>>> s.str.split(r".", expand=True)
           0    1
0  foojpgbar  jpg
  
>>> s.str.split(r"\.jpg", expand=True)
           0 1
0  foojpgbar
  When regex=True, pat is interpreted as a regex 
>>> s.str.split(r"\.jpg", regex=True, expand=True)
           0 1
0  foojpgbar
  A compiled regex can be passed as pat 
>>> import re
>>> s.str.split(re.compile(r"\.jpg"), expand=True)
           0 1
0  foojpgbar
  When regex=False, pat is interpreted as the string itself 
>>> s.str.split(r"\.jpg", regex=False, expand=True)
               0
0  foojpgbar.jpg
Match.group([group1, ...])  
Returns one or more subgroups of the match. If there is a single argument, the result is a single string; if there are multiple arguments, the result is a tuple with one item per argument. Without arguments, group1 defaults to zero (the whole match is returned). If a groupN argument is zero, the corresponding return value is the entire matching string; if it is in the inclusive range [1..99], it is the string matching the corresponding parenthesized group. If a group number is negative or larger than the number of groups defined in the pattern, an IndexError exception is raised. If a group is contained in a part of the pattern that did not match, the corresponding result is None. If a group is contained in a part of the pattern that matched multiple times, the last match is returned. >>> m = re.match(r"(\w+) (\w+)", "Isaac Newton, physicist")
>>> m.group(0)       # The entire match
'Isaac Newton'
>>> m.group(1)       # The first parenthesized subgroup.
'Isaac'
>>> m.group(2)       # The second parenthesized subgroup.
'Newton'
>>> m.group(1, 2)    # Multiple arguments give us a tuple.
('Isaac', 'Newton')
 If the regular expression uses the (?P<name>...) syntax, the groupN arguments may also be strings identifying groups by their group name. If a string argument is not used as a group name in the pattern, an IndexError exception is raised. A moderately complicated example: >>> m = re.match(r"(?P<first_name>\w+) (?P<last_name>\w+)", "Malcolm Reynolds")
>>> m.group('first_name')
'Malcolm'
>>> m.group('last_name')
'Reynolds'
 Named groups can also be referred to by their index: >>> m.group(1)
'Malcolm'
>>> m.group(2)
'Reynolds'
 If a group matches multiple times, only the last match is accessible: >>> m = re.match(r"(..)+", "a1b2c3")  # Matches 3 times.
>>> m.group(1)                        # Returns only the last match.
'c3'
Pattern.groups  
The number of capturing groups in the pattern.
def f_35622945(s):
    """regex, find "n"s only in the middle of string `s`
    """
    return  
 --------------------

def f_5306756():
    """display the float `1/3*100` as a percentage
    """
    return  
 --------------------

def f_2878084(mylist):
    """sort a list of dictionary `mylist` by the key `title`
    """
     
 --------------------

def f_2878084(l):
    """sort a list `l` of dicts by dict value 'title'
    """
     
 --------------------

def f_2878084(l):
    """sort a list of dictionaries by the value of keys 'title', 'title_url', 'id' in ascending order.
    """
     
 --------------------
Please refer to the following documentation to generate the code:
tf.raw_ops.ListDiff Computes the difference between two lists of numbers or strings.  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.raw_ops.ListDiff  
tf.raw_ops.ListDiff(
    x, y, out_idx=tf.dtypes.int32, name=None
)
 Given a list x and a list y, this operation returns a list out that represents all values that are in x but not in y. The returned list out is sorted in the same order that the numbers appear in x (duplicates are preserved). This operation also returns a list idx that represents the position of each out element in x. In other words: out[i] = x[idx[i]] for i in [0, 1, ..., len(out) - 1] For example, given this input: x = [1, 2, 3, 4, 5, 6]
y = [1, 3, 5]
 This operation would return: out ==> [2, 4, 6]
idx ==> [1, 3, 5]

 


 Args
  x   A Tensor. 1-D. Values to keep.  
  y   A Tensor. Must have the same type as x. 1-D. Values to remove.  
  out_idx   An optional tf.DType from: tf.int32, tf.int64. Defaults to tf.int32.  
  name   A name for the operation (optional).   
 


 Returns   A tuple of Tensor objects (out, idx).     out   A Tensor. Has the same type as x.  
  idx   A Tensor of type out_idx.
lstsq(A) -> (Tensor, Tensor)  
See torch.lstsq()
skimage.color.deltaE_cmc(lab1, lab2, kL=1, kC=1) [source]
 
Color difference from the CMC l:c standard. This color difference was developed by the Colour Measurement Committee (CMC) of the Society of Dyers and Colourists (United Kingdom). It is intended for use in the textile industry. The scale factors kL, kC set the weight given to differences in lightness and chroma relative to differences in hue. The usual values are kL=2, kC=1 for “acceptability” and kL=1, kC=1 for “imperceptibility”. Colors with dE > 1 are “different” for the given scale factors.  Parameters 
 
lab1array_like 

reference color (Lab colorspace)  
lab2array_like 

comparison color (Lab colorspace)    Returns 
 
dEarray_like 

distance between colors lab1 and lab2     Notes deltaE_cmc the defines the scales for the lightness, hue, and chroma in terms of the first color. Consequently deltaE_cmc(lab1, lab2) != deltaE_cmc(lab2, lab1) References  
1  
https://en.wikipedia.org/wiki/Color_difference  
2  
http://www.brucelindbloom.com/index.html?Eqn_DeltaE_CIE94.html  
3  
F. J. J. Clarke, R. McDonald, and B. Rigg, “Modification to the JPC79 colour-difference formula,” J. Soc. Dyers Colour. 100, 128-132 (1984).
plistlib.FMT_BINARY  
The binary format for plist files  New in version 3.4.
matplotlib.axis.Axis.get_tightbbox   Axis.get_tightbbox(renderer, *, for_layout_only=False)[source]
 
Return a bounding box that encloses the axis. It only accounts tick labels, axis label, and offsetText. If for_layout_only is True, then the width of the label (if this is an x-axis) or the height of the label (if this is a y-axis) is collapsed to near zero. This allows tight/constrained_layout to ignore too-long labels when doing their layout.
def f_9323159(l1, l2):
    """find 10 largest differences between each respective elements of list `l1` and list `l2`
    """
    return  
 --------------------

def f_29877663(soup):
    """BeautifulSoup find all 'span' elements in HTML string `soup` with class of 'starGryB sp'
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
pandas.DataFrame.to_sql   DataFrame.to_sql(name, con, schema=None, if_exists='fail', index=True, index_label=None, chunksize=None, dtype=None, method=None)[source]
 
Write records stored in a DataFrame to a SQL database. Databases supported by SQLAlchemy [1] are supported. Tables can be newly created, appended to, or overwritten.  Parameters 
 
name:str


Name of SQL table.  
con:sqlalchemy.engine.(Engine or Connection) or sqlite3.Connection


Using SQLAlchemy makes it possible to use any DB supported by that library. Legacy support is provided for sqlite3.Connection objects. The user is responsible for engine disposal and connection closure for the SQLAlchemy connectable See here.  
schema:str, optional


Specify the schema (if database flavor supports this). If None, use default schema.  
if_exists:{‘fail’, ‘replace’, ‘append’}, default ‘fail’


How to behave if the table already exists.  fail: Raise a ValueError. replace: Drop the table before inserting new values. append: Insert new values to the existing table.   
index:bool, default True


Write DataFrame index as a column. Uses index_label as the column name in the table.  
index_label:str or sequence, default None


Column label for index column(s). If None is given (default) and index is True, then the index names are used. A sequence should be given if the DataFrame uses MultiIndex.  
chunksize:int, optional


Specify the number of rows in each batch to be written at a time. By default, all rows will be written at once.  
dtype:dict or scalar, optional


Specifying the datatype for columns. If a dictionary is used, the keys should be the column names and the values should be the SQLAlchemy types or strings for the sqlite3 legacy mode. If a scalar is provided, it will be applied to all columns.  
method:{None, ‘multi’, callable}, optional


Controls the SQL insertion clause used:  None : Uses standard SQL INSERT clause (one per row). ‘multi’: Pass multiple values in a single INSERT clause. callable with signature (pd_table, conn, keys, data_iter).  Details and a sample callable implementation can be found in the section insert method.    Returns 
 None or int

Number of rows affected by to_sql. None is returned if the callable passed into method does not return the number of rows. The number of returned rows affected is the sum of the rowcount attribute of sqlite3.Cursor or SQLAlchemy connectable which may not reflect the exact number of written rows as stipulated in the sqlite3 or SQLAlchemy.  New in version 1.4.0.     Raises 
 ValueError

When the table already exists and if_exists is ‘fail’ (the default).      See also  read_sql

Read a DataFrame from a table.    Notes Timezone aware datetime columns will be written as Timestamp with timezone type with SQLAlchemy if supported by the database. Otherwise, the datetimes will be stored as timezone unaware timestamps local to the original timezone. References  1 
https://docs.sqlalchemy.org  2 
https://www.python.org/dev/peps/pep-0249/   Examples Create an in-memory SQLite database. 
>>> from sqlalchemy import create_engine
>>> engine = create_engine('sqlite://', echo=False)
  Create a table from scratch with 3 rows. 
>>> df = pd.DataFrame({'name' : ['User 1', 'User 2', 'User 3']})
>>> df
     name
0  User 1
1  User 2
2  User 3
  
>>> df.to_sql('users', con=engine)
3
>>> engine.execute("SELECT * FROM users").fetchall()
[(0, 'User 1'), (1, 'User 2'), (2, 'User 3')]
  An sqlalchemy.engine.Connection can also be passed to con: 
>>> with engine.begin() as connection:
...     df1 = pd.DataFrame({'name' : ['User 4', 'User 5']})
...     df1.to_sql('users', con=connection, if_exists='append')
2
  This is allowed to support operations that require that the same DBAPI connection is used for the entire operation. 
>>> df2 = pd.DataFrame({'name' : ['User 6', 'User 7']})
>>> df2.to_sql('users', con=engine, if_exists='append')
2
>>> engine.execute("SELECT * FROM users").fetchall()
[(0, 'User 1'), (1, 'User 2'), (2, 'User 3'),
 (0, 'User 4'), (1, 'User 5'), (0, 'User 6'),
 (1, 'User 7')]
  Overwrite the table with just df2. 
>>> df2.to_sql('users', con=engine, if_exists='replace',
...            index_label='id')
2
>>> engine.execute("SELECT * FROM users").fetchall()
[(0, 'User 6'), (1, 'User 7')]
  Specify the dtype (especially useful for integers with missing values). Notice that while pandas is forced to store the data as floating point, the database supports nullable integers. When fetching the data with Python, we get back integer scalars. 
>>> df = pd.DataFrame({"A": [1, None, 2]})
>>> df
     A
0  1.0
1  NaN
2  2.0
  
>>> from sqlalchemy.types import Integer
>>> df.to_sql('integers', con=engine, index=False,
...           dtype={"A": Integer()})
3
  
>>> engine.execute("SELECT * FROM integers").fetchall()
[(1,), (None,), (2,)]
pandas.Series.to_sql   Series.to_sql(name, con, schema=None, if_exists='fail', index=True, index_label=None, chunksize=None, dtype=None, method=None)[source]
 
Write records stored in a DataFrame to a SQL database. Databases supported by SQLAlchemy [1] are supported. Tables can be newly created, appended to, or overwritten.  Parameters 
 
name:str


Name of SQL table.  
con:sqlalchemy.engine.(Engine or Connection) or sqlite3.Connection


Using SQLAlchemy makes it possible to use any DB supported by that library. Legacy support is provided for sqlite3.Connection objects. The user is responsible for engine disposal and connection closure for the SQLAlchemy connectable See here.  
schema:str, optional


Specify the schema (if database flavor supports this). If None, use default schema.  
if_exists:{‘fail’, ‘replace’, ‘append’}, default ‘fail’


How to behave if the table already exists.  fail: Raise a ValueError. replace: Drop the table before inserting new values. append: Insert new values to the existing table.   
index:bool, default True


Write DataFrame index as a column. Uses index_label as the column name in the table.  
index_label:str or sequence, default None


Column label for index column(s). If None is given (default) and index is True, then the index names are used. A sequence should be given if the DataFrame uses MultiIndex.  
chunksize:int, optional


Specify the number of rows in each batch to be written at a time. By default, all rows will be written at once.  
dtype:dict or scalar, optional


Specifying the datatype for columns. If a dictionary is used, the keys should be the column names and the values should be the SQLAlchemy types or strings for the sqlite3 legacy mode. If a scalar is provided, it will be applied to all columns.  
method:{None, ‘multi’, callable}, optional


Controls the SQL insertion clause used:  None : Uses standard SQL INSERT clause (one per row). ‘multi’: Pass multiple values in a single INSERT clause. callable with signature (pd_table, conn, keys, data_iter).  Details and a sample callable implementation can be found in the section insert method.    Returns 
 None or int

Number of rows affected by to_sql. None is returned if the callable passed into method does not return the number of rows. The number of returned rows affected is the sum of the rowcount attribute of sqlite3.Cursor or SQLAlchemy connectable which may not reflect the exact number of written rows as stipulated in the sqlite3 or SQLAlchemy.  New in version 1.4.0.     Raises 
 ValueError

When the table already exists and if_exists is ‘fail’ (the default).      See also  read_sql

Read a DataFrame from a table.    Notes Timezone aware datetime columns will be written as Timestamp with timezone type with SQLAlchemy if supported by the database. Otherwise, the datetimes will be stored as timezone unaware timestamps local to the original timezone. References  1 
https://docs.sqlalchemy.org  2 
https://www.python.org/dev/peps/pep-0249/   Examples Create an in-memory SQLite database. 
>>> from sqlalchemy import create_engine
>>> engine = create_engine('sqlite://', echo=False)
  Create a table from scratch with 3 rows. 
>>> df = pd.DataFrame({'name' : ['User 1', 'User 2', 'User 3']})
>>> df
     name
0  User 1
1  User 2
2  User 3
  
>>> df.to_sql('users', con=engine)
3
>>> engine.execute("SELECT * FROM users").fetchall()
[(0, 'User 1'), (1, 'User 2'), (2, 'User 3')]
  An sqlalchemy.engine.Connection can also be passed to con: 
>>> with engine.begin() as connection:
...     df1 = pd.DataFrame({'name' : ['User 4', 'User 5']})
...     df1.to_sql('users', con=connection, if_exists='append')
2
  This is allowed to support operations that require that the same DBAPI connection is used for the entire operation. 
>>> df2 = pd.DataFrame({'name' : ['User 6', 'User 7']})
>>> df2.to_sql('users', con=engine, if_exists='append')
2
>>> engine.execute("SELECT * FROM users").fetchall()
[(0, 'User 1'), (1, 'User 2'), (2, 'User 3'),
 (0, 'User 4'), (1, 'User 5'), (0, 'User 6'),
 (1, 'User 7')]
  Overwrite the table with just df2. 
>>> df2.to_sql('users', con=engine, if_exists='replace',
...            index_label='id')
2
>>> engine.execute("SELECT * FROM users").fetchall()
[(0, 'User 6'), (1, 'User 7')]
  Specify the dtype (especially useful for integers with missing values). Notice that while pandas is forced to store the data as floating point, the database supports nullable integers. When fetching the data with Python, we get back integer scalars. 
>>> df = pd.DataFrame({"A": [1, None, 2]})
>>> df
     A
0  1.0
1  NaN
2  2.0
  
>>> from sqlalchemy.types import Integer
>>> df.to_sql('integers', con=engine, index=False,
...           dtype={"A": Integer()})
3
  
>>> engine.execute("SELECT * FROM integers").fetchall()
[(1,), (None,), (2,)]
pandas.ExcelWriter   classpandas.ExcelWriter(path, engine=None, date_format=None, datetime_format=None, mode='w', storage_options=None, if_sheet_exists=None, engine_kwargs=None, **kwargs)[source]
 
Class for writing DataFrame objects into excel sheets. Default is to use : * xlwt for xls * xlsxwriter for xlsx if xlsxwriter is installed otherwise openpyxl * odf for ods. See DataFrame.to_excel for typical usage. The writer should be used as a context manager. Otherwise, call close() to save and close any opened file handles.  Parameters 
 
path:str or typing.BinaryIO


Path to xls or xlsx or ods file.  
engine:str (optional)


Engine to use for writing. If None, defaults to io.excel.<extension>.writer. NOTE: can only be passed as a keyword argument.  Deprecated since version 1.2.0: As the xlwt package is no longer maintained, the xlwt engine will be removed in a future version of pandas.   
date_format:str, default None


Format string for dates written into Excel files (e.g. ‘YYYY-MM-DD’).  
datetime_format:str, default None


Format string for datetime objects written into Excel files. (e.g. ‘YYYY-MM-DD HH:MM:SS’).  
mode:{‘w’, ‘a’}, default ‘w’


File mode to use (write or append). Append does not work with fsspec URLs.  
storage_options:dict, optional


Extra options that make sense for a particular storage connection, e.g. host, port, username, password, etc., if using a URL that will be parsed by fsspec, e.g., starting “s3://”, “gcs://”.  New in version 1.2.0.   
if_sheet_exists:{‘error’, ‘new’, ‘replace’, ‘overlay’}, default ‘error’


How to behave when trying to write to a sheet that already exists (append mode only).  error: raise a ValueError. new: Create a new sheet, with a name determined by the engine. replace: Delete the contents of the sheet before writing to it. overlay: Write contents to the existing sheet without removing the old contents.   New in version 1.3.0.   Changed in version 1.4.0: Added overlay option   
engine_kwargs:dict, optional


Keyword arguments to be passed into the engine. These will be passed to the following functions of the respective engines:  xlsxwriter: xlsxwriter.Workbook(file, **engine_kwargs) openpyxl (write mode): openpyxl.Workbook(**engine_kwargs) openpyxl (append mode): openpyxl.load_workbook(file, **engine_kwargs) odswriter: odf.opendocument.OpenDocumentSpreadsheet(**engine_kwargs)   New in version 1.3.0.   
**kwargs:dict, optional


Keyword arguments to be passed into the engine.  Deprecated since version 1.3.0: Use engine_kwargs instead.      Notes None of the methods and properties are considered public. For compatibility with CSV writers, ExcelWriter serializes lists and dicts to strings before writing. Examples Default usage: 
>>> df = pd.DataFrame([["ABC", "XYZ"]], columns=["Foo", "Bar"])  
>>> with pd.ExcelWriter("path_to_file.xlsx") as writer:
...     df.to_excel(writer)  
  To write to separate sheets in a single file: 
>>> df1 = pd.DataFrame([["AAA", "BBB"]], columns=["Spam", "Egg"])  
>>> df2 = pd.DataFrame([["ABC", "XYZ"]], columns=["Foo", "Bar"])  
>>> with pd.ExcelWriter("path_to_file.xlsx") as writer:
...     df1.to_excel(writer, sheet_name="Sheet1")  
...     df2.to_excel(writer, sheet_name="Sheet2")  
  You can set the date format or datetime format: 
>>> from datetime import date, datetime  
>>> df = pd.DataFrame(
...     [
...         [date(2014, 1, 31), date(1999, 9, 24)],
...         [datetime(1998, 5, 26, 23, 33, 4), datetime(2014, 2, 28, 13, 5, 13)],
...     ],
...     index=["Date", "Datetime"],
...     columns=["X", "Y"],
... )  
>>> with pd.ExcelWriter(
...     "path_to_file.xlsx",
...     date_format="YYYY-MM-DD",
...     datetime_format="YYYY-MM-DD HH:MM:SS"
... ) as writer:
...     df.to_excel(writer)  
  You can also append to an existing Excel file: 
>>> with pd.ExcelWriter("path_to_file.xlsx", mode="a", engine="openpyxl") as writer:
...     df.to_excel(writer, sheet_name="Sheet3")  
  Here, the if_sheet_exists parameter can be set to replace a sheet if it already exists: 
>>> with ExcelWriter(
...     "path_to_file.xlsx",
...     mode="a",
...     engine="openpyxl",
...     if_sheet_exists="replace",
... ) as writer:
...     df.to_excel(writer, sheet_name="Sheet1")  
  You can also write multiple DataFrames to a single sheet. Note that the if_sheet_exists parameter needs to be set to overlay: 
>>> with ExcelWriter("path_to_file.xlsx",
...     mode="a",
...     engine="openpyxl",
...     if_sheet_exists="overlay",
... ) as writer:
...     df1.to_excel(writer, sheet_name="Sheet1")
...     df2.to_excel(writer, sheet_name="Sheet1", startcol=3)  
  You can store Excel file in RAM: 
>>> import io
>>> df = pd.DataFrame([["ABC", "XYZ"]], columns=["Foo", "Bar"])
>>> buffer = io.BytesIO()
>>> with pd.ExcelWriter(buffer) as writer:
...     df.to_excel(writer)
  You can pack Excel file into zip archive: 
>>> import zipfile  
>>> df = pd.DataFrame([["ABC", "XYZ"]], columns=["Foo", "Bar"])  
>>> with zipfile.ZipFile("path_to_file.zip", "w") as zf:
...     with zf.open("filename.xlsx", "w") as buffer:
...         with pd.ExcelWriter(buffer) as writer:
...             df.to_excel(writer)  
  You can specify additional arguments to the underlying engine: 
>>> with pd.ExcelWriter(
...     "path_to_file.xlsx",
...     engine="xlsxwriter",
...     engine_kwargs={"options": {"nan_inf_to_errors": True}}
... ) as writer:
...     df.to_excel(writer)  
  In append mode, engine_kwargs are passed through to openpyxl’s load_workbook: 
>>> with pd.ExcelWriter(
...     "path_to_file.xlsx",
...     engine="openpyxl",
...     mode="a",
...     engine_kwargs={"keep_vba": True}
... ) as writer:
...     df.to_excel(writer, sheet_name="Sheet2")  
  Attributes       
None     Methods       
None
renorm_(p, dim, maxnorm) → Tensor  
In-place version of renorm()
pandas.read_sql_query   pandas.read_sql_query(sql, con, index_col=None, coerce_float=True, params=None, parse_dates=None, chunksize=None, dtype=None)[source]
 
Read SQL query into a DataFrame. Returns a DataFrame corresponding to the result set of the query string. Optionally provide an index_col parameter to use one of the columns as the index, otherwise default integer index will be used.  Parameters 
 
sql:str SQL query or SQLAlchemy Selectable (select or text object)


SQL query to be executed.  
con:SQLAlchemy connectable, str, or sqlite3 connection


Using SQLAlchemy makes it possible to use any DB supported by that library. If a DBAPI2 object, only sqlite3 is supported.  
index_col:str or list of str, optional, default: None


Column(s) to set as index(MultiIndex).  
coerce_float:bool, default True


Attempts to convert values of non-string, non-numeric objects (like decimal.Decimal) to floating point. Useful for SQL result sets.  
params:list, tuple or dict, optional, default: None


List of parameters to pass to execute method. The syntax used to pass parameters is database driver dependent. Check your database driver documentation for which of the five syntax styles, described in PEP 249’s paramstyle, is supported. Eg. for psycopg2, uses %(name)s so use params={‘name’ : ‘value’}.  
parse_dates:list or dict, default: None


 List of column names to parse as dates. Dict of {column_name: format string} where format string is strftime compatible in case of parsing string times, or is one of (D, s, ns, ms, us) in case of parsing integer timestamps. Dict of {column_name: arg dict}, where the arg dict corresponds to the keyword arguments of pandas.to_datetime() Especially useful with databases without native Datetime support, such as SQLite.   
chunksize:int, default None


If specified, return an iterator where chunksize is the number of rows to include in each chunk.  
dtype:Type name or dict of columns


Data type for data or columns. E.g. np.float64 or {‘a’: np.float64, ‘b’: np.int32, ‘c’: ‘Int64’}.  New in version 1.3.0.     Returns 
 DataFrame or Iterator[DataFrame]
    See also  read_sql_table

Read SQL database table into a DataFrame.  read_sql

Read SQL query or database table into a DataFrame.    Notes Any datetime values with time zone information parsed via the parse_dates parameter will be converted to UTC.
def f_24189150(df, engine):
    """write records in dataframe `df` to table 'test' in schema 'a_schema' with `engine`
    """
     
 --------------------
Please refer to the following documentation to generate the code:
get_breaks(filename, lineno)  
Return all breakpoints for lineno in filename, or an empty list if none are set.
get_file_breaks(filename)  
Return all breakpoints in filename, or an empty list if none are set.
doctest.script_from_examples(s)  
Convert text with examples to a script. Argument s is a string containing doctest examples. The string is converted to a Python script, where doctest examples in s are converted to regular code, and everything else is converted to Python comments. The generated script is returned as a string. For example, import doctest
print(doctest.script_from_examples(r"""
    Set x and y to 1 and 2.
    >>> x, y = 1, 2

    Print their sum:
    >>> print(x+y)
    3
"""))
 displays: # Set x and y to 1 and 2.
x, y = 1, 2
#
# Print their sum:
print(x+y)
# Expected:
## 3
 This function is used internally by other functions (see below), but can also be useful when you want to transform an interactive Python session into a Python script.
token.RSQB  
Token value for "]".
get_examples(string, name='<string>')  
Extract all doctest examples from the given string, and return them as a list of Example objects. Line numbers are 0-based. The optional argument name is a name identifying this string, and is only used for error messages.
def f_30766151(s):
    """Extract brackets from string `s`
    """
    return  
 --------------------

def f_1143379(L):
    """remove duplicate elements from list 'L'
    """
    return  
 --------------------

def f_12330522(file):
    """read a file `file` without newlines
    """
    return  
 --------------------

def f_364621(testlist):
    """get the position of item 1 in `testlist`
    """
    return  
 --------------------

def f_364621(testlist):
    """get the position of item 1 in `testlist`
    """
    return  
 --------------------

def f_364621(testlist, element):
    """get the position of item `element` in list `testlist`
    """
    return  
 --------------------

def f_13145368(lis):
    """find the first element of the tuple with the maximum second element in a list of tuples `lis`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
max(iterable, *[, key, default])  
max(arg1, arg2, *args[, key])  
Return the largest item in an iterable or the largest of two or more arguments. If one positional argument is provided, it should be an iterable. The largest item in the iterable is returned. If two or more positional arguments are provided, the largest of the positional arguments is returned. There are two optional keyword-only arguments. The key argument specifies a one-argument ordering function like that used for list.sort(). The default argument specifies an object to return if the provided iterable is empty. If the iterable is empty and default is not provided, a ValueError is raised. If multiple items are maximal, the function returns the first one encountered. This is consistent with other sort-stability preserving tools such as sorted(iterable, key=keyfunc, reverse=True)[0] and heapq.nlargest(1, iterable, key=keyfunc).  New in version 3.4: The default keyword-only argument.   Changed in version 3.8: The key can be None.
values_list(*fields, flat=False, named=False)
calendar.timegm(tuple)  
An unrelated but handy function that takes a time tuple such as returned by the gmtime() function in the time module, and returns the corresponding Unix timestamp value, assuming an epoch of 1970, and the POSIX encoding. In fact, time.gmtime() and timegm() are each others’ inverse.
update() 
 Sets the elements of the color update(r, g, b) -> None update(r, g, b, a=255) -> None update(color_value) -> None  Sets the elements of the color. See parameters for pygame.Color() for the parameters of this function. If the alpha value was not set it will not change.  New in pygame 2.0.1.
types.prepare_class(name, bases=(), kwds=None)  
Calculates the appropriate metaclass and creates the class namespace. The arguments are the components that make up a class definition header: the class name, the base classes (in order) and the keyword arguments (such as metaclass). The return value is a 3-tuple: metaclass, namespace, kwds metaclass is the appropriate metaclass, namespace is the prepared class namespace and kwds is an updated copy of the passed in kwds argument with any 'metaclass' entry removed. If no kwds argument is passed in, this will be an empty dict.  New in version 3.3.   Changed in version 3.6: The default value for the namespace element of the returned tuple has changed. Now an insertion-order-preserving mapping is used when the metaclass does not have a __prepare__ method.
def f_13145368(lis):
    """get the item at index 0 from the tuple that has maximum value at index 1 in list `lis`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
coroutine asyncio.sleep(delay, result=None, *, loop=None)  
Block for delay seconds. If result is provided, it is returned to the caller when the coroutine completes. sleep() always suspends the current task, allowing other tasks to run.  Deprecated since version 3.8, will be removed in version 3.10: The loop parameter.  Example of coroutine displaying the current date every second for 5 seconds: import asyncio
import datetime

async def display_date():
    loop = asyncio.get_running_loop()
    end_time = loop.time() + 5.0
    while True:
        print(datetime.datetime.now())
        if (loop.time() + 1.0) >= end_time:
            break
        await asyncio.sleep(1)

asyncio.run(display_date())
curses.delay_output(ms)  
Insert an ms millisecond pause in output.
turtle.delay(delay=None)  
 Parameters 
delay – positive integer   Set or return the drawing delay in milliseconds. (This is approximately the time interval between two consecutive canvas updates.) The longer the drawing delay, the slower the animation. Optional argument: >>> screen.delay()
10
>>> screen.delay(5)
>>> screen.delay()
5
pygame.time.delay() 
 pause the program for an amount of time delay(milliseconds) -> time  Will pause for a given number of milliseconds. This function will use the processor (rather than sleeping) in order to make the delay more accurate than pygame.time.wait(). This returns the actual number of milliseconds used.
time.second  
In range(60).
def f_2689189():
    """Make a delay of 1 second
    """
     
 --------------------

def f_12485244(L):
    """convert list of tuples `L` to a string
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
Field.default
class AddField(model_name, name, field, preserve_default=True)
Field.initial
class AlterField(model_name, name, field, preserve_default=True)
Field.blank
def f_755857():
    """Django set default value of field `b` equal to '0000000'
    """
     
 --------------------
Please refer to the following documentation to generate the code:
deg_mark='^{\\circ}'
class Degrees(expression, **extra)
mpl_toolkits.axisartist.angle_helper.select_step_degree   mpl_toolkits.axisartist.angle_helper.select_step_degree(dv)[source]
deg_mark='^\\mathrm{h}'
tf.raw_ops.ListDiff Computes the difference between two lists of numbers or strings.  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.raw_ops.ListDiff  
tf.raw_ops.ListDiff(
    x, y, out_idx=tf.dtypes.int32, name=None
)
 Given a list x and a list y, this operation returns a list out that represents all values that are in x but not in y. The returned list out is sorted in the same order that the numbers appear in x (duplicates are preserved). This operation also returns a list idx that represents the position of each out element in x. In other words: out[i] = x[idx[i]] for i in [0, 1, ..., len(out) - 1] For example, given this input: x = [1, 2, 3, 4, 5, 6]
y = [1, 3, 5]
 This operation would return: out ==> [2, 4, 6]
idx ==> [1, 3, 5]

 


 Args
  x   A Tensor. 1-D. Values to keep.  
  y   A Tensor. Must have the same type as x. 1-D. Values to remove.  
  out_idx   An optional tf.DType from: tf.int32, tf.int64. Defaults to tf.int32.  
  name   A name for the operation (optional).   
 


 Returns   A tuple of Tensor objects (out, idx).     out   A Tensor. Has the same type as x.  
  idx   A Tensor of type out_idx.
def f_16193578(list5):
    """Sort lis `list5` in ascending order based on the degrees value of its elements
    """
    return  
 --------------------

def f_16041405(l):
    """convert a list `l` into a generator object
    """
    return  
 --------------------

def f_18837607(oldlist, removelist):
    """remove elements from list `oldlist` that have an index number mentioned in list `removelist`
    """
    return  
 --------------------

def f_4710067():
    """Open a file `yourfile.txt` in write mode
    """
    return  
 --------------------

def f_7373219(obj, attr):
    """get attribute 'attr' from object `obj`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
parser.tuple2st(sequence)  
This is the same function as sequence2st(). This entry point is maintained for backward compatibility.
ST.totuple(line_info=False, col_info=False)  
Same as st2tuple(st, line_info, col_info).
class tuple([iterable])  
Tuples may be constructed in a number of ways:  Using a pair of parentheses to denote the empty tuple: ()
 Using a trailing comma for a singleton tuple: a, or (a,)
 Separating items with commas: a, b, c or (a, b, c)
 Using the tuple() built-in: tuple() or tuple(iterable)
  The constructor builds a tuple whose items are the same and in the same order as iterable’s items. iterable may be either a sequence, a container that supports iteration, or an iterator object. If iterable is already a tuple, it is returned unchanged. For example, tuple('abc') returns ('a', 'b', 'c') and tuple( [1, 2, 3] ) returns (1, 2, 3). If no argument is given, the constructor creates a new empty tuple, (). Note that it is actually the comma which makes a tuple, not the parentheses. The parentheses are optional, except in the empty tuple case, or when they are needed to avoid syntactic ambiguity. For example, f(a, b, c) is a function call with three arguments, while f((a, b, c)) is a function call with a 3-tuple as the sole argument. Tuples implement all of the common sequence operations.
numpy.column_stack   numpy.column_stack(tup)[source]
 
Stack 1-D arrays as columns into a 2-D array. Take a sequence of 1-D arrays and stack them as columns to make a single 2-D array. 2-D arrays are stacked as-is, just like with hstack. 1-D arrays are turned into 2-D columns first.  Parameters 
 
tupsequence of 1-D or 2-D arrays.


Arrays to stack. All of them must have the same first dimension.    Returns 
 
stacked2-D array


The array formed by stacking the given arrays.      See also  
stack, hstack, vstack, concatenate

  Examples >>> a = np.array((1,2,3))
>>> b = np.array((2,3,4))
>>> np.column_stack((a,b))
array([[1, 2],
       [2, 3],
       [3, 4]])
numpy.row_stack   numpy.row_stack(tup)[source]
 
Stack arrays in sequence vertically (row wise). This is equivalent to concatenation along the first axis after 1-D arrays of shape (N,) have been reshaped to (1,N). Rebuilds arrays divided by vsplit. This function makes most sense for arrays with up to 3 dimensions. For instance, for pixel-data with a height (first axis), width (second axis), and r/g/b channels (third axis). The functions concatenate, stack and block provide more general stacking and concatenation operations.  Parameters 
 
tupsequence of ndarrays


The arrays must have the same shape along all but the first axis. 1-D arrays must have the same length.    Returns 
 
stackedndarray


The array formed by stacking the given arrays, will be at least 2-D.      See also  concatenate

Join a sequence of arrays along an existing axis.  stack

Join a sequence of arrays along a new axis.  block

Assemble an nd-array from nested lists of blocks.  hstack

Stack arrays in sequence horizontally (column wise).  dstack

Stack arrays in sequence depth wise (along third axis).  column_stack

Stack 1-D arrays as columns into a 2-D array.  vsplit

Split an array into multiple sub-arrays vertically (row-wise).    Examples >>> a = np.array([1, 2, 3])
>>> b = np.array([4, 5, 6])
>>> np.vstack((a,b))
array([[1, 2, 3],
       [4, 5, 6]])
 >>> a = np.array([[1], [2], [3]])
>>> b = np.array([[4], [5], [6]])
>>> np.vstack((a,b))
array([[1],
       [2],
       [3],
       [4],
       [5],
       [6]])
def f_8171751():
    """convert tuple of tuples `(('aa',), ('bb',), ('cc',))` to tuple
    """
    return  
 --------------------

def f_8171751():
    """convert tuple of tuples `(('aa',), ('bb',), ('cc',))` to list in one line
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
pandas.DataFrame.replace   DataFrame.replace(to_replace=None, value=NoDefault.no_default, inplace=False, limit=None, regex=False, method=NoDefault.no_default)[source]
 
Replace values given in to_replace with value. Values of the DataFrame are replaced with other values dynamically. This differs from updating with .loc or .iloc, which require you to specify a location to update with some value.  Parameters 
 
to_replace:str, regex, list, dict, Series, int, float, or None


How to find the values that will be replaced.  
numeric, str or regex:  
 numeric: numeric values equal to to_replace will be replaced with value str: string exactly matching to_replace will be replaced with value regex: regexs matching to_replace will be replaced with value  
  
list of str, regex, or numeric:  
 First, if to_replace and value are both lists, they must be the same length. Second, if regex=True then all of the strings in both lists will be interpreted as regexs otherwise they will match directly. This doesn’t matter much for value since there are only a few possible substitution regexes you can use. str, regex and numeric rules apply as above.  
  
dict:  
 Dicts can be used to specify different replacement values for different existing values. For example, {'a': 'b', 'y': 'z'} replaces the value ‘a’ with ‘b’ and ‘y’ with ‘z’. To use a dict in this way the value parameter should be None. For a DataFrame a dict can specify that different values should be replaced in different columns. For example, {'a': 1, 'b': 'z'} looks for the value 1 in column ‘a’ and the value ‘z’ in column ‘b’ and replaces these values with whatever is specified in value. The value parameter should not be None in this case. You can treat this as a special case of passing two lists except that you are specifying the column to search in. For a DataFrame nested dictionaries, e.g., {'a': {'b': np.nan}}, are read as follows: look in column ‘a’ for the value ‘b’ and replace it with NaN. The value parameter should be None to use a nested dict in this way. You can nest regular expressions as well. Note that column names (the top-level dictionary keys in a nested dictionary) cannot be regular expressions.  
  
None:  
 This means that the regex argument must be a string, compiled regular expression, or list, dict, ndarray or Series of such elements. If value is also None then this must be a nested dictionary or Series.  
   See the examples section for examples of each of these.  
value:scalar, dict, list, str, regex, default None


Value to replace any values matching to_replace with. For a DataFrame a dict of values can be used to specify which value to use for each column (columns not in the dict will not be filled). Regular expressions, strings and lists or dicts of such objects are also allowed.  
inplace:bool, default False


If True, performs operation inplace and returns None.  
limit:int, default None


Maximum size gap to forward or backward fill.  
regex:bool or same types as to_replace, default False


Whether to interpret to_replace and/or value as regular expressions. If this is True then to_replace must be a string. Alternatively, this could be a regular expression or a list, dict, or array of regular expressions in which case to_replace must be None.  
method:{‘pad’, ‘ffill’, ‘bfill’, None}


The method to use when for replacement, when to_replace is a scalar, list or tuple and value is None.  Changed in version 0.23.0: Added to DataFrame.     Returns 
 DataFrame

Object after replacement.    Raises 
 AssertionError

 If regex is not a bool and to_replace is not None.   TypeError

 If to_replace is not a scalar, array-like, dict, or None If to_replace is a dict and value is not a list, dict, ndarray, or Series If to_replace is None and regex is not compilable into a regular expression or is a list, dict, ndarray, or Series. When replacing multiple bool or datetime64 objects and the arguments to to_replace does not match the type of the value being replaced   ValueError

 If a list or an ndarray is passed to to_replace and value but they are not the same length.       See also  DataFrame.fillna

Fill NA values.  DataFrame.where

Replace values based on boolean condition.  Series.str.replace

Simple string replacement.    Notes  Regex substitution is performed under the hood with re.sub. The rules for substitution for re.sub are the same. Regular expressions will only substitute on strings, meaning you cannot provide, for example, a regular expression matching floating point numbers and expect the columns in your frame that have a numeric dtype to be matched. However, if those floating point numbers are strings, then you can do this. This method has a lot of options. You are encouraged to experiment and play with this method to gain intuition about how it works. When dict is used as the to_replace value, it is like key(s) in the dict are the to_replace part and value(s) in the dict are the value parameter.  Examples Scalar `to_replace` and `value` 
>>> s = pd.Series([1, 2, 3, 4, 5])
>>> s.replace(1, 5)
0    5
1    2
2    3
3    4
4    5
dtype: int64
  
>>> df = pd.DataFrame({'A': [0, 1, 2, 3, 4],
...                    'B': [5, 6, 7, 8, 9],
...                    'C': ['a', 'b', 'c', 'd', 'e']})
>>> df.replace(0, 5)
    A  B  C
0  5  5  a
1  1  6  b
2  2  7  c
3  3  8  d
4  4  9  e
  List-like `to_replace` 
>>> df.replace([0, 1, 2, 3], 4)
    A  B  C
0  4  5  a
1  4  6  b
2  4  7  c
3  4  8  d
4  4  9  e
  
>>> df.replace([0, 1, 2, 3], [4, 3, 2, 1])
    A  B  C
0  4  5  a
1  3  6  b
2  2  7  c
3  1  8  d
4  4  9  e
  
>>> s.replace([1, 2], method='bfill')
0    3
1    3
2    3
3    4
4    5
dtype: int64
  dict-like `to_replace` 
>>> df.replace({0: 10, 1: 100})
        A  B  C
0   10  5  a
1  100  6  b
2    2  7  c
3    3  8  d
4    4  9  e
  
>>> df.replace({'A': 0, 'B': 5}, 100)
        A    B  C
0  100  100  a
1    1    6  b
2    2    7  c
3    3    8  d
4    4    9  e
  
>>> df.replace({'A': {0: 100, 4: 400}})
        A  B  C
0  100  5  a
1    1  6  b
2    2  7  c
3    3  8  d
4  400  9  e
  Regular expression `to_replace` 
>>> df = pd.DataFrame({'A': ['bat', 'foo', 'bait'],
...                    'B': ['abc', 'bar', 'xyz']})
>>> df.replace(to_replace=r'^ba.$', value='new', regex=True)
        A    B
0   new  abc
1   foo  new
2  bait  xyz
  
>>> df.replace({'A': r'^ba.$'}, {'A': 'new'}, regex=True)
        A    B
0   new  abc
1   foo  bar
2  bait  xyz
  
>>> df.replace(regex=r'^ba.$', value='new')
        A    B
0   new  abc
1   foo  new
2  bait  xyz
  
>>> df.replace(regex={r'^ba.$': 'new', 'foo': 'xyz'})
        A    B
0   new  abc
1   xyz  new
2  bait  xyz
  
>>> df.replace(regex=[r'^ba.$', 'foo'], value='new')
        A    B
0   new  abc
1   new  new
2  bait  xyz
  Compare the behavior of s.replace({'a': None}) and s.replace('a', None) to understand the peculiarities of the to_replace parameter: 
>>> s = pd.Series([10, 'a', 'a', 'b', 'a'])
  When one uses a dict as the to_replace value, it is like the value(s) in the dict are equal to the value parameter. s.replace({'a': None}) is equivalent to s.replace(to_replace={'a': None}, value=None, method=None): 
>>> s.replace({'a': None})
0      10
1    None
2    None
3       b
4    None
dtype: object
  When value is not explicitly passed and to_replace is a scalar, list or tuple, replace uses the method parameter (default ‘pad’) to do the replacement. So this is why the ‘a’ values are being replaced by 10 in rows 1 and 2 and ‘b’ in row 4 in this case. 
>>> s.replace('a')
0    10
1    10
2    10
3     b
4     b
dtype: object
  On the other hand, if None is explicitly passed for value, it will be respected: 
>>> s.replace('a', None)
0      10
1    None
2    None
3       b
4    None
dtype: object
   
 Changed in version 1.4.0: Previously the explicit None was silently ignored.
pandas.Series.replace   Series.replace(to_replace=None, value=NoDefault.no_default, inplace=False, limit=None, regex=False, method=NoDefault.no_default)[source]
 
Replace values given in to_replace with value. Values of the Series are replaced with other values dynamically. This differs from updating with .loc or .iloc, which require you to specify a location to update with some value.  Parameters 
 
to_replace:str, regex, list, dict, Series, int, float, or None


How to find the values that will be replaced.  
numeric, str or regex:  
 numeric: numeric values equal to to_replace will be replaced with value str: string exactly matching to_replace will be replaced with value regex: regexs matching to_replace will be replaced with value  
  
list of str, regex, or numeric:  
 First, if to_replace and value are both lists, they must be the same length. Second, if regex=True then all of the strings in both lists will be interpreted as regexs otherwise they will match directly. This doesn’t matter much for value since there are only a few possible substitution regexes you can use. str, regex and numeric rules apply as above.  
  
dict:  
 Dicts can be used to specify different replacement values for different existing values. For example, {'a': 'b', 'y': 'z'} replaces the value ‘a’ with ‘b’ and ‘y’ with ‘z’. To use a dict in this way the value parameter should be None. For a DataFrame a dict can specify that different values should be replaced in different columns. For example, {'a': 1, 'b': 'z'} looks for the value 1 in column ‘a’ and the value ‘z’ in column ‘b’ and replaces these values with whatever is specified in value. The value parameter should not be None in this case. You can treat this as a special case of passing two lists except that you are specifying the column to search in. For a DataFrame nested dictionaries, e.g., {'a': {'b': np.nan}}, are read as follows: look in column ‘a’ for the value ‘b’ and replace it with NaN. The value parameter should be None to use a nested dict in this way. You can nest regular expressions as well. Note that column names (the top-level dictionary keys in a nested dictionary) cannot be regular expressions.  
  
None:  
 This means that the regex argument must be a string, compiled regular expression, or list, dict, ndarray or Series of such elements. If value is also None then this must be a nested dictionary or Series.  
   See the examples section for examples of each of these.  
value:scalar, dict, list, str, regex, default None


Value to replace any values matching to_replace with. For a DataFrame a dict of values can be used to specify which value to use for each column (columns not in the dict will not be filled). Regular expressions, strings and lists or dicts of such objects are also allowed.  
inplace:bool, default False


If True, performs operation inplace and returns None.  
limit:int, default None


Maximum size gap to forward or backward fill.  
regex:bool or same types as to_replace, default False


Whether to interpret to_replace and/or value as regular expressions. If this is True then to_replace must be a string. Alternatively, this could be a regular expression or a list, dict, or array of regular expressions in which case to_replace must be None.  
method:{‘pad’, ‘ffill’, ‘bfill’, None}


The method to use when for replacement, when to_replace is a scalar, list or tuple and value is None.  Changed in version 0.23.0: Added to DataFrame.     Returns 
 Series

Object after replacement.    Raises 
 AssertionError

 If regex is not a bool and to_replace is not None.   TypeError

 If to_replace is not a scalar, array-like, dict, or None If to_replace is a dict and value is not a list, dict, ndarray, or Series If to_replace is None and regex is not compilable into a regular expression or is a list, dict, ndarray, or Series. When replacing multiple bool or datetime64 objects and the arguments to to_replace does not match the type of the value being replaced   ValueError

 If a list or an ndarray is passed to to_replace and value but they are not the same length.       See also  Series.fillna

Fill NA values.  Series.where

Replace values based on boolean condition.  Series.str.replace

Simple string replacement.    Notes  Regex substitution is performed under the hood with re.sub. The rules for substitution for re.sub are the same. Regular expressions will only substitute on strings, meaning you cannot provide, for example, a regular expression matching floating point numbers and expect the columns in your frame that have a numeric dtype to be matched. However, if those floating point numbers are strings, then you can do this. This method has a lot of options. You are encouraged to experiment and play with this method to gain intuition about how it works. When dict is used as the to_replace value, it is like key(s) in the dict are the to_replace part and value(s) in the dict are the value parameter.  Examples Scalar `to_replace` and `value` 
>>> s = pd.Series([1, 2, 3, 4, 5])
>>> s.replace(1, 5)
0    5
1    2
2    3
3    4
4    5
dtype: int64
  
>>> df = pd.DataFrame({'A': [0, 1, 2, 3, 4],
...                    'B': [5, 6, 7, 8, 9],
...                    'C': ['a', 'b', 'c', 'd', 'e']})
>>> df.replace(0, 5)
    A  B  C
0  5  5  a
1  1  6  b
2  2  7  c
3  3  8  d
4  4  9  e
  List-like `to_replace` 
>>> df.replace([0, 1, 2, 3], 4)
    A  B  C
0  4  5  a
1  4  6  b
2  4  7  c
3  4  8  d
4  4  9  e
  
>>> df.replace([0, 1, 2, 3], [4, 3, 2, 1])
    A  B  C
0  4  5  a
1  3  6  b
2  2  7  c
3  1  8  d
4  4  9  e
  
>>> s.replace([1, 2], method='bfill')
0    3
1    3
2    3
3    4
4    5
dtype: int64
  dict-like `to_replace` 
>>> df.replace({0: 10, 1: 100})
        A  B  C
0   10  5  a
1  100  6  b
2    2  7  c
3    3  8  d
4    4  9  e
  
>>> df.replace({'A': 0, 'B': 5}, 100)
        A    B  C
0  100  100  a
1    1    6  b
2    2    7  c
3    3    8  d
4    4    9  e
  
>>> df.replace({'A': {0: 100, 4: 400}})
        A  B  C
0  100  5  a
1    1  6  b
2    2  7  c
3    3  8  d
4  400  9  e
  Regular expression `to_replace` 
>>> df = pd.DataFrame({'A': ['bat', 'foo', 'bait'],
...                    'B': ['abc', 'bar', 'xyz']})
>>> df.replace(to_replace=r'^ba.$', value='new', regex=True)
        A    B
0   new  abc
1   foo  new
2  bait  xyz
  
>>> df.replace({'A': r'^ba.$'}, {'A': 'new'}, regex=True)
        A    B
0   new  abc
1   foo  bar
2  bait  xyz
  
>>> df.replace(regex=r'^ba.$', value='new')
        A    B
0   new  abc
1   foo  new
2  bait  xyz
  
>>> df.replace(regex={r'^ba.$': 'new', 'foo': 'xyz'})
        A    B
0   new  abc
1   xyz  new
2  bait  xyz
  
>>> df.replace(regex=[r'^ba.$', 'foo'], value='new')
        A    B
0   new  abc
1   new  new
2  bait  xyz
  Compare the behavior of s.replace({'a': None}) and s.replace('a', None) to understand the peculiarities of the to_replace parameter: 
>>> s = pd.Series([10, 'a', 'a', 'b', 'a'])
  When one uses a dict as the to_replace value, it is like the value(s) in the dict are equal to the value parameter. s.replace({'a': None}) is equivalent to s.replace(to_replace={'a': None}, value=None, method=None): 
>>> s.replace({'a': None})
0      10
1    None
2    None
3       b
4    None
dtype: object
  When value is not explicitly passed and to_replace is a scalar, list or tuple, replace uses the method parameter (default ‘pad’) to do the replacement. So this is why the ‘a’ values are being replaced by 10 in rows 1 and 2 and ‘b’ in row 4 in this case. 
>>> s.replace('a')
0    10
1    10
2    10
3     b
4     b
dtype: object
  On the other hand, if None is explicitly passed for value, it will be respected: 
>>> s.replace('a', None)
0      10
1    None
2    None
3       b
4    None
dtype: object
   
 Changed in version 1.4.0: Previously the explicit None was silently ignored.
numpy.chararray.replace method   chararray.replace(old, new, count=None)[source]
 
For each element in self, return a copy of the string with all occurrences of substring old replaced by new.  See also  char.replace
numpy.char.chararray.replace method   char.chararray.replace(old, new, count=None)[source]
 
For each element in self, return a copy of the string with all occurrences of substring old replaced by new.  See also  char.replace
pandas.Series.str.replace   Series.str.replace(pat, repl, n=- 1, case=None, flags=0, regex=None)[source]
 
Replace each occurrence of pattern/regex in the Series/Index. Equivalent to str.replace() or re.sub(), depending on the regex value.  Parameters 
 
pat:str or compiled regex


String can be a character sequence or regular expression.  
repl:str or callable


Replacement string or a callable. The callable is passed the regex match object and must return a replacement string to be used. See re.sub().  
n:int, default -1 (all)


Number of replacements to make from start.  
case:bool, default None


Determines if replace is case sensitive:  If True, case sensitive (the default if pat is a string) Set to False for case insensitive Cannot be set if pat is a compiled regex.   
flags:int, default 0 (no flags)


Regex module flags, e.g. re.IGNORECASE. Cannot be set if pat is a compiled regex.  
regex:bool, default True


Determines if the passed-in pattern is a regular expression:  If True, assumes the passed-in pattern is a regular expression. If False, treats the pattern as a literal string Cannot be set to False if pat is a compiled regex or repl is a callable.   New in version 0.23.0.     Returns 
 Series or Index of object

A copy of the object with all matching occurrences of pat replaced by repl.    Raises 
 ValueError

 if regex is False and repl is a callable or pat is a compiled regex if pat is a compiled regex and case or flags is set      Notes When pat is a compiled regex, all flags should be included in the compiled regex. Use of case, flags, or regex=False with a compiled regex will raise an error. Examples When pat is a string and regex is True (the default), the given pat is compiled as a regex. When repl is a string, it replaces matching regex patterns as with re.sub(). NaN value(s) in the Series are left as is: 
>>> pd.Series(['foo', 'fuz', np.nan]).str.replace('f.', 'ba', regex=True)
0    bao
1    baz
2    NaN
dtype: object
  When pat is a string and regex is False, every pat is replaced with repl as with str.replace(): 
>>> pd.Series(['f.o', 'fuz', np.nan]).str.replace('f.', 'ba', regex=False)
0    bao
1    fuz
2    NaN
dtype: object
  When repl is a callable, it is called on every pat using re.sub(). The callable should expect one positional argument (a regex object) and return a string. To get the idea: 
>>> pd.Series(['foo', 'fuz', np.nan]).str.replace('f', repr, regex=True)
0    <re.Match object; span=(0, 1), match='f'>oo
1    <re.Match object; span=(0, 1), match='f'>uz
2                                            NaN
dtype: object
  Reverse every lowercase alphabetic word: 
>>> repl = lambda m: m.group(0)[::-1]
>>> ser = pd.Series(['foo 123', 'bar baz', np.nan])
>>> ser.str.replace(r'[a-z]+', repl, regex=True)
0    oof 123
1    rab zab
2        NaN
dtype: object
  Using regex groups (extract second group and swap case): 
>>> pat = r"(?P<one>\w+) (?P<two>\w+) (?P<three>\w+)"
>>> repl = lambda m: m.group('two').swapcase()
>>> ser = pd.Series(['One Two Three', 'Foo Bar Baz'])
>>> ser.str.replace(pat, repl, regex=True)
0    tWO
1    bAR
dtype: object
  Using a compiled regex with flags 
>>> import re
>>> regex_pat = re.compile(r'FUZ', flags=re.IGNORECASE)
>>> pd.Series(['foo', 'fuz', np.nan]).str.replace(regex_pat, 'bar', regex=True)
0    foo
1    bar
2    NaN
dtype: object
def f_28986489(df):
    """replace a characters in a column of a dataframe `df`
    """
     
 --------------------

def f_19339():
    """unzip the list `[('a', 1), ('b', 2), ('c', 3), ('d', 4)]`
    """
    return  
 --------------------

def f_19339(original):
    """unzip list `original`
    """
    return  
 --------------------

def f_19339(original):
    """unzip list `original` and return a generator
    """
    return  
 --------------------

def f_19339():
    """unzip list `[('a', 1), ('b', 2), ('c', 3), ('d', 4), ('e', )]`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
itertools.zip_longest(*iterables, fillvalue=None)  
Make an iterator that aggregates elements from each of the iterables. If the iterables are of uneven length, missing values are filled-in with fillvalue. Iteration continues until the longest iterable is exhausted. Roughly equivalent to: def zip_longest(*args, fillvalue=None):
    # zip_longest('ABCD', 'xy', fillvalue='-') --> Ax By C- D-
    iterators = [iter(it) for it in args]
    num_active = len(iterators)
    if not num_active:
        return
    while True:
        values = []
        for i, it in enumerate(iterators):
            try:
                value = next(it)
            except StopIteration:
                num_active -= 1
                if not num_active:
                    return
                iterators[i] = repeat(fillvalue)
                value = fillvalue
            values.append(value)
        yield tuple(values)
 If one of the iterables is potentially infinite, then the zip_longest() function should be wrapped with something that limits the number of calls (for example islice() or takewhile()). If not specified, fillvalue defaults to None.
zip(*iterables)  
Make an iterator that aggregates elements from each of the iterables. Returns an iterator of tuples, where the i-th tuple contains the i-th element from each of the argument sequences or iterables. The iterator stops when the shortest input iterable is exhausted. With a single iterable argument, it returns an iterator of 1-tuples. With no arguments, it returns an empty iterator. Equivalent to: def zip(*iterables):
    # zip('ABCD', 'xy') --> Ax By
    sentinel = object()
    iterators = [iter(it) for it in iterables]
    while iterators:
        result = []
        for it in iterators:
            elem = next(it, sentinel)
            if elem is sentinel:
                return
            result.append(elem)
        yield tuple(result)
 The left-to-right evaluation order of the iterables is guaranteed. This makes possible an idiom for clustering a data series into n-length groups using zip(*[iter(s)]*n). This repeats the same iterator n times so that each output tuple has the result of n calls to the iterator. This has the effect of dividing the input into n-length chunks. zip() should only be used with unequal length inputs when you don’t care about trailing, unmatched values from the longer iterables. If those values are important, use itertools.zip_longest() instead. zip() in conjunction with the * operator can be used to unzip a list: >>> x = [1, 2, 3]
>>> y = [4, 5, 6]
>>> zipped = zip(x, y)
>>> list(zipped)
[(1, 4), (2, 5), (3, 6)]
>>> x2, y2 = zip(*zip(x, y))
>>> x == list(x2) and y == list(y2)
True
tf.raw_ops.OrderedMapUnstageNoKey Op removes and returns the (key, value) element with the smallest  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.raw_ops.OrderedMapUnstageNoKey  
tf.raw_ops.OrderedMapUnstageNoKey(
    indices, dtypes, capacity=0, memory_limit=0, container='',
    shared_name='', name=None
)
 key from the underlying container. If the underlying container does not contain elements, the op will block until it does.
 


 Args
  indices   A Tensor of type int32.  
  dtypes   A list of tf.DTypes that has length >= 1.  
  capacity   An optional int that is >= 0. Defaults to 0.  
  memory_limit   An optional int that is >= 0. Defaults to 0.  
  container   An optional string. Defaults to "".  
  shared_name   An optional string. Defaults to "".  
  name   A name for the operation (optional).   
 


 Returns   A tuple of Tensor objects (key, values).     key   A Tensor of type int64.  
  values   A list of Tensor objects of type dtypes.
Unpacker.unpack_list(unpack_item)  
Unpacks and returns a list of homogeneous items. The list is unpacked one element at a time by first unpacking an unsigned integer flag. If the flag is 1, then the item is unpacked and appended to the list. A flag of 0 indicates the end of the list. unpack_item is the function that is called to unpack the items.
parser.tuple2st(sequence)  
This is the same function as sequence2st(). This entry point is maintained for backward compatibility.
def f_19339():
    """unzip list `[('a', 1), ('b', 2), ('c', 3), ('d', 4), ('e', )]` and fill empty results with None
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
canonical()  
Return the canonical encoding of the argument. Currently, the encoding of a Decimal instance is always canonical, so this operation returns its argument unchanged.
to_eng_string(context=None)  
Convert to a string, using engineering notation if an exponent is needed. Engineering notation has an exponent which is a multiple of 3. This can leave up to 3 digits to the left of the decimal place and may require the addition of either one or two trailing zeros. For example, this converts Decimal('123E+1') to Decimal('1.23E+3').
copy_decimal(num)  
Return a copy of the Decimal instance num.
decimal.HAVE_CONTEXTVAR  
The default value is True. If Python is compiled --without-decimal-contextvar, the C version uses a thread-local rather than a coroutine-local context and the value is False. This is slightly faster in some nested context scenarios.
Serializer fields  Each field in a Form class is responsible not only for validating data, but also for "cleaning" it — normalizing it to a consistent format. — Django documentation  Serializer fields handle converting between primitive values and internal datatypes. They also deal with validating input values, as well as retrieving and setting the values from their parent objects. Note: The serializer fields are declared in fields.py, but by convention you should import them using from rest_framework import serializers and refer to fields as serializers.<FieldName>. Core arguments Each serializer field class constructor takes at least these arguments. Some Field classes take additional, field-specific arguments, but the following should always be accepted: read_only Read-only fields are included in the API output, but should not be included in the input during create or update operations. Any 'read_only' fields that are incorrectly included in the serializer input will be ignored. Set this to True to ensure that the field is used when serializing a representation, but is not used when creating or updating an instance during deserialization. Defaults to False write_only Set this to True to ensure that the field may be used when updating or creating an instance, but is not included when serializing the representation. Defaults to False required Normally an error will be raised if a field is not supplied during deserialization. Set to false if this field is not required to be present during deserialization. Setting this to False also allows the object attribute or dictionary key to be omitted from output when serializing the instance. If the key is not present it will simply not be included in the output representation. Defaults to True. default If set, this gives the default value that will be used for the field if no input value is supplied. If not set the default behaviour is to not populate the attribute at all. The default is not applied during partial update operations. In the partial update case only fields that are provided in the incoming data will have a validated value returned. May be set to a function or other callable, in which case the value will be evaluated each time it is used. When called, it will receive no arguments. If the callable has a requires_context = True attribute, then the serializer field will be passed as an argument. For example: class CurrentUserDefault:
    """
    May be applied as a `default=...` value on a serializer field.
    Returns the current user.
    """
    requires_context = True

    def __call__(self, serializer_field):
        return serializer_field.context['request'].user
 When serializing the instance, default will be used if the object attribute or dictionary key is not present in the instance. Note that setting a default value implies that the field is not required. Including both the default and required keyword arguments is invalid and will raise an error. allow_null Normally an error will be raised if None is passed to a serializer field. Set this keyword argument to True if None should be considered a valid value. Note that, without an explicit default, setting this argument to True will imply a default value of null for serialization output, but does not imply a default for input deserialization. Defaults to False source The name of the attribute that will be used to populate the field. May be a method that only takes a self argument, such as URLField(source='get_absolute_url'), or may use dotted notation to traverse attributes, such as EmailField(source='user.email'). When serializing fields with dotted notation, it may be necessary to provide a default value if any object is not present or is empty during attribute traversal. The value source='*' has a special meaning, and is used to indicate that the entire object should be passed through to the field. This can be useful for creating nested representations, or for fields which require access to the complete object in order to determine the output representation. Defaults to the name of the field. validators A list of validator functions which should be applied to the incoming field input, and which either raise a validation error or simply return. Validator functions should typically raise serializers.ValidationError, but Django's built-in ValidationError is also supported for compatibility with validators defined in the Django codebase or third party Django packages. error_messages A dictionary of error codes to error messages. label A short text string that may be used as the name of the field in HTML form fields or other descriptive elements. help_text A text string that may be used as a description of the field in HTML form fields or other descriptive elements. initial A value that should be used for pre-populating the value of HTML form fields. You may pass a callable to it, just as you may do with any regular Django Field: import datetime
from rest_framework import serializers
class ExampleSerializer(serializers.Serializer):
    day = serializers.DateField(initial=datetime.date.today)
 style A dictionary of key-value pairs that can be used to control how renderers should render the field. Two examples here are 'input_type' and 'base_template': # Use <input type="password"> for the input.
password = serializers.CharField(
    style={'input_type': 'password'}
)

# Use a radio input instead of a select input.
color_channel = serializers.ChoiceField(
    choices=['red', 'green', 'blue'],
    style={'base_template': 'radio.html'}
)
 For more details see the HTML & Forms documentation. Boolean fields BooleanField A boolean representation. When using HTML encoded form input be aware that omitting a value will always be treated as setting a field to False, even if it has a default=True option specified. This is because HTML checkbox inputs represent the unchecked state by omitting the value, so REST framework treats omission as if it is an empty checkbox input. Note that Django 2.1 removed the blank kwarg from models.BooleanField. Prior to Django 2.1 models.BooleanField fields were always blank=True. Thus since Django 2.1 default serializers.BooleanField instances will be generated without the required kwarg (i.e. equivalent to required=True) whereas with previous versions of Django, default BooleanField instances will be generated with a required=False option. If you want to control this behaviour manually, explicitly declare the BooleanField on the serializer class, or use the extra_kwargs option to set the required flag. Corresponds to django.db.models.fields.BooleanField. Signature: BooleanField() NullBooleanField A boolean representation that also accepts None as a valid value. Corresponds to django.db.models.fields.NullBooleanField. Signature: NullBooleanField() String fields CharField A text representation. Optionally validates the text to be shorter than max_length and longer than min_length. Corresponds to django.db.models.fields.CharField or django.db.models.fields.TextField. Signature: CharField(max_length=None, min_length=None, allow_blank=False, trim_whitespace=True)  
max_length - Validates that the input contains no more than this number of characters. 
min_length - Validates that the input contains no fewer than this number of characters. 
allow_blank - If set to True then the empty string should be considered a valid value. If set to False then the empty string is considered invalid and will raise a validation error. Defaults to False. 
trim_whitespace - If set to True then leading and trailing whitespace is trimmed. Defaults to True.  The allow_null option is also available for string fields, although its usage is discouraged in favor of allow_blank. It is valid to set both allow_blank=True and allow_null=True, but doing so means that there will be two differing types of empty value permissible for string representations, which can lead to data inconsistencies and subtle application bugs. EmailField A text representation, validates the text to be a valid e-mail address. Corresponds to django.db.models.fields.EmailField Signature: EmailField(max_length=None, min_length=None, allow_blank=False) RegexField A text representation, that validates the given value matches against a certain regular expression. Corresponds to django.forms.fields.RegexField. Signature: RegexField(regex, max_length=None, min_length=None, allow_blank=False) The mandatory regex argument may either be a string, or a compiled python regular expression object. Uses Django's django.core.validators.RegexValidator for validation. SlugField A RegexField that validates the input against the pattern [a-zA-Z0-9_-]+. Corresponds to django.db.models.fields.SlugField. Signature: SlugField(max_length=50, min_length=None, allow_blank=False) URLField A RegexField that validates the input against a URL matching pattern. Expects fully qualified URLs of the form http://<host>/<path>. Corresponds to django.db.models.fields.URLField. Uses Django's django.core.validators.URLValidator for validation. Signature: URLField(max_length=200, min_length=None, allow_blank=False) UUIDField A field that ensures the input is a valid UUID string. The to_internal_value method will return a uuid.UUID instance. On output the field will return a string in the canonical hyphenated format, for example: "de305d54-75b4-431b-adb2-eb6b9e546013"
 Signature: UUIDField(format='hex_verbose')  
format: Determines the representation format of the uuid value 
'hex_verbose' - The canonical hex representation, including hyphens: "5ce0e9a5-5ffa-654b-cee0-1238041fb31a"
 
'hex' - The compact hex representation of the UUID, not including hyphens: "5ce0e9a55ffa654bcee01238041fb31a"
 
'int' - A 128 bit integer representation of the UUID: "123456789012312313134124512351145145114"
 
'urn' - RFC 4122 URN representation of the UUID: "urn:uuid:5ce0e9a5-5ffa-654b-cee0-1238041fb31a" Changing the format parameters only affects representation values. All formats are accepted by to_internal_value
    FilePathField A field whose choices are limited to the filenames in a certain directory on the filesystem Corresponds to django.forms.fields.FilePathField. Signature: FilePathField(path, match=None, recursive=False, allow_files=True, allow_folders=False, required=None, **kwargs)  
path - The absolute filesystem path to a directory from which this FilePathField should get its choice. 
match - A regular expression, as a string, that FilePathField will use to filter filenames. 
recursive - Specifies whether all subdirectories of path should be included. Default is False. 
allow_files - Specifies whether files in the specified location should be included. Default is True. Either this or allow_folders must be True. 
allow_folders - Specifies whether folders in the specified location should be included. Default is False. Either this or allow_files must be True.  IPAddressField A field that ensures the input is a valid IPv4 or IPv6 string. Corresponds to django.forms.fields.IPAddressField and django.forms.fields.GenericIPAddressField. Signature: IPAddressField(protocol='both', unpack_ipv4=False, **options)  
protocol Limits valid inputs to the specified protocol. Accepted values are 'both' (default), 'IPv4' or 'IPv6'. Matching is case insensitive. 
unpack_ipv4 Unpacks IPv4 mapped addresses like ::ffff:192.0.2.1. If this option is enabled that address would be unpacked to 192.0.2.1. Default is disabled. Can only be used when protocol is set to 'both'.  Numeric fields IntegerField An integer representation. Corresponds to django.db.models.fields.IntegerField, django.db.models.fields.SmallIntegerField, django.db.models.fields.PositiveIntegerField and django.db.models.fields.PositiveSmallIntegerField. Signature: IntegerField(max_value=None, min_value=None)  
max_value Validate that the number provided is no greater than this value. 
min_value Validate that the number provided is no less than this value.  FloatField A floating point representation. Corresponds to django.db.models.fields.FloatField. Signature: FloatField(max_value=None, min_value=None)  
max_value Validate that the number provided is no greater than this value. 
min_value Validate that the number provided is no less than this value.  DecimalField A decimal representation, represented in Python by a Decimal instance. Corresponds to django.db.models.fields.DecimalField. Signature: DecimalField(max_digits, decimal_places, coerce_to_string=None, max_value=None, min_value=None)  
max_digits The maximum number of digits allowed in the number. It must be either None or an integer greater than or equal to decimal_places. 
decimal_places The number of decimal places to store with the number. 
coerce_to_string Set to True if string values should be returned for the representation, or False if Decimal objects should be returned. Defaults to the same value as the COERCE_DECIMAL_TO_STRING settings key, which will be True unless overridden. If Decimal objects are returned by the serializer, then the final output format will be determined by the renderer. Note that setting localize will force the value to True. 
max_value Validate that the number provided is no greater than this value. 
min_value Validate that the number provided is no less than this value. 
localize Set to True to enable localization of input and output based on the current locale. This will also force coerce_to_string to True. Defaults to False. Note that data formatting is enabled if you have set USE_L10N=True in your settings file. 
rounding Sets the rounding mode used when quantising to the configured precision. Valid values are decimal module rounding modes. Defaults to None.  Example usage To validate numbers up to 999 with a resolution of 2 decimal places, you would use: serializers.DecimalField(max_digits=5, decimal_places=2)
 And to validate numbers up to anything less than one billion with a resolution of 10 decimal places: serializers.DecimalField(max_digits=19, decimal_places=10)
 This field also takes an optional argument, coerce_to_string. If set to True the representation will be output as a string. If set to False the representation will be left as a Decimal instance and the final representation will be determined by the renderer. If unset, this will default to the same value as the COERCE_DECIMAL_TO_STRING setting, which is True unless set otherwise. Date and time fields DateTimeField A date and time representation. Corresponds to django.db.models.fields.DateTimeField. Signature: DateTimeField(format=api_settings.DATETIME_FORMAT, input_formats=None, default_timezone=None)  
format - A string representing the output format. If not specified, this defaults to the same value as the DATETIME_FORMAT settings key, which will be 'iso-8601' unless set. Setting to a format string indicates that to_representation return values should be coerced to string output. Format strings are described below. Setting this value to None indicates that Python datetime objects should be returned by to_representation. In this case the datetime encoding will be determined by the renderer. 
input_formats - A list of strings representing the input formats which may be used to parse the date. If not specified, the DATETIME_INPUT_FORMATS setting will be used, which defaults to ['iso-8601']. 
default_timezone - A pytz.timezone representing the timezone. If not specified and the USE_TZ setting is enabled, this defaults to the current timezone. If USE_TZ is disabled, then datetime objects will be naive.  DateTimeField format strings. Format strings may either be Python strftime formats which explicitly specify the format, or the special string 'iso-8601', which indicates that ISO 8601 style datetimes should be used. (eg '2013-01-29T12:34:56.000000Z') When a value of None is used for the format datetime objects will be returned by to_representation and the final output representation will determined by the renderer class. auto_now and auto_now_add model fields. When using ModelSerializer or HyperlinkedModelSerializer, note that any model fields with auto_now=True or auto_now_add=True will use serializer fields that are read_only=True by default. If you want to override this behavior, you'll need to declare the DateTimeField explicitly on the serializer. For example: class CommentSerializer(serializers.ModelSerializer):
    created = serializers.DateTimeField()

    class Meta:
        model = Comment
 DateField A date representation. Corresponds to django.db.models.fields.DateField Signature: DateField(format=api_settings.DATE_FORMAT, input_formats=None)  
format - A string representing the output format. If not specified, this defaults to the same value as the DATE_FORMAT settings key, which will be 'iso-8601' unless set. Setting to a format string indicates that to_representation return values should be coerced to string output. Format strings are described below. Setting this value to None indicates that Python date objects should be returned by to_representation. In this case the date encoding will be determined by the renderer. 
input_formats - A list of strings representing the input formats which may be used to parse the date. If not specified, the DATE_INPUT_FORMATS setting will be used, which defaults to ['iso-8601'].  DateField format strings Format strings may either be Python strftime formats which explicitly specify the format, or the special string 'iso-8601', which indicates that ISO 8601 style dates should be used. (eg '2013-01-29') TimeField A time representation. Corresponds to django.db.models.fields.TimeField Signature: TimeField(format=api_settings.TIME_FORMAT, input_formats=None)  
format - A string representing the output format. If not specified, this defaults to the same value as the TIME_FORMAT settings key, which will be 'iso-8601' unless set. Setting to a format string indicates that to_representation return values should be coerced to string output. Format strings are described below. Setting this value to None indicates that Python time objects should be returned by to_representation. In this case the time encoding will be determined by the renderer. 
input_formats - A list of strings representing the input formats which may be used to parse the date. If not specified, the TIME_INPUT_FORMATS setting will be used, which defaults to ['iso-8601'].  TimeField format strings Format strings may either be Python strftime formats which explicitly specify the format, or the special string 'iso-8601', which indicates that ISO 8601 style times should be used. (eg '12:34:56.000000') DurationField A Duration representation. Corresponds to django.db.models.fields.DurationField The validated_data for these fields will contain a datetime.timedelta instance. The representation is a string following this format '[DD] [HH:[MM:]]ss[.uuuuuu]'. Signature: DurationField(max_value=None, min_value=None)  
max_value Validate that the duration provided is no greater than this value. 
min_value Validate that the duration provided is no less than this value.  Choice selection fields ChoiceField A field that can accept a value out of a limited set of choices. Used by ModelSerializer to automatically generate fields if the corresponding model field includes a choices=… argument. Signature: ChoiceField(choices)  
choices - A list of valid values, or a list of (key, display_name) tuples. 
allow_blank - If set to True then the empty string should be considered a valid value. If set to False then the empty string is considered invalid and will raise a validation error. Defaults to False. 
html_cutoff - If set this will be the maximum number of choices that will be displayed by a HTML select drop down. Can be used to ensure that automatically generated ChoiceFields with very large possible selections do not prevent a template from rendering. Defaults to None. 
html_cutoff_text - If set this will display a textual indicator if the maximum number of items have been cutoff in an HTML select drop down. Defaults to "More than {count} items…"
  Both the allow_blank and allow_null are valid options on ChoiceField, although it is highly recommended that you only use one and not both. allow_blank should be preferred for textual choices, and allow_null should be preferred for numeric or other non-textual choices. MultipleChoiceField A field that can accept a set of zero, one or many values, chosen from a limited set of choices. Takes a single mandatory argument. to_internal_value returns a set containing the selected values. Signature: MultipleChoiceField(choices)  
choices - A list of valid values, or a list of (key, display_name) tuples. 
allow_blank - If set to True then the empty string should be considered a valid value. If set to False then the empty string is considered invalid and will raise a validation error. Defaults to False. 
html_cutoff - If set this will be the maximum number of choices that will be displayed by a HTML select drop down. Can be used to ensure that automatically generated ChoiceFields with very large possible selections do not prevent a template from rendering. Defaults to None. 
html_cutoff_text - If set this will display a textual indicator if the maximum number of items have been cutoff in an HTML select drop down. Defaults to "More than {count} items…"
  As with ChoiceField, both the allow_blank and allow_null options are valid, although it is highly recommended that you only use one and not both. allow_blank should be preferred for textual choices, and allow_null should be preferred for numeric or other non-textual choices. File upload fields Parsers and file uploads. The FileField and ImageField classes are only suitable for use with MultiPartParser or FileUploadParser. Most parsers, such as e.g. JSON don't support file uploads. Django's regular FILE_UPLOAD_HANDLERS are used for handling uploaded files. FileField A file representation. Performs Django's standard FileField validation. Corresponds to django.forms.fields.FileField. Signature: FileField(max_length=None, allow_empty_file=False, use_url=UPLOADED_FILES_USE_URL)  
max_length - Designates the maximum length for the file name. 
allow_empty_file - Designates if empty files are allowed. 
use_url - If set to True then URL string values will be used for the output representation. If set to False then filename string values will be used for the output representation. Defaults to the value of the UPLOADED_FILES_USE_URL settings key, which is True unless set otherwise.  ImageField An image representation. Validates the uploaded file content as matching a known image format. Corresponds to django.forms.fields.ImageField. Signature: ImageField(max_length=None, allow_empty_file=False, use_url=UPLOADED_FILES_USE_URL)  
max_length - Designates the maximum length for the file name. 
allow_empty_file - Designates if empty files are allowed. 
use_url - If set to True then URL string values will be used for the output representation. If set to False then filename string values will be used for the output representation. Defaults to the value of the UPLOADED_FILES_USE_URL settings key, which is True unless set otherwise.  Requires either the Pillow package or PIL package. The Pillow package is recommended, as PIL is no longer actively maintained. Composite fields ListField A field class that validates a list of objects. Signature: ListField(child=<A_FIELD_INSTANCE>, allow_empty=True, min_length=None, max_length=None)  
child - A field instance that should be used for validating the objects in the list. If this argument is not provided then objects in the list will not be validated. 
allow_empty - Designates if empty lists are allowed. 
min_length - Validates that the list contains no fewer than this number of elements. 
max_length - Validates that the list contains no more than this number of elements.  For example, to validate a list of integers you might use something like the following: scores = serializers.ListField(
   child=serializers.IntegerField(min_value=0, max_value=100)
)
 The ListField class also supports a declarative style that allows you to write reusable list field classes. class StringListField(serializers.ListField):
    child = serializers.CharField()
 We can now reuse our custom StringListField class throughout our application, without having to provide a child argument to it. DictField A field class that validates a dictionary of objects. The keys in DictField are always assumed to be string values. Signature: DictField(child=<A_FIELD_INSTANCE>, allow_empty=True)  
child - A field instance that should be used for validating the values in the dictionary. If this argument is not provided then values in the mapping will not be validated. 
allow_empty - Designates if empty dictionaries are allowed.  For example, to create a field that validates a mapping of strings to strings, you would write something like this: document = DictField(child=CharField())
 You can also use the declarative style, as with ListField. For example: class DocumentField(DictField):
    child = CharField()
 HStoreField A preconfigured DictField that is compatible with Django's postgres HStoreField. Signature: HStoreField(child=<A_FIELD_INSTANCE>, allow_empty=True)  
child - A field instance that is used for validating the values in the dictionary. The default child field accepts both empty strings and null values. 
allow_empty - Designates if empty dictionaries are allowed.  Note that the child field must be an instance of CharField, as the hstore extension stores values as strings. JSONField A field class that validates that the incoming data structure consists of valid JSON primitives. In its alternate binary mode, it will represent and validate JSON-encoded binary strings. Signature: JSONField(binary, encoder)  
binary - If set to True then the field will output and validate a JSON encoded string, rather than a primitive data structure. Defaults to False. 
encoder - Use this JSON encoder to serialize input object. Defaults to None.  Miscellaneous fields ReadOnlyField A field class that simply returns the value of the field without modification. This field is used by default with ModelSerializer when including field names that relate to an attribute rather than a model field. Signature: ReadOnlyField() For example, if has_expired was a property on the Account model, then the following serializer would automatically generate it as a ReadOnlyField: class AccountSerializer(serializers.ModelSerializer):
    class Meta:
        model = Account
        fields = ['id', 'account_name', 'has_expired']
 HiddenField A field class that does not take a value based on user input, but instead takes its value from a default value or callable. Signature: HiddenField() For example, to include a field that always provides the current time as part of the serializer validated data, you would use the following: modified = serializers.HiddenField(default=timezone.now)
 The HiddenField class is usually only needed if you have some validation that needs to run based on some pre-provided field values, but you do not want to expose all of those fields to the end user. For further examples on HiddenField see the validators documentation. ModelField A generic field that can be tied to any arbitrary model field. The ModelField class delegates the task of serialization/deserialization to its associated model field. This field can be used to create serializer fields for custom model fields, without having to create a new custom serializer field. This field is used by ModelSerializer to correspond to custom model field classes. Signature: ModelField(model_field=<Django ModelField instance>) The ModelField class is generally intended for internal use, but can be used by your API if needed. In order to properly instantiate a ModelField, it must be passed a field that is attached to an instantiated model. For example: ModelField(model_field=MyModel()._meta.get_field('custom_field')) SerializerMethodField This is a read-only field. It gets its value by calling a method on the serializer class it is attached to. It can be used to add any sort of data to the serialized representation of your object. Signature: SerializerMethodField(method_name=None)  
method_name - The name of the method on the serializer to be called. If not included this defaults to get_<field_name>.  The serializer method referred to by the method_name argument should accept a single argument (in addition to self), which is the object being serialized. It should return whatever you want to be included in the serialized representation of the object. For example: from django.contrib.auth.models import User
from django.utils.timezone import now
from rest_framework import serializers

class UserSerializer(serializers.ModelSerializer):
    days_since_joined = serializers.SerializerMethodField()

    class Meta:
        model = User
        fields = '__all__'

    def get_days_since_joined(self, obj):
        return (now() - obj.date_joined).days
 Custom fields If you want to create a custom field, you'll need to subclass Field and then override either one or both of the .to_representation() and .to_internal_value() methods. These two methods are used to convert between the initial datatype, and a primitive, serializable datatype. Primitive datatypes will typically be any of a number, string, boolean, date/time/datetime or None. They may also be any list or dictionary like object that only contains other primitive objects. Other types might be supported, depending on the renderer that you are using. The .to_representation() method is called to convert the initial datatype into a primitive, serializable datatype. The .to_internal_value() method is called to restore a primitive datatype into its internal python representation. This method should raise a serializers.ValidationError if the data is invalid. Examples A Basic Custom Field Let's look at an example of serializing a class that represents an RGB color value: class Color:
    """
    A color represented in the RGB colorspace.
    """
    def __init__(self, red, green, blue):
        assert(red >= 0 and green >= 0 and blue >= 0)
        assert(red < 256 and green < 256 and blue < 256)
        self.red, self.green, self.blue = red, green, blue

class ColorField(serializers.Field):
    """
    Color objects are serialized into 'rgb(#, #, #)' notation.
    """
    def to_representation(self, value):
        return "rgb(%d, %d, %d)" % (value.red, value.green, value.blue)

    def to_internal_value(self, data):
        data = data.strip('rgb(').rstrip(')')
        red, green, blue = [int(col) for col in data.split(',')]
        return Color(red, green, blue)
 By default field values are treated as mapping to an attribute on the object. If you need to customize how the field value is accessed and set you need to override .get_attribute() and/or .get_value(). As an example, let's create a field that can be used to represent the class name of the object being serialized: class ClassNameField(serializers.Field):
    def get_attribute(self, instance):
        # We pass the object instance onto `to_representation`,
        # not just the field attribute.
        return instance

    def to_representation(self, value):
        """
        Serialize the value's class name.
        """
        return value.__class__.__name__
 Raising validation errors Our ColorField class above currently does not perform any data validation. To indicate invalid data, we should raise a serializers.ValidationError, like so: def to_internal_value(self, data):
    if not isinstance(data, str):
        msg = 'Incorrect type. Expected a string, but got %s'
        raise ValidationError(msg % type(data).__name__)

    if not re.match(r'^rgb\([0-9]+,[0-9]+,[0-9]+\)$', data):
        raise ValidationError('Incorrect format. Expected `rgb(#,#,#)`.')

    data = data.strip('rgb(').rstrip(')')
    red, green, blue = [int(col) for col in data.split(',')]

    if any([col > 255 or col < 0 for col in (red, green, blue)]):
        raise ValidationError('Value out of range. Must be between 0 and 255.')

    return Color(red, green, blue)
 The .fail() method is a shortcut for raising ValidationError that takes a message string from the error_messages dictionary. For example: default_error_messages = {
    'incorrect_type': 'Incorrect type. Expected a string, but got {input_type}',
    'incorrect_format': 'Incorrect format. Expected `rgb(#,#,#)`.',
    'out_of_range': 'Value out of range. Must be between 0 and 255.'
}

def to_internal_value(self, data):
    if not isinstance(data, str):
        self.fail('incorrect_type', input_type=type(data).__name__)

    if not re.match(r'^rgb\([0-9]+,[0-9]+,[0-9]+\)$', data):
        self.fail('incorrect_format')

    data = data.strip('rgb(').rstrip(')')
    red, green, blue = [int(col) for col in data.split(',')]

    if any([col > 255 or col < 0 for col in (red, green, blue)]):
        self.fail('out_of_range')

    return Color(red, green, blue)
 This style keeps your error messages cleaner and more separated from your code, and should be preferred. Using source='*' Here we'll take an example of a flat DataPoint model with x_coordinate and y_coordinate attributes. class DataPoint(models.Model):
    label = models.CharField(max_length=50)
    x_coordinate = models.SmallIntegerField()
    y_coordinate = models.SmallIntegerField()
 Using a custom field and source='*' we can provide a nested representation of the coordinate pair: class CoordinateField(serializers.Field):

    def to_representation(self, value):
        ret = {
            "x": value.x_coordinate,
            "y": value.y_coordinate
        }
        return ret

    def to_internal_value(self, data):
        ret = {
            "x_coordinate": data["x"],
            "y_coordinate": data["y"],
        }
        return ret


class DataPointSerializer(serializers.ModelSerializer):
    coordinates = CoordinateField(source='*')

    class Meta:
        model = DataPoint
        fields = ['label', 'coordinates']
 Note that this example doesn't handle validation. Partly for that reason, in a real project, the coordinate nesting might be better handled with a nested serializer using source='*', with two IntegerField instances, each with their own source pointing to the relevant field. The key points from the example, though, are:   to_representation is passed the entire DataPoint object and must map from that to the desired output. >>> instance = DataPoint(label='Example', x_coordinate=1, y_coordinate=2)
>>> out_serializer = DataPointSerializer(instance)
>>> out_serializer.data
ReturnDict([('label', 'Example'), ('coordinates', {'x': 1, 'y': 2})])
   Unless our field is to be read-only, to_internal_value must map back to a dict suitable for updating our target object. With source='*', the return from to_internal_value will update the root validated data dictionary, rather than a single key. >>> data = {
...     "label": "Second Example",
...     "coordinates": {
...         "x": 3,
...         "y": 4,
...     }
... }
>>> in_serializer = DataPointSerializer(data=data)
>>> in_serializer.is_valid()
True
>>> in_serializer.validated_data
OrderedDict([('label', 'Second Example'),
             ('y_coordinate', 4),
             ('x_coordinate', 3)])
   For completeness lets do the same thing again but with the nested serializer approach suggested above: class NestedCoordinateSerializer(serializers.Serializer):
    x = serializers.IntegerField(source='x_coordinate')
    y = serializers.IntegerField(source='y_coordinate')


class DataPointSerializer(serializers.ModelSerializer):
    coordinates = NestedCoordinateSerializer(source='*')

    class Meta:
        model = DataPoint
        fields = ['label', 'coordinates']
 Here the mapping between the target and source attribute pairs (x and x_coordinate, y and y_coordinate) is handled in the IntegerField declarations. It's our NestedCoordinateSerializer that takes source='*'. Our new DataPointSerializer exhibits the same behaviour as the custom field approach. Serializing: >>> out_serializer = DataPointSerializer(instance)
>>> out_serializer.data
ReturnDict([('label', 'testing'),
            ('coordinates', OrderedDict([('x', 1), ('y', 2)]))])
 Deserializing: >>> in_serializer = DataPointSerializer(data=data)
>>> in_serializer.is_valid()
True
>>> in_serializer.validated_data
OrderedDict([('label', 'still testing'),
             ('x_coordinate', 3),
             ('y_coordinate', 4)])
 But we also get the built-in validation for free: >>> invalid_data = {
...     "label": "still testing",
...     "coordinates": {
...         "x": 'a',
...         "y": 'b',
...     }
... }
>>> invalid_serializer = DataPointSerializer(data=invalid_data)
>>> invalid_serializer.is_valid()
False
>>> invalid_serializer.errors
ReturnDict([('coordinates',
             {'x': ['A valid integer is required.'],
              'y': ['A valid integer is required.']})])
 For this reason, the nested serializer approach would be the first to try. You would use the custom field approach when the nested serializer becomes infeasible or overly complex. Third party packages The following third party packages are also available. DRF Compound Fields The drf-compound-fields package provides "compound" serializer fields, such as lists of simple values, which can be described by other fields rather than serializers with the many=True option. Also provided are fields for typed dictionaries and values that can be either a specific type or a list of items of that type. DRF Extra Fields The drf-extra-fields package provides extra serializer fields for REST framework, including Base64ImageField and PointField classes. djangorestframework-recursive the djangorestframework-recursive package provides a RecursiveField for serializing and deserializing recursive structures django-rest-framework-gis The django-rest-framework-gis package provides geographic addons for django rest framework like a GeometryField field and a GeoJSON serializer. django-rest-framework-hstore The django-rest-framework-hstore package provides an HStoreField to support django-hstore DictionaryField model field. fields.py
def f_1960516():
    """encode `Decimal('3.9')` to a JSON string
    """
    return  
 --------------------

def f_1024847(d):
    """Add key "mynewkey" to dictionary `d` with value "mynewvalue"
    """
     
 --------------------

def f_1024847(data):
    """Add key 'a' to dictionary `data` with value 1
    """
     
 --------------------

def f_1024847(data):
    """Add key 'a' to dictionary `data` with value 1
    """
     
 --------------------

def f_1024847(data):
    """Add key 'a' to dictionary `data` with value 1
    """
     
 --------------------

def f_35837346(matrix):
    """find maximal value in matrix `matrix`
    """
    return  
 --------------------

def f_20457038(answer):
    """Round number `answer` to 2 precision after the decimal point
    """
     
 --------------------
Please refer to the following documentation to generate the code:
socket.inet_aton(ip_string)  
Convert an IPv4 address from dotted-quad string format (for example, ‘123.45.67.89’) to 32-bit packed binary format, as a bytes object four characters in length. This is useful when conversing with a program that uses the standard C library and needs objects of type struct in_addr, which is the C type for the 32-bit packed binary this function returns. inet_aton() also accepts strings with less than three dots; see the Unix manual page inet(3) for details. If the IPv4 address string passed to this function is invalid, OSError will be raised. Note that exactly what is valid depends on the underlying C implementation of inet_aton(). inet_aton() does not support IPv6, and inet_pton() should be used instead for IPv4/v6 dual stack support.
ip  
The address (IPv4Address) without network information. >>> interface = IPv4Interface('192.0.2.5/24')
>>> interface.ip
IPv4Address('192.0.2.5')
raw_decode(s)  
Decode a JSON document from s (a str beginning with a JSON document) and return a 2-tuple of the Python representation and the index in s where the document ended. This can be used to decode a JSON document from a string that may have extraneous data at the end.
email.utils.parseaddr(address)  
Parse address – which should be the value of some address-containing field such as To or Cc – into its constituent realname and email address parts. Returns a tuple of that information, unless the parse fails, in which case a 2-tuple of ('', '') is returned.
aliased_name(s)[source]
 
Return 'PROPNAME or alias' if s has an alias, else return 'PROPNAME'. e.g., for the line markerfacecolor property, which has an alias, return 'markerfacecolor or mfc' and for the transform property, which does not, return 'transform'.
def f_2890896(s):
    """extract ip address `ip` from an html string `s`
    """
     
 --------------------
Please refer to the following documentation to generate the code:
skimage.util.unique_rows(ar) [source]
 
Remove repeated rows from a 2D array. In particular, if given an array of coordinates of shape (Npoints, Ndim), it will remove repeated points.  Parameters 
 
ar2-D ndarray 

The input array.    Returns 
 
ar_out2-D ndarray 

A copy of the input array with repeated rows removed.    Raises 
 
ValueErrorif ar is not two-dimensional. 
   Notes The function will generate a copy of ar if it is not C-contiguous, which will negatively affect performance for large input arrays. Examples >>> ar = np.array([[1, 0, 1],
...                [0, 1, 0],
...                [1, 0, 1]], np.uint8)
>>> unique_rows(ar)
array([[0, 1, 0],
       [1, 0, 1]], dtype=uint8)
pandas.DataFrame.drop_duplicates   DataFrame.drop_duplicates(subset=None, keep='first', inplace=False, ignore_index=False)[source]
 
Return DataFrame with duplicate rows removed. Considering certain columns is optional. Indexes, including time indexes are ignored.  Parameters 
 
subset:column label or sequence of labels, optional


Only consider certain columns for identifying duplicates, by default use all of the columns.  
keep:{‘first’, ‘last’, False}, default ‘first’


Determines which duplicates (if any) to keep. - first : Drop duplicates except for the first occurrence. - last : Drop duplicates except for the last occurrence. - False : Drop all duplicates.  
inplace:bool, default False


Whether to drop duplicates in place or to return a copy.  
ignore_index:bool, default False


If True, the resulting axis will be labeled 0, 1, …, n - 1.  New in version 1.0.0.     Returns 
 DataFrame or None

DataFrame with duplicates removed or None if inplace=True.      See also  DataFrame.value_counts

Count unique combinations of columns.    Examples Consider dataset containing ramen rating. 
>>> df = pd.DataFrame({
...     'brand': ['Yum Yum', 'Yum Yum', 'Indomie', 'Indomie', 'Indomie'],
...     'style': ['cup', 'cup', 'cup', 'pack', 'pack'],
...     'rating': [4, 4, 3.5, 15, 5]
... })
>>> df
    brand style  rating
0  Yum Yum   cup     4.0
1  Yum Yum   cup     4.0
2  Indomie   cup     3.5
3  Indomie  pack    15.0
4  Indomie  pack     5.0
  By default, it removes duplicate rows based on all columns. 
>>> df.drop_duplicates()
    brand style  rating
0  Yum Yum   cup     4.0
2  Indomie   cup     3.5
3  Indomie  pack    15.0
4  Indomie  pack     5.0
  To remove duplicates on specific column(s), use subset. 
>>> df.drop_duplicates(subset=['brand'])
    brand style  rating
0  Yum Yum   cup     4.0
2  Indomie   cup     3.5
  To remove duplicates and keep last occurrences, use keep. 
>>> df.drop_duplicates(subset=['brand', 'style'], keep='last')
    brand style  rating
1  Yum Yum   cup     4.0
2  Indomie   cup     3.5
4  Indomie  pack     5.0
numpy.polynomial.legendre.Legendre.has_samecoef method   polynomial.legendre.Legendre.has_samecoef(other)[source]
 
Check if coefficients match.  New in version 1.6.0.   Parameters 
 
otherclass instance


The other class must have the coef attribute.    Returns 
 
boolboolean


True if the coefficients are the same, False otherwise.
Options.unique_together  
 Use UniqueConstraint with the constraints option instead. UniqueConstraint provides more functionality than unique_together. unique_together may be deprecated in the future.  Sets of field names that, taken together, must be unique: unique_together = [['driver', 'restaurant']]
 This is a list of lists that must be unique when considered together. It’s used in the Django admin and is enforced at the database level (i.e., the appropriate UNIQUE statements are included in the CREATE TABLE statement). For convenience, unique_together can be a single list when dealing with a single set of fields: unique_together = ['driver', 'restaurant']
 A ManyToManyField cannot be included in unique_together. (It’s not clear what that would even mean!) If you need to validate uniqueness related to a ManyToManyField, try using a signal or an explicit through model. The ValidationError raised during model validation when the constraint is violated has the unique_together error code.
curses.setupterm(term=None, fd=-1)  
Initialize the terminal. term is a string giving the terminal name, or None; if omitted or None, the value of the TERM environment variable will be used. fd is the file descriptor to which any initialization sequences will be sent; if not supplied or -1, the file descriptor for sys.stdout will be used.
def f_29836836(df):
    """filter dataframe `df` by values in column `A` that appear more than once
    """
    return  
 --------------------

def f_2545397(myfile):
    """append each line in file `myfile` into a list
    """
    return  
 --------------------

def f_2545397():
    """Get a list of integers `lst` from a file `filename.txt`
    """
     
 --------------------
Please refer to the following documentation to generate the code:
colorbar(mappable, *, ticks=None, **kwargs)[source]
colorbar(mappable, cax=None, ax=None, use_gridspec=True, **kw)[source]
 
Add a colorbar to a plot.  Parameters 
 mappable

The matplotlib.cm.ScalarMappable (i.e., AxesImage, ContourSet, etc.) described by this colorbar. This argument is mandatory for the Figure.colorbar method but optional for the pyplot.colorbar function, which sets the default to the current image. Note that one can create a ScalarMappable "on-the-fly" to generate colorbars not attached to a previously drawn artist, e.g. fig.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap), ax=ax)
  
caxAxes, optional


Axes into which the colorbar will be drawn.  
axAxes, list of Axes, optional


One or more parent axes from which space for a new colorbar axes will be stolen, if cax is None. This has no effect if cax is set.  
use_gridspecbool, optional


If cax is None, a new cax is created as an instance of Axes. If ax is an instance of Subplot and use_gridspec is True, cax is created as an instance of Subplot using the gridspec module.    Returns 
 
colorbarColorbar

   Notes Additional keyword arguments are of two kinds: axes properties:  locationNone or {'left', 'right', 'top', 'bottom'}


The location, relative to the parent axes, where the colorbar axes is created. It also determines the orientation of the colorbar (colorbars on the left and right are vertical, colorbars at the top and bottom are horizontal). If None, the location will come from the orientation if it is set (vertical colorbars on the right, horizontal ones at the bottom), or default to 'right' if orientation is unset.  orientationNone or {'vertical', 'horizontal'}


The orientation of the colorbar. It is preferable to set the location of the colorbar, as that also determines the orientation; passing incompatible values for location and orientation raises an exception.  fractionfloat, default: 0.15


Fraction of original axes to use for colorbar.  shrinkfloat, default: 1.0


Fraction by which to multiply the size of the colorbar.  aspectfloat, default: 20


Ratio of long to short dimensions.  padfloat, default: 0.05 if vertical, 0.15 if horizontal


Fraction of original axes between colorbar and new image axes.  anchor(float, float), optional


The anchor point of the colorbar axes. Defaults to (0.0, 0.5) if vertical; (0.5, 1.0) if horizontal.  panchor(float, float), or False, optional


The anchor point of the colorbar parent axes. If False, the parent axes' anchor will be unchanged. Defaults to (1.0, 0.5) if vertical; (0.5, 0.0) if horizontal.   colorbar properties:   
Property Description   
extend {'neither', 'both', 'min', 'max'} If not 'neither', make pointed end(s) for out-of- range values. These are set for a given colormap using the colormap set_under and set_over methods.  
extendfrac {None, 'auto', length, lengths} If set to None, both the minimum and maximum triangular colorbar extensions with have a length of 5% of the interior colorbar length (this is the default setting). If set to 'auto', makes the triangular colorbar extensions the same lengths as the interior boxes (when spacing is set to 'uniform') or the same lengths as the respective adjacent interior boxes (when spacing is set to 'proportional'). If a scalar, indicates the length of both the minimum and maximum triangular colorbar extensions as a fraction of the interior colorbar length. A two-element sequence of fractions may also be given, indicating the lengths of the minimum and maximum colorbar extensions respectively as a fraction of the interior colorbar length.  
extendrect bool If False the minimum and maximum colorbar extensions will be triangular (the default). If True the extensions will be rectangular.  
spacing {'uniform', 'proportional'} Uniform spacing gives each discrete color the same space; proportional makes the space proportional to the data interval.  
ticks None or list of ticks or Locator If None, ticks are determined automatically from the input.  
format None or str or Formatter If None, ScalarFormatter is used. If a format string is given, e.g., '%.3f', that is used. An alternative Formatter may be given instead.  
drawedges bool Whether to draw lines at color boundaries.  
label str The label on the colorbar's long axis.   The following will probably be useful only in the context of indexed colors (that is, when the mappable has norm=NoNorm()), or other unusual circumstances.   
Property Description   
boundaries None or a sequence  
values None or a sequence which must be of length 1 less than the sequence of boundaries. For each region delimited by adjacent entries in boundaries, the colormapped to the corresponding value in values will be used.   If mappable is a ContourSet, its extend kwarg is included automatically. The shrink kwarg provides a simple way to scale the colorbar with respect to the axes. Note that if cax is specified, it determines the size of the colorbar and shrink and aspect kwargs are ignored. For more precise control, you can manually specify the positions of the axes objects in which the mappable and the colorbar are drawn. In this case, do not use any of the axes properties kwargs. It is known that some vector graphics viewers (svg and pdf) renders white gaps between segments of the colorbar. This is due to bugs in the viewers, not Matplotlib. As a workaround, the colorbar can be rendered with overlapping segments: cbar = colorbar()
cbar.solids.set_edgecolor("face")
draw()
 However this has negative consequences in other circumstances, e.g. with semi-transparent images (alpha < 1) and colorbar extensions; therefore, this workaround is not used by default (see issue #1188).
colorbar(mappable, cax=None, ax=None, use_gridspec=True, **kw)[source]
 
Add a colorbar to a plot.  Parameters 
 mappable

The matplotlib.cm.ScalarMappable (i.e., AxesImage, ContourSet, etc.) described by this colorbar. This argument is mandatory for the Figure.colorbar method but optional for the pyplot.colorbar function, which sets the default to the current image. Note that one can create a ScalarMappable "on-the-fly" to generate colorbars not attached to a previously drawn artist, e.g. fig.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap), ax=ax)
  
caxAxes, optional


Axes into which the colorbar will be drawn.  
axAxes, list of Axes, optional


One or more parent axes from which space for a new colorbar axes will be stolen, if cax is None. This has no effect if cax is set.  
use_gridspecbool, optional


If cax is None, a new cax is created as an instance of Axes. If ax is an instance of Subplot and use_gridspec is True, cax is created as an instance of Subplot using the gridspec module.    Returns 
 
colorbarColorbar

   Notes Additional keyword arguments are of two kinds: axes properties:  locationNone or {'left', 'right', 'top', 'bottom'}


The location, relative to the parent axes, where the colorbar axes is created. It also determines the orientation of the colorbar (colorbars on the left and right are vertical, colorbars at the top and bottom are horizontal). If None, the location will come from the orientation if it is set (vertical colorbars on the right, horizontal ones at the bottom), or default to 'right' if orientation is unset.  orientationNone or {'vertical', 'horizontal'}


The orientation of the colorbar. It is preferable to set the location of the colorbar, as that also determines the orientation; passing incompatible values for location and orientation raises an exception.  fractionfloat, default: 0.15


Fraction of original axes to use for colorbar.  shrinkfloat, default: 1.0


Fraction by which to multiply the size of the colorbar.  aspectfloat, default: 20


Ratio of long to short dimensions.  padfloat, default: 0.05 if vertical, 0.15 if horizontal


Fraction of original axes between colorbar and new image axes.  anchor(float, float), optional


The anchor point of the colorbar axes. Defaults to (0.0, 0.5) if vertical; (0.5, 1.0) if horizontal.  panchor(float, float), or False, optional


The anchor point of the colorbar parent axes. If False, the parent axes' anchor will be unchanged. Defaults to (1.0, 0.5) if vertical; (0.5, 0.0) if horizontal.   colorbar properties:   
Property Description   
extend {'neither', 'both', 'min', 'max'} If not 'neither', make pointed end(s) for out-of- range values. These are set for a given colormap using the colormap set_under and set_over methods.  
extendfrac {None, 'auto', length, lengths} If set to None, both the minimum and maximum triangular colorbar extensions with have a length of 5% of the interior colorbar length (this is the default setting). If set to 'auto', makes the triangular colorbar extensions the same lengths as the interior boxes (when spacing is set to 'uniform') or the same lengths as the respective adjacent interior boxes (when spacing is set to 'proportional'). If a scalar, indicates the length of both the minimum and maximum triangular colorbar extensions as a fraction of the interior colorbar length. A two-element sequence of fractions may also be given, indicating the lengths of the minimum and maximum colorbar extensions respectively as a fraction of the interior colorbar length.  
extendrect bool If False the minimum and maximum colorbar extensions will be triangular (the default). If True the extensions will be rectangular.  
spacing {'uniform', 'proportional'} Uniform spacing gives each discrete color the same space; proportional makes the space proportional to the data interval.  
ticks None or list of ticks or Locator If None, ticks are determined automatically from the input.  
format None or str or Formatter If None, ScalarFormatter is used. If a format string is given, e.g., '%.3f', that is used. An alternative Formatter may be given instead.  
drawedges bool Whether to draw lines at color boundaries.  
label str The label on the colorbar's long axis.   The following will probably be useful only in the context of indexed colors (that is, when the mappable has norm=NoNorm()), or other unusual circumstances.   
Property Description   
boundaries None or a sequence  
values None or a sequence which must be of length 1 less than the sequence of boundaries. For each region delimited by adjacent entries in boundaries, the colormapped to the corresponding value in values will be used.   If mappable is a ContourSet, its extend kwarg is included automatically. The shrink kwarg provides a simple way to scale the colorbar with respect to the axes. Note that if cax is specified, it determines the size of the colorbar and shrink and aspect kwargs are ignored. For more precise control, you can manually specify the positions of the axes objects in which the mappable and the colorbar are drawn. In this case, do not use any of the axes properties kwargs. It is known that some vector graphics viewers (svg and pdf) renders white gaps between segments of the colorbar. This is due to bugs in the viewers, not Matplotlib. As a workaround, the colorbar can be rendered with overlapping segments: cbar = colorbar()
cbar.solids.set_edgecolor("face")
draw()
 However this has negative consequences in other circumstances, e.g. with semi-transparent images (alpha < 1) and colorbar extensions; therefore, this workaround is not used by default (see issue #1188).
colorbar(mappable, cax=None, ax=None, use_gridspec=True, **kw)[source]
 
Add a colorbar to a plot.  Parameters 
 mappable

The matplotlib.cm.ScalarMappable (i.e., AxesImage, ContourSet, etc.) described by this colorbar. This argument is mandatory for the Figure.colorbar method but optional for the pyplot.colorbar function, which sets the default to the current image. Note that one can create a ScalarMappable "on-the-fly" to generate colorbars not attached to a previously drawn artist, e.g. fig.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap), ax=ax)
  
caxAxes, optional


Axes into which the colorbar will be drawn.  
axAxes, list of Axes, optional


One or more parent axes from which space for a new colorbar axes will be stolen, if cax is None. This has no effect if cax is set.  
use_gridspecbool, optional


If cax is None, a new cax is created as an instance of Axes. If ax is an instance of Subplot and use_gridspec is True, cax is created as an instance of Subplot using the gridspec module.    Returns 
 
colorbarColorbar

   Notes Additional keyword arguments are of two kinds: axes properties:  locationNone or {'left', 'right', 'top', 'bottom'}


The location, relative to the parent axes, where the colorbar axes is created. It also determines the orientation of the colorbar (colorbars on the left and right are vertical, colorbars at the top and bottom are horizontal). If None, the location will come from the orientation if it is set (vertical colorbars on the right, horizontal ones at the bottom), or default to 'right' if orientation is unset.  orientationNone or {'vertical', 'horizontal'}


The orientation of the colorbar. It is preferable to set the location of the colorbar, as that also determines the orientation; passing incompatible values for location and orientation raises an exception.  fractionfloat, default: 0.15


Fraction of original axes to use for colorbar.  shrinkfloat, default: 1.0


Fraction by which to multiply the size of the colorbar.  aspectfloat, default: 20


Ratio of long to short dimensions.  padfloat, default: 0.05 if vertical, 0.15 if horizontal


Fraction of original axes between colorbar and new image axes.  anchor(float, float), optional


The anchor point of the colorbar axes. Defaults to (0.0, 0.5) if vertical; (0.5, 1.0) if horizontal.  panchor(float, float), or False, optional


The anchor point of the colorbar parent axes. If False, the parent axes' anchor will be unchanged. Defaults to (1.0, 0.5) if vertical; (0.5, 0.0) if horizontal.   colorbar properties:   
Property Description   
extend {'neither', 'both', 'min', 'max'} If not 'neither', make pointed end(s) for out-of- range values. These are set for a given colormap using the colormap set_under and set_over methods.  
extendfrac {None, 'auto', length, lengths} If set to None, both the minimum and maximum triangular colorbar extensions with have a length of 5% of the interior colorbar length (this is the default setting). If set to 'auto', makes the triangular colorbar extensions the same lengths as the interior boxes (when spacing is set to 'uniform') or the same lengths as the respective adjacent interior boxes (when spacing is set to 'proportional'). If a scalar, indicates the length of both the minimum and maximum triangular colorbar extensions as a fraction of the interior colorbar length. A two-element sequence of fractions may also be given, indicating the lengths of the minimum and maximum colorbar extensions respectively as a fraction of the interior colorbar length.  
extendrect bool If False the minimum and maximum colorbar extensions will be triangular (the default). If True the extensions will be rectangular.  
spacing {'uniform', 'proportional'} Uniform spacing gives each discrete color the same space; proportional makes the space proportional to the data interval.  
ticks None or list of ticks or Locator If None, ticks are determined automatically from the input.  
format None or str or Formatter If None, ScalarFormatter is used. If a format string is given, e.g., '%.3f', that is used. An alternative Formatter may be given instead.  
drawedges bool Whether to draw lines at color boundaries.  
label str The label on the colorbar's long axis.   The following will probably be useful only in the context of indexed colors (that is, when the mappable has norm=NoNorm()), or other unusual circumstances.   
Property Description   
boundaries None or a sequence  
values None or a sequence which must be of length 1 less than the sequence of boundaries. For each region delimited by adjacent entries in boundaries, the colormapped to the corresponding value in values will be used.   If mappable is a ContourSet, its extend kwarg is included automatically. The shrink kwarg provides a simple way to scale the colorbar with respect to the axes. Note that if cax is specified, it determines the size of the colorbar and shrink and aspect kwargs are ignored. For more precise control, you can manually specify the positions of the axes objects in which the mappable and the colorbar are drawn. In this case, do not use any of the axes properties kwargs. It is known that some vector graphics viewers (svg and pdf) renders white gaps between segments of the colorbar. This is due to bugs in the viewers, not Matplotlib. As a workaround, the colorbar can be rendered with overlapping segments: cbar = colorbar()
cbar.solids.set_edgecolor("face")
draw()
 However this has negative consequences in other circumstances, e.g. with semi-transparent images (alpha < 1) and colorbar extensions; therefore, this workaround is not used by default (see issue #1188).
matplotlib.pyplot.colorbar   matplotlib.pyplot.colorbar(mappable=None, cax=None, ax=None, **kw)[source]
 
Add a colorbar to a plot.  Parameters 
 mappable

The matplotlib.cm.ScalarMappable (i.e., AxesImage, ContourSet, etc.) described by this colorbar. This argument is mandatory for the Figure.colorbar method but optional for the pyplot.colorbar function, which sets the default to the current image. Note that one can create a ScalarMappable "on-the-fly" to generate colorbars not attached to a previously drawn artist, e.g. fig.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap), ax=ax)
  
caxAxes, optional


Axes into which the colorbar will be drawn.  
axAxes, list of Axes, optional


One or more parent axes from which space for a new colorbar axes will be stolen, if cax is None. This has no effect if cax is set.  
use_gridspecbool, optional


If cax is None, a new cax is created as an instance of Axes. If ax is an instance of Subplot and use_gridspec is True, cax is created as an instance of Subplot using the gridspec module.    Returns 
 
colorbarColorbar

   Notes Additional keyword arguments are of two kinds: axes properties:  locationNone or {'left', 'right', 'top', 'bottom'}


The location, relative to the parent axes, where the colorbar axes is created. It also determines the orientation of the colorbar (colorbars on the left and right are vertical, colorbars at the top and bottom are horizontal). If None, the location will come from the orientation if it is set (vertical colorbars on the right, horizontal ones at the bottom), or default to 'right' if orientation is unset.  orientationNone or {'vertical', 'horizontal'}


The orientation of the colorbar. It is preferable to set the location of the colorbar, as that also determines the orientation; passing incompatible values for location and orientation raises an exception.  fractionfloat, default: 0.15


Fraction of original axes to use for colorbar.  shrinkfloat, default: 1.0


Fraction by which to multiply the size of the colorbar.  aspectfloat, default: 20


Ratio of long to short dimensions.  padfloat, default: 0.05 if vertical, 0.15 if horizontal


Fraction of original axes between colorbar and new image axes.  anchor(float, float), optional


The anchor point of the colorbar axes. Defaults to (0.0, 0.5) if vertical; (0.5, 1.0) if horizontal.  panchor(float, float), or False, optional


The anchor point of the colorbar parent axes. If False, the parent axes' anchor will be unchanged. Defaults to (1.0, 0.5) if vertical; (0.5, 0.0) if horizontal.   colorbar properties:   
Property Description   
extend {'neither', 'both', 'min', 'max'} If not 'neither', make pointed end(s) for out-of- range values. These are set for a given colormap using the colormap set_under and set_over methods.  
extendfrac {None, 'auto', length, lengths} If set to None, both the minimum and maximum triangular colorbar extensions with have a length of 5% of the interior colorbar length (this is the default setting). If set to 'auto', makes the triangular colorbar extensions the same lengths as the interior boxes (when spacing is set to 'uniform') or the same lengths as the respective adjacent interior boxes (when spacing is set to 'proportional'). If a scalar, indicates the length of both the minimum and maximum triangular colorbar extensions as a fraction of the interior colorbar length. A two-element sequence of fractions may also be given, indicating the lengths of the minimum and maximum colorbar extensions respectively as a fraction of the interior colorbar length.  
extendrect bool If False the minimum and maximum colorbar extensions will be triangular (the default). If True the extensions will be rectangular.  
spacing {'uniform', 'proportional'} Uniform spacing gives each discrete color the same space; proportional makes the space proportional to the data interval.  
ticks None or list of ticks or Locator If None, ticks are determined automatically from the input.  
format None or str or Formatter If None, ScalarFormatter is used. If a format string is given, e.g., '%.3f', that is used. An alternative Formatter may be given instead.  
drawedges bool Whether to draw lines at color boundaries.  
label str The label on the colorbar's long axis.   The following will probably be useful only in the context of indexed colors (that is, when the mappable has norm=NoNorm()), or other unusual circumstances.   
Property Description   
boundaries None or a sequence  
values None or a sequence which must be of length 1 less than the sequence of boundaries. For each region delimited by adjacent entries in boundaries, the colormapped to the corresponding value in values will be used.   If mappable is a ContourSet, its extend kwarg is included automatically. The shrink kwarg provides a simple way to scale the colorbar with respect to the axes. Note that if cax is specified, it determines the size of the colorbar and shrink and aspect kwargs are ignored. For more precise control, you can manually specify the positions of the axes objects in which the mappable and the colorbar are drawn. In this case, do not use any of the axes properties kwargs. It is known that some vector graphics viewers (svg and pdf) renders white gaps between segments of the colorbar. This is due to bugs in the viewers, not Matplotlib. As a workaround, the colorbar can be rendered with overlapping segments: cbar = colorbar()
cbar.solids.set_edgecolor("face")
draw()
 However this has negative consequences in other circumstances, e.g. with semi-transparent images (alpha < 1) and colorbar extensions; therefore, this workaround is not used by default (see issue #1188). 
  Examples using matplotlib.pyplot.colorbar
 
   Subplots spacings and margins   

   Ellipse Collection   

   Axes Divider   

   Simple Colorbar   

   Image tutorial   

   Tight Layout guide
def f_35420052(plt, mappable, ax3):
    """add color bar with image `mappable` to plot `plt`
    """
     
 --------------------
Please refer to the following documentation to generate the code:
locale.ALT_DIGITS  
Get a representation of up to 100 values used to represent the values 0 to 99.
numpy.random.RandomState.standard_t method   random.RandomState.standard_t(df, size=None)
 
Draw samples from a standard Student’s t distribution with df degrees of freedom. A special case of the hyperbolic distribution. As df gets large, the result resembles that of the standard normal distribution (standard_normal).  Note New code should use the standard_t method of a default_rng() instance instead; please see the Quick Start.   Parameters 
 
dffloat or array_like of floats


Degrees of freedom, must be > 0.  
sizeint or tuple of ints, optional


Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if df is a scalar. Otherwise, np.array(df).size samples are drawn.    Returns 
 
outndarray or scalar


Drawn samples from the parameterized standard Student’s t distribution.      See also  Generator.standard_t

which should be used for new code.    Notes The probability density function for the t distribution is  \[P(x, df) = \frac{\Gamma(\frac{df+1}{2})}{\sqrt{\pi df} \Gamma(\frac{df}{2})}\Bigl( 1+\frac{x^2}{df} \Bigr)^{-(df+1)/2}\] The t test is based on an assumption that the data come from a Normal distribution. The t test provides a way to test whether the sample mean (that is the mean calculated from the data) is a good estimate of the true mean. The derivation of the t-distribution was first published in 1908 by William Gosset while working for the Guinness Brewery in Dublin. Due to proprietary issues, he had to publish under a pseudonym, and so he used the name Student. References  1 
Dalgaard, Peter, “Introductory Statistics With R”, Springer, 2002.  2 
Wikipedia, “Student’s t-distribution” https://en.wikipedia.org/wiki/Student’s_t-distribution   Examples From Dalgaard page 83 [1], suppose the daily energy intake for 11 women in kilojoules (kJ) is: >>> intake = np.array([5260., 5470, 5640, 6180, 6390, 6515, 6805, 7515, \
...                    7515, 8230, 8770])
 Does their energy intake deviate systematically from the recommended value of 7725 kJ? Our null hypothesis will be the absence of deviation, and the alternate hypothesis will be the presence of an effect that could be either positive or negative, hence making our test 2-tailed. Because we are estimating the mean and we have N=11 values in our sample, we have N-1=10 degrees of freedom. We set our significance level to 95% and compute the t statistic using the empirical mean and empirical standard deviation of our intake. We use a ddof of 1 to base the computation of our empirical standard deviation on an unbiased estimate of the variance (note: the final estimate is not unbiased due to the concave nature of the square root). >>> np.mean(intake)
6753.636363636364
>>> intake.std(ddof=1)
1142.1232221373727
>>> t = (np.mean(intake)-7725)/(intake.std(ddof=1)/np.sqrt(len(intake)))
>>> t
-2.8207540608310198
 We draw 1000000 samples from Student’s t distribution with the adequate degrees of freedom. >>> import matplotlib.pyplot as plt
>>> s = np.random.standard_t(10, size=1000000)
>>> h = plt.hist(s, bins=100, density=True)
 Does our t statistic land in one of the two critical regions found at both tails of the distribution? >>> np.sum(np.abs(t) < np.abs(s)) / float(len(s))
0.018318  #random < 0.05, statistic is in critical region
 The probability value for this 2-tailed test is about 1.83%, which is lower than the 5% pre-determined significance threshold. Therefore, the probability of observing values as extreme as our intake conditionally on the null hypothesis being true is too low, and we reject the null hypothesis of no deviation.
numpy.random.standard_t   random.standard_t(df, size=None)
 
Draw samples from a standard Student’s t distribution with df degrees of freedom. A special case of the hyperbolic distribution. As df gets large, the result resembles that of the standard normal distribution (standard_normal).  Note New code should use the standard_t method of a default_rng() instance instead; please see the Quick Start.   Parameters 
 
dffloat or array_like of floats


Degrees of freedom, must be > 0.  
sizeint or tuple of ints, optional


Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if df is a scalar. Otherwise, np.array(df).size samples are drawn.    Returns 
 
outndarray or scalar


Drawn samples from the parameterized standard Student’s t distribution.      See also  Generator.standard_t

which should be used for new code.    Notes The probability density function for the t distribution is  \[P(x, df) = \frac{\Gamma(\frac{df+1}{2})}{\sqrt{\pi df} \Gamma(\frac{df}{2})}\Bigl( 1+\frac{x^2}{df} \Bigr)^{-(df+1)/2}\] The t test is based on an assumption that the data come from a Normal distribution. The t test provides a way to test whether the sample mean (that is the mean calculated from the data) is a good estimate of the true mean. The derivation of the t-distribution was first published in 1908 by William Gosset while working for the Guinness Brewery in Dublin. Due to proprietary issues, he had to publish under a pseudonym, and so he used the name Student. References  1 
Dalgaard, Peter, “Introductory Statistics With R”, Springer, 2002.  2 
Wikipedia, “Student’s t-distribution” https://en.wikipedia.org/wiki/Student’s_t-distribution   Examples From Dalgaard page 83 [1], suppose the daily energy intake for 11 women in kilojoules (kJ) is: >>> intake = np.array([5260., 5470, 5640, 6180, 6390, 6515, 6805, 7515, \
...                    7515, 8230, 8770])
 Does their energy intake deviate systematically from the recommended value of 7725 kJ? Our null hypothesis will be the absence of deviation, and the alternate hypothesis will be the presence of an effect that could be either positive or negative, hence making our test 2-tailed. Because we are estimating the mean and we have N=11 values in our sample, we have N-1=10 degrees of freedom. We set our significance level to 95% and compute the t statistic using the empirical mean and empirical standard deviation of our intake. We use a ddof of 1 to base the computation of our empirical standard deviation on an unbiased estimate of the variance (note: the final estimate is not unbiased due to the concave nature of the square root). >>> np.mean(intake)
6753.636363636364
>>> intake.std(ddof=1)
1142.1232221373727
>>> t = (np.mean(intake)-7725)/(intake.std(ddof=1)/np.sqrt(len(intake)))
>>> t
-2.8207540608310198
 We draw 1000000 samples from Student’s t distribution with the adequate degrees of freedom. >>> import matplotlib.pyplot as plt
>>> s = np.random.standard_t(10, size=1000000)
>>> h = plt.hist(s, bins=100, density=True)
 Does our t statistic land in one of the two critical regions found at both tails of the distribution? >>> np.sum(np.abs(t) < np.abs(s)) / float(len(s))
0.018318  #random < 0.05, statistic is in critical region
 The probability value for this 2-tailed test is about 1.83%, which is lower than the 5% pre-determined significance threshold. Therefore, the probability of observing values as extreme as our intake conditionally on the null hypothesis being true is too low, and we reject the null hypothesis of no deviation.
tf.summary.text     View source on GitHub    Write a text summary. 
tf.summary.text(
    name, data, step=None, description=None
)

 


 Arguments
  name   A name for this summary. The summary tag used for TensorBoard will be this name prefixed by any active name scopes.  
  data   A UTF-8 string tensor value.  
  step   Explicit int64-castable monotonic step value for this summary. If omitted, this defaults to tf.summary.experimental.get_step(), which must not be None.  
  description   Optional long-form description for this summary, as a constant str. Markdown is supported. Defaults to empty.   
 


 Returns   True on success, or false if no summary was emitted because no default summary writer was available.  

 


 Raises
  ValueError   if a default writer exists, but no step was provided and tf.summary.experimental.get_step() is None.
numpy.random.Generator.standard_t method   random.Generator.standard_t(df, size=None)
 
Draw samples from a standard Student’s t distribution with df degrees of freedom. A special case of the hyperbolic distribution. As df gets large, the result resembles that of the standard normal distribution (standard_normal).  Parameters 
 
dffloat or array_like of floats


Degrees of freedom, must be > 0.  
sizeint or tuple of ints, optional


Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if df is a scalar. Otherwise, np.array(df).size samples are drawn.    Returns 
 
outndarray or scalar


Drawn samples from the parameterized standard Student’s t distribution.     Notes The probability density function for the t distribution is  \[P(x, df) = \frac{\Gamma(\frac{df+1}{2})}{\sqrt{\pi df} \Gamma(\frac{df}{2})}\Bigl( 1+\frac{x^2}{df} \Bigr)^{-(df+1)/2}\] The t test is based on an assumption that the data come from a Normal distribution. The t test provides a way to test whether the sample mean (that is the mean calculated from the data) is a good estimate of the true mean. The derivation of the t-distribution was first published in 1908 by William Gosset while working for the Guinness Brewery in Dublin. Due to proprietary issues, he had to publish under a pseudonym, and so he used the name Student. References  1 
Dalgaard, Peter, “Introductory Statistics With R”, Springer, 2002.  2 
Wikipedia, “Student’s t-distribution” https://en.wikipedia.org/wiki/Student’s_t-distribution   Examples From Dalgaard page 83 [1], suppose the daily energy intake for 11 women in kilojoules (kJ) is: >>> intake = np.array([5260., 5470, 5640, 6180, 6390, 6515, 6805, 7515, \
...                    7515, 8230, 8770])
 Does their energy intake deviate systematically from the recommended value of 7725 kJ? Our null hypothesis will be the absence of deviation, and the alternate hypothesis will be the presence of an effect that could be either positive or negative, hence making our test 2-tailed. Because we are estimating the mean and we have N=11 values in our sample, we have N-1=10 degrees of freedom. We set our significance level to 95% and compute the t statistic using the empirical mean and empirical standard deviation of our intake. We use a ddof of 1 to base the computation of our empirical standard deviation on an unbiased estimate of the variance (note: the final estimate is not unbiased due to the concave nature of the square root). >>> np.mean(intake)
6753.636363636364
>>> intake.std(ddof=1)
1142.1232221373727
>>> t = (np.mean(intake)-7725)/(intake.std(ddof=1)/np.sqrt(len(intake)))
>>> t
-2.8207540608310198
 We draw 1000000 samples from Student’s t distribution with the adequate degrees of freedom. >>> import matplotlib.pyplot as plt
>>> s = np.random.default_rng().standard_t(10, size=1000000)
>>> h = plt.hist(s, bins=100, density=True)
 Does our t statistic land in one of the two critical regions found at both tails of the distribution? >>> np.sum(np.abs(t) < np.abs(s)) / float(len(s))
0.018318  #random < 0.05, statistic is in critical region
 The probability value for this 2-tailed test is about 1.83%, which is lower than the 5% pre-determined significance threshold. Therefore, the probability of observing values as extreme as our intake conditionally on the null hypothesis being true is too low, and we reject the null hypothesis of no deviation.
def f_29903025(df):
    """count most frequent 100 words in column 'text' of dataframe `df`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
torch.combinations(input, r=2, with_replacement=False) → seq  
Compute combinations of length rr  of the given tensor. The behavior is similar to python’s itertools.combinations when with_replacement is set to False, and itertools.combinations_with_replacement when with_replacement is set to True.  Parameters 
 
input (Tensor) – 1D vector. 
r (int, optional) – number of elements to combine 
with_replacement (boolean, optional) – whether to allow duplication in combination   Returns 
A tensor equivalent to converting all the input tensors into lists, do itertools.combinations or itertools.combinations_with_replacement on these lists, and finally convert the resulting list into tensor.  Return type 
Tensor   Example: >>> a = [1, 2, 3]
>>> list(itertools.combinations(a, r=2))
[(1, 2), (1, 3), (2, 3)]
>>> list(itertools.combinations(a, r=3))
[(1, 2, 3)]
>>> list(itertools.combinations_with_replacement(a, r=2))
[(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)]
>>> tensor_a = torch.tensor(a)
>>> torch.combinations(tensor_a)
tensor([[1, 2],
        [1, 3],
        [2, 3]])
>>> torch.combinations(tensor_a, r=3)
tensor([[1, 2, 3]])
>>> torch.combinations(tensor_a, with_replacement=True)
tensor([[1, 1],
        [1, 2],
        [1, 3],
        [2, 2],
        [2, 3],
        [3, 3]])
itertools.combinations(iterable, r)  
Return r length subsequences of elements from the input iterable. The combination tuples are emitted in lexicographic ordering according to the order of the input iterable. So, if the input iterable is sorted, the combination tuples will be produced in sorted order. Elements are treated as unique based on their position, not on their value. So if the input elements are unique, there will be no repeat values in each combination. Roughly equivalent to: def combinations(iterable, r):
    # combinations('ABCD', 2) --> AB AC AD BC BD CD
    # combinations(range(4), 3) --> 012 013 023 123
    pool = tuple(iterable)
    n = len(pool)
    if r > n:
        return
    indices = list(range(r))
    yield tuple(pool[i] for i in indices)
    while True:
        for i in reversed(range(r)):
            if indices[i] != i + n - r:
                break
        else:
            return
        indices[i] += 1
        for j in range(i+1, r):
            indices[j] = indices[j-1] + 1
        yield tuple(pool[i] for i in indices)
 The code for combinations() can be also expressed as a subsequence of permutations() after filtering entries where the elements are not in sorted order (according to their position in the input pool): def combinations(iterable, r):
    pool = tuple(iterable)
    n = len(pool)
    for indices in permutations(range(n), r):
        if sorted(indices) == list(indices):
            yield tuple(pool[i] for i in indices)
 The number of items returned is n! / r! / (n-r)! when 0 <= r <= n or zero when r > n.
itertools.permutations(iterable, r=None)  
Return successive r length permutations of elements in the iterable. If r is not specified or is None, then r defaults to the length of the iterable and all possible full-length permutations are generated. The permutation tuples are emitted in lexicographic ordering according to the order of the input iterable. So, if the input iterable is sorted, the combination tuples will be produced in sorted order. Elements are treated as unique based on their position, not on their value. So if the input elements are unique, there will be no repeat values in each permutation. Roughly equivalent to: def permutations(iterable, r=None):
    # permutations('ABCD', 2) --> AB AC AD BA BC BD CA CB CD DA DB DC
    # permutations(range(3)) --> 012 021 102 120 201 210
    pool = tuple(iterable)
    n = len(pool)
    r = n if r is None else r
    if r > n:
        return
    indices = list(range(n))
    cycles = list(range(n, n-r, -1))
    yield tuple(pool[i] for i in indices[:r])
    while n:
        for i in reversed(range(r)):
            cycles[i] -= 1
            if cycles[i] == 0:
                indices[i:] = indices[i+1:] + indices[i:i+1]
                cycles[i] = n - i
            else:
                j = cycles[i]
                indices[i], indices[-j] = indices[-j], indices[i]
                yield tuple(pool[i] for i in indices[:r])
                break
        else:
            return
 The code for permutations() can be also expressed as a subsequence of product(), filtered to exclude entries with repeated elements (those from the same position in the input pool): def permutations(iterable, r=None):
    pool = tuple(iterable)
    n = len(pool)
    r = n if r is None else r
    for indices in product(range(n), repeat=r):
        if len(set(indices)) == r:
            yield tuple(pool[i] for i in indices)
 The number of items returned is n! / (n-r)! when 0 <= r <= n or zero when r > n.
itertools.combinations_with_replacement(iterable, r)  
Return r length subsequences of elements from the input iterable allowing individual elements to be repeated more than once. The combination tuples are emitted in lexicographic ordering according to the order of the input iterable. So, if the input iterable is sorted, the combination tuples will be produced in sorted order. Elements are treated as unique based on their position, not on their value. So if the input elements are unique, the generated combinations will also be unique. Roughly equivalent to: def combinations_with_replacement(iterable, r):
    # combinations_with_replacement('ABC', 2) --> AA AB AC BB BC CC
    pool = tuple(iterable)
    n = len(pool)
    if not n and r:
        return
    indices = [0] * r
    yield tuple(pool[i] for i in indices)
    while True:
        for i in reversed(range(r)):
            if indices[i] != n - 1:
                break
        else:
            return
        indices[i:] = [indices[i] + 1] * (r - i)
        yield tuple(pool[i] for i in indices)
 The code for combinations_with_replacement() can be also expressed as a subsequence of product() after filtering entries where the elements are not in sorted order (according to their position in the input pool): def combinations_with_replacement(iterable, r):
    pool = tuple(iterable)
    n = len(pool)
    for indices in product(range(n), repeat=r):
        if sorted(indices) == list(indices):
            yield tuple(pool[i] for i in indices)
 The number of items returned is (n+r-1)! / r! / (n-1)! when n > 0.  New in version 3.1.
torch.utils.data.random_split(dataset, lengths, generator=<torch._C.Generator object>) [source]
 
Randomly split a dataset into non-overlapping new datasets of given lengths. Optionally fix the generator for reproducible results, e.g.: >>> random_split(range(10), [3, 7], generator=torch.Generator().manual_seed(42))
  Parameters 
 
dataset (Dataset) – Dataset to be split 
lengths (sequence) – lengths of splits to be produced 
generator (Generator) – Generator used for the random permutation.
def f_7378180():
    """generate all 2-element subsets of tuple `(1, 2, 3)`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
classmethod datetime.today()  
Return the current local datetime, with tzinfo None. Equivalent to: datetime.fromtimestamp(time.time())
 See also now(), fromtimestamp(). This method is functionally equivalent to now(), but without a tz parameter.
classmethod datetime.utcnow()  
Return the current UTC date and time, with tzinfo None. This is like now(), but returns the current UTC date and time, as a naive datetime object. An aware current UTC datetime can be obtained by calling datetime.now(timezone.utc). See also now().  Warning Because naive datetime objects are treated by many datetime methods as local times, it is preferred to use aware datetimes to represent times in UTC. As such, the recommended way to create an object representing the current time in UTC is by calling datetime.now(timezone.utc).
pandas.Timestamp.today   classmethodTimestamp.today(cls, tz=None)
 
Return the current time in the local timezone. This differs from datetime.today() in that it can be localized to a passed timezone.  Parameters 
 
tz:str or timezone object, default None


Timezone to localize to.     Examples 
>>> pd.Timestamp.today()    
Timestamp('2020-11-16 22:37:39.969883')
  Analogous for pd.NaT: 
>>> pd.NaT.today()
NaT
classmethod date.today()  
Return the current local date. This is equivalent to date.fromtimestamp(time.time()).
classmethod datetime.now(tz=None)  
Return the current local date and time. If optional argument tz is None or not specified, this is like today(), but, if possible, supplies more precision than can be gotten from going through a time.time() timestamp (for example, this may be possible on platforms supplying the C gettimeofday() function). If tz is not None, it must be an instance of a tzinfo subclass, and the current date and time are converted to tz’s time zone. This function is preferred over today() and utcnow().
def f_4530069():
    """get a value of datetime.today() in the UTC time zone
    """
    return  
 --------------------

def f_4842956(list1):
    """Get a new list `list2`by removing empty list from a list of lists `list1`
    """
     
 --------------------

def f_4842956(list1):
    """Create `list2` to contain the lists from list `list1` excluding the empty lists from `list1`
    """
     
 --------------------
Please refer to the following documentation to generate the code:
class JsonResponse(data, encoder=DjangoJSONEncoder, safe=True, json_dumps_params=None, **kwargs)  
An HttpResponse subclass that helps to create a JSON-encoded response. It inherits most behavior from its superclass with a couple differences: Its default Content-Type header is set to application/json. The first parameter, data, should be a dict instance. If the safe parameter is set to False (see below) it can be any JSON-serializable object. The encoder, which defaults to django.core.serializers.json.DjangoJSONEncoder, will be used to serialize the data. See JSON serialization for more details about this serializer. The safe boolean parameter defaults to True. If it’s set to False, any object can be passed for serialization (otherwise only dict instances are allowed). If safe is True and a non-dict object is passed as the first argument, a TypeError will be raised. The json_dumps_params parameter is a dictionary of keyword arguments to pass to the json.dumps() call used to generate the response.
Responses  Unlike basic HttpResponse objects, TemplateResponse objects retain the details of the context that was provided by the view to compute the response. The final output of the response is not computed until it is needed, later in the response process. — Django documentation  REST framework supports HTTP content negotiation by providing a Response class which allows you to return content that can be rendered into multiple content types, depending on the client request. The Response class subclasses Django's SimpleTemplateResponse. Response objects are initialised with data, which should consist of native Python primitives. REST framework then uses standard HTTP content negotiation to determine how it should render the final response content. There's no requirement for you to use the Response class, you can also return regular HttpResponse or StreamingHttpResponse objects from your views if required. Using the Response class simply provides a nicer interface for returning content-negotiated Web API responses, that can be rendered to multiple formats. Unless you want to heavily customize REST framework for some reason, you should always use an APIView class or @api_view function for views that return Response objects. Doing so ensures that the view can perform content negotiation and select the appropriate renderer for the response, before it is returned from the view. Creating responses Response() Signature: Response(data, status=None, template_name=None, headers=None, content_type=None) Unlike regular HttpResponse objects, you do not instantiate Response objects with rendered content. Instead you pass in unrendered data, which may consist of any Python primitives. The renderers used by the Response class cannot natively handle complex datatypes such as Django model instances, so you need to serialize the data into primitive datatypes before creating the Response object. You can use REST framework's Serializer classes to perform this data serialization, or use your own custom serialization. Arguments:  
data: The serialized data for the response. 
status: A status code for the response. Defaults to 200. See also status codes. 
template_name: A template name to use if HTMLRenderer is selected. 
headers: A dictionary of HTTP headers to use in the response. 
content_type: The content type of the response. Typically, this will be set automatically by the renderer as determined by content negotiation, but there may be some cases where you need to specify the content type explicitly.  Attributes .data The unrendered, serialized data of the response. .status_code The numeric status code of the HTTP response. .content The rendered content of the response. The .render() method must have been called before .content can be accessed. .template_name The template_name, if supplied. Only required if HTMLRenderer or some other custom template renderer is the accepted renderer for the response. .accepted_renderer The renderer instance that will be used to render the response. Set automatically by the APIView or @api_view immediately before the response is returned from the view. .accepted_media_type The media type that was selected by the content negotiation stage. Set automatically by the APIView or @api_view immediately before the response is returned from the view. .renderer_context A dictionary of additional context information that will be passed to the renderer's .render() method. Set automatically by the APIView or @api_view immediately before the response is returned from the view. Standard HttpResponse attributes The Response class extends SimpleTemplateResponse, and all the usual attributes and methods are also available on the response. For example you can set headers on the response in the standard way: response = Response()
response['Cache-Control'] = 'no-cache'
 .render() Signature: .render() As with any other TemplateResponse, this method is called to render the serialized data of the response into the final response content. When .render() is called, the response content will be set to the result of calling the .render(data, accepted_media_type, renderer_context) method on the accepted_renderer instance. You won't typically need to call .render() yourself, as it's handled by Django's standard response cycle. response.py
flask.json.jsonify(*args, **kwargs)  
Serialize data to JSON and wrap it in a Response with the application/json mimetype. Uses dumps() to serialize the data, but args and kwargs are treated as data rather than arguments to json.dumps().  Single argument: Treated as a single value. Multiple arguments: Treated as a list of values. jsonify(1, 2, 3) is the same as jsonify([1, 2, 3]). Keyword arguments: Treated as a dict of values. jsonify(data=data, errors=errors) is the same as jsonify({"data": data, "errors": errors}). Passing both arguments and keyword arguments is not allowed as it’s not clear what should happen.  from flask import jsonify

@app.route("/users/me")
def get_current_user():
    return jsonify(
        username=g.user.username,
        email=g.user.email,
        id=g.user.id,
    )
 Will return a JSON response like this: {
  "username": "admin",
  "email": "admin@localhost",
  "id": 42
}
 The default output omits indents and spaces after separators. In debug mode or if JSONIFY_PRETTYPRINT_REGULAR is True, the output will be formatted to be easier to read.  Changelog Changed in version 0.11: Added support for serializing top-level arrays. This introduces a security risk in ancient browsers. See JSON Security.   New in version 0.2.   Parameters 
 
args (Any) –  
kwargs (Any) –    Return type 
Response
set_data(value)  
Sets a new string as response. The value must be a string or bytes. If a string is set it’s encoded to the charset of the response (utf-8 by default).  Changelog New in version 0.9.   Parameters 
value (Union[bytes, str]) –   Return type 
None
set_data(value)  
Sets a new string as response. The value must be a string or bytes. If a string is set it’s encoded to the charset of the response (utf-8 by default).  Changelog New in version 0.9.   Parameters 
value (Union[bytes, str]) –   Return type 
None
def f_9262278(data):
    """Django response with JSON `data`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
parse(string, name='<string>')  
Divide the given string into examples and intervening text, and return them as a list of alternating Examples and strings. Line numbers for the Examples are 0-based. The optional argument name is a name identifying this string, and is only used for error messages.
get_examples(string, name='<string>')  
Extract all doctest examples from the given string, and return them as a list of Example objects. Line numbers are 0-based. The optional argument name is a name identifying this string, and is only used for error messages.
email.utils.unquote(str)  
Return a new string which is an unquoted version of str. If str ends and begins with double quotes, they are stripped off. Likewise if str ends and begins with angle brackets, they are stripped off.
get_doctest(string, globs, name, filename, lineno)  
Extract all doctest examples from the given string, and collect them into a DocTest object. globs, name, filename, and lineno are attributes for the new DocTest object. See the documentation for DocTest for more information.
DocTestFailure.example  
The Example that failed.
def f_17284947(example_str):
    """get all text that is not enclosed within square brackets in string `example_str`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
parse(string, name='<string>')  
Divide the given string into examples and intervening text, and return them as a list of alternating Examples and strings. Line numbers for the Examples are 0-based. The optional argument name is a name identifying this string, and is only used for error messages.
get_examples(string, name='<string>')  
Extract all doctest examples from the given string, and return them as a list of Example objects. Line numbers are 0-based. The optional argument name is a name identifying this string, and is only used for error messages.
email.utils.unquote(str)  
Return a new string which is an unquoted version of str. If str ends and begins with double quotes, they are stripped off. Likewise if str ends and begins with angle brackets, they are stripped off.
get_doctest(string, globs, name, filename, lineno)  
Extract all doctest examples from the given string, and collect them into a DocTest object. globs, name, filename, and lineno are attributes for the new DocTest object. See the documentation for DocTest for more information.
DocTestFailure.example  
The Example that failed.
def f_17284947(example_str):
    """Use a regex to get all text in a string `example_str` that is not surrounded by square brackets
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
Match.__getitem__(g)  
This is identical to m.group(g). This allows easier access to an individual group from a match: >>> m = re.match(r"(\w+) (\w+)", "Isaac Newton, physicist")
>>> m[0]       # The entire match
'Isaac Newton'
>>> m[1]       # The first parenthesized subgroup.
'Isaac'
>>> m[2]       # The second parenthesized subgroup.
'Newton'
  New in version 3.6.
Match.span([group])  
For a match m, return the 2-tuple (m.start(group), m.end(group)). Note that if group did not contribute to the match, this is (-1, -1). group defaults to zero, the entire match.
email.utils.unquote(str)  
Return a new string which is an unquoted version of str. If str ends and begins with double quotes, they are stripped off. Likewise if str ends and begins with angle brackets, they are stripped off.
Match.groups(default=None)  
Return a tuple containing all the subgroups of the match, from 1 up to however many groups are in the pattern. The default argument is used for groups that did not participate in the match; it defaults to None. For example: >>> m = re.match(r"(\d+)\.(\d+)", "24.1632")
>>> m.groups()
('24', '1632')
 If we make the decimal place and everything after it optional, not all groups might participate in the match. These groups will default to None unless the default argument is given: >>> m = re.match(r"(\d+)\.?(\d+)?", "24")
>>> m.groups()      # Second group defaults to None.
('24', None)
>>> m.groups('0')   # Now, the second group defaults to '0'.
('24', '0')
statichexify(match)[source]
def f_14182339():
    """get whatever is between parentheses as a single match, and any char outside as an individual match in string '(zyx)bc'
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
re.M  
re.MULTILINE  
When specified, the pattern character '^' matches at the beginning of the string and at the beginning of each line (immediately following each newline); and the pattern character '$' matches at the end of the string and at the end of each line (immediately preceding each newline). By default, '^' matches only at the beginning of the string, and '$' only at the end of the string and immediately before the newline (if any) at the end of the string. Corresponds to the inline flag (?m).
re.M  
re.MULTILINE  
When specified, the pattern character '^' matches at the beginning of the string and at the beginning of each line (immediately following each newline); and the pattern character '$' matches at the end of the string and at the end of each line (immediately preceding each newline). By default, '^' matches only at the beginning of the string, and '$' only at the end of the string and immediately before the newline (if any) at the end of the string. Corresponds to the inline flag (?m).
numpy.polynomial.chebyshev.chebgrid3d   polynomial.chebyshev.chebgrid3d(x, y, z, c)[source]
 
Evaluate a 3-D Chebyshev series on the Cartesian product of x, y, and z. This function returns the values:  \[p(a,b,c) = \sum_{i,j,k} c_{i,j,k} * T_i(a) * T_j(b) * T_k(c)\] where the points (a, b, c) consist of all triples formed by taking a from x, b from y, and c from z. The resulting points form a grid with x in the first dimension, y in the second, and z in the third. The parameters x, y, and z are converted to arrays only if they are tuples or a lists, otherwise they are treated as a scalars. In either case, either x, y, and z or their elements must support multiplication and addition both with themselves and with the elements of c. If c has fewer than three dimensions, ones are implicitly appended to its shape to make it 3-D. The shape of the result will be c.shape[3:] + x.shape + y.shape + z.shape.  Parameters 
 
x, y, zarray_like, compatible objects


The three dimensional series is evaluated at the points in the Cartesian product of x, y, and z. If x,`y`, or z is a list or tuple, it is first converted to an ndarray, otherwise it is left unchanged and, if it isn’t an ndarray, it is treated as a scalar.  
carray_like


Array of coefficients ordered so that the coefficients for terms of degree i,j are contained in c[i,j]. If c has dimension greater than two the remaining indices enumerate multiple sets of coefficients.    Returns 
 
valuesndarray, compatible object


The values of the two dimensional polynomial at points in the Cartesian product of x and y.      See also  
chebval, chebval2d, chebgrid2d, chebval3d

  Notes  New in version 1.7.0.
locale.YESEXPR  
Get a regular expression that can be used with the regex function to recognize a positive response to a yes/no question.  Note The expression is in the syntax suitable for the regex() function from the C library, which might differ from the syntax used in re.
pandas.Series.str.extract   Series.str.extract(pat, flags=0, expand=True)[source]
 
Extract capture groups in the regex pat as columns in a DataFrame. For each subject string in the Series, extract groups from the first match of regular expression pat.  Parameters 
 
pat:str


Regular expression pattern with capturing groups.  
flags:int, default 0 (no flags)


Flags from the re module, e.g. re.IGNORECASE, that modify regular expression matching for things like case, spaces, etc. For more details, see re.  
expand:bool, default True


If True, return DataFrame with one column per capture group. If False, return a Series/Index if there is one capture group or DataFrame if there are multiple capture groups.    Returns 
 DataFrame or Series or Index

A DataFrame with one row for each subject string, and one column for each group. Any capture group names in regular expression pat will be used for column names; otherwise capture group numbers will be used. The dtype of each result column is always object, even when no match is found. If expand=False and pat has only one capture group, then return a Series (if subject is a Series) or Index (if subject is an Index).      See also  extractall

Returns all matches (not just the first match).    Examples A pattern with two groups will return a DataFrame with two columns. Non-matches will be NaN. 
>>> s = pd.Series(['a1', 'b2', 'c3'])
>>> s.str.extract(r'([ab])(\d)')
    0    1
0    a    1
1    b    2
2  NaN  NaN
  A pattern may contain optional groups. 
>>> s.str.extract(r'([ab])?(\d)')
    0  1
0    a  1
1    b  2
2  NaN  3
  Named groups will become column names in the result. 
>>> s.str.extract(r'(?P<letter>[ab])(?P<digit>\d)')
letter digit
0      a     1
1      b     2
2    NaN   NaN
  A pattern with one group will return a DataFrame with one column if expand=True. 
>>> s.str.extract(r'[ab](\d)', expand=True)
    0
0    1
1    2
2  NaN
  A pattern with one group will return a Series if expand=False. 
>>> s.str.extract(r'[ab](\d)', expand=False)
0      1
1      2
2    NaN
dtype: object
def f_14182339():
    """match regex '\\((.*?)\\)|(\\w)' with string '(zyx)bc'
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
re.findall(pattern, string, flags=0)  
Return all non-overlapping matches of pattern in string, as a list of strings. The string is scanned left-to-right, and matches are returned in the order found. If one or more groups are present in the pattern, return a list of groups; this will be a list of tuples if the pattern has more than one group. Empty matches are included in the result.  Changed in version 3.7: Non-empty matches can now start just after a previous empty match.
pandas.Series.str.findall   Series.str.findall(pat, flags=0)[source]
 
Find all occurrences of pattern or regular expression in the Series/Index. Equivalent to applying re.findall() to all the elements in the Series/Index.  Parameters 
 
pat:str


Pattern or regular expression.  
flags:int, default 0


Flags from re module, e.g. re.IGNORECASE (default is 0, which means no flags).    Returns 
 Series/Index of lists of strings

All non-overlapping matches of pattern or regular expression in each string of this Series/Index.      See also  count

Count occurrences of pattern or regular expression in each string of the Series/Index.  extractall

For each string in the Series, extract groups from all matches of regular expression and return a DataFrame with one row for each match and one column for each group.  re.findall

The equivalent re function to all non-overlapping matches of pattern or regular expression in string, as a list of strings.    Examples 
>>> s = pd.Series(['Lion', 'Monkey', 'Rabbit'])
  The search for the pattern ‘Monkey’ returns one match: 
>>> s.str.findall('Monkey')
0          []
1    [Monkey]
2          []
dtype: object
  On the other hand, the search for the pattern ‘MONKEY’ doesn’t return any match: 
>>> s.str.findall('MONKEY')
0    []
1    []
2    []
dtype: object
  Flags can be added to the pattern or regular expression. For instance, to find the pattern ‘MONKEY’ ignoring the case: 
>>> import re
>>> s.str.findall('MONKEY', flags=re.IGNORECASE)
0          []
1    [Monkey]
2          []
dtype: object
  When the pattern matches more than one string in the Series, all matches are returned: 
>>> s.str.findall('on')
0    [on]
1    [on]
2      []
dtype: object
  Regular expressions are supported too. For instance, the search for all the strings ending with the word ‘on’ is shown next: 
>>> s.str.findall('on$')
0    [on]
1      []
2      []
dtype: object
  If the pattern is found more than once in the same string, then a list of multiple strings is returned: 
>>> s.str.findall('b')
0        []
1        []
2    [b, b]
dtype: object
Pattern.findall(string[, pos[, endpos]])  
Similar to the findall() function, using the compiled pattern, but also accepts optional pos and endpos parameters that limit the search region like for search().
re.M  
re.MULTILINE  
When specified, the pattern character '^' matches at the beginning of the string and at the beginning of each line (immediately following each newline); and the pattern character '$' matches at the end of the string and at the end of each line (immediately preceding each newline). By default, '^' matches only at the beginning of the string, and '$' only at the end of the string and immediately before the newline (if any) at the end of the string. Corresponds to the inline flag (?m).
tf.strings.regex_full_match       View source on GitHub    Check if the input matches the regex pattern.  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.strings.regex_full_match  
tf.strings.regex_full_match(
    input, pattern, name=None
)
 The input is a string tensor of any shape. The pattern is a scalar string tensor which is applied to every element of the input tensor. The boolean values (True or False) of the output tensor indicate if the input matches the regex pattern provided. The pattern follows the re2 syntax (https://github.com/google/re2/wiki/Syntax) Examples: 
tf.strings.regex_full_match(["TF lib", "lib TF"], ".*lib$")
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True, False])>
tf.strings.regex_full_match(["TF lib", "lib TF"], ".*TF$")
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>

 


 Args
  input   A Tensor of type string. A string tensor of the text to be processed.  
  pattern   A Tensor of type string. A scalar string tensor containing the regular expression to match the input.  
  name   A name for the operation (optional).   
 


 Returns   A Tensor of type bool.
def f_14182339():
    """match multiple regex patterns with the alternation operator `|` in a string `(zyx)bc`
    """
    return  
 --------------------

def f_7126916(elements):
    """formate each string cin list `elements` into pattern '%{0}%'
    """
     
 --------------------
Please refer to the following documentation to generate the code:
coroutine asyncio.create_subprocess_shell(cmd, stdin=None, stdout=None, stderr=None, loop=None, limit=None, **kwds)  
Run the cmd shell command. The limit argument sets the buffer limit for StreamReader wrappers for Process.stdout and Process.stderr (if subprocess.PIPE is passed to stdout and stderr arguments). Return a Process instance. See the documentation of loop.subprocess_shell() for other parameters.  Important It is the application’s responsibility to ensure that all whitespace and special characters are quoted appropriately to avoid shell injection vulnerabilities. The shlex.quote() function can be used to properly escape whitespace and special shell characters in strings that are going to be used to construct shell commands.   Deprecated since version 3.8, will be removed in version 3.10: The loop parameter.
coroutine asyncio.create_subprocess_exec(program, *args, stdin=None, stdout=None, stderr=None, loop=None, limit=None, **kwds)  
Create a subprocess. The limit argument sets the buffer limit for StreamReader wrappers for Process.stdout and Process.stderr (if subprocess.PIPE is passed to stdout and stderr arguments). Return a Process instance. See the documentation of loop.subprocess_exec() for other parameters.  Deprecated since version 3.8, will be removed in version 3.10: The loop parameter.
test.support.script_helper.spawn_python(*args, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, **kw)  
Run a Python subprocess with the given arguments. kw is extra keyword args to pass to subprocess.Popen(). Returns a subprocess.Popen object.
start([initializer[, initargs]])  
Start a subprocess to start the manager. If initializer is not None then the subprocess will call initializer(*initargs) when it starts.
Subprocesses Source code: Lib/asyncio/subprocess.py, Lib/asyncio/base_subprocess.py This section describes high-level async/await asyncio APIs to create and manage subprocesses. Here’s an example of how asyncio can run a shell command and obtain its result: import asyncio

async def run(cmd):
    proc = await asyncio.create_subprocess_shell(
        cmd,
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.PIPE)

    stdout, stderr = await proc.communicate()

    print(f'[{cmd!r} exited with {proc.returncode}]')
    if stdout:
        print(f'[stdout]\n{stdout.decode()}')
    if stderr:
        print(f'[stderr]\n{stderr.decode()}')

asyncio.run(run('ls /zzz'))
 will print: ['ls /zzz' exited with 1]
[stderr]
ls: /zzz: No such file or directory
 Because all asyncio subprocess functions are asynchronous and asyncio provides many tools to work with such functions, it is easy to execute and monitor multiple subprocesses in parallel. It is indeed trivial to modify the above example to run several commands simultaneously: async def main():
    await asyncio.gather(
        run('ls /zzz'),
        run('sleep 1; echo "hello"'))

asyncio.run(main())
 See also the Examples subsection. Creating Subprocesses  
coroutine asyncio.create_subprocess_exec(program, *args, stdin=None, stdout=None, stderr=None, loop=None, limit=None, **kwds)  
Create a subprocess. The limit argument sets the buffer limit for StreamReader wrappers for Process.stdout and Process.stderr (if subprocess.PIPE is passed to stdout and stderr arguments). Return a Process instance. See the documentation of loop.subprocess_exec() for other parameters.  Deprecated since version 3.8, will be removed in version 3.10: The loop parameter.  
  
coroutine asyncio.create_subprocess_shell(cmd, stdin=None, stdout=None, stderr=None, loop=None, limit=None, **kwds)  
Run the cmd shell command. The limit argument sets the buffer limit for StreamReader wrappers for Process.stdout and Process.stderr (if subprocess.PIPE is passed to stdout and stderr arguments). Return a Process instance. See the documentation of loop.subprocess_shell() for other parameters.  Important It is the application’s responsibility to ensure that all whitespace and special characters are quoted appropriately to avoid shell injection vulnerabilities. The shlex.quote() function can be used to properly escape whitespace and special shell characters in strings that are going to be used to construct shell commands.   Deprecated since version 3.8, will be removed in version 3.10: The loop parameter.  
  Note Subprocesses are available for Windows if a ProactorEventLoop is used. See Subprocess Support on Windows for details.   See also asyncio also has the following low-level APIs to work with subprocesses: loop.subprocess_exec(), loop.subprocess_shell(), loop.connect_read_pipe(), loop.connect_write_pipe(), as well as the Subprocess Transports and Subprocess Protocols.  Constants  
asyncio.subprocess.PIPE  
Can be passed to the stdin, stdout or stderr parameters. If PIPE is passed to stdin argument, the Process.stdin attribute will point to a StreamWriter instance. If PIPE is passed to stdout or stderr arguments, the Process.stdout and Process.stderr attributes will point to StreamReader instances. 
  
asyncio.subprocess.STDOUT  
Special value that can be used as the stderr argument and indicates that standard error should be redirected into standard output. 
  
asyncio.subprocess.DEVNULL  
Special value that can be used as the stdin, stdout or stderr argument to process creation functions. It indicates that the special file os.devnull will be used for the corresponding subprocess stream. 
 Interacting with Subprocesses Both create_subprocess_exec() and create_subprocess_shell() functions return instances of the Process class. Process is a high-level wrapper that allows communicating with subprocesses and watching for their completion.  
class asyncio.subprocess.Process  
An object that wraps OS processes created by the create_subprocess_exec() and create_subprocess_shell() functions. This class is designed to have a similar API to the subprocess.Popen class, but there are some notable differences:  unlike Popen, Process instances do not have an equivalent to the poll() method; the communicate() and wait() methods don’t have a timeout parameter: use the wait_for() function; the Process.wait() method is asynchronous, whereas subprocess.Popen.wait() method is implemented as a blocking busy loop; the universal_newlines parameter is not supported.  This class is not thread safe. See also the Subprocess and Threads section.  
coroutine wait()  
Wait for the child process to terminate. Set and return the returncode attribute.  Note This method can deadlock when using stdout=PIPE or stderr=PIPE and the child process generates so much output that it blocks waiting for the OS pipe buffer to accept more data. Use the communicate() method when using pipes to avoid this condition.  
  
coroutine communicate(input=None)  
Interact with process:  send data to stdin (if input is not None); read data from stdout and stderr, until EOF is reached; wait for process to terminate.  The optional input argument is the data (bytes object) that will be sent to the child process. Return a tuple (stdout_data, stderr_data). If either BrokenPipeError or ConnectionResetError exception is raised when writing input into stdin, the exception is ignored. This condition occurs when the process exits before all data are written into stdin. If it is desired to send data to the process’ stdin, the process needs to be created with stdin=PIPE. Similarly, to get anything other than None in the result tuple, the process has to be created with stdout=PIPE and/or stderr=PIPE arguments. Note, that the data read is buffered in memory, so do not use this method if the data size is large or unlimited. 
  
send_signal(signal)  
Sends the signal signal to the child process.  Note On Windows, SIGTERM is an alias for terminate(). CTRL_C_EVENT and CTRL_BREAK_EVENT can be sent to processes started with a creationflags parameter which includes CREATE_NEW_PROCESS_GROUP.  
  
terminate()  
Stop the child process. On POSIX systems this method sends signal.SIGTERM to the child process. On Windows the Win32 API function TerminateProcess() is called to stop the child process. 
  
kill()  
Kill the child process. On POSIX systems this method sends SIGKILL to the child process. On Windows this method is an alias for terminate(). 
  
stdin  
Standard input stream (StreamWriter) or None if the process was created with stdin=None. 
  
stdout  
Standard output stream (StreamReader) or None if the process was created with stdout=None. 
  
stderr  
Standard error stream (StreamReader) or None if the process was created with stderr=None. 
  Warning Use the communicate() method rather than process.stdin.write(), await process.stdout.read() or await process.stderr.read. This avoids deadlocks due to streams pausing reading or writing and blocking the child process.   
pid  
Process identification number (PID). Note that for processes created by the create_subprocess_shell() function, this attribute is the PID of the spawned shell. 
  
returncode  
Return code of the process when it exits. A None value indicates that the process has not terminated yet. A negative value -N indicates that the child was terminated by signal N (POSIX only). 
 
 Subprocess and Threads Standard asyncio event loop supports running subprocesses from different threads by default. On Windows subprocesses are provided by ProactorEventLoop only (default), SelectorEventLoop has no subprocess support. On UNIX child watchers are used for subprocess finish waiting, see Process Watchers for more info.  Changed in version 3.8: UNIX switched to use ThreadedChildWatcher for spawning subprocesses from different threads without any limitation. Spawning a subprocess with inactive current child watcher raises RuntimeError.  Note that alternative event loop implementations might have own limitations; please refer to their documentation.  See also The Concurrency and multithreading in asyncio section.  Examples An example using the Process class to control a subprocess and the StreamReader class to read from its standard output. The subprocess is created by the create_subprocess_exec() function: import asyncio
import sys

async def get_date():
    code = 'import datetime; print(datetime.datetime.now())'

    # Create the subprocess; redirect the standard output
    # into a pipe.
    proc = await asyncio.create_subprocess_exec(
        sys.executable, '-c', code,
        stdout=asyncio.subprocess.PIPE)

    # Read one line of output.
    data = await proc.stdout.readline()
    line = data.decode('ascii').rstrip()

    # Wait for the subprocess exit.
    await proc.wait()
    return line

date = asyncio.run(get_date())
print(f"Current date: {date}")
 See also the same example written using low-level APIs.
def f_3595685():
    """Open a background process 'background-process' with arguments 'arguments'
    """
    return  
 --------------------

def f_18453566(mydict, mykeys):
    """get list of values from dictionary 'mydict' w.r.t. list of keys 'mykeys'
    """
    return  
 --------------------

def f_12692135():
    """convert list `[('Name', 'Joe'), ('Age', 22)]` into a dictionary
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
statistics.mean(data)  
Return the sample arithmetic mean of data which can be a sequence or iterable. The arithmetic mean is the sum of the data divided by the number of data points. It is commonly called “the average”, although it is only one of many different mathematical averages. It is a measure of the central location of the data. If data is empty, StatisticsError will be raised. Some examples of use: >>> mean([1, 2, 3, 4, 4])
2.8
>>> mean([-1.0, 2.5, 3.25, 5.75])
2.625

>>> from fractions import Fraction as F
>>> mean([F(3, 7), F(1, 21), F(5, 3), F(1, 3)])
Fraction(13, 21)

>>> from decimal import Decimal as D
>>> mean([D("0.5"), D("0.75"), D("0.625"), D("0.375")])
Decimal('0.5625')
  Note The mean is strongly affected by outliers and is not a robust estimator for central location: the mean is not necessarily a typical example of the data points. For more robust measures of central location, see median() and mode(). The sample mean gives an unbiased estimate of the true population mean, so that when taken on average over all the possible samples, mean(sample) converges on the true mean of the entire population. If data represents the entire population rather than a sample, then mean(data) is equivalent to calculating the true population mean μ.
class RegrAvgX(y, x, filter=None, default=None)  
Returns the average of the independent variable (sum(x)/N) as a float, or default if there aren’t any matching rows.
class RegrAvgY(y, x, filter=None, default=None)  
Returns the average of the dependent variable (sum(y)/N) as a float, or default if there aren’t any matching rows.
statistics.fmean(data)  
Convert data to floats and compute the arithmetic mean. This runs faster than the mean() function and it always returns a float. The data may be a sequence or iterable. If the input dataset is empty, raises a StatisticsError. >>> fmean([3.5, 4.0, 5.25])
4.25
  New in version 3.8.
statistics.median(data)  
Return the median (middle value) of numeric data, using the common “mean of middle two” method. If data is empty, StatisticsError is raised. data can be a sequence or iterable. The median is a robust measure of central location and is less affected by the presence of outliers. When the number of data points is odd, the middle data point is returned: >>> median([1, 3, 5])
3
 When the number of data points is even, the median is interpolated by taking the average of the two middle values: >>> median([1, 3, 5, 7])
4.0
 This is suited for when your data is discrete, and you don’t mind that the median may not be an actual data point. If the data is ordinal (supports order operations) but not numeric (doesn’t support addition), consider using median_low() or median_high() instead.
def f_14401047(data):
    """average each two columns of array `data`
    """
    return  
 --------------------

def f_18886596(s):
    """double backslash escape all double quotes in string `s`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
shlex.split(s, comments=False, posix=True)  
Split the string s using shell-like syntax. If comments is False (the default), the parsing of comments in the given string will be disabled (setting the commenters attribute of the shlex instance to the empty string). This function operates in POSIX mode by default, but uses non-POSIX mode if the posix argument is false.  Note Since the split() function instantiates a shlex instance, passing None for s will read the string to split from standard input.   Deprecated since version 3.9: Passing None for s will raise an exception in future Python versions.
str.split(sep=None, maxsplit=-1)  
Return a list of the words in the string, using sep as the delimiter string. If maxsplit is given, at most maxsplit splits are done (thus, the list will have at most maxsplit+1 elements). If maxsplit is not specified or -1, then there is no limit on the number of splits (all possible splits are made). If sep is given, consecutive delimiters are not grouped together and are deemed to delimit empty strings (for example, '1,,2'.split(',') returns ['1', '', '2']). The sep argument may consist of multiple characters (for example, '1<>2<>3'.split('<>') returns ['1', '2', '3']). Splitting an empty string with a specified separator returns ['']. For example: >>> '1,2,3'.split(',')
['1', '2', '3']
>>> '1,2,3'.split(',', maxsplit=1)
['1', '2,3']
>>> '1,2,,3,'.split(',')
['1', '2', '', '3', '']
 If sep is not specified or is None, a different splitting algorithm is applied: runs of consecutive whitespace are regarded as a single separator, and the result will contain no empty strings at the start or end if the string has leading or trailing whitespace. Consequently, splitting an empty string or a string consisting of just whitespace with a None separator returns []. For example: >>> '1 2 3'.split()
['1', '2', '3']
>>> '1 2 3'.split(maxsplit=1)
['1', '2 3']
>>> '   1   2   3   '.split()
['1', '2', '3']
HttpRequest.META  
A dictionary containing all available HTTP headers. Available headers depend on the client and server, but here are some examples:  
CONTENT_LENGTH – The length of the request body (as a string). 
CONTENT_TYPE – The MIME type of the request body. 
HTTP_ACCEPT – Acceptable content types for the response. 
HTTP_ACCEPT_ENCODING – Acceptable encodings for the response. 
HTTP_ACCEPT_LANGUAGE – Acceptable languages for the response. 
HTTP_HOST – The HTTP Host header sent by the client. 
HTTP_REFERER – The referring page, if any. 
HTTP_USER_AGENT – The client’s user-agent string. 
QUERY_STRING – The query string, as a single (unparsed) string. 
REMOTE_ADDR – The IP address of the client. 
REMOTE_HOST – The hostname of the client. 
REMOTE_USER – The user authenticated by the web server, if any. 
REQUEST_METHOD – A string such as "GET" or "POST". 
SERVER_NAME – The hostname of the server. 
SERVER_PORT – The port of the server (as a string).  With the exception of CONTENT_LENGTH and CONTENT_TYPE, as given above, any HTTP headers in the request are converted to META keys by converting all characters to uppercase, replacing any hyphens with underscores and adding an HTTP_ prefix to the name. So, for example, a header called X-Bender would be mapped to the META key HTTP_X_BENDER. Note that runserver strips all headers with underscores in the name, so you won’t see them in META. This prevents header-spoofing based on ambiguity between underscores and dashes both being normalizing to underscores in WSGI environment variables. It matches the behavior of web servers like Nginx and Apache 2.4+. HttpRequest.headers is a simpler way to access all HTTP-prefixed headers, plus CONTENT_LENGTH and CONTENT_TYPE.
flask.escape()  
Replace the characters &, <, >, ', and " in the string with HTML-safe sequences. Use this if you need to display text that might contain such characters in HTML. If the object has an __html__ method, it is called and the return value is assumed to already be safe for HTML.  Parameters 
s – An object to be converted to a string and escaped.  Returns 
A Markup string with the escaped text.
str.rsplit(sep=None, maxsplit=-1)  
Return a list of the words in the string, using sep as the delimiter string. If maxsplit is given, at most maxsplit splits are done, the rightmost ones. If sep is not specified or None, any whitespace string is a separator. Except for splitting from the right, rsplit() behaves like split() which is described in detail below.
def f_5932059(s):
    """split a string `s` into a list of words and whitespace
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
matplotlib.axes.Axes.stackplot   Axes.stackplot(x, *args, labels=(), colors=None, baseline='zero', data=None, **kwargs)[source]
 
Draw a stacked area plot.  Parameters 
 
x(N,) array-like


y(M, N) array-like


The data is assumed to be unstacked. Each of the following calls is legal: stackplot(x, y)           # where y has shape (M, N)
stackplot(x, y1, y2, y3)  # where y1, y2, y3, y4 have length N
  
baseline{'zero', 'sym', 'wiggle', 'weighted_wiggle'}


Method used to calculate the baseline:  
'zero': Constant zero baseline, i.e. a simple stacked plot. 
'sym': Symmetric around zero and is sometimes called 'ThemeRiver'. 
'wiggle': Minimizes the sum of the squared slopes. 
'weighted_wiggle': Does the same but weights to account for size of each layer. It is also called 'Streamgraph'-layout. More details can be found at http://leebyron.com/streamgraph/.   
labelslist of str, optional


A sequence of labels to assign to each data series. If unspecified, then no labels will be applied to artists.  
colorslist of color, optional


A sequence of colors to be cycled through and used to color the stacked areas. The sequence need not be exactly the same length as the number of provided y, in which case the colors will repeat from the beginning. If not specified, the colors from the Axes property cycle will be used.  
dataindexable object, optional


If given, all parameters also accept a string s, which is interpreted as data[s] (unless this raises an exception).  **kwargs

All other keyword arguments are passed to Axes.fill_between.    Returns 
 list of PolyCollection


A list of PolyCollection instances, one for each element in the stacked area plot.     
  Examples using matplotlib.axes.Axes.stackplot
 
   Stackplots and streamgraphs
matplotlib.pyplot.stackplot   matplotlib.pyplot.stackplot(x, *args, labels=(), colors=None, baseline='zero', data=None, **kwargs)[source]
 
Draw a stacked area plot.  Parameters 
 
x(N,) array-like


y(M, N) array-like


The data is assumed to be unstacked. Each of the following calls is legal: stackplot(x, y)           # where y has shape (M, N)
stackplot(x, y1, y2, y3)  # where y1, y2, y3, y4 have length N
  
baseline{'zero', 'sym', 'wiggle', 'weighted_wiggle'}


Method used to calculate the baseline:  
'zero': Constant zero baseline, i.e. a simple stacked plot. 
'sym': Symmetric around zero and is sometimes called 'ThemeRiver'. 
'wiggle': Minimizes the sum of the squared slopes. 
'weighted_wiggle': Does the same but weights to account for size of each layer. It is also called 'Streamgraph'-layout. More details can be found at http://leebyron.com/streamgraph/.   
labelslist of str, optional


A sequence of labels to assign to each data series. If unspecified, then no labels will be applied to artists.  
colorslist of color, optional


A sequence of colors to be cycled through and used to color the stacked areas. The sequence need not be exactly the same length as the number of provided y, in which case the colors will repeat from the beginning. If not specified, the colors from the Axes property cycle will be used.  
dataindexable object, optional


If given, all parameters also accept a string s, which is interpreted as data[s] (unless this raises an exception).  **kwargs

All other keyword arguments are passed to Axes.fill_between.    Returns 
 list of PolyCollection


A list of PolyCollection instances, one for each element in the stacked area plot.
pandas.DataFrame.plot.bar   DataFrame.plot.bar(x=None, y=None, **kwargs)[source]
 
Vertical bar plot. A bar plot is a plot that presents categorical data with rectangular bars with lengths proportional to the values that they represent. A bar plot shows comparisons among discrete categories. One axis of the plot shows the specific categories being compared, and the other axis represents a measured value.  Parameters 
 
x:label or position, optional


Allows plotting of one column versus another. If not specified, the index of the DataFrame is used.  
y:label or position, optional


Allows plotting of one column versus another. If not specified, all numerical columns are used.  
color:str, array-like, or dict, optional


The color for each of the DataFrame’s columns. Possible values are:  
 A single color string referred to by name, RGB or RGBA code,

for instance ‘red’ or ‘#a98d19’.    
 A sequence of color strings referred to by name, RGB or RGBA

code, which will be used for each column recursively. For instance [‘green’,’yellow’] each column’s bar will be filled in green or yellow, alternatively. If there is only a single column to be plotted, then only the first color from the color list will be used.    
 A dict of the form {column name:color}, so that each column will be


colored accordingly. For example, if your columns are called a and b, then passing {‘a’: ‘green’, ‘b’: ‘red’} will color bars for column a in green and bars for column b in red.      New in version 1.1.0.   **kwargs

Additional keyword arguments are documented in DataFrame.plot().    Returns 
 matplotlib.axes.Axes or np.ndarray of them

An ndarray is returned with one matplotlib.axes.Axes per column when subplots=True.      See also  DataFrame.plot.barh

Horizontal bar plot.  DataFrame.plot

Make plots of a DataFrame.  matplotlib.pyplot.bar

Make a bar plot with matplotlib.    Examples Basic plot. 
>>> df = pd.DataFrame({'lab':['A', 'B', 'C'], 'val':[10, 30, 20]})
>>> ax = df.plot.bar(x='lab', y='val', rot=0)
     Plot a whole dataframe to a bar plot. Each column is assigned a distinct color, and each row is nested in a group along the horizontal axis. 
>>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]
>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]
>>> index = ['snail', 'pig', 'elephant',
...          'rabbit', 'giraffe', 'coyote', 'horse']
>>> df = pd.DataFrame({'speed': speed,
...                    'lifespan': lifespan}, index=index)
>>> ax = df.plot.bar(rot=0)
     Plot stacked bar charts for the DataFrame 
>>> ax = df.plot.bar(stacked=True)
     Instead of nesting, the figure can be split by column with subplots=True. In this case, a numpy.ndarray of matplotlib.axes.Axes are returned. 
>>> axes = df.plot.bar(rot=0, subplots=True)
>>> axes[1].legend(loc=2)  
     If you don’t like the default colours, you can specify how you’d like each column to be colored. 
>>> axes = df.plot.bar(
...     rot=0, subplots=True, color={"speed": "red", "lifespan": "green"}
... )
>>> axes[1].legend(loc=2)  
     Plot a single column. 
>>> ax = df.plot.bar(y='speed', rot=0)
     Plot only selected categories for the DataFrame. 
>>> ax = df.plot.bar(x='lifespan', rot=0)
pandas.DataFrame.plot.barh   DataFrame.plot.barh(x=None, y=None, **kwargs)[source]
 
Make a horizontal bar plot. A horizontal bar plot is a plot that presents quantitative data with rectangular bars with lengths proportional to the values that they represent. A bar plot shows comparisons among discrete categories. One axis of the plot shows the specific categories being compared, and the other axis represents a measured value.  Parameters 
 
x:label or position, optional


Allows plotting of one column versus another. If not specified, the index of the DataFrame is used.  
y:label or position, optional


Allows plotting of one column versus another. If not specified, all numerical columns are used.  
color:str, array-like, or dict, optional


The color for each of the DataFrame’s columns. Possible values are:  
 A single color string referred to by name, RGB or RGBA code,

for instance ‘red’ or ‘#a98d19’.    
 A sequence of color strings referred to by name, RGB or RGBA

code, which will be used for each column recursively. For instance [‘green’,’yellow’] each column’s bar will be filled in green or yellow, alternatively. If there is only a single column to be plotted, then only the first color from the color list will be used.    
 A dict of the form {column name:color}, so that each column will be


colored accordingly. For example, if your columns are called a and b, then passing {‘a’: ‘green’, ‘b’: ‘red’} will color bars for column a in green and bars for column b in red.      New in version 1.1.0.   **kwargs

Additional keyword arguments are documented in DataFrame.plot().    Returns 
 matplotlib.axes.Axes or np.ndarray of them

An ndarray is returned with one matplotlib.axes.Axes per column when subplots=True.      See also  DataFrame.plot.bar

Vertical bar plot.  DataFrame.plot

Make plots of DataFrame using matplotlib.  matplotlib.axes.Axes.bar

Plot a vertical bar plot using matplotlib.    Examples Basic example 
>>> df = pd.DataFrame({'lab': ['A', 'B', 'C'], 'val': [10, 30, 20]})
>>> ax = df.plot.barh(x='lab', y='val')
     Plot a whole DataFrame to a horizontal bar plot 
>>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]
>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]
>>> index = ['snail', 'pig', 'elephant',
...          'rabbit', 'giraffe', 'coyote', 'horse']
>>> df = pd.DataFrame({'speed': speed,
...                    'lifespan': lifespan}, index=index)
>>> ax = df.plot.barh()
     Plot stacked barh charts for the DataFrame 
>>> ax = df.plot.barh(stacked=True)
     We can specify colors for each column 
>>> ax = df.plot.barh(color={"speed": "red", "lifespan": "green"})
     Plot a column of the DataFrame to a horizontal bar plot 
>>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]
>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]
>>> index = ['snail', 'pig', 'elephant',
...          'rabbit', 'giraffe', 'coyote', 'horse']
>>> df = pd.DataFrame({'speed': speed,
...                    'lifespan': lifespan}, index=index)
>>> ax = df.plot.barh(y='speed')
     Plot DataFrame versus the desired column 
>>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]
>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]
>>> index = ['snail', 'pig', 'elephant',
...          'rabbit', 'giraffe', 'coyote', 'horse']
>>> df = pd.DataFrame({'speed': speed,
...                    'lifespan': lifespan}, index=index)
>>> ax = df.plot.barh(x='lifespan')
pandas.Series.plot.bar   Series.plot.bar(x=None, y=None, **kwargs)[source]
 
Vertical bar plot. A bar plot is a plot that presents categorical data with rectangular bars with lengths proportional to the values that they represent. A bar plot shows comparisons among discrete categories. One axis of the plot shows the specific categories being compared, and the other axis represents a measured value.  Parameters 
 
x:label or position, optional


Allows plotting of one column versus another. If not specified, the index of the DataFrame is used.  
y:label or position, optional


Allows plotting of one column versus another. If not specified, all numerical columns are used.  
color:str, array-like, or dict, optional


The color for each of the DataFrame’s columns. Possible values are:  
 A single color string referred to by name, RGB or RGBA code,

for instance ‘red’ or ‘#a98d19’.    
 A sequence of color strings referred to by name, RGB or RGBA

code, which will be used for each column recursively. For instance [‘green’,’yellow’] each column’s bar will be filled in green or yellow, alternatively. If there is only a single column to be plotted, then only the first color from the color list will be used.    
 A dict of the form {column name:color}, so that each column will be


colored accordingly. For example, if your columns are called a and b, then passing {‘a’: ‘green’, ‘b’: ‘red’} will color bars for column a in green and bars for column b in red.      New in version 1.1.0.   **kwargs

Additional keyword arguments are documented in DataFrame.plot().    Returns 
 matplotlib.axes.Axes or np.ndarray of them

An ndarray is returned with one matplotlib.axes.Axes per column when subplots=True.      See also  DataFrame.plot.barh

Horizontal bar plot.  DataFrame.plot

Make plots of a DataFrame.  matplotlib.pyplot.bar

Make a bar plot with matplotlib.    Examples Basic plot. 
>>> df = pd.DataFrame({'lab':['A', 'B', 'C'], 'val':[10, 30, 20]})
>>> ax = df.plot.bar(x='lab', y='val', rot=0)
     Plot a whole dataframe to a bar plot. Each column is assigned a distinct color, and each row is nested in a group along the horizontal axis. 
>>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]
>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]
>>> index = ['snail', 'pig', 'elephant',
...          'rabbit', 'giraffe', 'coyote', 'horse']
>>> df = pd.DataFrame({'speed': speed,
...                    'lifespan': lifespan}, index=index)
>>> ax = df.plot.bar(rot=0)
     Plot stacked bar charts for the DataFrame 
>>> ax = df.plot.bar(stacked=True)
     Instead of nesting, the figure can be split by column with subplots=True. In this case, a numpy.ndarray of matplotlib.axes.Axes are returned. 
>>> axes = df.plot.bar(rot=0, subplots=True)
>>> axes[1].legend(loc=2)  
     If you don’t like the default colours, you can specify how you’d like each column to be colored. 
>>> axes = df.plot.bar(
...     rot=0, subplots=True, color={"speed": "red", "lifespan": "green"}
... )
>>> axes[1].legend(loc=2)  
     Plot a single column. 
>>> ax = df.plot.bar(y='speed', rot=0)
     Plot only selected categories for the DataFrame. 
>>> ax = df.plot.bar(x='lifespan', rot=0)
def f_9938130(df):
    """plotting stacked barplots on a panda data frame `df`
    """
    return  
 --------------------

def f_35945473(myDictionary):
    """reverse the keys and values in a dictionary `myDictionary`
    """
    return  
 --------------------

def f_30729735(myList):
    """finding the index of elements containing substring 'how' and 'what' in a list of strings 'myList'.
    """
    return  
 --------------------

def f_1303243(obj):
    """check if object `obj` is a string
    """
    return  
 --------------------

def f_1303243(o):
    """check if object `o` is a string
    """
    return  
 --------------------

def f_1303243(o):
    """check if object `o` is a string
    """
    return  
 --------------------

def f_1303243(obj_to_test):
    """check if `obj_to_test` is a string
    """
    return  
 --------------------

def f_8177079(list1, list2):
    """append list `list1` to `list2`
    """
     
 --------------------

def f_8177079(mylog, list1):
    """append list `mylog` to `list1`
    """
     
 --------------------

def f_8177079(a, c):
    """append list `a` to `c`
    """
     
 --------------------

def f_8177079(mylog, list1):
    """append items in list `mylog` to `list1`
    """
     
 --------------------

def f_4126227(a, b):
    """append a tuple of elements from list `a` with indexes '[0][0] [0][2]' to list `b`
    """
     
 --------------------
Please refer to the following documentation to generate the code:
secret_key  
If a secret key is set, cryptographic components can use this to sign cookies and other things. Set this to a complex random value when you want to use the secure cookie for instance. This attribute can also be configured from the config with the SECRET_KEY configuration key. Defaults to None.
SECRET_KEY  
A secret key that will be used for securely signing the session cookie and can be used for any other security related needs by extensions or your application. It should be a long random bytes or str. For example, copy the output of this to your config: $ python -c 'import os; print(os.urandom(16))'
b'_5#y2L"F4Q8z\n\xec]/'
 Do not reveal the secret key when posting questions or committing code. Default: None
salt = 'cookie-session'  
the salt that should be applied on top of the secret key for the signing of cookie based sessions.
AppConfig.ready()  
Subclasses can override this method to perform initialization tasks such as registering signals. It is called as soon as the registry is fully populated. Although you can’t import models at the module-level where AppConfig classes are defined, you can import them in ready(), using either an import statement or get_model(). If you’re registering model signals, you can refer to the sender by its string label instead of using the model class itself. Example: from django.apps import AppConfig
from django.db.models.signals import pre_save


class RockNRollConfig(AppConfig):
    # ...

    def ready(self):
        # importing model classes
        from .models import MyModel  # or...
        MyModel = self.get_model('MyModel')

        # registering signals with the model's string label
        pre_save.connect(receiver, sender='app_label.MyModel')
  Warning Although you can access model classes as described above, avoid interacting with the database in your ready() implementation. This includes model methods that execute queries (save(), delete(), manager methods etc.), and also raw SQL queries via django.db.connection. Your ready() method will run during startup of every management command. For example, even though the test database configuration is separate from the production settings, manage.py test would still execute some queries against your production database!   Note In the usual initialization process, the ready method is only called once by Django. But in some corner cases, particularly in tests which are fiddling with installed applications, ready might be called more than once. In that case, either write idempotent methods, or put a flag on your AppConfig classes to prevent re-running code which should be executed exactly one time.
ENV  
What environment the app is running in. Flask and extensions may enable behaviors based on the environment, such as enabling debug mode. The env attribute maps to this config key. This is set by the FLASK_ENV environment variable and may not behave as expected if set in code. Do not enable development when deploying in production. Default: 'production'  Changelog New in version 1.0.
def f_34902378(app):
    """Initialize `SECRET_KEY` in flask config with `Your_secret_string `
    """
     
 --------------------
Please refer to the following documentation to generate the code:
torch.column_stack(tensors, *, out=None) → Tensor  
Creates a new tensor by horizontally stacking the tensors in tensors. Equivalent to torch.hstack(tensors), except each zero or one dimensional tensor t in tensors is first reshaped into a (t.numel(), 1) column before being stacked horizontally.  Parameters 
tensors (sequence of Tensors) – sequence of tensors to concatenate  Keyword Arguments 
out (Tensor, optional) – the output tensor.   Example: >>> a = torch.tensor([1, 2, 3])
>>> b = torch.tensor([4, 5, 6])
>>> torch.column_stack((a, b))
tensor([[1, 4],
    [2, 5],
    [3, 6]])
>>> a = torch.arange(5)
>>> b = torch.arange(10).reshape(5, 2)
>>> torch.column_stack((a, b, b))
tensor([[0, 0, 1, 0, 1],
        [1, 2, 3, 2, 3],
        [2, 4, 5, 4, 5],
        [3, 6, 7, 6, 7],
        [4, 8, 9, 8, 9]])
pandas.wide_to_long   pandas.wide_to_long(df, stubnames, i, j, sep='', suffix='\\d+')[source]
 
Unpivot a DataFrame from wide to long format. Less flexible but more user-friendly than melt. With stubnames [‘A’, ‘B’], this function expects to find one or more group of columns with format A-suffix1, A-suffix2,…, B-suffix1, B-suffix2,… You specify what you want to call this suffix in the resulting long format with j (for example j=’year’) Each row of these wide variables are assumed to be uniquely identified by i (can be a single column name or a list of column names) All remaining variables in the data frame are left intact.  Parameters 
 
df:DataFrame


The wide-format DataFrame.  
stubnames:str or list-like


The stub name(s). The wide format variables are assumed to start with the stub names.  
i:str or list-like


Column(s) to use as id variable(s).  
j:str


The name of the sub-observation variable. What you wish to name your suffix in the long format.  
sep:str, default “”


A character indicating the separation of the variable names in the wide format, to be stripped from the names in the long format. For example, if your column names are A-suffix1, A-suffix2, you can strip the hyphen by specifying sep=’-’.  
suffix:str, default ‘\d+’


A regular expression capturing the wanted suffixes. ‘\d+’ captures numeric suffixes. Suffixes with no numbers could be specified with the negated character class ‘\D+’. You can also further disambiguate suffixes, for example, if your wide variables are of the form A-one, B-two,.., and you have an unrelated column A-rating, you can ignore the last one by specifying suffix=’(!?one|two)’. When all suffixes are numeric, they are cast to int64/float64.    Returns 
 DataFrame

A DataFrame that contains each stub name as a variable, with new index (i, j).      See also  melt

Unpivot a DataFrame from wide to long format, optionally leaving identifiers set.  pivot

Create a spreadsheet-style pivot table as a DataFrame.  DataFrame.pivot

Pivot without aggregation that can handle non-numeric data.  DataFrame.pivot_table

Generalization of pivot that can handle duplicate values for one index/column pair.  DataFrame.unstack

Pivot based on the index values instead of a column.    Notes All extra variables are left untouched. This simply uses pandas.melt under the hood, but is hard-coded to “do the right thing” in a typical case. Examples 
>>> np.random.seed(123)
>>> df = pd.DataFrame({"A1970" : {0 : "a", 1 : "b", 2 : "c"},
...                    "A1980" : {0 : "d", 1 : "e", 2 : "f"},
...                    "B1970" : {0 : 2.5, 1 : 1.2, 2 : .7},
...                    "B1980" : {0 : 3.2, 1 : 1.3, 2 : .1},
...                    "X"     : dict(zip(range(3), np.random.randn(3)))
...                   })
>>> df["id"] = df.index
>>> df
  A1970 A1980  B1970  B1980         X  id
0     a     d    2.5    3.2 -1.085631   0
1     b     e    1.2    1.3  0.997345   1
2     c     f    0.7    0.1  0.282978   2
>>> pd.wide_to_long(df, ["A", "B"], i="id", j="year")
... 
                X  A    B
id year
0  1970 -1.085631  a  2.5
1  1970  0.997345  b  1.2
2  1970  0.282978  c  0.7
0  1980 -1.085631  d  3.2
1  1980  0.997345  e  1.3
2  1980  0.282978  f  0.1
  With multiple id columns 
>>> df = pd.DataFrame({
...     'famid': [1, 1, 1, 2, 2, 2, 3, 3, 3],
...     'birth': [1, 2, 3, 1, 2, 3, 1, 2, 3],
...     'ht1': [2.8, 2.9, 2.2, 2, 1.8, 1.9, 2.2, 2.3, 2.1],
...     'ht2': [3.4, 3.8, 2.9, 3.2, 2.8, 2.4, 3.3, 3.4, 2.9]
... })
>>> df
   famid  birth  ht1  ht2
0      1      1  2.8  3.4
1      1      2  2.9  3.8
2      1      3  2.2  2.9
3      2      1  2.0  3.2
4      2      2  1.8  2.8
5      2      3  1.9  2.4
6      3      1  2.2  3.3
7      3      2  2.3  3.4
8      3      3  2.1  2.9
>>> l = pd.wide_to_long(df, stubnames='ht', i=['famid', 'birth'], j='age')
>>> l
... 
                  ht
famid birth age
1     1     1    2.8
            2    3.4
      2     1    2.9
            2    3.8
      3     1    2.2
            2    2.9
2     1     1    2.0
            2    3.2
      2     1    1.8
            2    2.8
      3     1    1.9
            2    2.4
3     1     1    2.2
            2    3.3
      2     1    2.3
            2    3.4
      3     1    2.1
            2    2.9
  Going from long back to wide just takes some creative use of unstack 
>>> w = l.unstack()
>>> w.columns = w.columns.map('{0[0]}{0[1]}'.format)
>>> w.reset_index()
   famid  birth  ht1  ht2
0      1      1  2.8  3.4
1      1      2  2.9  3.8
2      1      3  2.2  2.9
3      2      1  2.0  3.2
4      2      2  1.8  2.8
5      2      3  1.9  2.4
6      3      1  2.2  3.3
7      3      2  2.3  3.4
8      3      3  2.1  2.9
  Less wieldy column names are also handled 
>>> np.random.seed(0)
>>> df = pd.DataFrame({'A(weekly)-2010': np.random.rand(3),
...                    'A(weekly)-2011': np.random.rand(3),
...                    'B(weekly)-2010': np.random.rand(3),
...                    'B(weekly)-2011': np.random.rand(3),
...                    'X' : np.random.randint(3, size=3)})
>>> df['id'] = df.index
>>> df 
   A(weekly)-2010  A(weekly)-2011  B(weekly)-2010  B(weekly)-2011  X  id
0        0.548814        0.544883        0.437587        0.383442  0   0
1        0.715189        0.423655        0.891773        0.791725  1   1
2        0.602763        0.645894        0.963663        0.528895  1   2
  
>>> pd.wide_to_long(df, ['A(weekly)', 'B(weekly)'], i='id',
...                 j='year', sep='-')
... 
         X  A(weekly)  B(weekly)
id year
0  2010  0   0.548814   0.437587
1  2010  1   0.715189   0.891773
2  2010  1   0.602763   0.963663
0  2011  0   0.544883   0.383442
1  2011  1   0.423655   0.791725
2  2011  1   0.645894   0.528895
  If we have many columns, we could also use a regex to find our stubnames and pass that list on to wide_to_long 
>>> stubnames = sorted(
...     set([match[0] for match in df.columns.str.findall(
...         r'[A-B]\(.*\)').values if match != []])
... )
>>> list(stubnames)
['A(weekly)', 'B(weekly)']
  All of the above examples have integers as suffixes. It is possible to have non-integers as suffixes. 
>>> df = pd.DataFrame({
...     'famid': [1, 1, 1, 2, 2, 2, 3, 3, 3],
...     'birth': [1, 2, 3, 1, 2, 3, 1, 2, 3],
...     'ht_one': [2.8, 2.9, 2.2, 2, 1.8, 1.9, 2.2, 2.3, 2.1],
...     'ht_two': [3.4, 3.8, 2.9, 3.2, 2.8, 2.4, 3.3, 3.4, 2.9]
... })
>>> df
   famid  birth  ht_one  ht_two
0      1      1     2.8     3.4
1      1      2     2.9     3.8
2      1      3     2.2     2.9
3      2      1     2.0     3.2
4      2      2     1.8     2.8
5      2      3     1.9     2.4
6      3      1     2.2     3.3
7      3      2     2.3     3.4
8      3      3     2.1     2.9
  
>>> l = pd.wide_to_long(df, stubnames='ht', i=['famid', 'birth'], j='age',
...                     sep='_', suffix=r'\w+')
>>> l
... 
                  ht
famid birth age
1     1     one  2.8
            two  3.4
      2     one  2.9
            two  3.8
      3     one  2.2
            two  2.9
2     1     one  2.0
            two  3.2
      2     one  1.8
            two  2.8
      3     one  1.9
            two  2.4
3     1     one  2.2
            two  3.3
      2     one  2.3
            two  3.4
      3     one  2.1
            two  2.9
tf.raw_ops.OutfeedDequeueTupleV2 Retrieve multiple values from the computation outfeed. Device ordinal is a  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.raw_ops.OutfeedDequeueTupleV2  
tf.raw_ops.OutfeedDequeueTupleV2(
    device_ordinal, dtypes, shapes, name=None
)
 tensor allowing dynamic outfeed. This operation will block indefinitely until data is available. Output i corresponds to XLA tuple element i. Args: device_ordinal: A Tensor of type int32. An int scalar tensor, representing the TPU device to use. This should be -1 when the Op is running on a TPU device, and >= 0 when the Op is running on the CPU device. dtypes: A list of tf.DTypes that has length >= 1. The element types of each element in outputs. shapes: A list of shapes (each a tf.TensorShape or list of ints). The shapes of each tensor in outputs. name: A name for the operation (optional). Returns: A list of Tensor objects of type dtypes.
pandas.melt   pandas.melt(frame, id_vars=None, value_vars=None, var_name=None, value_name='value', col_level=None, ignore_index=True)[source]
 
Unpivot a DataFrame from wide to long format, optionally leaving identifiers set. This function is useful to massage a DataFrame into a format where one or more columns are identifier variables (id_vars), while all other columns, considered measured variables (value_vars), are “unpivoted” to the row axis, leaving just two non-identifier columns, ‘variable’ and ‘value’.  Parameters 
 
id_vars:tuple, list, or ndarray, optional


Column(s) to use as identifier variables.  
value_vars:tuple, list, or ndarray, optional


Column(s) to unpivot. If not specified, uses all columns that are not set as id_vars.  
var_name:scalar


Name to use for the ‘variable’ column. If None it uses frame.columns.name or ‘variable’.  
value_name:scalar, default ‘value’


Name to use for the ‘value’ column.  
col_level:int or str, optional


If columns are a MultiIndex then use this level to melt.  
ignore_index:bool, default True


If True, original index is ignored. If False, the original index is retained. Index labels will be repeated as necessary.  New in version 1.1.0.     Returns 
 DataFrame

Unpivoted DataFrame.      See also  DataFrame.melt

Identical method.  pivot_table

Create a spreadsheet-style pivot table as a DataFrame.  DataFrame.pivot

Return reshaped DataFrame organized by given index / column values.  DataFrame.explode

Explode a DataFrame from list-like columns to long format.    Examples 
>>> df = pd.DataFrame({'A': {0: 'a', 1: 'b', 2: 'c'},
...                    'B': {0: 1, 1: 3, 2: 5},
...                    'C': {0: 2, 1: 4, 2: 6}})
>>> df
   A  B  C
0  a  1  2
1  b  3  4
2  c  5  6
  
>>> pd.melt(df, id_vars=['A'], value_vars=['B'])
   A variable  value
0  a        B      1
1  b        B      3
2  c        B      5
  
>>> pd.melt(df, id_vars=['A'], value_vars=['B', 'C'])
   A variable  value
0  a        B      1
1  b        B      3
2  c        B      5
3  a        C      2
4  b        C      4
5  c        C      6
  The names of ‘variable’ and ‘value’ columns can be customized: 
>>> pd.melt(df, id_vars=['A'], value_vars=['B'],
...         var_name='myVarname', value_name='myValname')
   A myVarname  myValname
0  a         B          1
1  b         B          3
2  c         B          5
  Original index values can be kept around: 
>>> pd.melt(df, id_vars=['A'], value_vars=['B', 'C'], ignore_index=False)
   A variable  value
0  a        B      1
1  b        B      3
2  c        B      5
0  a        C      2
1  b        C      4
2  c        C      6
  If you have multi-index columns: 
>>> df.columns = [list('ABC'), list('DEF')]
>>> df
   A  B  C
   D  E  F
0  a  1  2
1  b  3  4
2  c  5  6
  
>>> pd.melt(df, col_level=0, id_vars=['A'], value_vars=['B'])
   A variable  value
0  a        B      1
1  b        B      3
2  c        B      5
  
>>> pd.melt(df, id_vars=[('A', 'D')], value_vars=[('B', 'E')])
  (A, D) variable_0 variable_1  value
0      a          B          E      1
1      b          B          E      3
2      c          B          E      5
tf.experimental.numpy.outer TensorFlow variant of NumPy's outer. 
tf.experimental.numpy.outer(
    a, b
)
 Unsupported arguments: out. See the NumPy documentation for numpy.outer.
def f_22799300(out):
    """unpack a series of tuples in pandas `out` into a DataFrame with column names 'out-1' and 'out-2'
    """
    return  
 --------------------

def f_1762484(stocks_list):
    """find the index of an element 'MSFT' in a list `stocks_list`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
get_rotate_label(text)[source]
get_axislabel_pos_angle(axes)[source]
invert_ticklabel_direction()[source]
set_axis_direction(label_direction)[source]
 
Adjust the text angle and text alignment of ticklabels according to the matplotlib convention. The label_direction must be one of [left, right, bottom, top].   
property left bottom right top   
ticklabels angle 90 0 -90 180  
ticklabel va center baseline center baseline  
ticklabel ha right center right center   Note that the text angles are actually relative to (90 + angle of the direction to the ticklabel), which gives 0 for bottom axis.
get_rlabel_position()[source]
 
 Returns 
 float

The theta position of the radius labels in degrees.
def f_3464359(ax, labels):
    """rotate the xtick `labels` of matplotlib plot `ax` by `45` degrees to make long labels readable
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
matplotlib.cbook.strip_math(s)[source]
 
Remove latex formatting from mathtext. Only handles fully math and fully non-math strings.
staticfix_minus(s)[source]
 
Some classes may want to replace a hyphen for minus with the proper unicode symbol (U+2212) for typographical correctness. This is a helper method to perform such a replacement when it is enabled via rcParams["axes.unicode_minus"] (default: True).
str.rstrip([chars])  
Return a copy of the string with trailing characters removed. The chars argument is a string specifying the set of characters to be removed. If omitted or None, the chars argument defaults to removing whitespace. The chars argument is not a suffix; rather, all combinations of its values are stripped: >>> '   spacious   '.rstrip()
'   spacious'
>>> 'mississippi'.rstrip('ipz')
'mississ'
 See str.removesuffix() for a method that will remove a single suffix string rather than all of a set of characters. For example: >>> 'Monty Python'.rstrip(' Python')
'M'
>>> 'Monty Python'.removesuffix(' Python')
'Monty'
str.strip([chars])  
Return a copy of the string with the leading and trailing characters removed. The chars argument is a string specifying the set of characters to be removed. If omitted or None, the chars argument defaults to removing whitespace. The chars argument is not a prefix or suffix; rather, all combinations of its values are stripped: >>> '   spacious   '.strip()
'spacious'
>>> 'www.example.com'.strip('cmowz.')
'example'
 The outermost leading and trailing chars argument values are stripped from the string. Characters are removed from the leading end until reaching a string character that is not contained in the set of characters in chars. A similar action takes place on the trailing end. For example: >>> comment_string = '#....... Section 3.2.1 Issue #32 .......'
>>> comment_string.strip('.#! ')
'Section 3.2.1 Issue #32'
mark_safe(s) [source]
 
Explicitly mark a string as safe for (HTML) output purposes. The returned object can be used everywhere a string is appropriate. Can be called multiple times on a single string. Can also be used as a decorator. For building up fragments of HTML, you should normally be using django.utils.html.format_html() instead. String marked safe will become unsafe again if modified. For example: >>> mystr = '<b>Hello World</b>   '
>>> mystr = mark_safe(mystr)
>>> type(mystr)
<class 'django.utils.safestring.SafeString'>

>>> mystr = mystr.strip()  # removing whitespace
>>> type(mystr)
<type 'str'>
def f_875968(s):
    """remove symbols from a string `s`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
string.octdigits  
The string '01234567'.
oct(x)  
Convert an integer number to an octal string prefixed with “0o”. The result is a valid Python expression. If x is not a Python int object, it has to define an __index__() method that returns an integer. For example: >>> oct(8)
'0o10'
>>> oct(-56)
'-0o70'
 If you want to convert an integer number to octal string either with prefix “0o” or not, you can use either of the following ways. >>> '%#o' % 10, '%o' % 10
('0o12', '12')
>>> format(10, '#o'), format(10, 'o')
('0o12', '12')
>>> f'{10:#o}', f'{10:o}'
('0o12', '12')
 See also format() for more information.
email.utils.decode_rfc2231(s)  
Decode the string s according to RFC 2231.
statichexify(match)[source]
re.S  
re.DOTALL  
Make the '.' special character match any character at all, including a newline; without this flag, '.' will match anything except a newline. Corresponds to the inline flag (?s).
def f_34750084(s):
    """Find octal characters matches from a string `s` using regex
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
torch.nn.functional.instance_norm(input, running_mean=None, running_var=None, weight=None, bias=None, use_input_stats=True, momentum=0.1, eps=1e-05) [source]
 
Applies Instance Normalization for each channel in each data sample in a batch. See InstanceNorm1d, InstanceNorm2d, InstanceNorm3d for details.
torch.ravel(input) → Tensor  
Return a contiguous flattened tensor. A copy is made only if needed.  Parameters 
input (Tensor) – the input tensor.   Example: >>> t = torch.tensor([[[1, 2],
...                    [3, 4]],
...                   [[5, 6],
...                    [7, 8]]])
>>> torch.ravel(t)
tensor([1, 2, 3, 4, 5, 6, 7, 8])
tf.sparse.split     View source on GitHub    Split a SparseTensor into num_split tensors along axis. 
tf.sparse.split(
    sp_input=None, num_split=None, axis=None, name=None
)
 If the sp_input.dense_shape[axis] is not an integer multiple of num_split each slice starting from 0:shape[axis] % num_split gets extra one dimension. For example: 
indices = [[0, 2], [0, 4], [0, 5], [1, 0], [1, 1]]
values = [1, 2, 3, 4, 5]
t = tf.SparseTensor(indices=indices, values=values, dense_shape=[2, 7])
tf.sparse.to_dense(t)
<tf.Tensor: shape=(2, 7), dtype=int32, numpy=
array([[0, 0, 1, 0, 2, 3, 0],
       [4, 5, 0, 0, 0, 0, 0]], dtype=int32)>
 
output = tf.sparse.split(sp_input=t, num_split=2, axis=1)
tf.sparse.to_dense(output[0])
<tf.Tensor: shape=(2, 4), dtype=int32, numpy=
array([[0, 0, 1, 0],
       [4, 5, 0, 0]], dtype=int32)>
tf.sparse.to_dense(output[1])
<tf.Tensor: shape=(2, 3), dtype=int32, numpy=
array([[2, 3, 0],
       [0, 0, 0]], dtype=int32)>
 
output = tf.sparse.split(sp_input=t, num_split=2, axis=0)
tf.sparse.to_dense(output[0])
<tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[0, 0, 1, 0, 2, 3, 0]],
dtype=int32)>
tf.sparse.to_dense(output[1])
<tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[4, 5, 0, 0, 0, 0, 0]],
dtype=int32)>
 
output = tf.sparse.split(sp_input=t, num_split=2, axis=-1)
tf.sparse.to_dense(output[0])
<tf.Tensor: shape=(2, 4), dtype=int32, numpy=
array([[0, 0, 1, 0],
       [4, 5, 0, 0]], dtype=int32)>
tf.sparse.to_dense(output[1])
<tf.Tensor: shape=(2, 3), dtype=int32, numpy=
array([[2, 3, 0],
       [0, 0, 0]], dtype=int32)>

 


 Args
  sp_input   The SparseTensor to split.  
  num_split   A Python integer. The number of ways to split.  
  axis   A 0-D int32 Tensor. The dimension along which to split. Must be in range [-rank, rank), where rank is the number of dimensions in the input SparseTensor.  
  name   A name for the operation (optional).   
 


 Returns   num_split SparseTensor objects resulting from splitting value.  

 


 Raises
  TypeError   If sp_input is not a SparseTensor.
torch.atleast_3d(*tensors) [source]
 
Returns a 3-dimensional view of each input tensor with zero dimensions. Input tensors with three or more dimensions are returned as-is. :param input: :type input: Tensor or list of Tensors  Returns 
output (Tensor or tuple of Tensors)   Example >>> x = torch.tensor(0.5)
>>> x
tensor(0.5000)
>>> torch.atleast_3d(x)
tensor([[[0.5000]]])
>>> y = torch.randn(2,2)
>>> y
tensor([[-0.8079,  0.7460],
        [-1.1647,  1.4734]])
>>> torch.atleast_3d(y)
tensor([[[-0.8079],
        [ 0.7460]],

        [[-1.1647],
        [ 1.4734]]])
>>> x = torch.randn(1,1,1)
>>> x
tensor([[[-1.5689]]])
>>> torch.atleast_3d(x)
tensor([[[-1.5689]]])
>>> x = torch.tensor(0.5)
>>> y = torch.tensor(1.)
>>> torch.atleast_3d((x,y))
(tensor([[[0.5000]]]), tensor([[[1.]]]))
torch.tile(input, reps) → Tensor  
Constructs a tensor by repeating the elements of input. The reps argument specifies the number of repetitions in each dimension. If reps specifies fewer dimensions than input has, then ones are prepended to reps until all dimensions are specified. For example, if input has shape (8, 6, 4, 2) and reps is (2, 2), then reps is treated as (1, 1, 2, 2). Analogously, if input has fewer dimensions than reps specifies, then input is treated as if it were unsqueezed at dimension zero until it has as many dimensions as reps specifies. For example, if input has shape (4, 2) and reps is (3, 3, 2, 2), then input is treated as if it had the shape (1, 1, 4, 2).  Note This function is similar to NumPy’s tile function.   Parameters 
 
input (Tensor) – the tensor whose elements to repeat. 
reps (tuple) – the number of repetitions per dimension.    Example: >>> x = torch.tensor([1, 2, 3])
>>> x.tile((2,))
tensor([1, 2, 3, 1, 2, 3])
>>> y = torch.tensor([[1, 2], [3, 4]])
>>> torch.tile(y, (2, 2))
tensor([[1, 2, 1, 2],
        [3, 4, 3, 4],
        [1, 2, 1, 2],
        [3, 4, 3, 4]])
def f_13209288(input):
    """split string `input` based on occurrences of regex pattern '[ ](?=[A-Z]+\\b)'
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
tf.sparse.slice     View source on GitHub    Slice a SparseTensor based on the start and `size.  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.sparse.slice, tf.compat.v1.sparse_slice  
tf.sparse.slice(
    sp_input, start, size, name=None
)
 For example, if the input is input_tensor = shape = [2, 7]
[    a   d e  ]
[b c          ]
 Graphically the output tensors are: sparse.slice([0, 0], [2, 4]) = shape = [2, 4]
[    a  ]
[b c    ]

sparse.slice([0, 4], [2, 3]) = shape = [2, 3]
[ d e  ]
[      ]

 


 Args
  sp_input   The SparseTensor to split.  
  start   1-D. tensor represents the start of the slice.  
  size   1-D. tensor represents the size of the slice.  
  name   A name for the operation (optional).   
 


 Returns   A SparseTensor objects resulting from splicing.  

 


 Raises
  TypeError   If sp_input is not a SparseTensor.
tf.raw_ops.StringUpper Converts all lowercase characters into their respective uppercase replacements.  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.raw_ops.StringUpper  
tf.raw_ops.StringUpper(
    input, encoding='', name=None
)
 Example: 
tf.strings.upper("CamelCase string and ALL CAPS")
<tf.Tensor: shape=(), dtype=string, numpy=b'CAMELCASE STRING AND ALL CAPS'>

 


 Args
  input   A Tensor of type string.  
  encoding   An optional string. Defaults to "".  
  name   A name for the operation (optional).   
 


 Returns   A Tensor of type string.
torch.flipud(input) → Tensor  
Flip tensor in the up/down direction, returning a new tensor. Flip the entries in each column in the up/down direction. Rows are preserved, but appear in a different order than before.  Note Requires the tensor to be at least 1-D.   Note torch.flipud makes a copy of input’s data. This is different from NumPy’s np.flipud, which returns a view in constant time. Since copying a tensor’s data is more work than viewing that data, torch.flipud is expected to be slower than np.flipud.   Parameters 
input (Tensor) – Must be at least 1-dimensional.   Example: >>> x = torch.arange(4).view(2, 2)
>>> x
tensor([[0, 1],
        [2, 3]])
>>> torch.flipud(x)
tensor([[2, 3],
        [0, 1]])
torch.chunk(input, chunks, dim=0) → List of Tensors  
Splits a tensor into a specific number of chunks. Each chunk is a view of the input tensor. Last chunk will be smaller if the tensor size along the given dimension dim is not divisible by chunks.  Parameters 
 
input (Tensor) – the tensor to split 
chunks (int) – number of chunks to return 
dim (int) – dimension along which to split the tensor
tf.strings.upper Converts all lowercase characters into their respective uppercase replacements.  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.strings.upper  
tf.strings.upper(
    input, encoding='', name=None
)
 Example: 
tf.strings.upper("CamelCase string and ALL CAPS")
<tf.Tensor: shape=(), dtype=string, numpy=b'CAMELCASE STRING AND ALL CAPS'>

 


 Args
  input   A Tensor of type string.  
  encoding   An optional string. Defaults to "".  
  name   A name for the operation (optional).   
 


 Returns   A Tensor of type string.
def f_13209288(input):
    """Split string `input` at every space followed by an upper-case letter
    """
    return  
 --------------------

def f_24642040(url, files, headers, data):
    """send multipart encoded file `files` to url `url` with headers `headers` and metadata `data`
    """
    return  
 --------------------

def f_4290716(filename, bytes_):
    """write bytes `bytes_` to a file `filename` in python 3
    """
    return  
 --------------------

def f_33078554(lst, dct):
    """get a list from a list `lst` with values mapped into a dictionary `dct`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
numpy.lib.recfunctions.find_duplicates(a, key=None, ignoremask=True, return_index=False)[source]
 
Find the duplicates in a structured array along a given key  Parameters 
 
aarray-like


Input array  
key{string, None}, optional


Name of the fields along which to check the duplicates. If None, the search is performed by records  
ignoremask{True, False}, optional


Whether masked data should be discarded or considered as duplicates.  
return_index{False, True}, optional


Whether to return the indices of the duplicated values.     Examples >>> from numpy.lib import recfunctions as rfn
>>> ndtype = [('a', int)]
>>> a = np.ma.array([1, 1, 1, 2, 2, 3, 3],
...         mask=[0, 0, 1, 0, 0, 0, 1]).view(ndtype)
>>> rfn.find_duplicates(a, ignoremask=True, return_index=True)
(masked_array(data=[(1,), (1,), (2,), (2,)],
             mask=[(False,), (False,), (False,), (False,)],
       fill_value=(999999,),
            dtype=[('a', '<i8')]), array([0, 1, 3, 4]))
tf.raw_ops.DeepCopy Makes a copy of x.  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.raw_ops.DeepCopy  
tf.raw_ops.DeepCopy(
    x, name=None
)

 


 Args
  x   A Tensor. The source tensor of type T.  
  name   A name for the operation (optional).   
 


 Returns   A Tensor. Has the same type as x.
copy.deepcopy(x[, memo])  
Return a deep copy of x.
tf.compat.v1.flags.FlagNameConflictsWithMethodError Raised when a flag name conflicts with FlagValues methods. Inherits From: Error  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.app.flags.FlagNameConflictsWithMethodError
pandas.Index.set_names   Index.set_names(names, level=None, inplace=False)[source]
 
Set Index or MultiIndex name. Able to set new names partially and by level.  Parameters 
 
names:label or list of label or dict-like for MultiIndex


Name(s) to set.  Changed in version 1.3.0.   
level:int, label or list of int or label, optional


If the index is a MultiIndex and names is not dict-like, level(s) to set (None for all levels). Otherwise level must be None.  Changed in version 1.3.0.   
inplace:bool, default False


Modifies the object directly, instead of creating a new Index or MultiIndex.    Returns 
 Index or None

The same type as the caller or None if inplace=True.      See also  Index.rename

Able to set new names without level.    Examples 
>>> idx = pd.Index([1, 2, 3, 4])
>>> idx
Int64Index([1, 2, 3, 4], dtype='int64')
>>> idx.set_names('quarter')
Int64Index([1, 2, 3, 4], dtype='int64', name='quarter')
  
>>> idx = pd.MultiIndex.from_product([['python', 'cobra'],
...                                   [2018, 2019]])
>>> idx
MultiIndex([('python', 2018),
            ('python', 2019),
            ( 'cobra', 2018),
            ( 'cobra', 2019)],
           )
>>> idx.set_names(['kind', 'year'], inplace=True)
>>> idx
MultiIndex([('python', 2018),
            ('python', 2019),
            ( 'cobra', 2018),
            ( 'cobra', 2019)],
           names=['kind', 'year'])
>>> idx.set_names('species', level=0)
MultiIndex([('python', 2018),
            ('python', 2019),
            ( 'cobra', 2018),
            ( 'cobra', 2019)],
           names=['species', 'year'])
  When renaming levels with a dict, levels can not be passed. 
>>> idx.set_names({'kind': 'snake'})
MultiIndex([('python', 2018),
            ('python', 2019),
            ( 'cobra', 2018),
            ( 'cobra', 2019)],
           names=['snake', 'year'])
def f_15247628(x):
    """find duplicate names in column 'name' of the dataframe `x`
    """
    return  
 --------------------

def f_783897():
    """truncate float 1.923328437452 to 3 decimal places
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
dates(field, kind, order='ASC')
get_date_list(queryset, date_type=None, ordering='ASC')  
Returns the list of dates of type date_type for which queryset contains entries. For example, get_date_list(qs, 'year') will return the list of years for which qs has entries. If date_type isn’t provided, the result of get_date_list_period() is used. date_type and ordering are passed to QuerySet.dates().
class BaseDateListView  
A base class that provides common behavior for all date-based views. There won’t normally be a reason to instantiate BaseDateListView; instantiate one of the subclasses instead. While this view (and its subclasses) are executing, self.object_list will contain the list of objects that the view is operating upon, and self.date_list will contain the list of dates for which data is available. Mixins  DateMixin MultipleObjectMixin  Methods and Attributes  
allow_empty  
A boolean specifying whether to display the page if no objects are available. If this is True and no objects are available, the view will display an empty page instead of raising a 404. This is identical to django.views.generic.list.MultipleObjectMixin.allow_empty, except for the default value, which is False. 
  
date_list_period  
Optional A string defining the aggregation period for date_list. It must be one of 'year' (default), 'month', or 'day'. 
  
get_dated_items()  
Returns a 3-tuple containing (date_list, object_list, extra_context). date_list is the list of dates for which data is available. object_list is the list of objects. extra_context is a dictionary of context data that will be added to any context data provided by the MultipleObjectMixin. 
  
get_dated_queryset(**lookup)  
Returns a queryset, filtered using the query arguments defined by lookup. Enforces any restrictions on the queryset, such as allow_empty and allow_future. 
  
get_date_list_period()  
Returns the aggregation period for date_list. Returns date_list_period by default. 
  
get_date_list(queryset, date_type=None, ordering='ASC')  
Returns the list of dates of type date_type for which queryset contains entries. For example, get_date_list(qs, 'year') will return the list of years for which qs has entries. If date_type isn’t provided, the result of get_date_list_period() is used. date_type and ordering are passed to QuerySet.dates().
invoke(cli=None, args=None, **kwargs)  
Invokes a CLI command in an isolated environment. See CliRunner.invoke for full method documentation. See Testing CLI Commands for examples. If the obj argument is not given, passes an instance of ScriptInfo that knows how to load the Flask app being tested.  Parameters 
 
cli (Optional[Any]) – Command object to invoke. Default is the app’s cli group. 
args (Optional[Any]) – List of strings to invoke the command with. 
kwargs (Any) –    Returns 
a Result object.  Return type 
Any
logits [source]
def f_22859493(li):
    """sort list `li` in descending order based on the date value in second element of each list in list `li`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
classmatplotlib.projections.polar.RadialTick(*args, **kwargs)[source]
 
Bases: matplotlib.axis.YTick A radial-axis tick. This subclass of YTick provides radial ticks with some small modification to their re-positioning such that ticks are rotated based on axes limits. This results in ticks that are correctly perpendicular to the spine. Labels are also rotated to be perpendicular to the spine, when 'auto' rotation is enabled. bbox is the Bound2D bounding box in display coords of the Axes loc is the tick location in data coords size is the tick size in points   set(*, agg_filter=<UNSET>, alpha=<UNSET>, animated=<UNSET>, clip_box=<UNSET>, clip_on=<UNSET>, clip_path=<UNSET>, gid=<UNSET>, in_layout=<UNSET>, label=<UNSET>, label1=<UNSET>, label2=<UNSET>, pad=<UNSET>, path_effects=<UNSET>, picker=<UNSET>, rasterized=<UNSET>, sketch_params=<UNSET>, snap=<UNSET>, transform=<UNSET>, url=<UNSET>, visible=<UNSET>, zorder=<UNSET>)[source]
 
Set multiple properties at once. Supported properties are   
Property Description   
agg_filter a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array  
alpha scalar or None  
animated bool  
clip_box Bbox  
clip_on bool  
clip_path Patch or (Path, Transform) or None  
figure Figure  
gid str  
in_layout bool  
label str  
label1 str  
label2 str  
pad float  
path_effects AbstractPathEffect  
picker None or bool or float or callable  
rasterized bool  
sketch_params (scale: float, length: float, randomness: float)  
snap bool or None  
transform Transform  
url str  
visible bool  
zorder float   
   update_position(loc)[source]
 
Set the location of tick in data coords with scalar loc.
set_rticks(*args, **kwargs)[source]
matplotlib.pyplot.minorticks_on   matplotlib.pyplot.minorticks_on()[source]
 
Display minor ticks on the Axes. Displaying minor ticks may reduce performance; you may turn them off using minorticks_off() if drawing speed is a problem.
classmatplotlib.projections.polar.ThetaTick(axes, *args, **kwargs)[source]
 
Bases: matplotlib.axis.XTick A theta-axis tick. This subclass of XTick provides angular ticks with some small modification to their re-positioning such that ticks are rotated based on tick location. This results in ticks that are correctly perpendicular to the arc spine. When 'auto' rotation is enabled, labels are also rotated to be parallel to the spine. The label padding is also applied here since it's not possible to use a generic axes transform to produce tick-specific padding. bbox is the Bound2D bounding box in display coords of the Axes loc is the tick location in data coords size is the tick size in points   set(*, agg_filter=<UNSET>, alpha=<UNSET>, animated=<UNSET>, clip_box=<UNSET>, clip_on=<UNSET>, clip_path=<UNSET>, gid=<UNSET>, in_layout=<UNSET>, label=<UNSET>, label1=<UNSET>, label2=<UNSET>, pad=<UNSET>, path_effects=<UNSET>, picker=<UNSET>, rasterized=<UNSET>, sketch_params=<UNSET>, snap=<UNSET>, transform=<UNSET>, url=<UNSET>, visible=<UNSET>, zorder=<UNSET>)[source]
 
Set multiple properties at once. Supported properties are   
Property Description   
agg_filter a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array  
alpha scalar or None  
animated bool  
clip_box Bbox  
clip_on bool  
clip_path Patch or (Path, Transform) or None  
figure Figure  
gid str  
in_layout bool  
label str  
label1 str  
label2 str  
pad float  
path_effects AbstractPathEffect  
picker None or bool or float or callable  
rasterized bool  
sketch_params (scale: float, length: float, randomness: float)  
snap bool or None  
transform Transform  
url str  
visible bool  
zorder float   
   update_position(loc)[source]
 
Set the location of tick in data coords with scalar loc.
matplotlib.pyplot.minorticks_off   matplotlib.pyplot.minorticks_off()[source]
 
Remove minor ticks from the Axes.
def f_29394552(ax):
    """place the radial ticks in plot `ax` at 135 degrees
    """
     
 --------------------
Please refer to the following documentation to generate the code:
os.path.isabs(path)  
Return True if path is an absolute pathname. On Unix, that means it begins with a slash, on Windows that it begins with a (back)slash after chopping off a potential drive letter.  Changed in version 3.6: Accepts a path-like object.
PurePath.is_absolute()  
Return whether the path is absolute or not. A path is considered absolute if it has both a root and (if the flavour allows) a drive: >>> PurePosixPath('/a/b').is_absolute()
True
>>> PurePosixPath('a/b').is_absolute()
False

>>> PureWindowsPath('c:/a/b').is_absolute()
True
>>> PureWindowsPath('/a/b').is_absolute()
False
>>> PureWindowsPath('c:').is_absolute()
False
>>> PureWindowsPath('//some/share').is_absolute()
True
Path.is_mount()  
Return True if the path is a mount point: a point in a file system where a different file system has been mounted. On POSIX, the function checks whether path’s parent, path/.., is on a different device than path, or whether path/.. and path point to the same i-node on the same device — this should detect mount points for all Unix and POSIX variants. Not implemented on Windows.  New in version 3.7.
os.path.ismount(path)  
Return True if pathname path is a mount point: a point in a file system where a different file system has been mounted. On POSIX, the function checks whether path’s parent, path/.., is on a different device than path, or whether path/.. and path point to the same i-node on the same device — this should detect mount points for all Unix and POSIX variants. It is not able to reliably detect bind mounts on the same filesystem. On Windows, a drive letter root and a share UNC are always mount points, and for any other path GetVolumePathName is called to see if it is different from the input path.  New in version 3.4: Support for detecting non-root mount points on Windows.   Changed in version 3.6: Accepts a path-like object.
PurePath.is_relative_to(*other)  
Return whether or not this path is relative to the other path. >>> p = PurePath('/etc/passwd')
>>> p.is_relative_to('/etc')
True
>>> p.is_relative_to('/usr')
False
  New in version 3.9.
def f_3320406(my_path):
    """check if path `my_path` is an absolute path
    """
    return  
 --------------------

def f_2212433(yourdict):
    """get number of keys in dictionary `yourdict`
    """
    return  
 --------------------

def f_2212433(yourdictfile):
    """count the number of keys in dictionary `yourdictfile`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
pandas.core.groupby.GroupBy.groups   propertyGroupBy.groups
 
Dict {group name -> group labels}.
pandas.core.resample.Resampler.get_group   Resampler.get_group(name, obj=None)[source]
 
Construct DataFrame from group with provided name.  Parameters 
 
name:object


The name of the group to get as a DataFrame.  
obj:DataFrame, default None


The DataFrame to take the DataFrame out of. If it is None, the object groupby was called on will be used.    Returns 
 
group:same type as obj
pandas.core.groupby.GroupBy.head   finalGroupBy.head(n=5)[source]
 
Return first n rows of each group. Similar to .apply(lambda x: x.head(n)), but it returns a subset of rows from the original DataFrame with original index and order preserved (as_index flag is ignored).  Parameters 
 
n:int


If positive: number of entries to include from start of each group. If negative: number of entries to exclude from end of each group.    Returns 
 Series or DataFrame

Subset of original Series or DataFrame as determined by n.      See also  Series.groupby

Apply a function groupby to a Series.  DataFrame.groupby

Apply a function groupby to each row or column of a DataFrame.    Examples 
>>> df = pd.DataFrame([[1, 2], [1, 4], [5, 6]],
...                   columns=['A', 'B'])
>>> df.groupby('A').head(1)
   A  B
0  1  2
2  5  6
>>> df.groupby('A').head(-1)
   A  B
0  1  2
pandas.core.groupby.GroupBy.first   finalGroupBy.first(numeric_only=False, min_count=- 1)[source]
 
Compute first of group values.  Parameters 
 
numeric_only:bool, default False


Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data.  
min_count:int, default -1


The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA.    Returns 
 Series or DataFrame

Computed first of values within each group.
pandas.core.groupby.GroupBy.get_group   GroupBy.get_group(name, obj=None)[source]
 
Construct DataFrame from group with provided name.  Parameters 
 
name:object


The name of the group to get as a DataFrame.  
obj:DataFrame, default None


The DataFrame to take the DataFrame out of. If it is None, the object groupby was called on will be used.    Returns 
 
group:same type as obj
def f_20067636(df):
    """pandas dataframe `df` get first row of each group by 'id'
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
pandas.DataFrame.explode   DataFrame.explode(column, ignore_index=False)[source]
 
Transform each element of a list-like to a row, replicating index values.  New in version 0.25.0.   Parameters 
 
column:IndexLabel


Column(s) to explode. For multiple columns, specify a non-empty list with each element be str or tuple, and all specified columns their list-like data on same row of the frame must have matching length.  New in version 1.3.0: Multi-column explode   
ignore_index:bool, default False


If True, the resulting index will be labeled 0, 1, …, n - 1.  New in version 1.1.0.     Returns 
 DataFrame

Exploded lists to rows of the subset columns; index will be duplicated for these rows.    Raises 
 ValueError :

 If columns of the frame are not unique. If specified columns to explode is empty list. If specified columns to explode have not matching count of elements rowwise in the frame.       See also  DataFrame.unstack

Pivot a level of the (necessarily hierarchical) index labels.  DataFrame.melt

Unpivot a DataFrame from wide format to long format.  Series.explode

Explode a DataFrame from list-like columns to long format.    Notes This routine will explode list-likes including lists, tuples, sets, Series, and np.ndarray. The result dtype of the subset rows will be object. Scalars will be returned unchanged, and empty list-likes will result in a np.nan for that row. In addition, the ordering of rows in the output will be non-deterministic when exploding sets. Examples 
>>> df = pd.DataFrame({'A': [[0, 1, 2], 'foo', [], [3, 4]],
...                    'B': 1,
...                    'C': [['a', 'b', 'c'], np.nan, [], ['d', 'e']]})
>>> df
           A  B          C
0  [0, 1, 2]  1  [a, b, c]
1        foo  1        NaN
2         []  1         []
3     [3, 4]  1     [d, e]
  Single-column explode. 
>>> df.explode('A')
     A  B          C
0    0  1  [a, b, c]
0    1  1  [a, b, c]
0    2  1  [a, b, c]
1  foo  1        NaN
2  NaN  1         []
3    3  1     [d, e]
3    4  1     [d, e]
  Multi-column explode. 
>>> df.explode(list('AC'))
     A  B    C
0    0  1    a
0    1  1    b
0    2  1    c
1  foo  1  NaN
2  NaN  1  NaN
3    3  1    d
3    4  1    e
split(split_size, dim=0) [source]
 
See torch.split()
class sklearn.compose.ColumnTransformer(transformers, *, remainder='drop', sparse_threshold=0.3, n_jobs=None, transformer_weights=None, verbose=False) [source]
 
Applies transformers to columns of an array or pandas DataFrame. This estimator allows different columns or column subsets of the input to be transformed separately and the features generated by each transformer will be concatenated to form a single feature space. This is useful for heterogeneous or columnar data, to combine several feature extraction mechanisms or transformations into a single transformer. Read more in the User Guide.  New in version 0.20.   Parameters 
 
transformerslist of tuples 

List of (name, transformer, columns) tuples specifying the transformer objects to be applied to subsets of the data.  
namestr 

Like in Pipeline and FeatureUnion, this allows the transformer and its parameters to be set using set_params and searched in grid search.  
transformer{‘drop’, ‘passthrough’} or estimator 

Estimator must support fit and transform. Special-cased strings ‘drop’ and ‘passthrough’ are accepted as well, to indicate to drop the columns or to pass them through untransformed, respectively.  
columnsstr, array-like of str, int, array-like of int, array-like of bool, slice or callable 

Indexes the data on its second axis. Integers are interpreted as positional columns, while strings can reference DataFrame columns by name. A scalar string or int should be used where transformer expects X to be a 1d array-like (vector), otherwise a 2d array will be passed to the transformer. A callable is passed the input data X and can return any of the above. To select multiple columns by name or dtype, you can use make_column_selector.    
remainder{‘drop’, ‘passthrough’} or estimator, default=’drop’ 

By default, only the specified columns in transformers are transformed and combined in the output, and the non-specified columns are dropped. (default of 'drop'). By specifying remainder='passthrough', all remaining columns that were not specified in transformers will be automatically passed through. This subset of columns is concatenated with the output of the transformers. By setting remainder to be an estimator, the remaining non-specified columns will use the remainder estimator. The estimator must support fit and transform. Note that using this feature requires that the DataFrame columns input at fit and transform have identical order.  
sparse_thresholdfloat, default=0.3 

If the output of the different transformers contains sparse matrices, these will be stacked as a sparse matrix if the overall density is lower than this value. Use sparse_threshold=0 to always return dense. When the transformed output consists of all dense data, the stacked result will be dense, and this keyword will be ignored.  
n_jobsint, default=None 

Number of jobs to run in parallel. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details.  
transformer_weightsdict, default=None 

Multiplicative weights for features per transformer. The output of the transformer is multiplied by these weights. Keys are transformer names, values the weights.  
verbosebool, default=False 

If True, the time elapsed while fitting each transformer will be printed as it is completed.    Attributes 
 
transformers_list 

The collection of fitted transformers as tuples of (name, fitted_transformer, column). fitted_transformer can be an estimator, ‘drop’, or ‘passthrough’. In case there were no columns selected, this will be the unfitted transformer. If there are remaining columns, the final element is a tuple of the form: (‘remainder’, transformer, remaining_columns) corresponding to the remainder parameter. If there are remaining columns, then len(transformers_)==len(transformers)+1, otherwise len(transformers_)==len(transformers).  
named_transformers_Bunch 

Access the fitted transformer by name.  
sparse_output_bool 

Boolean flag indicating whether the output of transform is a sparse matrix or a dense numpy array, which depends on the output of the individual transformers and the sparse_threshold keyword.      See also  
make_column_transformer


Convenience function for combining the outputs of multiple transformer objects applied to column subsets of the original feature space.  
make_column_selector


Convenience function for selecting columns based on datatype or the columns name with a regex pattern.    Notes The order of the columns in the transformed feature matrix follows the order of how the columns are specified in the transformers list. Columns of the original feature matrix that are not specified are dropped from the resulting transformed feature matrix, unless specified in the passthrough keyword. Those columns specified with passthrough are added at the right to the output of the transformers. Examples >>> import numpy as np
>>> from sklearn.compose import ColumnTransformer
>>> from sklearn.preprocessing import Normalizer
>>> ct = ColumnTransformer(
...     [("norm1", Normalizer(norm='l1'), [0, 1]),
...      ("norm2", Normalizer(norm='l1'), slice(2, 4))])
>>> X = np.array([[0., 1., 2., 2.],
...               [1., 1., 0., 1.]])
>>> # Normalizer scales each row of X to unit norm. A separate scaling
>>> # is applied for the two first and two last elements of each
>>> # row independently.
>>> ct.fit_transform(X)
array([[0. , 1. , 0.5, 0.5],
       [0.5, 0.5, 0. , 1. ]])
 Methods  
fit(X[, y]) Fit all transformers using X.  
fit_transform(X[, y]) Fit all transformers, transform the data and concatenate results.  
get_feature_names() Get feature names from all transformers.  
get_params([deep]) Get parameters for this estimator.  
set_params(**kwargs) Set the parameters of this estimator.  
transform(X) Transform X separately by each transformer, concatenate results.    
fit(X, y=None) [source]
 
Fit all transformers using X.  Parameters 
 
X{array-like, dataframe} of shape (n_samples, n_features) 

Input data, of which specified subsets are used to fit the transformers.  
yarray-like of shape (n_samples,…), default=None 

Targets for supervised learning.    Returns 
 
selfColumnTransformer 

This estimator     
  
fit_transform(X, y=None) [source]
 
Fit all transformers, transform the data and concatenate results.  Parameters 
 
X{array-like, dataframe} of shape (n_samples, n_features) 

Input data, of which specified subsets are used to fit the transformers.  
yarray-like of shape (n_samples,), default=None 

Targets for supervised learning.    Returns 
 
X_t{array-like, sparse matrix} of shape (n_samples, sum_n_components) 

hstack of results of transformers. sum_n_components is the sum of n_components (output dimension) over transformers. If any result is a sparse matrix, everything will be converted to sparse matrices.     
  
get_feature_names() [source]
 
Get feature names from all transformers.  Returns 
 
feature_nameslist of strings 

Names of the features produced by transform.     
  
get_params(deep=True) [source]
 
Get parameters for this estimator. Returns the parameters given in the constructor as well as the estimators contained within the transformers of the ColumnTransformer.  Parameters 
 
deepbool, default=True 

If True, will return the parameters for this estimator and contained subobjects that are estimators.    Returns 
 
paramsdict 

Parameter names mapped to their values.     
  
property named_transformers_  
Access the fitted transformer by name. Read-only attribute to access any transformer by given name. Keys are transformer names and values are the fitted transformer objects. 
  
set_params(**kwargs) [source]
 
Set the parameters of this estimator. Valid parameter keys can be listed with get_params(). Note that you can directly set the parameters of the estimators contained in transformers of ColumnTransformer.  Returns 
 self
   
  
transform(X) [source]
 
Transform X separately by each transformer, concatenate results.  Parameters 
 
X{array-like, dataframe} of shape (n_samples, n_features) 

The data to be transformed by subset.    Returns 
 
X_t{array-like, sparse matrix} of shape (n_samples, sum_n_components) 

hstack of results of transformers. sum_n_components is the sum of n_components (output dimension) over transformers. If any result is a sparse matrix, everything will be converted to sparse matrices.
pandas.melt   pandas.melt(frame, id_vars=None, value_vars=None, var_name=None, value_name='value', col_level=None, ignore_index=True)[source]
 
Unpivot a DataFrame from wide to long format, optionally leaving identifiers set. This function is useful to massage a DataFrame into a format where one or more columns are identifier variables (id_vars), while all other columns, considered measured variables (value_vars), are “unpivoted” to the row axis, leaving just two non-identifier columns, ‘variable’ and ‘value’.  Parameters 
 
id_vars:tuple, list, or ndarray, optional


Column(s) to use as identifier variables.  
value_vars:tuple, list, or ndarray, optional


Column(s) to unpivot. If not specified, uses all columns that are not set as id_vars.  
var_name:scalar


Name to use for the ‘variable’ column. If None it uses frame.columns.name or ‘variable’.  
value_name:scalar, default ‘value’


Name to use for the ‘value’ column.  
col_level:int or str, optional


If columns are a MultiIndex then use this level to melt.  
ignore_index:bool, default True


If True, original index is ignored. If False, the original index is retained. Index labels will be repeated as necessary.  New in version 1.1.0.     Returns 
 DataFrame

Unpivoted DataFrame.      See also  DataFrame.melt

Identical method.  pivot_table

Create a spreadsheet-style pivot table as a DataFrame.  DataFrame.pivot

Return reshaped DataFrame organized by given index / column values.  DataFrame.explode

Explode a DataFrame from list-like columns to long format.    Examples 
>>> df = pd.DataFrame({'A': {0: 'a', 1: 'b', 2: 'c'},
...                    'B': {0: 1, 1: 3, 2: 5},
...                    'C': {0: 2, 1: 4, 2: 6}})
>>> df
   A  B  C
0  a  1  2
1  b  3  4
2  c  5  6
  
>>> pd.melt(df, id_vars=['A'], value_vars=['B'])
   A variable  value
0  a        B      1
1  b        B      3
2  c        B      5
  
>>> pd.melt(df, id_vars=['A'], value_vars=['B', 'C'])
   A variable  value
0  a        B      1
1  b        B      3
2  c        B      5
3  a        C      2
4  b        C      4
5  c        C      6
  The names of ‘variable’ and ‘value’ columns can be customized: 
>>> pd.melt(df, id_vars=['A'], value_vars=['B'],
...         var_name='myVarname', value_name='myValname')
   A myVarname  myValname
0  a         B          1
1  b         B          3
2  c         B          5
  Original index values can be kept around: 
>>> pd.melt(df, id_vars=['A'], value_vars=['B', 'C'], ignore_index=False)
   A variable  value
0  a        B      1
1  b        B      3
2  c        B      5
0  a        C      2
1  b        C      4
2  c        C      6
  If you have multi-index columns: 
>>> df.columns = [list('ABC'), list('DEF')]
>>> df
   A  B  C
   D  E  F
0  a  1  2
1  b  3  4
2  c  5  6
  
>>> pd.melt(df, col_level=0, id_vars=['A'], value_vars=['B'])
   A variable  value
0  a        B      1
1  b        B      3
2  c        B      5
  
>>> pd.melt(df, id_vars=[('A', 'D')], value_vars=[('B', 'E')])
  (A, D) variable_0 variable_1  value
0      a          B          E      1
1      b          B          E      3
2      c          B          E      5
staticget_default_size()[source]
 
Return the default font size.
def f_40924332(df):
    """split a list in first column into multiple columns keeping other columns as well in pandas data frame `df`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
get_script_prefix()
numpy.distutils.misc_util.get_script_files(scripts)[source]
extra_js
numpy.distutils.misc_util.get_data_files(data)[source]
static source_to_code(data, path='<string>')  
Create a code object from Python source. The data argument can be whatever the compile() function supports (i.e. string or bytes). The path argument should be the “path” to where the source code originated from, which can be an abstract concept (e.g. location in a zip file). With the subsequent code object one can execute it in a module by running exec(code, module.__dict__).  New in version 3.4.   Changed in version 3.5: Made the method static.
def f_30759776(data):
    """extract attributes 'src="js/([^"]*\\bjquery\\b[^"]*)"' from string `data`
    """
    return  
 --------------------

def f_25388796():
    """Sum integers contained in strings in list `['', '3.4', '', '', '1.0']`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
coroutine asyncio.create_subprocess_shell(cmd, stdin=None, stdout=None, stderr=None, loop=None, limit=None, **kwds)  
Run the cmd shell command. The limit argument sets the buffer limit for StreamReader wrappers for Process.stdout and Process.stderr (if subprocess.PIPE is passed to stdout and stderr arguments). Return a Process instance. See the documentation of loop.subprocess_shell() for other parameters.  Important It is the application’s responsibility to ensure that all whitespace and special characters are quoted appropriately to avoid shell injection vulnerabilities. The shlex.quote() function can be used to properly escape whitespace and special shell characters in strings that are going to be used to construct shell commands.   Deprecated since version 3.8, will be removed in version 3.10: The loop parameter.
ctypes.util.find_msvcrt()  
Windows only: return the filename of the VC runtime library used by Python, and by the extension modules. If the name of the library cannot be determined, None is returned. If you need to free memory, for example, allocated by an extension module with a call to the free(void *), it is important that you use the function in the same library that allocated the memory.
socket.AF_VSOCK  
socket.IOCTL_VM_SOCKETS_GET_LOCAL_CID  
VMADDR*  
SO_VM*  
Constants for Linux host/guest communication. Availability: Linux >= 4.8.  New in version 3.7.
msvcrt — Useful routines from the MS VC++ runtime These functions provide access to some useful capabilities on Windows platforms. Some higher-level modules use these functions to build the Windows implementations of their services. For example, the getpass module uses this in the implementation of the getpass() function. Further documentation on these functions can be found in the Platform API documentation. The module implements both the normal and wide char variants of the console I/O api. The normal API deals only with ASCII characters and is of limited use for internationalized applications. The wide char API should be used where ever possible.  Changed in version 3.3: Operations in this module now raise OSError where IOError was raised.  File Operations  
msvcrt.locking(fd, mode, nbytes)  
Lock part of a file based on file descriptor fd from the C runtime. Raises OSError on failure. The locked region of the file extends from the current file position for nbytes bytes, and may continue beyond the end of the file. mode must be one of the LK_* constants listed below. Multiple regions in a file may be locked at the same time, but may not overlap. Adjacent regions are not merged; they must be unlocked individually. Raises an auditing event msvcrt.locking with arguments fd, mode, nbytes. 
  
msvcrt.LK_LOCK  
msvcrt.LK_RLCK  
Locks the specified bytes. If the bytes cannot be locked, the program immediately tries again after 1 second. If, after 10 attempts, the bytes cannot be locked, OSError is raised. 
  
msvcrt.LK_NBLCK  
msvcrt.LK_NBRLCK  
Locks the specified bytes. If the bytes cannot be locked, OSError is raised. 
  
msvcrt.LK_UNLCK  
Unlocks the specified bytes, which must have been previously locked. 
  
msvcrt.setmode(fd, flags)  
Set the line-end translation mode for the file descriptor fd. To set it to text mode, flags should be os.O_TEXT; for binary, it should be os.O_BINARY. 
  
msvcrt.open_osfhandle(handle, flags)  
Create a C runtime file descriptor from the file handle handle. The flags parameter should be a bitwise OR of os.O_APPEND, os.O_RDONLY, and os.O_TEXT. The returned file descriptor may be used as a parameter to os.fdopen() to create a file object. Raises an auditing event msvcrt.open_osfhandle with arguments handle, flags. 
  
msvcrt.get_osfhandle(fd)  
Return the file handle for the file descriptor fd. Raises OSError if fd is not recognized. Raises an auditing event msvcrt.get_osfhandle with argument fd. 
 Console I/O  
msvcrt.kbhit()  
Return True if a keypress is waiting to be read. 
  
msvcrt.getch()  
Read a keypress and return the resulting character as a byte string. Nothing is echoed to the console. This call will block if a keypress is not already available, but will not wait for Enter to be pressed. If the pressed key was a special function key, this will return '\000' or '\xe0'; the next call will return the keycode. The Control-C keypress cannot be read with this function. 
  
msvcrt.getwch()  
Wide char variant of getch(), returning a Unicode value. 
  
msvcrt.getche()  
Similar to getch(), but the keypress will be echoed if it represents a printable character. 
  
msvcrt.getwche()  
Wide char variant of getche(), returning a Unicode value. 
  
msvcrt.putch(char)  
Print the byte string char to the console without buffering. 
  
msvcrt.putwch(unicode_char)  
Wide char variant of putch(), accepting a Unicode value. 
  
msvcrt.ungetch(char)  
Cause the byte string char to be “pushed back” into the console buffer; it will be the next character read by getch() or getche(). 
  
msvcrt.ungetwch(unicode_char)  
Wide char variant of ungetch(), accepting a Unicode value. 
 Other Functions  
msvcrt.heapmin()  
Force the malloc() heap to clean itself up and return unused blocks to the operating system. On failure, this raises OSError.
socket.AF_VSOCK  
socket.IOCTL_VM_SOCKETS_GET_LOCAL_CID  
VMADDR*  
SO_VM*  
Constants for Linux host/guest communication. Availability: Linux >= 4.8.  New in version 3.7.
def f_804995():
    """Call a subprocess with arguments `c:\\Program Files\\VMware\\VMware Server\\vmware-cmd.bat` that may contain spaces
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
heapq — Heap queue algorithm Source code: Lib/heapq.py This module provides an implementation of the heap queue algorithm, also known as the priority queue algorithm. Heaps are binary trees for which every parent node has a value less than or equal to any of its children. This implementation uses arrays for which heap[k] <= heap[2*k+1] and heap[k] <= heap[2*k+2] for all k, counting elements from zero. For the sake of comparison, non-existing elements are considered to be infinite. The interesting property of a heap is that its smallest element is always the root, heap[0]. The API below differs from textbook heap algorithms in two aspects: (a) We use zero-based indexing. This makes the relationship between the index for a node and the indexes for its children slightly less obvious, but is more suitable since Python uses zero-based indexing. (b) Our pop method returns the smallest item, not the largest (called a “min heap” in textbooks; a “max heap” is more common in texts because of its suitability for in-place sorting). These two make it possible to view the heap as a regular Python list without surprises: heap[0] is the smallest item, and heap.sort() maintains the heap invariant! To create a heap, use a list initialized to [], or you can transform a populated list into a heap via function heapify(). The following functions are provided:  
heapq.heappush(heap, item)  
Push the value item onto the heap, maintaining the heap invariant. 
  
heapq.heappop(heap)  
Pop and return the smallest item from the heap, maintaining the heap invariant. If the heap is empty, IndexError is raised. To access the smallest item without popping it, use heap[0]. 
  
heapq.heappushpop(heap, item)  
Push item on the heap, then pop and return the smallest item from the heap. The combined action runs more efficiently than heappush() followed by a separate call to heappop(). 
  
heapq.heapify(x)  
Transform list x into a heap, in-place, in linear time. 
  
heapq.heapreplace(heap, item)  
Pop and return the smallest item from the heap, and also push the new item. The heap size doesn’t change. If the heap is empty, IndexError is raised. This one step operation is more efficient than a heappop() followed by heappush() and can be more appropriate when using a fixed-size heap. The pop/push combination always returns an element from the heap and replaces it with item. The value returned may be larger than the item added. If that isn’t desired, consider using heappushpop() instead. Its push/pop combination returns the smaller of the two values, leaving the larger value on the heap. 
 The module also offers three general purpose functions based on heaps.  
heapq.merge(*iterables, key=None, reverse=False)  
Merge multiple sorted inputs into a single sorted output (for example, merge timestamped entries from multiple log files). Returns an iterator over the sorted values. Similar to sorted(itertools.chain(*iterables)) but returns an iterable, does not pull the data into memory all at once, and assumes that each of the input streams is already sorted (smallest to largest). Has two optional arguments which must be specified as keyword arguments. key specifies a key function of one argument that is used to extract a comparison key from each input element. The default value is None (compare the elements directly). reverse is a boolean value. If set to True, then the input elements are merged as if each comparison were reversed. To achieve behavior similar to sorted(itertools.chain(*iterables), reverse=True), all iterables must be sorted from largest to smallest.  Changed in version 3.5: Added the optional key and reverse parameters.  
  
heapq.nlargest(n, iterable, key=None)  
Return a list with the n largest elements from the dataset defined by iterable. key, if provided, specifies a function of one argument that is used to extract a comparison key from each element in iterable (for example, key=str.lower). Equivalent to: sorted(iterable, key=key,
reverse=True)[:n]. 
  
heapq.nsmallest(n, iterable, key=None)  
Return a list with the n smallest elements from the dataset defined by iterable. key, if provided, specifies a function of one argument that is used to extract a comparison key from each element in iterable (for example, key=str.lower). Equivalent to: sorted(iterable, key=key)[:n]. 
 The latter two functions perform best for smaller values of n. For larger values, it is more efficient to use the sorted() function. Also, when n==1, it is more efficient to use the built-in min() and max() functions. If repeated usage of these functions is required, consider turning the iterable into an actual heap. Basic Examples A heapsort can be implemented by pushing all values onto a heap and then popping off the smallest values one at a time: >>> def heapsort(iterable):
...     h = []
...     for value in iterable:
...         heappush(h, value)
...     return [heappop(h) for i in range(len(h))]
...
>>> heapsort([1, 3, 5, 7, 9, 2, 4, 6, 8, 0])
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
 This is similar to sorted(iterable), but unlike sorted(), this implementation is not stable. Heap elements can be tuples. This is useful for assigning comparison values (such as task priorities) alongside the main record being tracked: >>> h = []
>>> heappush(h, (5, 'write code'))
>>> heappush(h, (7, 'release product'))
>>> heappush(h, (1, 'write spec'))
>>> heappush(h, (3, 'create tests'))
>>> heappop(h)
(1, 'write spec')
 Priority Queue Implementation Notes A priority queue is common use for a heap, and it presents several implementation challenges:  Sort stability: how do you get two tasks with equal priorities to be returned in the order they were originally added? Tuple comparison breaks for (priority, task) pairs if the priorities are equal and the tasks do not have a default comparison order. If the priority of a task changes, how do you move it to a new position in the heap? Or if a pending task needs to be deleted, how do you find it and remove it from the queue?  A solution to the first two challenges is to store entries as 3-element list including the priority, an entry count, and the task. The entry count serves as a tie-breaker so that two tasks with the same priority are returned in the order they were added. And since no two entry counts are the same, the tuple comparison will never attempt to directly compare two tasks. Another solution to the problem of non-comparable tasks is to create a wrapper class that ignores the task item and only compares the priority field: from dataclasses import dataclass, field
from typing import Any

@dataclass(order=True)
class PrioritizedItem:
    priority: int
    item: Any=field(compare=False)
 The remaining challenges revolve around finding a pending task and making changes to its priority or removing it entirely. Finding a task can be done with a dictionary pointing to an entry in the queue. Removing the entry or changing its priority is more difficult because it would break the heap structure invariants. So, a possible solution is to mark the entry as removed and add a new entry with the revised priority: pq = []                         # list of entries arranged in a heap
entry_finder = {}               # mapping of tasks to entries
REMOVED = '<removed-task>'      # placeholder for a removed task
counter = itertools.count()     # unique sequence count

def add_task(task, priority=0):
    'Add a new task or update the priority of an existing task'
    if task in entry_finder:
        remove_task(task)
    count = next(counter)
    entry = [priority, count, task]
    entry_finder[task] = entry
    heappush(pq, entry)

def remove_task(task):
    'Mark an existing task as REMOVED.  Raise KeyError if not found.'
    entry = entry_finder.pop(task)
    entry[-1] = REMOVED

def pop_task():
    'Remove and return the lowest priority task. Raise KeyError if empty.'
    while pq:
        priority, count, task = heappop(pq)
        if task is not REMOVED:
            del entry_finder[task]
            return task
    raise KeyError('pop from an empty priority queue')
 Theory Heaps are arrays for which a[k] <= a[2*k+1] and a[k] <= a[2*k+2] for all k, counting elements from 0. For the sake of comparison, non-existing elements are considered to be infinite. The interesting property of a heap is that a[0] is always its smallest element. The strange invariant above is meant to be an efficient memory representation for a tournament. The numbers below are k, not a[k]:                                0

              1                                 2

      3               4                5               6

  7       8       9       10      11      12      13      14

15 16   17 18   19 20   21 22   23 24   25 26   27 28   29 30
 In the tree above, each cell k is topping 2*k+1 and 2*k+2. In a usual binary tournament we see in sports, each cell is the winner over the two cells it tops, and we can trace the winner down the tree to see all opponents s/he had. However, in many computer applications of such tournaments, we do not need to trace the history of a winner. To be more memory efficient, when a winner is promoted, we try to replace it by something else at a lower level, and the rule becomes that a cell and the two cells it tops contain three different items, but the top cell “wins” over the two topped cells. If this heap invariant is protected at all time, index 0 is clearly the overall winner. The simplest algorithmic way to remove it and find the “next” winner is to move some loser (let’s say cell 30 in the diagram above) into the 0 position, and then percolate this new 0 down the tree, exchanging values, until the invariant is re-established. This is clearly logarithmic on the total number of items in the tree. By iterating over all items, you get an O(n log n) sort. A nice feature of this sort is that you can efficiently insert new items while the sort is going on, provided that the inserted items are not “better” than the last 0’th element you extracted. This is especially useful in simulation contexts, where the tree holds all incoming events, and the “win” condition means the smallest scheduled time. When an event schedules other events for execution, they are scheduled into the future, so they can easily go into the heap. So, a heap is a good structure for implementing schedulers (this is what I used for my MIDI sequencer :-). Various structures for implementing schedulers have been extensively studied, and heaps are good for this, as they are reasonably speedy, the speed is almost constant, and the worst case is not much different than the average case. However, there are other representations which are more efficient overall, yet the worst cases might be terrible. Heaps are also very useful in big disk sorts. You most probably all know that a big sort implies producing “runs” (which are pre-sorted sequences, whose size is usually related to the amount of CPU memory), followed by a merging passes for these runs, which merging is often very cleverly organised 1. It is very important that the initial sort produces the longest runs possible. Tournaments are a good way to achieve that. If, using all the memory available to hold a tournament, you replace and percolate items that happen to fit the current run, you’ll produce runs which are twice the size of the memory for random input, and much better for input fuzzily ordered. Moreover, if you output the 0’th item on disk and get an input which may not fit in the current tournament (because the value “wins” over the last output value), it cannot fit in the heap, so the size of the heap decreases. The freed memory could be cleverly reused immediately for progressively building a second heap, which grows at exactly the same rate the first heap is melting. When the first heap completely vanishes, you switch heaps and start a new run. Clever and quite effective! In a word, heaps are useful memory structures to know. I use them in a few applications, and I think it is good to keep a ‘heap’ module around. :-) Footnotes  
1  
The disk balancing algorithms which are current, nowadays, are more annoying than clever, and this is a consequence of the seeking capabilities of the disks. On devices which cannot seek, like big tape drives, the story was quite different, and one had to be very clever to ensure (far in advance) that each tape movement will be the most effective possible (that is, will best participate at “progressing” the merge). Some tapes were even able to read backwards, and this was also used to avoid the rewinding time. Believe me, real good tape sorts were quite spectacular to watch! From all times, sorting has always been a Great Art! :-)
reverse()  
Reverse the elements of the deque in-place and then return None.  New in version 3.2.
reverse()
heapq.heapify(x)  
Transform list x into a heap, in-place, in linear time.
reverse_order()  
This method for the Stats class reverses the ordering of the basic list within the object. Note that by default ascending vs descending order is properly selected based on the sort key of choice.
def f_26441253(q):
    """reverse a priority queue `q` in python without using classes
    """
     
 --------------------
Please refer to the following documentation to generate the code:
pandas.core.groupby.DataFrameGroupBy.boxplot   DataFrameGroupBy.boxplot(subplots=True, column=None, fontsize=None, rot=0, grid=True, ax=None, figsize=None, layout=None, sharex=False, sharey=True, backend=None, **kwargs)[source]
 
Make box plots from DataFrameGroupBy data.  Parameters 
 
grouped:Grouped DataFrame


subplots:bool


 False - no subplots will be used True - create a subplot for each group.   
column:column name or list of names, or vector


Can be any valid input to groupby.  
fontsize:int or str


rot:label rotation angle


grid:Setting this to True will show the grid


ax:Matplotlib axis object, default None


figsize:A tuple (width, height) in inches


layout:tuple (optional)


The layout of the plot: (rows, columns).  
sharex:bool, default False


Whether x-axes will be shared among subplots.  
sharey:bool, default True


Whether y-axes will be shared among subplots.  
backend:str, default None


Backend to use instead of the backend specified in the option plotting.backend. For instance, ‘matplotlib’. Alternatively, to specify the plotting.backend for the whole session, set pd.options.plotting.backend.  New in version 1.0.0.   **kwargs

All other plotting keyword arguments to be passed to matplotlib’s boxplot function.    Returns 
 dict of key/value = group key/DataFrame.boxplot return value
or DataFrame.boxplot return value in case subplots=figures=False
   Examples You can create boxplots for grouped data and show them as separate subplots: 
>>> import itertools
>>> tuples = [t for t in itertools.product(range(1000), range(4))]
>>> index = pd.MultiIndex.from_tuples(tuples, names=['lvl0', 'lvl1'])
>>> data = np.random.randn(len(index),4)
>>> df = pd.DataFrame(data, columns=list('ABCD'), index=index)
>>> grouped = df.groupby(level='lvl1')
>>> grouped.boxplot(rot=45, fontsize=12, figsize=(8,10))  
     The subplots=False option shows the boxplots in a single figure. 
>>> grouped.boxplot(subplots=False, rot=45, fontsize=12)
pandas.DataFrame.plot.bar   DataFrame.plot.bar(x=None, y=None, **kwargs)[source]
 
Vertical bar plot. A bar plot is a plot that presents categorical data with rectangular bars with lengths proportional to the values that they represent. A bar plot shows comparisons among discrete categories. One axis of the plot shows the specific categories being compared, and the other axis represents a measured value.  Parameters 
 
x:label or position, optional


Allows plotting of one column versus another. If not specified, the index of the DataFrame is used.  
y:label or position, optional


Allows plotting of one column versus another. If not specified, all numerical columns are used.  
color:str, array-like, or dict, optional


The color for each of the DataFrame’s columns. Possible values are:  
 A single color string referred to by name, RGB or RGBA code,

for instance ‘red’ or ‘#a98d19’.    
 A sequence of color strings referred to by name, RGB or RGBA

code, which will be used for each column recursively. For instance [‘green’,’yellow’] each column’s bar will be filled in green or yellow, alternatively. If there is only a single column to be plotted, then only the first color from the color list will be used.    
 A dict of the form {column name:color}, so that each column will be


colored accordingly. For example, if your columns are called a and b, then passing {‘a’: ‘green’, ‘b’: ‘red’} will color bars for column a in green and bars for column b in red.      New in version 1.1.0.   **kwargs

Additional keyword arguments are documented in DataFrame.plot().    Returns 
 matplotlib.axes.Axes or np.ndarray of them

An ndarray is returned with one matplotlib.axes.Axes per column when subplots=True.      See also  DataFrame.plot.barh

Horizontal bar plot.  DataFrame.plot

Make plots of a DataFrame.  matplotlib.pyplot.bar

Make a bar plot with matplotlib.    Examples Basic plot. 
>>> df = pd.DataFrame({'lab':['A', 'B', 'C'], 'val':[10, 30, 20]})
>>> ax = df.plot.bar(x='lab', y='val', rot=0)
     Plot a whole dataframe to a bar plot. Each column is assigned a distinct color, and each row is nested in a group along the horizontal axis. 
>>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]
>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]
>>> index = ['snail', 'pig', 'elephant',
...          'rabbit', 'giraffe', 'coyote', 'horse']
>>> df = pd.DataFrame({'speed': speed,
...                    'lifespan': lifespan}, index=index)
>>> ax = df.plot.bar(rot=0)
     Plot stacked bar charts for the DataFrame 
>>> ax = df.plot.bar(stacked=True)
     Instead of nesting, the figure can be split by column with subplots=True. In this case, a numpy.ndarray of matplotlib.axes.Axes are returned. 
>>> axes = df.plot.bar(rot=0, subplots=True)
>>> axes[1].legend(loc=2)  
     If you don’t like the default colours, you can specify how you’d like each column to be colored. 
>>> axes = df.plot.bar(
...     rot=0, subplots=True, color={"speed": "red", "lifespan": "green"}
... )
>>> axes[1].legend(loc=2)  
     Plot a single column. 
>>> ax = df.plot.bar(y='speed', rot=0)
     Plot only selected categories for the DataFrame. 
>>> ax = df.plot.bar(x='lifespan', rot=0)
pandas.Series.plot.bar   Series.plot.bar(x=None, y=None, **kwargs)[source]
 
Vertical bar plot. A bar plot is a plot that presents categorical data with rectangular bars with lengths proportional to the values that they represent. A bar plot shows comparisons among discrete categories. One axis of the plot shows the specific categories being compared, and the other axis represents a measured value.  Parameters 
 
x:label or position, optional


Allows plotting of one column versus another. If not specified, the index of the DataFrame is used.  
y:label or position, optional


Allows plotting of one column versus another. If not specified, all numerical columns are used.  
color:str, array-like, or dict, optional


The color for each of the DataFrame’s columns. Possible values are:  
 A single color string referred to by name, RGB or RGBA code,

for instance ‘red’ or ‘#a98d19’.    
 A sequence of color strings referred to by name, RGB or RGBA

code, which will be used for each column recursively. For instance [‘green’,’yellow’] each column’s bar will be filled in green or yellow, alternatively. If there is only a single column to be plotted, then only the first color from the color list will be used.    
 A dict of the form {column name:color}, so that each column will be


colored accordingly. For example, if your columns are called a and b, then passing {‘a’: ‘green’, ‘b’: ‘red’} will color bars for column a in green and bars for column b in red.      New in version 1.1.0.   **kwargs

Additional keyword arguments are documented in DataFrame.plot().    Returns 
 matplotlib.axes.Axes or np.ndarray of them

An ndarray is returned with one matplotlib.axes.Axes per column when subplots=True.      See also  DataFrame.plot.barh

Horizontal bar plot.  DataFrame.plot

Make plots of a DataFrame.  matplotlib.pyplot.bar

Make a bar plot with matplotlib.    Examples Basic plot. 
>>> df = pd.DataFrame({'lab':['A', 'B', 'C'], 'val':[10, 30, 20]})
>>> ax = df.plot.bar(x='lab', y='val', rot=0)
     Plot a whole dataframe to a bar plot. Each column is assigned a distinct color, and each row is nested in a group along the horizontal axis. 
>>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]
>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]
>>> index = ['snail', 'pig', 'elephant',
...          'rabbit', 'giraffe', 'coyote', 'horse']
>>> df = pd.DataFrame({'speed': speed,
...                    'lifespan': lifespan}, index=index)
>>> ax = df.plot.bar(rot=0)
     Plot stacked bar charts for the DataFrame 
>>> ax = df.plot.bar(stacked=True)
     Instead of nesting, the figure can be split by column with subplots=True. In this case, a numpy.ndarray of matplotlib.axes.Axes are returned. 
>>> axes = df.plot.bar(rot=0, subplots=True)
>>> axes[1].legend(loc=2)  
     If you don’t like the default colours, you can specify how you’d like each column to be colored. 
>>> axes = df.plot.bar(
...     rot=0, subplots=True, color={"speed": "red", "lifespan": "green"}
... )
>>> axes[1].legend(loc=2)  
     Plot a single column. 
>>> ax = df.plot.bar(y='speed', rot=0)
     Plot only selected categories for the DataFrame. 
>>> ax = df.plot.bar(x='lifespan', rot=0)
pandas.DataFrame.plot.barh   DataFrame.plot.barh(x=None, y=None, **kwargs)[source]
 
Make a horizontal bar plot. A horizontal bar plot is a plot that presents quantitative data with rectangular bars with lengths proportional to the values that they represent. A bar plot shows comparisons among discrete categories. One axis of the plot shows the specific categories being compared, and the other axis represents a measured value.  Parameters 
 
x:label or position, optional


Allows plotting of one column versus another. If not specified, the index of the DataFrame is used.  
y:label or position, optional


Allows plotting of one column versus another. If not specified, all numerical columns are used.  
color:str, array-like, or dict, optional


The color for each of the DataFrame’s columns. Possible values are:  
 A single color string referred to by name, RGB or RGBA code,

for instance ‘red’ or ‘#a98d19’.    
 A sequence of color strings referred to by name, RGB or RGBA

code, which will be used for each column recursively. For instance [‘green’,’yellow’] each column’s bar will be filled in green or yellow, alternatively. If there is only a single column to be plotted, then only the first color from the color list will be used.    
 A dict of the form {column name:color}, so that each column will be


colored accordingly. For example, if your columns are called a and b, then passing {‘a’: ‘green’, ‘b’: ‘red’} will color bars for column a in green and bars for column b in red.      New in version 1.1.0.   **kwargs

Additional keyword arguments are documented in DataFrame.plot().    Returns 
 matplotlib.axes.Axes or np.ndarray of them

An ndarray is returned with one matplotlib.axes.Axes per column when subplots=True.      See also  DataFrame.plot.bar

Vertical bar plot.  DataFrame.plot

Make plots of DataFrame using matplotlib.  matplotlib.axes.Axes.bar

Plot a vertical bar plot using matplotlib.    Examples Basic example 
>>> df = pd.DataFrame({'lab': ['A', 'B', 'C'], 'val': [10, 30, 20]})
>>> ax = df.plot.barh(x='lab', y='val')
     Plot a whole DataFrame to a horizontal bar plot 
>>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]
>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]
>>> index = ['snail', 'pig', 'elephant',
...          'rabbit', 'giraffe', 'coyote', 'horse']
>>> df = pd.DataFrame({'speed': speed,
...                    'lifespan': lifespan}, index=index)
>>> ax = df.plot.barh()
     Plot stacked barh charts for the DataFrame 
>>> ax = df.plot.barh(stacked=True)
     We can specify colors for each column 
>>> ax = df.plot.barh(color={"speed": "red", "lifespan": "green"})
     Plot a column of the DataFrame to a horizontal bar plot 
>>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]
>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]
>>> index = ['snail', 'pig', 'elephant',
...          'rabbit', 'giraffe', 'coyote', 'horse']
>>> df = pd.DataFrame({'speed': speed,
...                    'lifespan': lifespan}, index=index)
>>> ax = df.plot.barh(y='speed')
     Plot DataFrame versus the desired column 
>>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]
>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]
>>> index = ['snail', 'pig', 'elephant',
...          'rabbit', 'giraffe', 'coyote', 'horse']
>>> df = pd.DataFrame({'speed': speed,
...                    'lifespan': lifespan}, index=index)
>>> ax = df.plot.barh(x='lifespan')
pandas.Series.plot.barh   Series.plot.barh(x=None, y=None, **kwargs)[source]
 
Make a horizontal bar plot. A horizontal bar plot is a plot that presents quantitative data with rectangular bars with lengths proportional to the values that they represent. A bar plot shows comparisons among discrete categories. One axis of the plot shows the specific categories being compared, and the other axis represents a measured value.  Parameters 
 
x:label or position, optional


Allows plotting of one column versus another. If not specified, the index of the DataFrame is used.  
y:label or position, optional


Allows plotting of one column versus another. If not specified, all numerical columns are used.  
color:str, array-like, or dict, optional


The color for each of the DataFrame’s columns. Possible values are:  
 A single color string referred to by name, RGB or RGBA code,

for instance ‘red’ or ‘#a98d19’.    
 A sequence of color strings referred to by name, RGB or RGBA

code, which will be used for each column recursively. For instance [‘green’,’yellow’] each column’s bar will be filled in green or yellow, alternatively. If there is only a single column to be plotted, then only the first color from the color list will be used.    
 A dict of the form {column name:color}, so that each column will be


colored accordingly. For example, if your columns are called a and b, then passing {‘a’: ‘green’, ‘b’: ‘red’} will color bars for column a in green and bars for column b in red.      New in version 1.1.0.   **kwargs

Additional keyword arguments are documented in DataFrame.plot().    Returns 
 matplotlib.axes.Axes or np.ndarray of them

An ndarray is returned with one matplotlib.axes.Axes per column when subplots=True.      See also  DataFrame.plot.bar

Vertical bar plot.  DataFrame.plot

Make plots of DataFrame using matplotlib.  matplotlib.axes.Axes.bar

Plot a vertical bar plot using matplotlib.    Examples Basic example 
>>> df = pd.DataFrame({'lab': ['A', 'B', 'C'], 'val': [10, 30, 20]})
>>> ax = df.plot.barh(x='lab', y='val')
     Plot a whole DataFrame to a horizontal bar plot 
>>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]
>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]
>>> index = ['snail', 'pig', 'elephant',
...          'rabbit', 'giraffe', 'coyote', 'horse']
>>> df = pd.DataFrame({'speed': speed,
...                    'lifespan': lifespan}, index=index)
>>> ax = df.plot.barh()
     Plot stacked barh charts for the DataFrame 
>>> ax = df.plot.barh(stacked=True)
     We can specify colors for each column 
>>> ax = df.plot.barh(color={"speed": "red", "lifespan": "green"})
     Plot a column of the DataFrame to a horizontal bar plot 
>>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]
>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]
>>> index = ['snail', 'pig', 'elephant',
...          'rabbit', 'giraffe', 'coyote', 'horse']
>>> df = pd.DataFrame({'speed': speed,
...                    'lifespan': lifespan}, index=index)
>>> ax = df.plot.barh(y='speed')
     Plot DataFrame versus the desired column 
>>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]
>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]
>>> index = ['snail', 'pig', 'elephant',
...          'rabbit', 'giraffe', 'coyote', 'horse']
>>> df = pd.DataFrame({'speed': speed,
...                    'lifespan': lifespan}, index=index)
>>> ax = df.plot.barh(x='lifespan')
def f_18897261(df):
    """make a barplot of data in column `group` of dataframe `df` colour-coded according to list `color`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
statichexify(match)[source]
statistics.multimode(data)  
Return a list of the most frequently occurring values in the order they were first encountered in the data. Will return more than one result if there are multiple modes or an empty list if the data is empty: >>> multimode('aabbbbccddddeeffffgg')
['b', 'd', 'f']
>>> multimode('')
[]
  New in version 3.8.
residuals(data, params=None) [source]
 
Determine residuals of data to model. For each point, the shortest (orthogonal) distance to the line is returned. It is obtained by projecting the data onto the line.  Parameters 
 
data(N, dim) array 

N points in a space of dimension dim.  
params(2, ) array, optional 

Optional custom parameter set in the form (origin, direction).    Returns 
 
residuals(N, ) array 

Residual for each data point.
estimate(data) [source]
 
Estimate line model from data. This minimizes the sum of shortest (orthogonal) distances from the given data points to the estimated line.  Parameters 
 
data(N, dim) array 

N points in a space of dimensionality dim >= 2.    Returns 
 
successbool 

True, if model estimation succeeds.
werkzeug.http.generate_etag(data)  
Generate an etag for some data.  Changed in version 2.0: Use SHA-1. MD5 may not be available in some environments.   Parameters 
data (bytes) –   Return type 
str
def f_373194(data):
    """find all matches of regex pattern '([a-fA-F\\d]{32})' in string `data`
    """
    return  
 --------------------

def f_518021(my_list):
    """Get the length of list `my_list`
    """
    return  
 --------------------

def f_518021(l):
    """Getting the length of array `l`
    """
    return  
 --------------------

def f_518021(s):
    """Getting the length of array `s`
    """
    return  
 --------------------

def f_518021(my_tuple):
    """Getting the length of `my_tuple`
    """
    return  
 --------------------

def f_518021(my_string):
    """Getting the length of `my_string`
    """
    return  
 --------------------

def f_40452956():
    """remove escape character from string "\\a"
    """
    return  
 --------------------

def f_8687018():
    """replace each 'a' with 'b' and each 'b' with 'a' in the string 'obama' in a single pass.
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
remove_folder(folder)  
Delete the folder whose name is folder. If the folder contains any messages, a NotEmptyError exception will be raised and the folder will not be deleted.
remove_folder(folder)  
Delete the folder whose name is folder. If the folder contains any messages, a NotEmptyError exception will be raised and the folder will not be deleted.
test.support.rmtree(path)  
Call shutil.rmtree() on path or call os.lstat() and os.rmdir() to remove a path and its contents. On Windows platforms, this is wrapped with a wait loop that checks for the existence of the files.
tf.raw_ops.IFFT3D Inverse 3D fast Fourier transform.  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.raw_ops.IFFT3D  
tf.raw_ops.IFFT3D(
    input, name=None
)
 Computes the inverse 3-dimensional discrete Fourier transform over the inner-most 3 dimensions of input.
 


 Args
  input   A Tensor. Must be one of the following types: complex64, complex128. A complex tensor.  
  name   A name for the operation (optional).   
 


 Returns   A Tensor. Has the same type as input.
tf.raw_ops.IFFT2D Inverse 2D fast Fourier transform.  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.raw_ops.IFFT2D  
tf.raw_ops.IFFT2D(
    input, name=None
)
 Computes the inverse 2-dimensional discrete Fourier transform over the inner-most 2 dimensions of input.
 


 Args
  input   A Tensor. Must be one of the following types: complex64, complex128. A complex tensor.  
  name   A name for the operation (optional).   
 


 Returns   A Tensor. Has the same type as input.
def f_303200():
    """remove directory tree '/folder_name'
    """
     
 --------------------
Please refer to the following documentation to generate the code:
pandas.tseries.offsets.FY5253.weekday   FY5253.weekday
pandas.tseries.offsets.FY5253Quarter.weekday   FY5253Quarter.weekday
pandas.tseries.offsets.WeekOfMonth.weekday   WeekOfMonth.weekday
pandas.tseries.offsets.Week.weekday   Week.weekday
pandas.PeriodIndex.weekday   propertyPeriodIndex.weekday
 
The day of the week with Monday=0, Sunday=6.
def f_13740672(data):
    """create a new column `weekday` in pandas data frame `data` based on the values in column `my_dt`
    """
     
 --------------------

def f_20950650(x):
    """reverse sort Counter `x` by values
    """
    return  
 --------------------

def f_20950650(x):
    """reverse sort counter `x` by value
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
numpy.ma.append   ma.append(a, b, axis=None)[source]
 
Append values to the end of an array.  New in version 1.9.0.   Parameters 
 
aarray_like


Values are appended to a copy of this array.  
barray_like


These values are appended to a copy of a. It must be of the correct shape (the same shape as a, excluding axis). If axis is not specified, b can be any shape and will be flattened before use.  
axisint, optional


The axis along which v are appended. If axis is not given, both a and b are flattened before use.    Returns 
 
appendMaskedArray


A copy of a with b appended to axis. Note that append does not occur in-place: a new array is allocated and filled. If axis is None, the result is a flattened array.      See also  numpy.append

Equivalent function in the top-level NumPy module.    Examples >>> import numpy.ma as ma
>>> a = ma.masked_values([1, 2, 3], 2)
>>> b = ma.masked_values([[4, 5, 6], [7, 8, 9]], 7)
>>> ma.append(a, b)
masked_array(data=[1, --, 3, 4, 5, 6, --, 8, 9],
             mask=[False,  True, False, False, False, False,  True, False,
                   False],
       fill_value=999999)
tf.experimental.numpy.append TensorFlow variant of NumPy's append. 
tf.experimental.numpy.append(
    arr, values, axis=None
)
 See the NumPy documentation for numpy.append.
numpy.append   numpy.append(arr, values, axis=None)[source]
 
Append values to the end of an array.  Parameters 
 
arrarray_like


Values are appended to a copy of this array.  
valuesarray_like


These values are appended to a copy of arr. It must be of the correct shape (the same shape as arr, excluding axis). If axis is not specified, values can be any shape and will be flattened before use.  
axisint, optional


The axis along which values are appended. If axis is not given, both arr and values are flattened before use.    Returns 
 
appendndarray


A copy of arr with values appended to axis. Note that append does not occur in-place: a new array is allocated and filled. If axis is None, out is a flattened array.      See also  insert

Insert elements into an array.  delete

Delete elements from an array.    Examples >>> np.append([1, 2, 3], [[4, 5, 6], [7, 8, 9]])
array([1, 2, 3, ..., 7, 8, 9])
 When axis is specified, values must have the correct shape. >>> np.append([[1, 2, 3], [4, 5, 6]], [[7, 8, 9]], axis=0)
array([[1, 2, 3],
       [4, 5, 6],
       [7, 8, 9]])
>>> np.append([[1, 2, 3], [4, 5, 6]], [7, 8, 9], axis=0)
Traceback (most recent call last):
    ...
ValueError: all the input arrays must have same number of dimensions, but
the array at index 0 has 2 dimension(s) and the array at index 1 has 1
dimension(s)
operator.iconcat(a, b)  
operator.__iconcat__(a, b)  
a = iconcat(a, b) is equivalent to a += b for a and b sequences.
operator.iconcat(a, b)  
operator.__iconcat__(a, b)  
a = iconcat(a, b) is equivalent to a += b for a and b sequences.
def f_9775297(a, b):
    """append a numpy array 'b' to a numpy array 'a'
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
numpy.ma.append   ma.append(a, b, axis=None)[source]
 
Append values to the end of an array.  New in version 1.9.0.   Parameters 
 
aarray_like


Values are appended to a copy of this array.  
barray_like


These values are appended to a copy of a. It must be of the correct shape (the same shape as a, excluding axis). If axis is not specified, b can be any shape and will be flattened before use.  
axisint, optional


The axis along which v are appended. If axis is not given, both a and b are flattened before use.    Returns 
 
appendMaskedArray


A copy of a with b appended to axis. Note that append does not occur in-place: a new array is allocated and filled. If axis is None, the result is a flattened array.      See also  numpy.append

Equivalent function in the top-level NumPy module.    Examples >>> import numpy.ma as ma
>>> a = ma.masked_values([1, 2, 3], 2)
>>> b = ma.masked_values([[4, 5, 6], [7, 8, 9]], 7)
>>> ma.append(a, b)
masked_array(data=[1, --, 3, 4, 5, 6, --, 8, 9],
             mask=[False,  True, False, False, False, False,  True, False,
                   False],
       fill_value=999999)
tf.experimental.numpy.concatenate TensorFlow variant of NumPy's concatenate. 
tf.experimental.numpy.concatenate(
    arys, axis=0
)
 See the NumPy documentation for numpy.concatenate.
numpy.c_   numpy.c_ = <numpy.lib.index_tricks.CClass object>
 
Translates slice objects to concatenation along the second axis. This is short-hand for np.r_['-1,2,0', index expression], which is useful because of its common occurrence. In particular, arrays will be stacked along their last axis after being upgraded to at least 2-D with 1’s post-pended to the shape (column vectors made out of 1-D arrays).  See also  column_stack

Stack 1-D arrays as columns into a 2-D array.  r_

For more detailed documentation.    Examples >>> np.c_[np.array([1,2,3]), np.array([4,5,6])]
array([[1, 4],
       [2, 5],
       [3, 6]])
>>> np.c_[np.array([[1,2,3]]), 0, 0, np.array([[4,5,6]])]
array([[1, 2, 3, ..., 4, 5, 6]])
numpy.concatenate   numpy.concatenate((a1, a2, ...), axis=0, out=None, dtype=None, casting="same_kind")
 
Join a sequence of arrays along an existing axis.  Parameters 
 
a1, a2, …sequence of array_like


The arrays must have the same shape, except in the dimension corresponding to axis (the first, by default).  
axisint, optional


The axis along which the arrays will be joined. If axis is None, arrays are flattened before use. Default is 0.  
outndarray, optional


If provided, the destination to place the result. The shape must be correct, matching that of what concatenate would have returned if no out argument were specified.  
dtypestr or dtype


If provided, the destination array will have this dtype. Cannot be provided together with out.  New in version 1.20.0.   
casting{‘no’, ‘equiv’, ‘safe’, ‘same_kind’, ‘unsafe’}, optional


Controls what kind of data casting may occur. Defaults to ‘same_kind’.  New in version 1.20.0.     Returns 
 
resndarray


The concatenated array.      See also  ma.concatenate

Concatenate function that preserves input masks.  array_split

Split an array into multiple sub-arrays of equal or near-equal size.  split

Split array into a list of multiple sub-arrays of equal size.  hsplit

Split array into multiple sub-arrays horizontally (column wise).  vsplit

Split array into multiple sub-arrays vertically (row wise).  dsplit

Split array into multiple sub-arrays along the 3rd axis (depth).  stack

Stack a sequence of arrays along a new axis.  block

Assemble arrays from blocks.  hstack

Stack arrays in sequence horizontally (column wise).  vstack

Stack arrays in sequence vertically (row wise).  dstack

Stack arrays in sequence depth wise (along third dimension).  column_stack

Stack 1-D arrays as columns into a 2-D array.    Notes When one or more of the arrays to be concatenated is a MaskedArray, this function will return a MaskedArray object instead of an ndarray, but the input masks are not preserved. In cases where a MaskedArray is expected as input, use the ma.concatenate function from the masked array module instead. Examples >>> a = np.array([[1, 2], [3, 4]])
>>> b = np.array([[5, 6]])
>>> np.concatenate((a, b), axis=0)
array([[1, 2],
       [3, 4],
       [5, 6]])
>>> np.concatenate((a, b.T), axis=1)
array([[1, 2, 5],
       [3, 4, 6]])
>>> np.concatenate((a, b), axis=None)
array([1, 2, 3, 4, 5, 6])
 This function will not preserve masking of MaskedArray inputs. >>> a = np.ma.arange(3)
>>> a[1] = np.ma.masked
>>> b = np.arange(2, 5)
>>> a
masked_array(data=[0, --, 2],
             mask=[False,  True, False],
       fill_value=999999)
>>> b
array([2, 3, 4])
>>> np.concatenate([a, b])
masked_array(data=[0, 1, 2, 2, 3, 4],
             mask=False,
       fill_value=999999)
>>> np.ma.concatenate([a, b])
masked_array(data=[0, --, 2, 2, 3, 4],
             mask=[False,  True, False, False, False, False],
       fill_value=999999)
numpy.ma.concatenate   ma.concatenate(arrays, axis=0)[source]
 
Concatenate a sequence of arrays along the given axis.  Parameters 
 
arrayssequence of array_like


The arrays must have the same shape, except in the dimension corresponding to axis (the first, by default).  
axisint, optional


The axis along which the arrays will be joined. Default is 0.    Returns 
 
resultMaskedArray


The concatenated array with any masked entries preserved.      See also  numpy.concatenate

Equivalent function in the top-level NumPy module.    Examples >>> import numpy.ma as ma
>>> a = ma.arange(3)
>>> a[1] = ma.masked
>>> b = ma.arange(2, 5)
>>> a
masked_array(data=[0, --, 2],
             mask=[False,  True, False],
       fill_value=999999)
>>> b
masked_array(data=[2, 3, 4],
             mask=False,
       fill_value=999999)
>>> ma.concatenate([a, b])
masked_array(data=[0, --, 2, 2, 3, 4],
             mask=[False,  True, False, False, False, False],
       fill_value=999999)
def f_21887754(a, b):
    """numpy concatenate two arrays `a` and `b` along the first axis
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
numpy.ma.append   ma.append(a, b, axis=None)[source]
 
Append values to the end of an array.  New in version 1.9.0.   Parameters 
 
aarray_like


Values are appended to a copy of this array.  
barray_like


These values are appended to a copy of a. It must be of the correct shape (the same shape as a, excluding axis). If axis is not specified, b can be any shape and will be flattened before use.  
axisint, optional


The axis along which v are appended. If axis is not given, both a and b are flattened before use.    Returns 
 
appendMaskedArray


A copy of a with b appended to axis. Note that append does not occur in-place: a new array is allocated and filled. If axis is None, the result is a flattened array.      See also  numpy.append

Equivalent function in the top-level NumPy module.    Examples >>> import numpy.ma as ma
>>> a = ma.masked_values([1, 2, 3], 2)
>>> b = ma.masked_values([[4, 5, 6], [7, 8, 9]], 7)
>>> ma.append(a, b)
masked_array(data=[1, --, 3, 4, 5, 6, --, 8, 9],
             mask=[False,  True, False, False, False, False,  True, False,
                   False],
       fill_value=999999)
numpy.c_   numpy.c_ = <numpy.lib.index_tricks.CClass object>
 
Translates slice objects to concatenation along the second axis. This is short-hand for np.r_['-1,2,0', index expression], which is useful because of its common occurrence. In particular, arrays will be stacked along their last axis after being upgraded to at least 2-D with 1’s post-pended to the shape (column vectors made out of 1-D arrays).  See also  column_stack

Stack 1-D arrays as columns into a 2-D array.  r_

For more detailed documentation.    Examples >>> np.c_[np.array([1,2,3]), np.array([4,5,6])]
array([[1, 4],
       [2, 5],
       [3, 6]])
>>> np.c_[np.array([[1,2,3]]), 0, 0, np.array([[4,5,6]])]
array([[1, 2, 3, ..., 4, 5, 6]])
tf.experimental.numpy.concatenate TensorFlow variant of NumPy's concatenate. 
tf.experimental.numpy.concatenate(
    arys, axis=0
)
 See the NumPy documentation for numpy.concatenate.
numpy.concatenate   numpy.concatenate((a1, a2, ...), axis=0, out=None, dtype=None, casting="same_kind")
 
Join a sequence of arrays along an existing axis.  Parameters 
 
a1, a2, …sequence of array_like


The arrays must have the same shape, except in the dimension corresponding to axis (the first, by default).  
axisint, optional


The axis along which the arrays will be joined. If axis is None, arrays are flattened before use. Default is 0.  
outndarray, optional


If provided, the destination to place the result. The shape must be correct, matching that of what concatenate would have returned if no out argument were specified.  
dtypestr or dtype


If provided, the destination array will have this dtype. Cannot be provided together with out.  New in version 1.20.0.   
casting{‘no’, ‘equiv’, ‘safe’, ‘same_kind’, ‘unsafe’}, optional


Controls what kind of data casting may occur. Defaults to ‘same_kind’.  New in version 1.20.0.     Returns 
 
resndarray


The concatenated array.      See also  ma.concatenate

Concatenate function that preserves input masks.  array_split

Split an array into multiple sub-arrays of equal or near-equal size.  split

Split array into a list of multiple sub-arrays of equal size.  hsplit

Split array into multiple sub-arrays horizontally (column wise).  vsplit

Split array into multiple sub-arrays vertically (row wise).  dsplit

Split array into multiple sub-arrays along the 3rd axis (depth).  stack

Stack a sequence of arrays along a new axis.  block

Assemble arrays from blocks.  hstack

Stack arrays in sequence horizontally (column wise).  vstack

Stack arrays in sequence vertically (row wise).  dstack

Stack arrays in sequence depth wise (along third dimension).  column_stack

Stack 1-D arrays as columns into a 2-D array.    Notes When one or more of the arrays to be concatenated is a MaskedArray, this function will return a MaskedArray object instead of an ndarray, but the input masks are not preserved. In cases where a MaskedArray is expected as input, use the ma.concatenate function from the masked array module instead. Examples >>> a = np.array([[1, 2], [3, 4]])
>>> b = np.array([[5, 6]])
>>> np.concatenate((a, b), axis=0)
array([[1, 2],
       [3, 4],
       [5, 6]])
>>> np.concatenate((a, b.T), axis=1)
array([[1, 2, 5],
       [3, 4, 6]])
>>> np.concatenate((a, b), axis=None)
array([1, 2, 3, 4, 5, 6])
 This function will not preserve masking of MaskedArray inputs. >>> a = np.ma.arange(3)
>>> a[1] = np.ma.masked
>>> b = np.arange(2, 5)
>>> a
masked_array(data=[0, --, 2],
             mask=[False,  True, False],
       fill_value=999999)
>>> b
array([2, 3, 4])
>>> np.concatenate([a, b])
masked_array(data=[0, 1, 2, 2, 3, 4],
             mask=False,
       fill_value=999999)
>>> np.ma.concatenate([a, b])
masked_array(data=[0, --, 2, 2, 3, 4],
             mask=[False,  True, False, False, False, False],
       fill_value=999999)
numpy.ma.concatenate   ma.concatenate(arrays, axis=0)[source]
 
Concatenate a sequence of arrays along the given axis.  Parameters 
 
arrayssequence of array_like


The arrays must have the same shape, except in the dimension corresponding to axis (the first, by default).  
axisint, optional


The axis along which the arrays will be joined. Default is 0.    Returns 
 
resultMaskedArray


The concatenated array with any masked entries preserved.      See also  numpy.concatenate

Equivalent function in the top-level NumPy module.    Examples >>> import numpy.ma as ma
>>> a = ma.arange(3)
>>> a[1] = ma.masked
>>> b = ma.arange(2, 5)
>>> a
masked_array(data=[0, --, 2],
             mask=[False,  True, False],
       fill_value=999999)
>>> b
masked_array(data=[2, 3, 4],
             mask=False,
       fill_value=999999)
>>> ma.concatenate([a, b])
masked_array(data=[0, --, 2, 2, 3, 4],
             mask=[False,  True, False, False, False, False],
       fill_value=999999)
def f_21887754(a, b):
    """numpy concatenate two arrays `a` and `b` along the second axis
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
numpy.ma.append   ma.append(a, b, axis=None)[source]
 
Append values to the end of an array.  New in version 1.9.0.   Parameters 
 
aarray_like


Values are appended to a copy of this array.  
barray_like


These values are appended to a copy of a. It must be of the correct shape (the same shape as a, excluding axis). If axis is not specified, b can be any shape and will be flattened before use.  
axisint, optional


The axis along which v are appended. If axis is not given, both a and b are flattened before use.    Returns 
 
appendMaskedArray


A copy of a with b appended to axis. Note that append does not occur in-place: a new array is allocated and filled. If axis is None, the result is a flattened array.      See also  numpy.append

Equivalent function in the top-level NumPy module.    Examples >>> import numpy.ma as ma
>>> a = ma.masked_values([1, 2, 3], 2)
>>> b = ma.masked_values([[4, 5, 6], [7, 8, 9]], 7)
>>> ma.append(a, b)
masked_array(data=[1, --, 3, 4, 5, 6, --, 8, 9],
             mask=[False,  True, False, False, False, False,  True, False,
                   False],
       fill_value=999999)
tf.experimental.numpy.concatenate TensorFlow variant of NumPy's concatenate. 
tf.experimental.numpy.concatenate(
    arys, axis=0
)
 See the NumPy documentation for numpy.concatenate.
numpy.c_   numpy.c_ = <numpy.lib.index_tricks.CClass object>
 
Translates slice objects to concatenation along the second axis. This is short-hand for np.r_['-1,2,0', index expression], which is useful because of its common occurrence. In particular, arrays will be stacked along their last axis after being upgraded to at least 2-D with 1’s post-pended to the shape (column vectors made out of 1-D arrays).  See also  column_stack

Stack 1-D arrays as columns into a 2-D array.  r_

For more detailed documentation.    Examples >>> np.c_[np.array([1,2,3]), np.array([4,5,6])]
array([[1, 4],
       [2, 5],
       [3, 6]])
>>> np.c_[np.array([[1,2,3]]), 0, 0, np.array([[4,5,6]])]
array([[1, 2, 3, ..., 4, 5, 6]])
numpy.concatenate   numpy.concatenate((a1, a2, ...), axis=0, out=None, dtype=None, casting="same_kind")
 
Join a sequence of arrays along an existing axis.  Parameters 
 
a1, a2, …sequence of array_like


The arrays must have the same shape, except in the dimension corresponding to axis (the first, by default).  
axisint, optional


The axis along which the arrays will be joined. If axis is None, arrays are flattened before use. Default is 0.  
outndarray, optional


If provided, the destination to place the result. The shape must be correct, matching that of what concatenate would have returned if no out argument were specified.  
dtypestr or dtype


If provided, the destination array will have this dtype. Cannot be provided together with out.  New in version 1.20.0.   
casting{‘no’, ‘equiv’, ‘safe’, ‘same_kind’, ‘unsafe’}, optional


Controls what kind of data casting may occur. Defaults to ‘same_kind’.  New in version 1.20.0.     Returns 
 
resndarray


The concatenated array.      See also  ma.concatenate

Concatenate function that preserves input masks.  array_split

Split an array into multiple sub-arrays of equal or near-equal size.  split

Split array into a list of multiple sub-arrays of equal size.  hsplit

Split array into multiple sub-arrays horizontally (column wise).  vsplit

Split array into multiple sub-arrays vertically (row wise).  dsplit

Split array into multiple sub-arrays along the 3rd axis (depth).  stack

Stack a sequence of arrays along a new axis.  block

Assemble arrays from blocks.  hstack

Stack arrays in sequence horizontally (column wise).  vstack

Stack arrays in sequence vertically (row wise).  dstack

Stack arrays in sequence depth wise (along third dimension).  column_stack

Stack 1-D arrays as columns into a 2-D array.    Notes When one or more of the arrays to be concatenated is a MaskedArray, this function will return a MaskedArray object instead of an ndarray, but the input masks are not preserved. In cases where a MaskedArray is expected as input, use the ma.concatenate function from the masked array module instead. Examples >>> a = np.array([[1, 2], [3, 4]])
>>> b = np.array([[5, 6]])
>>> np.concatenate((a, b), axis=0)
array([[1, 2],
       [3, 4],
       [5, 6]])
>>> np.concatenate((a, b.T), axis=1)
array([[1, 2, 5],
       [3, 4, 6]])
>>> np.concatenate((a, b), axis=None)
array([1, 2, 3, 4, 5, 6])
 This function will not preserve masking of MaskedArray inputs. >>> a = np.ma.arange(3)
>>> a[1] = np.ma.masked
>>> b = np.arange(2, 5)
>>> a
masked_array(data=[0, --, 2],
             mask=[False,  True, False],
       fill_value=999999)
>>> b
array([2, 3, 4])
>>> np.concatenate([a, b])
masked_array(data=[0, 1, 2, 2, 3, 4],
             mask=False,
       fill_value=999999)
>>> np.ma.concatenate([a, b])
masked_array(data=[0, --, 2, 2, 3, 4],
             mask=[False,  True, False, False, False, False],
       fill_value=999999)
numpy.ma.concatenate   ma.concatenate(arrays, axis=0)[source]
 
Concatenate a sequence of arrays along the given axis.  Parameters 
 
arrayssequence of array_like


The arrays must have the same shape, except in the dimension corresponding to axis (the first, by default).  
axisint, optional


The axis along which the arrays will be joined. Default is 0.    Returns 
 
resultMaskedArray


The concatenated array with any masked entries preserved.      See also  numpy.concatenate

Equivalent function in the top-level NumPy module.    Examples >>> import numpy.ma as ma
>>> a = ma.arange(3)
>>> a[1] = ma.masked
>>> b = ma.arange(2, 5)
>>> a
masked_array(data=[0, --, 2],
             mask=[False,  True, False],
       fill_value=999999)
>>> b
masked_array(data=[2, 3, 4],
             mask=False,
       fill_value=999999)
>>> ma.concatenate([a, b])
masked_array(data=[0, --, 2, 2, 3, 4],
             mask=[False,  True, False, False, False, False],
       fill_value=999999)
def f_21887754(a, b):
    """numpy concatenate two arrays `a` and `b` along the first axis
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
numpy.ma.append   ma.append(a, b, axis=None)[source]
 
Append values to the end of an array.  New in version 1.9.0.   Parameters 
 
aarray_like


Values are appended to a copy of this array.  
barray_like


These values are appended to a copy of a. It must be of the correct shape (the same shape as a, excluding axis). If axis is not specified, b can be any shape and will be flattened before use.  
axisint, optional


The axis along which v are appended. If axis is not given, both a and b are flattened before use.    Returns 
 
appendMaskedArray


A copy of a with b appended to axis. Note that append does not occur in-place: a new array is allocated and filled. If axis is None, the result is a flattened array.      See also  numpy.append

Equivalent function in the top-level NumPy module.    Examples >>> import numpy.ma as ma
>>> a = ma.masked_values([1, 2, 3], 2)
>>> b = ma.masked_values([[4, 5, 6], [7, 8, 9]], 7)
>>> ma.append(a, b)
masked_array(data=[1, --, 3, 4, 5, 6, --, 8, 9],
             mask=[False,  True, False, False, False, False,  True, False,
                   False],
       fill_value=999999)
tf.experimental.numpy.concatenate TensorFlow variant of NumPy's concatenate. 
tf.experimental.numpy.concatenate(
    arys, axis=0
)
 See the NumPy documentation for numpy.concatenate.
numpy.c_   numpy.c_ = <numpy.lib.index_tricks.CClass object>
 
Translates slice objects to concatenation along the second axis. This is short-hand for np.r_['-1,2,0', index expression], which is useful because of its common occurrence. In particular, arrays will be stacked along their last axis after being upgraded to at least 2-D with 1’s post-pended to the shape (column vectors made out of 1-D arrays).  See also  column_stack

Stack 1-D arrays as columns into a 2-D array.  r_

For more detailed documentation.    Examples >>> np.c_[np.array([1,2,3]), np.array([4,5,6])]
array([[1, 4],
       [2, 5],
       [3, 6]])
>>> np.c_[np.array([[1,2,3]]), 0, 0, np.array([[4,5,6]])]
array([[1, 2, 3, ..., 4, 5, 6]])
numpy.concatenate   numpy.concatenate((a1, a2, ...), axis=0, out=None, dtype=None, casting="same_kind")
 
Join a sequence of arrays along an existing axis.  Parameters 
 
a1, a2, …sequence of array_like


The arrays must have the same shape, except in the dimension corresponding to axis (the first, by default).  
axisint, optional


The axis along which the arrays will be joined. If axis is None, arrays are flattened before use. Default is 0.  
outndarray, optional


If provided, the destination to place the result. The shape must be correct, matching that of what concatenate would have returned if no out argument were specified.  
dtypestr or dtype


If provided, the destination array will have this dtype. Cannot be provided together with out.  New in version 1.20.0.   
casting{‘no’, ‘equiv’, ‘safe’, ‘same_kind’, ‘unsafe’}, optional


Controls what kind of data casting may occur. Defaults to ‘same_kind’.  New in version 1.20.0.     Returns 
 
resndarray


The concatenated array.      See also  ma.concatenate

Concatenate function that preserves input masks.  array_split

Split an array into multiple sub-arrays of equal or near-equal size.  split

Split array into a list of multiple sub-arrays of equal size.  hsplit

Split array into multiple sub-arrays horizontally (column wise).  vsplit

Split array into multiple sub-arrays vertically (row wise).  dsplit

Split array into multiple sub-arrays along the 3rd axis (depth).  stack

Stack a sequence of arrays along a new axis.  block

Assemble arrays from blocks.  hstack

Stack arrays in sequence horizontally (column wise).  vstack

Stack arrays in sequence vertically (row wise).  dstack

Stack arrays in sequence depth wise (along third dimension).  column_stack

Stack 1-D arrays as columns into a 2-D array.    Notes When one or more of the arrays to be concatenated is a MaskedArray, this function will return a MaskedArray object instead of an ndarray, but the input masks are not preserved. In cases where a MaskedArray is expected as input, use the ma.concatenate function from the masked array module instead. Examples >>> a = np.array([[1, 2], [3, 4]])
>>> b = np.array([[5, 6]])
>>> np.concatenate((a, b), axis=0)
array([[1, 2],
       [3, 4],
       [5, 6]])
>>> np.concatenate((a, b.T), axis=1)
array([[1, 2, 5],
       [3, 4, 6]])
>>> np.concatenate((a, b), axis=None)
array([1, 2, 3, 4, 5, 6])
 This function will not preserve masking of MaskedArray inputs. >>> a = np.ma.arange(3)
>>> a[1] = np.ma.masked
>>> b = np.arange(2, 5)
>>> a
masked_array(data=[0, --, 2],
             mask=[False,  True, False],
       fill_value=999999)
>>> b
array([2, 3, 4])
>>> np.concatenate([a, b])
masked_array(data=[0, 1, 2, 2, 3, 4],
             mask=False,
       fill_value=999999)
>>> np.ma.concatenate([a, b])
masked_array(data=[0, --, 2, 2, 3, 4],
             mask=[False,  True, False, False, False, False],
       fill_value=999999)
numpy.ma.concatenate   ma.concatenate(arrays, axis=0)[source]
 
Concatenate a sequence of arrays along the given axis.  Parameters 
 
arrayssequence of array_like


The arrays must have the same shape, except in the dimension corresponding to axis (the first, by default).  
axisint, optional


The axis along which the arrays will be joined. Default is 0.    Returns 
 
resultMaskedArray


The concatenated array with any masked entries preserved.      See also  numpy.concatenate

Equivalent function in the top-level NumPy module.    Examples >>> import numpy.ma as ma
>>> a = ma.arange(3)
>>> a[1] = ma.masked
>>> b = ma.arange(2, 5)
>>> a
masked_array(data=[0, --, 2],
             mask=[False,  True, False],
       fill_value=999999)
>>> b
masked_array(data=[2, 3, 4],
             mask=False,
       fill_value=999999)
>>> ma.concatenate([a, b])
masked_array(data=[0, --, 2, 2, 3, 4],
             mask=[False,  True, False, False, False, False],
       fill_value=999999)
def f_21887754(a, b):
    """numpy concatenate two arrays `a` and `b` along the first axis
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
coroutine loop.getnameinfo(sockaddr, flags=0)  
Asynchronous version of socket.getnameinfo().
coroutine loop.getaddrinfo(host, port, *, family=0, type=0, proto=0, flags=0)  
Asynchronous version of socket.getaddrinfo().
socket.gethostbyaddr(ip_address)  
Return a triple (hostname, aliaslist, ipaddrlist) where hostname is the primary host name responding to the given ip_address, aliaslist is a (possibly empty) list of alternative host names for the same address, and ipaddrlist is a list of IPv4/v6 addresses for the same interface on the same host (most likely containing only a single address). To find the fully qualified domain name, use the function getfqdn(). gethostbyaddr() supports both IPv4 and IPv6. Raises an auditing event socket.gethostbyaddr with argument ip_address.
socket.gethostbyname_ex(hostname)  
Translate a host name to IPv4 address format, extended interface. Return a triple (hostname, aliaslist, ipaddrlist) where hostname is the primary host name responding to the given ip_address, aliaslist is a (possibly empty) list of alternative host names for the same address, and ipaddrlist is a list of IPv4 addresses for the same interface on the same host (often but not always a single address). gethostbyname_ex() does not support IPv6 name resolution, and getaddrinfo() should be used instead for IPv4/v6 dual stack support. Raises an auditing event socket.gethostbyname with argument hostname.
socket.getaddrinfo(host, port, family=0, type=0, proto=0, flags=0)  
Translate the host/port argument into a sequence of 5-tuples that contain all the necessary arguments for creating a socket connected to that service. host is a domain name, a string representation of an IPv4/v6 address or None. port is a string service name such as 'http', a numeric port number or None. By passing None as the value of host and port, you can pass NULL to the underlying C API. The family, type and proto arguments can be optionally specified in order to narrow the list of addresses returned. Passing zero as a value for each of these arguments selects the full range of results. The flags argument can be one or several of the AI_* constants, and will influence how results are computed and returned. For example, AI_NUMERICHOST will disable domain name resolution and will raise an error if host is a domain name. The function returns a list of 5-tuples with the following structure: (family, type, proto, canonname, sockaddr) In these tuples, family, type, proto are all integers and are meant to be passed to the socket() function. canonname will be a string representing the canonical name of the host if AI_CANONNAME is part of the flags argument; else canonname will be empty. sockaddr is a tuple describing a socket address, whose format depends on the returned family (a (address, port) 2-tuple for AF_INET, a (address, port, flowinfo, scope_id) 4-tuple for AF_INET6), and is meant to be passed to the socket.connect() method. Raises an auditing event socket.getaddrinfo with arguments host, port, family, type, protocol. The following example fetches address information for a hypothetical TCP connection to example.org on port 80 (results may differ on your system if IPv6 isn’t enabled): >>> socket.getaddrinfo("example.org", 80, proto=socket.IPPROTO_TCP)
[(<AddressFamily.AF_INET6: 10>, <SocketType.SOCK_STREAM: 1>,
 6, '', ('2606:2800:220:1:248:1893:25c8:1946', 80, 0, 0)),
 (<AddressFamily.AF_INET: 2>, <SocketType.SOCK_STREAM: 1>,
 6, '', ('93.184.216.34', 80))]
  Changed in version 3.2: parameters can now be passed using keyword arguments.   Changed in version 3.7: for IPv6 multicast addresses, string representing an address will not contain %scope_id part.
def f_2805231():
    """fetch address information for host 'google.com' ion port 80
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
pandas.tseries.offsets.CDay   pandas.tseries.offsets.CDay
 
alias of pandas._libs.tslibs.offsets.CustomBusinessDay
pandas.tseries.offsets.CustomBusinessDay.name   CustomBusinessDay.name
pandas.tseries.offsets.CustomBusinessDay.apply   CustomBusinessDay.apply()
tf.image.adjust_saturation     View source on GitHub    Adjust saturation of RGB images.  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.image.adjust_saturation  
tf.image.adjust_saturation(
    image, saturation_factor, name=None
)
 This is a convenience method that converts RGB images to float representation, converts them to HSV, adds an offset to the saturation channel, converts back to RGB and then back to the original data type. If several adjustments are chained it is advisable to minimize the number of redundant conversions. image is an RGB image or images. The image saturation is adjusted by converting the images to HSV and multiplying the saturation (S) channel by saturation_factor and clipping. The images are then converted back to RGB. Usage Example: 
x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.adjust_saturation(x, 0.5)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[ 2. ,  2.5,  3. ],
        [ 5. ,  5.5,  6. ]],
       [[ 8. ,  8.5,  9. ],
        [11. , 11.5, 12. ]]], dtype=float32)>

 


 Args
  image   RGB image or images. The size of the last dimension must be 3.  
  saturation_factor   float. Factor to multiply the saturation by.  
  name   A name for this operation (optional).   
 


 Returns   Adjusted image(s), same shape and DType as image.  

 


 Raises
  InvalidArgumentError   input must have 3 channels
pandas.tseries.offsets.CustomBusinessDay.__call__   CustomBusinessDay.__call__(*args, **kwargs)
 
Call self as a function.
def f_17552997(df):
    """add a column 'day' with value 'sat' to dataframe `df`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
Exceptions  Exceptions… allow error handling to be organized cleanly in a central or high-level place within the program structure. — Doug Hellmann, Python Exception Handling Techniques  Exception handling in REST framework views REST framework's views handle various exceptions, and deal with returning appropriate error responses. The handled exceptions are:  Subclasses of APIException raised inside REST framework. Django's Http404 exception. Django's PermissionDenied exception.  In each case, REST framework will return a response with an appropriate status code and content-type. The body of the response will include any additional details regarding the nature of the error. Most error responses will include a key detail in the body of the response. For example, the following request: DELETE http://api.example.com/foo/bar HTTP/1.1
Accept: application/json
 Might receive an error response indicating that the DELETE method is not allowed on that resource: HTTP/1.1 405 Method Not Allowed
Content-Type: application/json
Content-Length: 42

{"detail": "Method 'DELETE' not allowed."}
 Validation errors are handled slightly differently, and will include the field names as the keys in the response. If the validation error was not specific to a particular field then it will use the "non_field_errors" key, or whatever string value has been set for the NON_FIELD_ERRORS_KEY setting. An example validation error might look like this: HTTP/1.1 400 Bad Request
Content-Type: application/json
Content-Length: 94

{"amount": ["A valid integer is required."], "description": ["This field may not be blank."]}
 Custom exception handling You can implement custom exception handling by creating a handler function that converts exceptions raised in your API views into response objects. This allows you to control the style of error responses used by your API. The function must take a pair of arguments, the first is the exception to be handled, and the second is a dictionary containing any extra context such as the view currently being handled. The exception handler function should either return a Response object, or return None if the exception cannot be handled. If the handler returns None then the exception will be re-raised and Django will return a standard HTTP 500 'server error' response. For example, you might want to ensure that all error responses include the HTTP status code in the body of the response, like so: HTTP/1.1 405 Method Not Allowed
Content-Type: application/json
Content-Length: 62

{"status_code": 405, "detail": "Method 'DELETE' not allowed."}
 In order to alter the style of the response, you could write the following custom exception handler: from rest_framework.views import exception_handler

def custom_exception_handler(exc, context):
    # Call REST framework's default exception handler first,
    # to get the standard error response.
    response = exception_handler(exc, context)

    # Now add the HTTP status code to the response.
    if response is not None:
        response.data['status_code'] = response.status_code

    return response
 The context argument is not used by the default handler, but can be useful if the exception handler needs further information such as the view currently being handled, which can be accessed as context['view']. The exception handler must also be configured in your settings, using the EXCEPTION_HANDLER setting key. For example: REST_FRAMEWORK = {
    'EXCEPTION_HANDLER': 'my_project.my_app.utils.custom_exception_handler'
}
 If not specified, the 'EXCEPTION_HANDLER' setting defaults to the standard exception handler provided by REST framework: REST_FRAMEWORK = {
    'EXCEPTION_HANDLER': 'rest_framework.views.exception_handler'
}
 Note that the exception handler will only be called for responses generated by raised exceptions. It will not be used for any responses returned directly by the view, such as the HTTP_400_BAD_REQUEST responses that are returned by the generic views when serializer validation fails. API Reference APIException Signature: APIException() The base class for all exceptions raised inside an APIView class or @api_view. To provide a custom exception, subclass APIException and set the .status_code, .default_detail, and default_code attributes on the class. For example, if your API relies on a third party service that may sometimes be unreachable, you might want to implement an exception for the "503 Service Unavailable" HTTP response code. You could do this like so: from rest_framework.exceptions import APIException

class ServiceUnavailable(APIException):
    status_code = 503
    default_detail = 'Service temporarily unavailable, try again later.'
    default_code = 'service_unavailable'
 Inspecting API exceptions There are a number of different properties available for inspecting the status of an API exception. You can use these to build custom exception handling for your project. The available attributes and methods are:  
.detail - Return the textual description of the error. 
.get_codes() - Return the code identifier of the error. 
.get_full_details() - Return both the textual description and the code identifier.  In most cases the error detail will be a simple item: >>> print(exc.detail)
You do not have permission to perform this action.
>>> print(exc.get_codes())
permission_denied
>>> print(exc.get_full_details())
{'message':'You do not have permission to perform this action.','code':'permission_denied'}
 In the case of validation errors the error detail will be either a list or dictionary of items: >>> print(exc.detail)
{"name":"This field is required.","age":"A valid integer is required."}
>>> print(exc.get_codes())
{"name":"required","age":"invalid"}
>>> print(exc.get_full_details())
{"name":{"message":"This field is required.","code":"required"},"age":{"message":"A valid integer is required.","code":"invalid"}}
 ParseError Signature: ParseError(detail=None, code=None) Raised if the request contains malformed data when accessing request.data. By default this exception results in a response with the HTTP status code "400 Bad Request". AuthenticationFailed Signature: AuthenticationFailed(detail=None, code=None) Raised when an incoming request includes incorrect authentication. By default this exception results in a response with the HTTP status code "401 Unauthenticated", but it may also result in a "403 Forbidden" response, depending on the authentication scheme in use. See the authentication documentation for more details. NotAuthenticated Signature: NotAuthenticated(detail=None, code=None) Raised when an unauthenticated request fails the permission checks. By default this exception results in a response with the HTTP status code "401 Unauthenticated", but it may also result in a "403 Forbidden" response, depending on the authentication scheme in use. See the authentication documentation for more details. PermissionDenied Signature: PermissionDenied(detail=None, code=None) Raised when an authenticated request fails the permission checks. By default this exception results in a response with the HTTP status code "403 Forbidden". NotFound Signature: NotFound(detail=None, code=None) Raised when a resource does not exists at the given URL. This exception is equivalent to the standard Http404 Django exception. By default this exception results in a response with the HTTP status code "404 Not Found". MethodNotAllowed Signature: MethodNotAllowed(method, detail=None, code=None) Raised when an incoming request occurs that does not map to a handler method on the view. By default this exception results in a response with the HTTP status code "405 Method Not Allowed". NotAcceptable Signature: NotAcceptable(detail=None, code=None) Raised when an incoming request occurs with an Accept header that cannot be satisfied by any of the available renderers. By default this exception results in a response with the HTTP status code "406 Not Acceptable". UnsupportedMediaType Signature: UnsupportedMediaType(media_type, detail=None, code=None) Raised if there are no parsers that can handle the content type of the request data when accessing request.data. By default this exception results in a response with the HTTP status code "415 Unsupported Media Type". Throttled Signature: Throttled(wait=None, detail=None, code=None) Raised when an incoming request fails the throttling checks. By default this exception results in a response with the HTTP status code "429 Too Many Requests". ValidationError Signature: ValidationError(detail, code=None) The ValidationError exception is slightly different from the other APIException classes:  The detail argument is mandatory, not optional. The detail argument may be a list or dictionary of error details, and may also be a nested data structure. By using a dictionary, you can specify field-level errors while performing object-level validation in the validate() method of a serializer. For example. raise serializers.ValidationError({'name': 'Please enter a valid name.'})
 By convention you should import the serializers module and use a fully qualified ValidationError style, in order to differentiate it from Django's built-in validation error. For example. raise serializers.ValidationError('This field must be an integer value.')
  The ValidationError class should be used for serializer and field validation, and by validator classes. It is also raised when calling serializer.is_valid with the raise_exception keyword argument: serializer.is_valid(raise_exception=True)
 The generic views use the raise_exception=True flag, which means that you can override the style of validation error responses globally in your API. To do so, use a custom exception handler, as described above. By default this exception results in a response with the HTTP status code "400 Bad Request". Generic Error Views Django REST Framework provides two error views suitable for providing generic JSON 500 Server Error and 400 Bad Request responses. (Django's default error views provide HTML responses, which may not be appropriate for an API-only application.) Use these as per Django's Customizing error views documentation. rest_framework.exceptions.server_error Returns a response with status code 500 and application/json content type. Set as handler500: handler500 = 'rest_framework.exceptions.server_error'
 rest_framework.exceptions.bad_request Returns a response with status code 400 and application/json content type. Set as handler400: handler400 = 'rest_framework.exceptions.bad_request'
 exceptions.py
HTTPBasicAuthHandler.http_error_401(req, fp, code, msg, hdrs)  
Retry the request with authentication information, if available.
HTTPDigestAuthHandler.http_error_401(req, fp, code, msg, hdrs)  
Retry the request with authentication information, if available.
Exceptions  Exceptions… allow error handling to be organized cleanly in a central or high-level place within the program structure. — Doug Hellmann, Python Exception Handling Techniques  Exception handling in REST framework views REST framework's views handle various exceptions, and deal with returning appropriate error responses. The handled exceptions are:  Subclasses of APIException raised inside REST framework. Django's Http404 exception. Django's PermissionDenied exception.  In each case, REST framework will return a response with an appropriate status code and content-type. The body of the response will include any additional details regarding the nature of the error. Most error responses will include a key detail in the body of the response. For example, the following request: DELETE http://api.example.com/foo/bar HTTP/1.1
Accept: application/json
 Might receive an error response indicating that the DELETE method is not allowed on that resource: HTTP/1.1 405 Method Not Allowed
Content-Type: application/json
Content-Length: 42

{"detail": "Method 'DELETE' not allowed."}
 Validation errors are handled slightly differently, and will include the field names as the keys in the response. If the validation error was not specific to a particular field then it will use the "non_field_errors" key, or whatever string value has been set for the NON_FIELD_ERRORS_KEY setting. An example validation error might look like this: HTTP/1.1 400 Bad Request
Content-Type: application/json
Content-Length: 94

{"amount": ["A valid integer is required."], "description": ["This field may not be blank."]}
 Custom exception handling You can implement custom exception handling by creating a handler function that converts exceptions raised in your API views into response objects. This allows you to control the style of error responses used by your API. The function must take a pair of arguments, the first is the exception to be handled, and the second is a dictionary containing any extra context such as the view currently being handled. The exception handler function should either return a Response object, or return None if the exception cannot be handled. If the handler returns None then the exception will be re-raised and Django will return a standard HTTP 500 'server error' response. For example, you might want to ensure that all error responses include the HTTP status code in the body of the response, like so: HTTP/1.1 405 Method Not Allowed
Content-Type: application/json
Content-Length: 62

{"status_code": 405, "detail": "Method 'DELETE' not allowed."}
 In order to alter the style of the response, you could write the following custom exception handler: from rest_framework.views import exception_handler

def custom_exception_handler(exc, context):
    # Call REST framework's default exception handler first,
    # to get the standard error response.
    response = exception_handler(exc, context)

    # Now add the HTTP status code to the response.
    if response is not None:
        response.data['status_code'] = response.status_code

    return response
 The context argument is not used by the default handler, but can be useful if the exception handler needs further information such as the view currently being handled, which can be accessed as context['view']. The exception handler must also be configured in your settings, using the EXCEPTION_HANDLER setting key. For example: REST_FRAMEWORK = {
    'EXCEPTION_HANDLER': 'my_project.my_app.utils.custom_exception_handler'
}
 If not specified, the 'EXCEPTION_HANDLER' setting defaults to the standard exception handler provided by REST framework: REST_FRAMEWORK = {
    'EXCEPTION_HANDLER': 'rest_framework.views.exception_handler'
}
 Note that the exception handler will only be called for responses generated by raised exceptions. It will not be used for any responses returned directly by the view, such as the HTTP_400_BAD_REQUEST responses that are returned by the generic views when serializer validation fails. API Reference APIException Signature: APIException() The base class for all exceptions raised inside an APIView class or @api_view. To provide a custom exception, subclass APIException and set the .status_code, .default_detail, and default_code attributes on the class. For example, if your API relies on a third party service that may sometimes be unreachable, you might want to implement an exception for the "503 Service Unavailable" HTTP response code. You could do this like so: from rest_framework.exceptions import APIException

class ServiceUnavailable(APIException):
    status_code = 503
    default_detail = 'Service temporarily unavailable, try again later.'
    default_code = 'service_unavailable'
 Inspecting API exceptions There are a number of different properties available for inspecting the status of an API exception. You can use these to build custom exception handling for your project. The available attributes and methods are:  
.detail - Return the textual description of the error. 
.get_codes() - Return the code identifier of the error. 
.get_full_details() - Return both the textual description and the code identifier.  In most cases the error detail will be a simple item: >>> print(exc.detail)
You do not have permission to perform this action.
>>> print(exc.get_codes())
permission_denied
>>> print(exc.get_full_details())
{'message':'You do not have permission to perform this action.','code':'permission_denied'}
 In the case of validation errors the error detail will be either a list or dictionary of items: >>> print(exc.detail)
{"name":"This field is required.","age":"A valid integer is required."}
>>> print(exc.get_codes())
{"name":"required","age":"invalid"}
>>> print(exc.get_full_details())
{"name":{"message":"This field is required.","code":"required"},"age":{"message":"A valid integer is required.","code":"invalid"}}
 ParseError Signature: ParseError(detail=None, code=None) Raised if the request contains malformed data when accessing request.data. By default this exception results in a response with the HTTP status code "400 Bad Request". AuthenticationFailed Signature: AuthenticationFailed(detail=None, code=None) Raised when an incoming request includes incorrect authentication. By default this exception results in a response with the HTTP status code "401 Unauthenticated", but it may also result in a "403 Forbidden" response, depending on the authentication scheme in use. See the authentication documentation for more details. NotAuthenticated Signature: NotAuthenticated(detail=None, code=None) Raised when an unauthenticated request fails the permission checks. By default this exception results in a response with the HTTP status code "401 Unauthenticated", but it may also result in a "403 Forbidden" response, depending on the authentication scheme in use. See the authentication documentation for more details. PermissionDenied Signature: PermissionDenied(detail=None, code=None) Raised when an authenticated request fails the permission checks. By default this exception results in a response with the HTTP status code "403 Forbidden". NotFound Signature: NotFound(detail=None, code=None) Raised when a resource does not exists at the given URL. This exception is equivalent to the standard Http404 Django exception. By default this exception results in a response with the HTTP status code "404 Not Found". MethodNotAllowed Signature: MethodNotAllowed(method, detail=None, code=None) Raised when an incoming request occurs that does not map to a handler method on the view. By default this exception results in a response with the HTTP status code "405 Method Not Allowed". NotAcceptable Signature: NotAcceptable(detail=None, code=None) Raised when an incoming request occurs with an Accept header that cannot be satisfied by any of the available renderers. By default this exception results in a response with the HTTP status code "406 Not Acceptable". UnsupportedMediaType Signature: UnsupportedMediaType(media_type, detail=None, code=None) Raised if there are no parsers that can handle the content type of the request data when accessing request.data. By default this exception results in a response with the HTTP status code "415 Unsupported Media Type". Throttled Signature: Throttled(wait=None, detail=None, code=None) Raised when an incoming request fails the throttling checks. By default this exception results in a response with the HTTP status code "429 Too Many Requests". ValidationError Signature: ValidationError(detail, code=None) The ValidationError exception is slightly different from the other APIException classes:  The detail argument is mandatory, not optional. The detail argument may be a list or dictionary of error details, and may also be a nested data structure. By using a dictionary, you can specify field-level errors while performing object-level validation in the validate() method of a serializer. For example. raise serializers.ValidationError({'name': 'Please enter a valid name.'})
 By convention you should import the serializers module and use a fully qualified ValidationError style, in order to differentiate it from Django's built-in validation error. For example. raise serializers.ValidationError('This field must be an integer value.')
  The ValidationError class should be used for serializer and field validation, and by validator classes. It is also raised when calling serializer.is_valid with the raise_exception keyword argument: serializer.is_valid(raise_exception=True)
 The generic views use the raise_exception=True flag, which means that you can override the style of validation error responses globally in your API. To do so, use a custom exception handler, as described above. By default this exception results in a response with the HTTP status code "400 Bad Request". Generic Error Views Django REST Framework provides two error views suitable for providing generic JSON 500 Server Error and 400 Bad Request responses. (Django's default error views provide HTML responses, which may not be appropriate for an API-only application.) Use these as per Django's Customizing error views documentation. rest_framework.exceptions.server_error Returns a response with status code 500 and application/json content type. Set as handler500: handler500 = 'rest_framework.exceptions.server_error'
 rest_framework.exceptions.bad_request Returns a response with status code 400 and application/json content type. Set as handler400: handler400 = 'rest_framework.exceptions.bad_request'
 exceptions.py
Authentication  Auth needs to be pluggable. — Jacob Kaplan-Moss, "REST worst practices"  Authentication is the mechanism of associating an incoming request with a set of identifying credentials, such as the user the request came from, or the token that it was signed with. The permission and throttling policies can then use those credentials to determine if the request should be permitted. REST framework provides several authentication schemes out of the box, and also allows you to implement custom schemes. Authentication always runs at the very start of the view, before the permission and throttling checks occur, and before any other code is allowed to proceed. The request.user property will typically be set to an instance of the contrib.auth package's User class. The request.auth property is used for any additional authentication information, for example, it may be used to represent an authentication token that the request was signed with. Note: Don't forget that authentication by itself won't allow or disallow an incoming request, it simply identifies the credentials that the request was made with. For information on how to set up the permission policies for your API please see the permissions documentation. How authentication is determined The authentication schemes are always defined as a list of classes. REST framework will attempt to authenticate with each class in the list, and will set request.user and request.auth using the return value of the first class that successfully authenticates. If no class authenticates, request.user will be set to an instance of django.contrib.auth.models.AnonymousUser, and request.auth will be set to None. The value of request.user and request.auth for unauthenticated requests can be modified using the UNAUTHENTICATED_USER and UNAUTHENTICATED_TOKEN settings. Setting the authentication scheme The default authentication schemes may be set globally, using the DEFAULT_AUTHENTICATION_CLASSES setting. For example. REST_FRAMEWORK = {
    'DEFAULT_AUTHENTICATION_CLASSES': [
        'rest_framework.authentication.BasicAuthentication',
        'rest_framework.authentication.SessionAuthentication',
    ]
}
 You can also set the authentication scheme on a per-view or per-viewset basis, using the APIView class-based views. from rest_framework.authentication import SessionAuthentication, BasicAuthentication
from rest_framework.permissions import IsAuthenticated
from rest_framework.response import Response
from rest_framework.views import APIView

class ExampleView(APIView):
    authentication_classes = [SessionAuthentication, BasicAuthentication]
    permission_classes = [IsAuthenticated]

    def get(self, request, format=None):
        content = {
            'user': str(request.user),  # `django.contrib.auth.User` instance.
            'auth': str(request.auth),  # None
        }
        return Response(content)
 Or, if you're using the @api_view decorator with function based views. @api_view(['GET'])
@authentication_classes([SessionAuthentication, BasicAuthentication])
@permission_classes([IsAuthenticated])
def example_view(request, format=None):
    content = {
        'user': str(request.user),  # `django.contrib.auth.User` instance.
        'auth': str(request.auth),  # None
    }
    return Response(content)
 Unauthorized and Forbidden responses When an unauthenticated request is denied permission there are two different error codes that may be appropriate.  HTTP 401 Unauthorized HTTP 403 Permission Denied  HTTP 401 responses must always include a WWW-Authenticate header, that instructs the client how to authenticate. HTTP 403 responses do not include the WWW-Authenticate header. The kind of response that will be used depends on the authentication scheme. Although multiple authentication schemes may be in use, only one scheme may be used to determine the type of response. The first authentication class set on the view is used when determining the type of response. Note that when a request may successfully authenticate, but still be denied permission to perform the request, in which case a 403 Permission Denied response will always be used, regardless of the authentication scheme. Apache mod_wsgi specific configuration Note that if deploying to Apache using mod_wsgi, the authorization header is not passed through to a WSGI application by default, as it is assumed that authentication will be handled by Apache, rather than at an application level. If you are deploying to Apache, and using any non-session based authentication, you will need to explicitly configure mod_wsgi to pass the required headers through to the application. This can be done by specifying the WSGIPassAuthorization directive in the appropriate context and setting it to 'On'. # this can go in either server config, virtual host, directory or .htaccess
WSGIPassAuthorization On
 API Reference BasicAuthentication This authentication scheme uses HTTP Basic Authentication, signed against a user's username and password. Basic authentication is generally only appropriate for testing. If successfully authenticated, BasicAuthentication provides the following credentials.  
request.user will be a Django User instance. 
request.auth will be None.  Unauthenticated responses that are denied permission will result in an HTTP 401 Unauthorized response with an appropriate WWW-Authenticate header. For example: WWW-Authenticate: Basic realm="api"
 Note: If you use BasicAuthentication in production you must ensure that your API is only available over https. You should also ensure that your API clients will always re-request the username and password at login, and will never store those details to persistent storage. TokenAuthentication This authentication scheme uses a simple token-based HTTP Authentication scheme. Token authentication is appropriate for client-server setups, such as native desktop and mobile clients. To use the TokenAuthentication scheme you'll need to configure the authentication classes to include TokenAuthentication, and additionally include rest_framework.authtoken in your INSTALLED_APPS setting: INSTALLED_APPS = [
    ...
    'rest_framework.authtoken'
]
 Note: Make sure to run manage.py migrate after changing your settings. The rest_framework.authtoken app provides Django database migrations. You'll also need to create tokens for your users. from rest_framework.authtoken.models import Token

token = Token.objects.create(user=...)
print(token.key)
 For clients to authenticate, the token key should be included in the Authorization HTTP header. The key should be prefixed by the string literal "Token", with whitespace separating the two strings. For example: Authorization: Token 9944b09199c62bcf9418ad846dd0e4bbdfc6ee4b
 Note: If you want to use a different keyword in the header, such as Bearer, simply subclass TokenAuthentication and set the keyword class variable. If successfully authenticated, TokenAuthentication provides the following credentials.  
request.user will be a Django User instance. 
request.auth will be a rest_framework.authtoken.models.Token instance.  Unauthenticated responses that are denied permission will result in an HTTP 401 Unauthorized response with an appropriate WWW-Authenticate header. For example: WWW-Authenticate: Token
 The curl command line tool may be useful for testing token authenticated APIs. For example: curl -X GET http://127.0.0.1:8000/api/example/ -H 'Authorization: Token 9944b09199c62bcf9418ad846dd0e4bbdfc6ee4b'
 Note: If you use TokenAuthentication in production you must ensure that your API is only available over https. Generating Tokens By using signals If you want every user to have an automatically generated Token, you can simply catch the User's post_save signal. from django.conf import settings
from django.db.models.signals import post_save
from django.dispatch import receiver
from rest_framework.authtoken.models import Token

@receiver(post_save, sender=settings.AUTH_USER_MODEL)
def create_auth_token(sender, instance=None, created=False, **kwargs):
    if created:
        Token.objects.create(user=instance)
 Note that you'll want to ensure you place this code snippet in an installed models.py module, or some other location that will be imported by Django on startup. If you've already created some users, you can generate tokens for all existing users like this: from django.contrib.auth.models import User
from rest_framework.authtoken.models import Token

for user in User.objects.all():
    Token.objects.get_or_create(user=user)
 By exposing an api endpoint When using TokenAuthentication, you may want to provide a mechanism for clients to obtain a token given the username and password. REST framework provides a built-in view to provide this behaviour. To use it, add the obtain_auth_token view to your URLconf: from rest_framework.authtoken import views
urlpatterns += [
    path('api-token-auth/', views.obtain_auth_token)
]
 Note that the URL part of the pattern can be whatever you want to use. The obtain_auth_token view will return a JSON response when valid username and password fields are POSTed to the view using form data or JSON: { 'token' : '9944b09199c62bcf9418ad846dd0e4bbdfc6ee4b' }
 Note that the default obtain_auth_token view explicitly uses JSON requests and responses, rather than using default renderer and parser classes in your settings. By default, there are no permissions or throttling applied to the obtain_auth_token view. If you do wish to apply to throttle you'll need to override the view class, and include them using the throttle_classes attribute. If you need a customized version of the obtain_auth_token view, you can do so by subclassing the ObtainAuthToken view class, and using that in your url conf instead. For example, you may return additional user information beyond the token value: from rest_framework.authtoken.views import ObtainAuthToken
from rest_framework.authtoken.models import Token
from rest_framework.response import Response

class CustomAuthToken(ObtainAuthToken):

    def post(self, request, *args, **kwargs):
        serializer = self.serializer_class(data=request.data,
                                           context={'request': request})
        serializer.is_valid(raise_exception=True)
        user = serializer.validated_data['user']
        token, created = Token.objects.get_or_create(user=user)
        return Response({
            'token': token.key,
            'user_id': user.pk,
            'email': user.email
        })
 And in your urls.py: urlpatterns += [
    path('api-token-auth/', CustomAuthToken.as_view())
]
 With Django admin It is also possible to create Tokens manually through the admin interface. In case you are using a large user base, we recommend that you monkey patch the TokenAdmin class customize it to your needs, more specifically by declaring the user field as raw_field. your_app/admin.py: from rest_framework.authtoken.admin import TokenAdmin

TokenAdmin.raw_id_fields = ['user']
 Using Django manage.py command Since version 3.6.4 it's possible to generate a user token using the following command: ./manage.py drf_create_token <username>
 this command will return the API token for the given user, creating it if it doesn't exist: Generated token 9944b09199c62bcf9418ad846dd0e4bbdfc6ee4b for user user1
 In case you want to regenerate the token (for example if it has been compromised or leaked) you can pass an additional parameter: ./manage.py drf_create_token -r <username>
 SessionAuthentication This authentication scheme uses Django's default session backend for authentication. Session authentication is appropriate for AJAX clients that are running in the same session context as your website. If successfully authenticated, SessionAuthentication provides the following credentials.  
request.user will be a Django User instance. 
request.auth will be None.  Unauthenticated responses that are denied permission will result in an HTTP 403 Forbidden response. If you're using an AJAX-style API with SessionAuthentication, you'll need to make sure you include a valid CSRF token for any "unsafe" HTTP method calls, such as PUT, PATCH, POST or DELETE requests. See the Django CSRF documentation for more details. Warning: Always use Django's standard login view when creating login pages. This will ensure your login views are properly protected. CSRF validation in REST framework works slightly differently from standard Django due to the need to support both session and non-session based authentication to the same views. This means that only authenticated requests require CSRF tokens, and anonymous requests may be sent without CSRF tokens. This behaviour is not suitable for login views, which should always have CSRF validation applied. RemoteUserAuthentication This authentication scheme allows you to delegate authentication to your web server, which sets the REMOTE_USER environment variable. To use it, you must have django.contrib.auth.backends.RemoteUserBackend (or a subclass) in your AUTHENTICATION_BACKENDS setting. By default, RemoteUserBackend creates User objects for usernames that don't already exist. To change this and other behaviour, consult the Django documentation. If successfully authenticated, RemoteUserAuthentication provides the following credentials:  
request.user will be a Django User instance. 
request.auth will be None.  Consult your web server's documentation for information about configuring an authentication method, e.g.:  Apache Authentication How-To NGINX (Restricting Access)  Custom authentication To implement a custom authentication scheme, subclass BaseAuthentication and override the .authenticate(self, request) method. The method should return a two-tuple of (user, auth) if authentication succeeds, or None otherwise. In some circumstances instead of returning None, you may want to raise an AuthenticationFailed exception from the .authenticate() method. Typically the approach you should take is:  If authentication is not attempted, return None. Any other authentication schemes also in use will still be checked. If authentication is attempted but fails, raise an AuthenticationFailed exception. An error response will be returned immediately, regardless of any permissions checks, and without checking any other authentication schemes.  You may also override the .authenticate_header(self, request) method. If implemented, it should return a string that will be used as the value of the WWW-Authenticate header in a HTTP 401 Unauthorized response. If the .authenticate_header() method is not overridden, the authentication scheme will return HTTP 403 Forbidden responses when an unauthenticated request is denied access. Note: When your custom authenticator is invoked by the request object's .user or .auth properties, you may see an AttributeError re-raised as a WrappedAttributeError. This is necessary to prevent the original exception from being suppressed by the outer property access. Python will not recognize that the AttributeError originates from your custom authenticator and will instead assume that the request object does not have a .user or .auth property. These errors should be fixed or otherwise handled by your authenticator. Example The following example will authenticate any incoming request as the user given by the username in a custom request header named 'X-USERNAME'. from django.contrib.auth.models import User
from rest_framework import authentication
from rest_framework import exceptions

class ExampleAuthentication(authentication.BaseAuthentication):
    def authenticate(self, request):
        username = request.META.get('HTTP_X_USERNAME')
        if not username:
            return None

        try:
            user = User.objects.get(username=username)
        except User.DoesNotExist:
            raise exceptions.AuthenticationFailed('No such user')

        return (user, None)
 Third party packages The following third-party packages are also available. Django OAuth Toolkit The Django OAuth Toolkit package provides OAuth 2.0 support and works with Python 3.4+. The package is maintained by jazzband and uses the excellent OAuthLib. The package is well documented, and well supported and is currently our recommended package for OAuth 2.0 support. Installation & configuration Install using pip. pip install django-oauth-toolkit
 Add the package to your INSTALLED_APPS and modify your REST framework settings. INSTALLED_APPS = [
    ...
    'oauth2_provider',
]

REST_FRAMEWORK = {
    'DEFAULT_AUTHENTICATION_CLASSES': [
        'oauth2_provider.contrib.rest_framework.OAuth2Authentication',
    ]
}
 For more details see the Django REST framework - Getting started documentation. Django REST framework OAuth The Django REST framework OAuth package provides both OAuth1 and OAuth2 support for REST framework. This package was previously included directly in the REST framework but is now supported and maintained as a third-party package. Installation & configuration Install the package using pip. pip install djangorestframework-oauth
 For details on configuration and usage see the Django REST framework OAuth documentation for authentication and permissions. JSON Web Token Authentication JSON Web Token is a fairly new standard which can be used for token-based authentication. Unlike the built-in TokenAuthentication scheme, JWT Authentication doesn't need to use a database to validate a token. A package for JWT authentication is djangorestframework-simplejwt which provides some features as well as a pluggable token blacklist app. Hawk HTTP Authentication The HawkREST library builds on the Mohawk library to let you work with Hawk signed requests and responses in your API. Hawk lets two parties securely communicate with each other using messages signed by a shared key. It is based on HTTP MAC access authentication (which was based on parts of OAuth 1.0). HTTP Signature Authentication HTTP Signature (currently a IETF draft) provides a way to achieve origin authentication and message integrity for HTTP messages. Similar to Amazon's HTTP Signature scheme, used by many of its services, it permits stateless, per-request authentication. Elvio Toccalino maintains the djangorestframework-httpsignature (outdated) package which provides an easy to use HTTP Signature Authentication mechanism. You can use the updated fork version of djangorestframework-httpsignature, which is drf-httpsig. Djoser Djoser library provides a set of views to handle basic actions such as registration, login, logout, password reset and account activation. The package works with a custom user model and uses token-based authentication. This is ready to use REST implementation of the Django authentication system. django-rest-auth / dj-rest-auth This library provides a set of REST API endpoints for registration, authentication (including social media authentication), password reset, retrieve and update user details, etc. By having these API endpoints, your client apps such as AngularJS, iOS, Android, and others can communicate to your Django backend site independently via REST APIs for user management. There are currently two forks of this project.  
Django-rest-auth is the original project, but is not currently receiving updates. 
Dj-rest-auth is a newer fork of the project.  django-rest-framework-social-oauth2 Django-rest-framework-social-oauth2 library provides an easy way to integrate social plugins (facebook, twitter, google, etc.) to your authentication system and an easy oauth2 setup. With this library, you will be able to authenticate users based on external tokens (e.g. facebook access token), convert these tokens to "in-house" oauth2 tokens and use and generate oauth2 tokens to authenticate your users. django-rest-knox Django-rest-knox library provides models and views to handle token-based authentication in a more secure and extensible way than the built-in TokenAuthentication scheme - with Single Page Applications and Mobile clients in mind. It provides per-client tokens, and views to generate them when provided some other authentication (usually basic authentication), to delete the token (providing a server enforced logout) and to delete all tokens (logs out all clients that a user is logged into). drfpasswordless drfpasswordless adds (Medium, Square Cash inspired) passwordless support to Django REST Framework's TokenAuthentication scheme. Users log in and sign up with a token sent to a contact point like an email address or a mobile number. django-rest-authemail django-rest-authemail provides a RESTful API interface for user signup and authentication. Email addresses are used for authentication, rather than usernames. API endpoints are available for signup, signup email verification, login, logout, password reset, password reset verification, email change, email change verification, password change, and user detail. A fully functional example project and detailed instructions are included. Django-Rest-Durin Django-Rest-Durin is built with the idea to have one library that does token auth for multiple Web/CLI/Mobile API clients via one interface but allows different token configuration for each API Client that consumes the API. It provides support for multiple tokens per user via custom models, views, permissions that work with Django-Rest-Framework. The token expiration time can be different per API client and is customizable via the Django Admin Interface. More information can be found in the Documentation. authentication.py
def f_4356842():
    """return a 401 unauthorized in django
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
template_folder  
The path to the templates folder, relative to root_path, to add to the template loader. None if templates should not be added.
template_folder  
The path to the templates folder, relative to root_path, to add to the template loader. None if templates should not be added.
class filesystem.Loader  
Loads templates from the filesystem, according to DIRS. This loader is enabled by default. However it won’t find any templates until you set DIRS to a non-empty list: TEMPLATES = [{
    'BACKEND': 'django.template.backends.django.DjangoTemplates',
    'DIRS': [BASE_DIR / 'templates'],
}]
 You can also override 'DIRS' and specify specific directories for a particular filesystem loader: TEMPLATES = [{
    'BACKEND': 'django.template.backends.django.DjangoTemplates',
    'OPTIONS': {
        'loaders': [
            (
                'django.template.loaders.filesystem.Loader',
                [BASE_DIR / 'templates'],
            ),
        ],
    },
}]
jinja_environment  
alias of flask.templating.Environment
Foreword Read this before you get started with Flask. This hopefully answers some questions about the purpose and goals of the project, and when you should or should not be using it. What does “micro” mean? “Micro” does not mean that your whole web application has to fit into a single Python file (although it certainly can), nor does it mean that Flask is lacking in functionality. The “micro” in microframework means Flask aims to keep the core simple but extensible. Flask won’t make many decisions for you, such as what database to use. Those decisions that it does make, such as what templating engine to use, are easy to change. Everything else is up to you, so that Flask can be everything you need and nothing you don’t. By default, Flask does not include a database abstraction layer, form validation or anything else where different libraries already exist that can handle that. Instead, Flask supports extensions to add such functionality to your application as if it was implemented in Flask itself. Numerous extensions provide database integration, form validation, upload handling, various open authentication technologies, and more. Flask may be “micro”, but it’s ready for production use on a variety of needs. Configuration and Conventions Flask has many configuration values, with sensible defaults, and a few conventions when getting started. By convention, templates and static files are stored in subdirectories within the application’s Python source tree, with the names templates and static respectively. While this can be changed, you usually don’t have to, especially when getting started. Growing with Flask Once you have Flask up and running, you’ll find a variety of extensions available in the community to integrate your project for production. As your codebase grows, you are free to make the design decisions appropriate for your project. Flask will continue to provide a very simple glue layer to the best that Python has to offer. You can implement advanced patterns in SQLAlchemy or another database tool, introduce non-relational data persistence as appropriate, and take advantage of framework-agnostic tools built for WSGI, the Python web interface. Flask includes many hooks to customize its behavior. Should you need more customization, the Flask class is built for subclassing. If you are interested in that, check out the Becoming Big chapter. If you are curious about the Flask design principles, head over to the section about Design Decisions in Flask.
def f_13598363():
    """Flask set folder 'wherever' as the default template folder
    """
    return  
 --------------------

def f_3398589(c2):
    """sort a list of lists 'c2' such that third row comes first
    """
     
 --------------------

def f_3398589(c2):
    """sort a list of lists 'c2' in reversed row order
    """
     
 --------------------

def f_3398589(c2):
    """Sorting a list of lists `c2`, each by the third and second row
    """
     
 --------------------
Please refer to the following documentation to generate the code:
matplotlib.testing.set_font_settings_for_testing()[source]
tkinter.font.NORMAL  
tkinter.font.BOLD  
tkinter.font.ITALIC  
tkinter.font.ROMAN
tkinter.font.NORMAL  
tkinter.font.BOLD  
tkinter.font.ITALIC  
tkinter.font.ROMAN
tkinter.font.NORMAL  
tkinter.font.BOLD  
tkinter.font.ITALIC  
tkinter.font.ROMAN
tkinter.font.NORMAL  
tkinter.font.BOLD  
tkinter.font.ITALIC  
tkinter.font.ROMAN
def f_10960463():
    """set font `Arial` to display non-ascii characters in matplotlib
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
date.toordinal()  
Return the proleptic Gregorian ordinal of the date, where January 1 of year 1 has ordinal 1. For any date object d, date.fromordinal(d.toordinal()) == d.
pandas.Timestamp.toordinal   Timestamp.toordinal()
 
Return proleptic Gregorian ordinal. January 1 of year 1 is day 1.
datetime.toordinal()  
Return the proleptic Gregorian ordinal of the date. The same as self.date().toordinal().
pandas.Period.ordinal   Period.ordinal
pandas.Series.dt.day   Series.dt.day
 
The day of the datetime. Examples 
>>> datetime_series = pd.Series(
...     pd.date_range("2000-01-01", periods=3, freq="D")
... )
>>> datetime_series
0   2000-01-01
1   2000-01-02
2   2000-01-03
dtype: datetime64[ns]
>>> datetime_series.dt.day
0    1
1    2
2    3
dtype: int64
def f_20576618(df):
    """Convert  DateTime column 'date' of pandas dataframe 'df' to ordinal
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
pandas.DataFrame.at   propertyDataFrame.at
 
Access a single value for a row/column label pair. Similar to loc, in that both provide label-based lookups. Use at if you only need to get or set a single value in a DataFrame or Series.  Raises 
 KeyError

If ‘label’ does not exist in DataFrame.      See also  DataFrame.iat

Access a single value for a row/column pair by integer position.  DataFrame.loc

Access a group of rows and columns by label(s).  Series.at

Access a single value using a label.    Examples 
>>> df = pd.DataFrame([[0, 2, 3], [0, 4, 1], [10, 20, 30]],
...                   index=[4, 5, 6], columns=['A', 'B', 'C'])
>>> df
    A   B   C
4   0   2   3
5   0   4   1
6  10  20  30
  Get value at specified row/column pair 
>>> df.at[4, 'B']
2
  Set value at specified row/column pair 
>>> df.at[4, 'B'] = 10
>>> df.at[4, 'B']
10
  Get value within a Series 
>>> df.loc[5].at['B']
4
pandas.Series.at   propertySeries.at
 
Access a single value for a row/column label pair. Similar to loc, in that both provide label-based lookups. Use at if you only need to get or set a single value in a DataFrame or Series.  Raises 
 KeyError

If ‘label’ does not exist in DataFrame.      See also  DataFrame.iat

Access a single value for a row/column pair by integer position.  DataFrame.loc

Access a group of rows and columns by label(s).  Series.at

Access a single value using a label.    Examples 
>>> df = pd.DataFrame([[0, 2, 3], [0, 4, 1], [10, 20, 30]],
...                   index=[4, 5, 6], columns=['A', 'B', 'C'])
>>> df
    A   B   C
4   0   2   3
5   0   4   1
6  10  20  30
  Get value at specified row/column pair 
>>> df.at[4, 'B']
2
  Set value at specified row/column pair 
>>> df.at[4, 'B'] = 10
>>> df.at[4, 'B']
10
  Get value within a Series 
>>> df.loc[5].at['B']
4
pandas.Index.asof   finalIndex.asof(label)[source]
 
Return the label from the index, or, if not present, the previous one. Assuming that the index is sorted, return the passed index label if it is in the index, or return the previous index label if the passed one is not in the index.  Parameters 
 
label:object


The label up to which the method returns the latest index label.    Returns 
 object

The passed label if it is in the index. The previous label if the passed label is not in the sorted index or NaN if there is no such label.      See also  Series.asof

Return the latest value in a Series up to the passed index.  merge_asof

Perform an asof merge (similar to left join but it matches on nearest key rather than equal key).  Index.get_loc

An asof is a thin wrapper around get_loc with method=’pad’.    Examples Index.asof returns the latest index label up to the passed label. 
>>> idx = pd.Index(['2013-12-31', '2014-01-02', '2014-01-03'])
>>> idx.asof('2014-01-01')
'2013-12-31'
  If the label is in the index, the method returns the passed label. 
>>> idx.asof('2014-01-02')
'2014-01-02'
  If all of the labels in the index are later than the passed label, NaN is returned. 
>>> idx.asof('1999-01-02')
nan
  If the index is not sorted, an error is raised. 
>>> idx_not_sorted = pd.Index(['2013-12-31', '2015-01-02',
...                            '2014-01-03'])
>>> idx_not_sorted.asof('2013-12-31')
Traceback (most recent call last):
ValueError: index must be monotonic increasing or decreasing
pandas.Index.asof_locs   Index.asof_locs(where, mask)[source]
 
Return the locations (indices) of labels in the index. As in the asof function, if the label (a particular entry in where) is not in the index, the latest index label up to the passed label is chosen and its index returned. If all of the labels in the index are later than a label in where, -1 is returned. mask is used to ignore NA values in the index during calculation.  Parameters 
 
where:Index


An Index consisting of an array of timestamps.  
mask:np.ndarray[bool]


Array of booleans denoting where values in the original data are not NA.    Returns 
 np.ndarray[np.intp]

An array of locations (indices) of the labels from the Index which correspond to the return values of the asof function for every element in where.
pandas.DataFrame.iat   propertyDataFrame.iat
 
Access a single value for a row/column pair by integer position. Similar to iloc, in that both provide integer-based lookups. Use iat if you only need to get or set a single value in a DataFrame or Series.  Raises 
 IndexError

When integer position is out of bounds.      See also  DataFrame.at

Access a single value for a row/column label pair.  DataFrame.loc

Access a group of rows and columns by label(s).  DataFrame.iloc

Access a group of rows and columns by integer position(s).    Examples 
>>> df = pd.DataFrame([[0, 2, 3], [0, 4, 1], [10, 20, 30]],
...                   columns=['A', 'B', 'C'])
>>> df
    A   B   C
0   0   2   3
1   0   4   1
2  10  20  30
  Get value at specified row/column pair 
>>> df.iat[1, 2]
1
  Set value at specified row/column pair 
>>> df.iat[1, 2] = 10
>>> df.iat[1, 2]
10
  Get value within a series 
>>> df.loc[0].iat[1]
2
def f_31793195(df):
    """Get the integer location of a key `bob` in a pandas data frame `df`
    """
    return  
 --------------------

def f_10487278(my_dict):
    """add an item with key 'third_key' and value 1 to an dictionary `my_dict`
    """
     
 --------------------

def f_10487278():
    """declare an array `my_list`
    """
     
 --------------------

def f_10487278(my_list):
    """Insert item `12` to a list `my_list`
    """
     
 --------------------

def f_10155684(myList):
    """add an entry 'wuggah' at the beginning of list `myList`
    """
     
 --------------------

def f_3519125(hex_str):
    """convert a hex-string representation `hex_str` to actual bytes
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
class LastValue(expression, **extra)
pandas.core.groupby.GroupBy.last   finalGroupBy.last(numeric_only=False, min_count=- 1)[source]
 
Compute last of group values.  Parameters 
 
numeric_only:bool, default False


Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data.  
min_count:int, default -1


The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA.    Returns 
 Series or DataFrame

Computed last of values within each group.
last()
pandas.core.resample.Resampler.last   Resampler.last(_method='last', min_count=0, *args, **kwargs)[source]
 
Compute last of group values.  Parameters 
 
numeric_only:bool, default False


Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data.  
min_count:int, default -1


The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA.    Returns 
 Series or DataFrame

Computed last of values within each group.
pandas.DataFrame.last   DataFrame.last(offset)[source]
 
Select final periods of time series data based on a date offset. For a DataFrame with a sorted DatetimeIndex, this function selects the last few rows based on a date offset.  Parameters 
 
offset:str, DateOffset, dateutil.relativedelta


The offset length of the data that will be selected. For instance, ‘3D’ will display all the rows having their index within the last 3 days.    Returns 
 Series or DataFrame

A subset of the caller.    Raises 
 TypeError

If the index is not a DatetimeIndex      See also  first

Select initial periods of time series based on a date offset.  at_time

Select values at a particular time of the day.  between_time

Select values between particular times of the day.    Examples 
>>> i = pd.date_range('2018-04-09', periods=4, freq='2D')
>>> ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i)
>>> ts
            A
2018-04-09  1
2018-04-11  2
2018-04-13  3
2018-04-15  4
  Get the rows for the last 3 days: 
>>> ts.last('3D')
            A
2018-04-13  3
2018-04-15  4
  Notice the data for 3 last calendar days were returned, not the last 3 observed days in the dataset, and therefore data for 2018-04-11 was not returned.
def f_40144769(df):
    """select the last column of dataframe `df`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
stringprep.in_table_c11_c12(code)  
Determine whether code is in tableC.1 (Space characters, union of C.1.1 and C.1.2).
stringprep.in_table_c4(code)  
Determine whether code is in tableC.4 (Non-character code points).
class torch.distributions.chi2.Chi2(df, validate_args=None) [source]
 
Bases: torch.distributions.gamma.Gamma Creates a Chi2 distribution parameterized by shape parameter df. This is exactly equivalent to Gamma(alpha=0.5*df, beta=0.5) Example: >>> m = Chi2(torch.tensor([1.0]))
>>> m.sample()  # Chi2 distributed with shape df=1
tensor([ 0.1046])
  Parameters 
df (float or Tensor) – shape parameter of the distribution    
arg_constraints = {'df': GreaterThan(lower_bound=0.0)} 
  
property df 
  
expand(batch_shape, _instance=None) [source]
stringprep.in_table_c21_c22(code)  
Determine whether code is in tableC.2 (Control characters, union of C.2.1 and C.2.2).
stringprep.in_table_c3(code)  
Determine whether code is in tableC.3 (Private use).
def f_30787901(df):
    """get the first value from dataframe `df` where column 'Letters' is equal to 'C'
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
numpy.bmat   numpy.bmat(obj, ldict=None, gdict=None)[source]
 
Build a matrix object from a string, nested sequence, or array.  Parameters 
 
objstr or array_like


Input data. If a string, variables in the current scope may be referenced by name.  
ldictdict, optional


A dictionary that replaces local operands in current frame. Ignored if obj is not a string or gdict is None.  
gdictdict, optional


A dictionary that replaces global operands in current frame. Ignored if obj is not a string.    Returns 
 
outmatrix


Returns a matrix object, which is a specialized 2-D array.      See also  block

A generalization of this function for N-d arrays, that returns normal ndarrays.    Examples >>> A = np.mat('1 1; 1 1')
>>> B = np.mat('2 2; 2 2')
>>> C = np.mat('3 4; 5 6')
>>> D = np.mat('7 8; 9 0')
 All the following expressions construct the same block matrix: >>> np.bmat([[A, B], [C, D]])
matrix([[1, 1, 2, 2],
        [1, 1, 2, 2],
        [3, 4, 7, 8],
        [5, 6, 9, 0]])
>>> np.bmat(np.r_[np.c_[A, B], np.c_[C, D]])
matrix([[1, 1, 2, 2],
        [1, 1, 2, 2],
        [3, 4, 7, 8],
        [5, 6, 9, 0]])
>>> np.bmat('A,B; C,D')
matrix([[1, 1, 2, 2],
        [1, 1, 2, 2],
        [3, 4, 7, 8],
        [5, 6, 9, 0]])
tf.keras.utils.to_categorical     View source on GitHub    Converts a class vector (integers) to binary class matrix.  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.keras.utils.to_categorical  
tf.keras.utils.to_categorical(
    y, num_classes=None, dtype='float32'
)
 E.g. for use with categorical_crossentropy.
 


 Arguments
  y   class vector to be converted into a matrix (integers from 0 to num_classes).  
  num_classes   total number of classes. If None, this would be inferred as the (largest number in y) + 1.  
  dtype   The data type expected by the input. Default: 'float32'.   
 


 Returns   A binary matrix representation of the input. The classes axis is placed last.  
 Example: 
a = tf.keras.utils.to_categorical([0, 1, 2, 3], num_classes=4)
a = tf.constant(a, shape=[4, 4])
print(a)
tf.Tensor(
  [[1. 0. 0. 0.]
   [0. 1. 0. 0.]
   [0. 0. 1. 0.]
   [0. 0. 0. 1.]], shape=(4, 4), dtype=float32)
 
b = tf.constant([.9, .04, .03, .03,
                 .3, .45, .15, .13,
                 .04, .01, .94, .05,
                 .12, .21, .5, .17],
                shape=[4, 4])
loss = tf.keras.backend.categorical_crossentropy(a, b)
print(np.around(loss, 5))
[0.10536 0.82807 0.1011  1.77196]
 
loss = tf.keras.backend.categorical_crossentropy(a, a)
print(np.around(loss, 5))
[0. 0. 0. 0.]

 


 Raises   Value Error: If input contains string value
curses.getsyx()  
Return the current coordinates of the virtual screen cursor as a tuple (y, x). If leaveok is currently True, then return (-1, -1).
get_canvas_width_height()[source]
 
Return the canvas width and height in display coords.
tf.compat.v1.initialize_variables See tf.compat.v1.variables_initializer. (deprecated) 
tf.compat.v1.initialize_variables(
    var_list, name='init'
)
 Warning: THIS FUNCTION IS DEPRECATED. It will be removed after 2017-03-02. Instructions for updating: Use tf.variables_initializer instead.
Note: The output of this function should be used. If it is not, a warning will be logged or an error may be raised. To mark the output as used, call its .mark_used() method.
def f_18730044():
    """converting two lists `[1, 2, 3]` and `[4, 5, 6]` into a matrix
    """
    return  
 --------------------

def f_402504(i):
    """get the type of `i`
    """
    return  
 --------------------

def f_402504(v):
    """determine the type of variable `v`
    """
    return  
 --------------------

def f_402504(v):
    """determine the type of variable `v`
    """
    return  
 --------------------

def f_402504(variable_name):
    """get the type of variable `variable_name`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
inspect.getgeneratorstate(generator)  
Get current state of a generator-iterator.  Possible states are:

 GEN_CREATED: Waiting to start execution. GEN_RUNNING: Currently being executed by the interpreter. GEN_SUSPENDED: Currently suspended at a yield expression. GEN_CLOSED: Execution has completed.     New in version 3.2.
flask.stream_with_context(generator_or_function)  
Request contexts disappear when the response is started on the server. This is done for efficiency reasons and to make it less likely to encounter memory leaks with badly written WSGI middlewares. The downside is that if you are using streamed responses, the generator cannot access request bound information any more. This function however can help you keep the context around for longer: from flask import stream_with_context, request, Response

@app.route('/stream')
def streamed_response():
    @stream_with_context
    def generate():
        yield 'Hello '
        yield request.args['name']
        yield '!'
    return Response(generate())
 Alternatively it can also be used around a specific generator: from flask import stream_with_context, request, Response

@app.route('/stream')
def streamed_response():
    def generate():
        yield 'Hello '
        yield request.args['name']
        yield '!'
    return Response(stream_with_context(generate()))
  Changelog New in version 0.9.   Parameters 
generator_or_function (Union[Generator, Callable]) –   Return type 
Generator
class typing.Generator(Iterator[T_co], Generic[T_co, T_contra, V_co])  
A generator can be annotated by the generic type Generator[YieldType, SendType, ReturnType]. For example: def echo_round() -> Generator[int, float, str]:
    sent = yield 0
    while sent >= 0:
        sent = yield round(sent)
    return 'Done'
 Note that unlike many other generics in the typing module, the SendType of Generator behaves contravariantly, not covariantly or invariantly. If your generator will only yield values, set the SendType and ReturnType to None: def infinite_stream(start: int) -> Generator[int, None, None]:
    while True:
        yield start
        start += 1
 Alternatively, annotate your generator as having a return type of either Iterable[YieldType] or Iterator[YieldType]: def infinite_stream(start: int) -> Iterator[int]:
    while True:
        yield start
        start += 1
  Deprecated since version 3.9: collections.abc.Generator now supports []. See PEP 585 and Generic Alias Type.
Match.__getitem__(g)  
This is identical to m.group(g). This allows easier access to an individual group from a match: >>> m = re.match(r"(\w+) (\w+)", "Isaac Newton, physicist")
>>> m[0]       # The entire match
'Isaac Newton'
>>> m[1]       # The first parenthesized subgroup.
'Isaac'
>>> m[2]       # The second parenthesized subgroup.
'Newton'
  New in version 3.6.
flask.appcontext_pushed  
This signal is sent when an application context is pushed. The sender is the application. This is usually useful for unittests in order to temporarily hook in information. For instance it can be used to set a resource early onto the g object. Example usage: from contextlib import contextmanager
from flask import appcontext_pushed

@contextmanager
def user_set(app, user):
    def handler(sender, **kwargs):
        g.user = user
    with appcontext_pushed.connected_to(handler, app):
        yield
 And in the testcode: def test_user_me(self):
    with user_set(app, 'john'):
        c = app.test_client()
        resp = c.get('/users/me')
        assert resp.data == 'username=john'
  Changelog New in version 0.10.
def f_2300756(g):
    """get the 5th item of a generator `g`
    """
    return  
 --------------------

def f_20056548(word):
    """return a string `word` with string format
    """
    return  
 --------------------

def f_8546245(list):
    """join a list of strings `list` using a space ' '
    """
    return  
 --------------------

def f_2276416():
    """create list `y` containing two empty lists
    """
     
 --------------------

def f_3925614(filename):
    """read a file `filename` into a list `data`
    """
     
 --------------------

def f_22187233():
    """delete all occurrences of character 'i' in string 'it is icy'
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
i1i2i3 
 Gets or sets the I1I2I3 representation of the Color. i1i2i3 -> tuple  The I1I2I3 representation of the Color. The I1I2I3 components are in the ranges I1 = [0, 1], I2 = [-0.5, 0.5], I3 = [-0.5, 0.5]. Note that this will not return the absolutely exact I1I2I3 values for the set RGB values in all cases. Due to the RGB mapping from 0-255 and the I1I2I3 mapping from 0-1 rounding errors may cause the I1I2I3 values to differ slightly from what you might expect.
tf.raw_ops.InplaceSub Subtracts v into specified rows of x.  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.raw_ops.InplaceSub  
tf.raw_ops.InplaceSub(
    x, i, v, name=None
)
 Computes y = x; y[i, :] -= v; return y.
 


 Args
  x   A Tensor. A Tensor of type T.  
  i   A Tensor of type int32. A vector. Indices into the left-most dimension of x.  
  v   A Tensor. Must have the same type as x. A Tensor of type T. Same dimension sizes as x except the first dimension, which must be the same as i's size.  
  name   A name for the operation (optional).   
 


 Returns   A Tensor. Has the same type as x.
curses.has_ic()  
Return True if the terminal has insert- and delete-character capabilities. This function is included for historical reasons only, as all modern software terminal emulators have such capabilities.
set_rticks(*args, **kwargs)[source]
colorsys.yiq_to_rgb(y, i, q)  
Convert the color from YIQ coordinates to RGB coordinates.
def f_22187233():
    """delete all instances of a character 'i' in a string 'it is icy'
    """
    return  
 --------------------

def f_22187233():
    """delete all characters "i" in string "it is icy"
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
pandas.DataFrame.dropna   DataFrame.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)[source]
 
Remove missing values. See the User Guide for more on which values are considered missing, and how to work with missing data.  Parameters 
 
axis:{0 or ‘index’, 1 or ‘columns’}, default 0


Determine if rows or columns which contain missing values are removed.  0, or ‘index’ : Drop rows which contain missing values. 1, or ‘columns’ : Drop columns which contain missing value.   Changed in version 1.0.0: Pass tuple or list to drop on multiple axes. Only a single axis is allowed.   
how:{‘any’, ‘all’}, default ‘any’


Determine if row or column is removed from DataFrame, when we have at least one NA or all NA.  ‘any’ : If any NA values are present, drop that row or column. ‘all’ : If all values are NA, drop that row or column.   
thresh:int, optional


Require that many non-NA values.  
subset:column label or sequence of labels, optional


Labels along other axis to consider, e.g. if you are dropping rows these would be a list of columns to include.  
inplace:bool, default False


If True, do operation inplace and return None.    Returns 
 DataFrame or None

DataFrame with NA entries dropped from it or None if inplace=True.      See also  DataFrame.isna

Indicate missing values.  DataFrame.notna

Indicate existing (non-missing) values.  DataFrame.fillna

Replace missing values.  Series.dropna

Drop missing values.  Index.dropna

Drop missing indices.    Examples 
>>> df = pd.DataFrame({"name": ['Alfred', 'Batman', 'Catwoman'],
...                    "toy": [np.nan, 'Batmobile', 'Bullwhip'],
...                    "born": [pd.NaT, pd.Timestamp("1940-04-25"),
...                             pd.NaT]})
>>> df
       name        toy       born
0    Alfred        NaN        NaT
1    Batman  Batmobile 1940-04-25
2  Catwoman   Bullwhip        NaT
  Drop the rows where at least one element is missing. 
>>> df.dropna()
     name        toy       born
1  Batman  Batmobile 1940-04-25
  Drop the columns where at least one element is missing. 
>>> df.dropna(axis='columns')
       name
0    Alfred
1    Batman
2  Catwoman
  Drop the rows where all elements are missing. 
>>> df.dropna(how='all')
       name        toy       born
0    Alfred        NaN        NaT
1    Batman  Batmobile 1940-04-25
2  Catwoman   Bullwhip        NaT
  Keep only the rows with at least 2 non-NA values. 
>>> df.dropna(thresh=2)
       name        toy       born
1    Batman  Batmobile 1940-04-25
2  Catwoman   Bullwhip        NaT
  Define in which columns to look for missing values. 
>>> df.dropna(subset=['name', 'toy'])
       name        toy       born
1    Batman  Batmobile 1940-04-25
2  Catwoman   Bullwhip        NaT
  Keep the DataFrame with valid entries in the same variable. 
>>> df.dropna(inplace=True)
>>> df
     name        toy       born
1  Batman  Batmobile 1940-04-25
pandas.Index.dropna   Index.dropna(how='any')[source]
 
Return Index without NA/NaN values.  Parameters 
 
how:{‘any’, ‘all’}, default ‘any’


If the Index is a MultiIndex, drop the value when any or all levels are NaN.    Returns 
 Index
pandas.Series.dropna   Series.dropna(axis=0, inplace=False, how=None)[source]
 
Return a new Series with missing values removed. See the User Guide for more on which values are considered missing, and how to work with missing data.  Parameters 
 
axis:{0 or ‘index’}, default 0


There is only one axis to drop values from.  
inplace:bool, default False


If True, do operation inplace and return None.  
how:str, optional


Not in use. Kept for compatibility.    Returns 
 Series or None

Series with NA entries dropped from it or None if inplace=True.      See also  Series.isna

Indicate missing values.  Series.notna

Indicate existing (non-missing) values.  Series.fillna

Replace missing values.  DataFrame.dropna

Drop rows or columns which contain NA values.  Index.dropna

Drop missing indices.    Examples 
>>> ser = pd.Series([1., 2., np.nan])
>>> ser
0    1.0
1    2.0
2    NaN
dtype: float64
  Drop NA values from a Series. 
>>> ser.dropna()
0    1.0
1    2.0
dtype: float64
  Keep the Series with valid entries in the same variable. 
>>> ser.dropna(inplace=True)
>>> ser
0    1.0
1    2.0
dtype: float64
  Empty strings are not considered NA values. None is considered an NA value. 
>>> ser = pd.Series([np.NaN, 2, pd.NaT, '', None, 'I stay'])
>>> ser
0       NaN
1         2
2       NaT
3
4      None
5    I stay
dtype: object
>>> ser.dropna()
1         2
3
5    I stay
dtype: object
pandas.api.extensions.ExtensionArray.dropna   ExtensionArray.dropna()[source]
 
Return ExtensionArray without NA values.  Returns 
 
valid:ExtensionArray
pandas.DataFrame.sparse.to_dense   DataFrame.sparse.to_dense()[source]
 
Convert a DataFrame with sparse values to dense.  New in version 0.25.0.   Returns 
 DataFrame

A DataFrame with the same values stored as dense arrays.     Examples 
>>> df = pd.DataFrame({"A": pd.arrays.SparseArray([0, 1, 0])})
>>> df.sparse.to_dense()
   A
0  0
1  1
2  0
def f_13413590(df):
    """Drop rows of pandas dataframe `df` having NaN in column at index "1"
    """
    return  
 --------------------

def f_598398(myList):
    """get elements from list `myList`, that have a field `n` value 30
    """
    return  
 --------------------

def f_10351772(intstringlist):
    """converting list of strings `intstringlist` to list of integer `nums`
    """
     
 --------------------
Please refer to the following documentation to generate the code:
re.S  
re.DOTALL  
Make the '.' special character match any character at all, including a newline; without this flag, '.' will match anything except a newline. Corresponds to the inline flag (?s).
re.S  
re.DOTALL  
Make the '.' special character match any character at all, including a newline; without this flag, '.' will match anything except a newline. Corresponds to the inline flag (?s).
token.DOT  
Token value for ".".
turtle.tracer(n=None, delay=None)  
 Parameters 
 
n – nonnegative integer 
delay – nonnegative integer    Turn turtle animation on/off and set delay for update drawings. If n is given, only each n-th regular screen update is really performed. (Can be used to accelerate the drawing of complex graphics.) When called without arguments, returns the currently stored value of n. Second argument sets delay value (see delay()). >>> screen.tracer(8, 25)
>>> dist = 2
>>> for i in range(200):
...     fd(dist)
...     rt(90)
...     dist += 2
Cmd.emptyline()  
Method called when an empty line is entered in response to the prompt. If this method is not overridden, it repeats the last nonempty command entered.
def f_493386():
    """print "." without newline
    """
    return  
 --------------------

def f_6569528():
    """round off the float that is the product of `2.52 * 100` and convert it to an int
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
test.support.findfile(filename, subdir=None)  
Return the path to the file named filename. If no match is found filename is returned. This does not equal a failure since it could be the path to the file. Setting subdir indicates a relative path to use to find the file rather than looking directly in the path directories.
numpy.distutils.misc_util.get_script_files(scripts)[source]
matplotlib.font_manager.list_fonts(directory, extensions)[source]
 
Return a list of all fonts matching any of the extensions, found recursively under the directory.
tf.keras.preprocessing.text_dataset_from_directory Generates a tf.data.Dataset from text files in a directory. 
tf.keras.preprocessing.text_dataset_from_directory(
    directory, labels='inferred', label_mode='int',
    class_names=None, batch_size=32, max_length=None, shuffle=True, seed=None,
    validation_split=None, subset=None, follow_links=False
)
 If your directory structure is: main_directory/
...class_a/
......a_text_1.txt
......a_text_2.txt
...class_b/
......b_text_1.txt
......b_text_2.txt
 Then calling text_dataset_from_directory(main_directory, labels='inferred') will return a tf.data.Dataset that yields batches of texts from the subdirectories class_a and class_b, together with labels 0 and 1 (0 corresponding to class_a and 1 corresponding to class_b). Only .txt files are supported at this time.
 


 Arguments
  directory   Directory where the data is located. If labels is "inferred", it should contain subdirectories, each containing text files for a class. Otherwise, the directory structure is ignored.  
  labels   Either "inferred" (labels are generated from the directory structure), or a list/tuple of integer labels of the same size as the number of text files found in the directory. Labels should be sorted according to the alphanumeric order of the text file paths (obtained via os.walk(directory) in Python).  
  label_mode    'int': means that the labels are encoded as integers (e.g. for sparse_categorical_crossentropy loss). 'categorical' means that the labels are encoded as a categorical vector (e.g. for categorical_crossentropy loss). 'binary' means that the labels (there can be only 2) are encoded as float32 scalars with values 0 or 1 (e.g. for binary_crossentropy). None (no labels). 

 
  class_names   Only valid if "labels" is "inferred". This is the explict list of class names (must match names of subdirectories). Used to control the order of the classes (otherwise alphanumerical order is used).  
  batch_size   Size of the batches of data. Default: 32.  
  max_length   Maximum size of a text string. Texts longer than this will be truncated to max_length.  
  shuffle   Whether to shuffle the data. Default: True. If set to False, sorts the data in alphanumeric order.  
  seed   Optional random seed for shuffling and transformations.  
  validation_split   Optional float between 0 and 1, fraction of data to reserve for validation.  
  subset   One of "training" or "validation". Only used if validation_split is set.  
  follow_links   Whether to visits subdirectories pointed to by symlinks. Defaults to False.   
 


 Returns   A tf.data.Dataset object.  If label_mode is None, it yields string tensors of shape (batch_size,), containing the contents of a batch of text files. Otherwise, it yields a tuple (texts, labels), where texts has shape (batch_size,) and labels follows the format described below. 

 
 Rules regarding labels format:  if label_mode is int, the labels are an int32 tensor of shape (batch_size,). if label_mode is binary, the labels are a float32 tensor of 1s and 0s of shape (batch_size, 1). if label_mode is categorial, the labels are a float32 tensor of shape (batch_size, num_classes), representing a one-hot encoding of the class index.
torch.divide(input, other, *, rounding_mode=None, out=None) → Tensor  
Alias for torch.div().
def f_3964681():
    """Find all files `files` in directory '/mydir' with extension '.txt'
    """
     
 --------------------
Please refer to the following documentation to generate the code:
fnmatch.fnmatch(filename, pattern)  
Test whether the filename string matches the pattern string, returning True or False. Both parameters are case-normalized using os.path.normcase(). fnmatchcase() can be used to perform a case-sensitive comparison, regardless of whether that’s standard for the operating system. This example will print all file names in the current directory with the extension .txt: import fnmatch
import os

for file in os.listdir('.'):
    if fnmatch.fnmatch(file, '*.txt'):
        print(file)
findtext(match, default=None, namespaces=None)  
Same as Element.findtext(), starting at the root of the tree.
test.support.findfile(filename, subdir=None)  
Return the path to the file named filename. If no match is found filename is returned. This does not equal a failure since it could be the path to the file. Setting subdir indicates a relative path to use to find the file rather than looking directly in the path directories.
tf.keras.preprocessing.text_dataset_from_directory Generates a tf.data.Dataset from text files in a directory. 
tf.keras.preprocessing.text_dataset_from_directory(
    directory, labels='inferred', label_mode='int',
    class_names=None, batch_size=32, max_length=None, shuffle=True, seed=None,
    validation_split=None, subset=None, follow_links=False
)
 If your directory structure is: main_directory/
...class_a/
......a_text_1.txt
......a_text_2.txt
...class_b/
......b_text_1.txt
......b_text_2.txt
 Then calling text_dataset_from_directory(main_directory, labels='inferred') will return a tf.data.Dataset that yields batches of texts from the subdirectories class_a and class_b, together with labels 0 and 1 (0 corresponding to class_a and 1 corresponding to class_b). Only .txt files are supported at this time.
 


 Arguments
  directory   Directory where the data is located. If labels is "inferred", it should contain subdirectories, each containing text files for a class. Otherwise, the directory structure is ignored.  
  labels   Either "inferred" (labels are generated from the directory structure), or a list/tuple of integer labels of the same size as the number of text files found in the directory. Labels should be sorted according to the alphanumeric order of the text file paths (obtained via os.walk(directory) in Python).  
  label_mode    'int': means that the labels are encoded as integers (e.g. for sparse_categorical_crossentropy loss). 'categorical' means that the labels are encoded as a categorical vector (e.g. for categorical_crossentropy loss). 'binary' means that the labels (there can be only 2) are encoded as float32 scalars with values 0 or 1 (e.g. for binary_crossentropy). None (no labels). 

 
  class_names   Only valid if "labels" is "inferred". This is the explict list of class names (must match names of subdirectories). Used to control the order of the classes (otherwise alphanumerical order is used).  
  batch_size   Size of the batches of data. Default: 32.  
  max_length   Maximum size of a text string. Texts longer than this will be truncated to max_length.  
  shuffle   Whether to shuffle the data. Default: True. If set to False, sorts the data in alphanumeric order.  
  seed   Optional random seed for shuffling and transformations.  
  validation_split   Optional float between 0 and 1, fraction of data to reserve for validation.  
  subset   One of "training" or "validation". Only used if validation_split is set.  
  follow_links   Whether to visits subdirectories pointed to by symlinks. Defaults to False.   
 


 Returns   A tf.data.Dataset object.  If label_mode is None, it yields string tensors of shape (batch_size,), containing the contents of a batch of text files. Otherwise, it yields a tuple (texts, labels), where texts has shape (batch_size,) and labels follows the format described below. 

 
 Rules regarding labels format:  if label_mode is int, the labels are an int32 tensor of shape (batch_size,). if label_mode is binary, the labels are a float32 tensor of 1s and 0s of shape (batch_size, 1). if label_mode is categorial, the labels are a float32 tensor of shape (batch_size, num_classes), representing a one-hot encoding of the class index.
close()  
Maildir instances do not keep any open files and the underlying mailboxes do not support locking, so this method does nothing.
def f_3964681():
    """Find all files in directory "/mydir" with extension ".txt"
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
fnmatch.fnmatch(filename, pattern)  
Test whether the filename string matches the pattern string, returning True or False. Both parameters are case-normalized using os.path.normcase(). fnmatchcase() can be used to perform a case-sensitive comparison, regardless of whether that’s standard for the operating system. This example will print all file names in the current directory with the extension .txt: import fnmatch
import os

for file in os.listdir('.'):
    if fnmatch.fnmatch(file, '*.txt'):
        print(file)
findtext(match, default=None, namespaces=None)  
Same as Element.findtext(), starting at the root of the tree.
test.support.findfile(filename, subdir=None)  
Return the path to the file named filename. If no match is found filename is returned. This does not equal a failure since it could be the path to the file. Setting subdir indicates a relative path to use to find the file rather than looking directly in the path directories.
tf.keras.preprocessing.text_dataset_from_directory Generates a tf.data.Dataset from text files in a directory. 
tf.keras.preprocessing.text_dataset_from_directory(
    directory, labels='inferred', label_mode='int',
    class_names=None, batch_size=32, max_length=None, shuffle=True, seed=None,
    validation_split=None, subset=None, follow_links=False
)
 If your directory structure is: main_directory/
...class_a/
......a_text_1.txt
......a_text_2.txt
...class_b/
......b_text_1.txt
......b_text_2.txt
 Then calling text_dataset_from_directory(main_directory, labels='inferred') will return a tf.data.Dataset that yields batches of texts from the subdirectories class_a and class_b, together with labels 0 and 1 (0 corresponding to class_a and 1 corresponding to class_b). Only .txt files are supported at this time.
 


 Arguments
  directory   Directory where the data is located. If labels is "inferred", it should contain subdirectories, each containing text files for a class. Otherwise, the directory structure is ignored.  
  labels   Either "inferred" (labels are generated from the directory structure), or a list/tuple of integer labels of the same size as the number of text files found in the directory. Labels should be sorted according to the alphanumeric order of the text file paths (obtained via os.walk(directory) in Python).  
  label_mode    'int': means that the labels are encoded as integers (e.g. for sparse_categorical_crossentropy loss). 'categorical' means that the labels are encoded as a categorical vector (e.g. for categorical_crossentropy loss). 'binary' means that the labels (there can be only 2) are encoded as float32 scalars with values 0 or 1 (e.g. for binary_crossentropy). None (no labels). 

 
  class_names   Only valid if "labels" is "inferred". This is the explict list of class names (must match names of subdirectories). Used to control the order of the classes (otherwise alphanumerical order is used).  
  batch_size   Size of the batches of data. Default: 32.  
  max_length   Maximum size of a text string. Texts longer than this will be truncated to max_length.  
  shuffle   Whether to shuffle the data. Default: True. If set to False, sorts the data in alphanumeric order.  
  seed   Optional random seed for shuffling and transformations.  
  validation_split   Optional float between 0 and 1, fraction of data to reserve for validation.  
  subset   One of "training" or "validation". Only used if validation_split is set.  
  follow_links   Whether to visits subdirectories pointed to by symlinks. Defaults to False.   
 


 Returns   A tf.data.Dataset object.  If label_mode is None, it yields string tensors of shape (batch_size,), containing the contents of a batch of text files. Otherwise, it yields a tuple (texts, labels), where texts has shape (batch_size,) and labels follows the format described below. 

 
 Rules regarding labels format:  if label_mode is int, the labels are an int32 tensor of shape (batch_size,). if label_mode is binary, the labels are a float32 tensor of 1s and 0s of shape (batch_size, 1). if label_mode is categorial, the labels are a float32 tensor of shape (batch_size, num_classes), representing a one-hot encoding of the class index.
close()  
Maildir instances do not keep any open files and the underlying mailboxes do not support locking, so this method does nothing.
def f_3964681():
    """Find all files in directory "/mydir" with extension ".txt"
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
pandas.DataFrame.plot   DataFrame.plot(*args, **kwargs)[source]
 
Make plots of Series or DataFrame. Uses the backend specified by the option plotting.backend. By default, matplotlib is used.  Parameters 
 
data:Series or DataFrame


The object for which the method is called.  
x:label or position, default None


Only used if data is a DataFrame.  
y:label, position or list of label, positions, default None


Allows plotting of one column versus another. Only used if data is a DataFrame.  
kind:str


The kind of plot to produce:  ‘line’ : line plot (default) ‘bar’ : vertical bar plot ‘barh’ : horizontal bar plot ‘hist’ : histogram ‘box’ : boxplot ‘kde’ : Kernel Density Estimation plot ‘density’ : same as ‘kde’ ‘area’ : area plot ‘pie’ : pie plot ‘scatter’ : scatter plot (DataFrame only) ‘hexbin’ : hexbin plot (DataFrame only)   
ax:matplotlib axes object, default None


An axes of the current figure.  
subplots:bool, default False


Make separate subplots for each column.  
sharex:bool, default True if ax is None else False


In case subplots=True, share x axis and set some x axis labels to invisible; defaults to True if ax is None otherwise False if an ax is passed in; Be aware, that passing in both an ax and sharex=True will alter all x axis labels for all axis in a figure.  
sharey:bool, default False


In case subplots=True, share y axis and set some y axis labels to invisible.  
layout:tuple, optional


(rows, columns) for the layout of subplots.  
figsize:a tuple (width, height) in inches


Size of a figure object.  
use_index:bool, default True


Use index as ticks for x axis.  
title:str or list


Title to use for the plot. If a string is passed, print the string at the top of the figure. If a list is passed and subplots is True, print each item in the list above the corresponding subplot.  
grid:bool, default None (matlab style default)


Axis grid lines.  
legend:bool or {‘reverse’}


Place legend on axis subplots.  
style:list or dict


The matplotlib line style per column.  
logx:bool or ‘sym’, default False


Use log scaling or symlog scaling on x axis. .. versionchanged:: 0.25.0  
logy:bool or ‘sym’ default False


Use log scaling or symlog scaling on y axis. .. versionchanged:: 0.25.0  
loglog:bool or ‘sym’, default False


Use log scaling or symlog scaling on both x and y axes. .. versionchanged:: 0.25.0  
xticks:sequence


Values to use for the xticks.  
yticks:sequence


Values to use for the yticks.  
xlim:2-tuple/list


Set the x limits of the current axes.  
ylim:2-tuple/list


Set the y limits of the current axes.  
xlabel:label, optional


Name to use for the xlabel on x-axis. Default uses index name as xlabel, or the x-column name for planar plots.  New in version 1.1.0.   Changed in version 1.2.0: Now applicable to planar plots (scatter, hexbin).   
ylabel:label, optional


Name to use for the ylabel on y-axis. Default will show no ylabel, or the y-column name for planar plots.  New in version 1.1.0.   Changed in version 1.2.0: Now applicable to planar plots (scatter, hexbin).   
rot:int, default None


Rotation for ticks (xticks for vertical, yticks for horizontal plots).  
fontsize:int, default None


Font size for xticks and yticks.  
colormap:str or matplotlib colormap object, default None


Colormap to select colors from. If string, load colormap with that name from matplotlib.  
colorbar:bool, optional


If True, plot colorbar (only relevant for ‘scatter’ and ‘hexbin’ plots).  
position:float


Specify relative alignments for bar plot layout. From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5 (center).  
table:bool, Series or DataFrame, default False


If True, draw a table using the data in the DataFrame and the data will be transposed to meet matplotlib’s default layout. If a Series or DataFrame is passed, use passed data to draw a table.  
yerr:DataFrame, Series, array-like, dict and str


See Plotting with Error Bars for detail.  
xerr:DataFrame, Series, array-like, dict and str


Equivalent to yerr.  
stacked:bool, default False in line and bar plots, and True in area plot


If True, create stacked plot.  
sort_columns:bool, default False


Sort column names to determine plot ordering.  
secondary_y:bool or sequence, default False


Whether to plot on the secondary y-axis if a list/tuple, which columns to plot on secondary y-axis.  
mark_right:bool, default True


When using a secondary_y axis, automatically mark the column labels with “(right)” in the legend.  
include_bool:bool, default is False


If True, boolean values can be plotted.  
backend:str, default None


Backend to use instead of the backend specified in the option plotting.backend. For instance, ‘matplotlib’. Alternatively, to specify the plotting.backend for the whole session, set pd.options.plotting.backend.  New in version 1.0.0.   **kwargs

Options to pass to matplotlib plotting method.    Returns 
 
matplotlib.axes.Axes or numpy.ndarray of them

If the backend is not the default matplotlib one, the return value will be the object returned by the backend.     Notes  See matplotlib documentation online for more on this subject If kind = ‘bar’ or ‘barh’, you can specify relative alignments for bar plot layout by position keyword. From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5 (center)
pandas.Series.plot   Series.plot(*args, **kwargs)[source]
 
Make plots of Series or DataFrame. Uses the backend specified by the option plotting.backend. By default, matplotlib is used.  Parameters 
 
data:Series or DataFrame


The object for which the method is called.  
x:label or position, default None


Only used if data is a DataFrame.  
y:label, position or list of label, positions, default None


Allows plotting of one column versus another. Only used if data is a DataFrame.  
kind:str


The kind of plot to produce:  ‘line’ : line plot (default) ‘bar’ : vertical bar plot ‘barh’ : horizontal bar plot ‘hist’ : histogram ‘box’ : boxplot ‘kde’ : Kernel Density Estimation plot ‘density’ : same as ‘kde’ ‘area’ : area plot ‘pie’ : pie plot ‘scatter’ : scatter plot (DataFrame only) ‘hexbin’ : hexbin plot (DataFrame only)   
ax:matplotlib axes object, default None


An axes of the current figure.  
subplots:bool, default False


Make separate subplots for each column.  
sharex:bool, default True if ax is None else False


In case subplots=True, share x axis and set some x axis labels to invisible; defaults to True if ax is None otherwise False if an ax is passed in; Be aware, that passing in both an ax and sharex=True will alter all x axis labels for all axis in a figure.  
sharey:bool, default False


In case subplots=True, share y axis and set some y axis labels to invisible.  
layout:tuple, optional


(rows, columns) for the layout of subplots.  
figsize:a tuple (width, height) in inches


Size of a figure object.  
use_index:bool, default True


Use index as ticks for x axis.  
title:str or list


Title to use for the plot. If a string is passed, print the string at the top of the figure. If a list is passed and subplots is True, print each item in the list above the corresponding subplot.  
grid:bool, default None (matlab style default)


Axis grid lines.  
legend:bool or {‘reverse’}


Place legend on axis subplots.  
style:list or dict


The matplotlib line style per column.  
logx:bool or ‘sym’, default False


Use log scaling or symlog scaling on x axis. .. versionchanged:: 0.25.0  
logy:bool or ‘sym’ default False


Use log scaling or symlog scaling on y axis. .. versionchanged:: 0.25.0  
loglog:bool or ‘sym’, default False


Use log scaling or symlog scaling on both x and y axes. .. versionchanged:: 0.25.0  
xticks:sequence


Values to use for the xticks.  
yticks:sequence


Values to use for the yticks.  
xlim:2-tuple/list


Set the x limits of the current axes.  
ylim:2-tuple/list


Set the y limits of the current axes.  
xlabel:label, optional


Name to use for the xlabel on x-axis. Default uses index name as xlabel, or the x-column name for planar plots.  New in version 1.1.0.   Changed in version 1.2.0: Now applicable to planar plots (scatter, hexbin).   
ylabel:label, optional


Name to use for the ylabel on y-axis. Default will show no ylabel, or the y-column name for planar plots.  New in version 1.1.0.   Changed in version 1.2.0: Now applicable to planar plots (scatter, hexbin).   
rot:int, default None


Rotation for ticks (xticks for vertical, yticks for horizontal plots).  
fontsize:int, default None


Font size for xticks and yticks.  
colormap:str or matplotlib colormap object, default None


Colormap to select colors from. If string, load colormap with that name from matplotlib.  
colorbar:bool, optional


If True, plot colorbar (only relevant for ‘scatter’ and ‘hexbin’ plots).  
position:float


Specify relative alignments for bar plot layout. From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5 (center).  
table:bool, Series or DataFrame, default False


If True, draw a table using the data in the DataFrame and the data will be transposed to meet matplotlib’s default layout. If a Series or DataFrame is passed, use passed data to draw a table.  
yerr:DataFrame, Series, array-like, dict and str


See Plotting with Error Bars for detail.  
xerr:DataFrame, Series, array-like, dict and str


Equivalent to yerr.  
stacked:bool, default False in line and bar plots, and True in area plot


If True, create stacked plot.  
sort_columns:bool, default False


Sort column names to determine plot ordering.  
secondary_y:bool or sequence, default False


Whether to plot on the secondary y-axis if a list/tuple, which columns to plot on secondary y-axis.  
mark_right:bool, default True


When using a secondary_y axis, automatically mark the column labels with “(right)” in the legend.  
include_bool:bool, default is False


If True, boolean values can be plotted.  
backend:str, default None


Backend to use instead of the backend specified in the option plotting.backend. For instance, ‘matplotlib’. Alternatively, to specify the plotting.backend for the whole session, set pd.options.plotting.backend.  New in version 1.0.0.   **kwargs

Options to pass to matplotlib plotting method.    Returns 
 
matplotlib.axes.Axes or numpy.ndarray of them

If the backend is not the default matplotlib one, the return value will be the object returned by the backend.     Notes  See matplotlib documentation online for more on this subject If kind = ‘bar’ or ‘barh’, you can specify relative alignments for bar plot layout by position keyword. From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5 (center)
Plotting The following functions are contained in the pandas.plotting module.       
andrews_curves(frame, class_column[, ax, ...]) Generate a matplotlib plot of Andrews curves, for visualising clusters of multivariate data.  
autocorrelation_plot(series[, ax]) Autocorrelation plot for time series.  
bootstrap_plot(series[, fig, size, samples]) Bootstrap plot on mean, median and mid-range statistics.  
boxplot(data[, column, by, ax, fontsize, ...]) Make a box plot from DataFrame columns.  
deregister_matplotlib_converters() Remove pandas formatters and converters.  
lag_plot(series[, lag, ax]) Lag plot for time series.  
parallel_coordinates(frame, class_column[, ...]) Parallel coordinates plotting.  
plot_params Stores pandas plotting options.  
radviz(frame, class_column[, ax, color, ...]) Plot a multidimensional dataset in 2D.  
register_matplotlib_converters() Register pandas formatters and converters with matplotlib.  
scatter_matrix(frame[, alpha, figsize, ax, ...]) Draw a matrix of scatter plots.  
table(ax, data[, rowLabels, colLabels]) Helper function to convert DataFrame and Series to matplotlib.table.
matplotlib.axes.Axes.plot   Axes.plot(*args, scalex=True, scaley=True, data=None, **kwargs)[source]
 
Plot y versus x as lines and/or markers. Call signatures: plot([x], y, [fmt], *, data=None, **kwargs)
plot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)
 The coordinates of the points or line nodes are given by x, y. The optional parameter fmt is a convenient way for defining basic formatting like color, marker and linestyle. It's a shortcut string notation described in the Notes section below. >>> plot(x, y)        # plot x and y using default line style and color
>>> plot(x, y, 'bo')  # plot x and y using blue circle markers
>>> plot(y)           # plot y using x as index array 0..N-1
>>> plot(y, 'r+')     # ditto, but with red plusses
 You can use Line2D properties as keyword arguments for more control on the appearance. Line properties and fmt can be mixed. The following two calls yield identical results: >>> plot(x, y, 'go--', linewidth=2, markersize=12)
>>> plot(x, y, color='green', marker='o', linestyle='dashed',
...      linewidth=2, markersize=12)
 When conflicting with fmt, keyword arguments take precedence. Plotting labelled data There's a convenient way for plotting objects with labelled data (i.e. data that can be accessed by index obj['y']). Instead of giving the data in x and y, you can provide the object in the data parameter and just give the labels for x and y: >>> plot('xlabel', 'ylabel', data=obj)
 All indexable objects are supported. This could e.g. be a dict, a pandas.DataFrame or a structured numpy array. Plotting multiple sets of data There are various ways to plot multiple sets of data.  
The most straight forward way is just to call plot multiple times. Example: >>> plot(x1, y1, 'bo')
>>> plot(x2, y2, 'go')
  
If x and/or y are 2D arrays a separate data set will be drawn for every column. If both x and y are 2D, they must have the same shape. If only one of them is 2D with shape (N, m) the other must have length N and will be used for every data set m. Example: >>> x = [1, 2, 3]
>>> y = np.array([[1, 2], [3, 4], [5, 6]])
>>> plot(x, y)
 is equivalent to: >>> for col in range(y.shape[1]):
...     plot(x, y[:, col])
  
The third way is to specify multiple sets of [x], y, [fmt] groups: >>> plot(x1, y1, 'g^', x2, y2, 'g-')
 In this case, any additional keyword argument applies to all datasets. Also this syntax cannot be combined with the data parameter.   By default, each line is assigned a different style specified by a 'style cycle'. The fmt and line property parameters are only necessary if you want explicit deviations from these defaults. Alternatively, you can also change the style cycle using rcParams["axes.prop_cycle"] (default: cycler('color', ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])).  Parameters 
 
x, yarray-like or scalar


The horizontal / vertical coordinates of the data points. x values are optional and default to range(len(y)). Commonly, these parameters are 1D arrays. They can also be scalars, or two-dimensional (in that case, the columns represent separate data sets). These arguments cannot be passed as keywords.  
fmtstr, optional


A format string, e.g. 'ro' for red circles. See the Notes section for a full description of the format strings. Format strings are just an abbreviation for quickly setting basic line properties. All of these and more can also be controlled by keyword arguments. This argument cannot be passed as keyword.  
dataindexable object, optional


An object with labelled data. If given, provide the label names to plot in x and y.  Note Technically there's a slight ambiguity in calls where the second label is a valid fmt. plot('n', 'o', data=obj) could be plt(x, y) or plt(y, fmt). In such cases, the former interpretation is chosen, but a warning is issued. You may suppress the warning by adding an empty format string plot('n', 'o', '', data=obj).     Returns 
 list of Line2D


A list of lines representing the plotted data.    Other Parameters 
 
scalex, scaleybool, default: True


These parameters determine if the view limits are adapted to the data limits. The values are passed on to autoscale_view.  
**kwargsLine2D properties, optional


kwargs are used to specify properties like a line label (for auto legends), linewidth, antialiasing, marker face color. Example: >>> plot([1, 2, 3], [1, 2, 3], 'go-', label='line 1', linewidth=2)
>>> plot([1, 2, 3], [1, 4, 9], 'rs', label='line 2')
 If you specify multiple lines with one plot call, the kwargs apply to all those lines. In case the label object is iterable, each element is used as labels for each set of data. Here is a list of available Line2D properties:   
Property Description   
agg_filter a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array  
alpha scalar or None  
animated bool  
antialiased or aa bool  
clip_box Bbox  
clip_on bool  
clip_path Patch or (Path, Transform) or None  
color or c color  
dash_capstyle CapStyle or {'butt', 'projecting', 'round'}  
dash_joinstyle JoinStyle or {'miter', 'round', 'bevel'}  
dashes sequence of floats (on/off ink in points) or (None, None)  
data (2, N) array or two 1D arrays  
drawstyle or ds {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'  
figure Figure  
fillstyle {'full', 'left', 'right', 'bottom', 'top', 'none'}  
gid str  
in_layout bool  
label object  
linestyle or ls {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}  
linewidth or lw float  
marker marker style string, Path or MarkerStyle  
markeredgecolor or mec color  
markeredgewidth or mew float  
markerfacecolor or mfc color  
markerfacecoloralt or mfcalt color  
markersize or ms float  
markevery None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]  
path_effects AbstractPathEffect  
picker float or callable[[Artist, Event], tuple[bool, dict]]  
pickradius float  
rasterized bool  
sketch_params (scale: float, length: float, randomness: float)  
snap bool or None  
solid_capstyle CapStyle or {'butt', 'projecting', 'round'}  
solid_joinstyle JoinStyle or {'miter', 'round', 'bevel'}  
transform unknown  
url str  
visible bool  
xdata 1D array  
ydata 1D array  
zorder float        See also  scatter

XY scatter plot with markers of varying size and/or color ( sometimes also called bubble chart).    Notes Format Strings A format string consists of a part for color, marker and line: fmt = '[marker][line][color]'
 Each of them is optional. If not provided, the value from the style cycle is used. Exception: If line is given, but no marker, the data will be a line without markers. Other combinations such as [color][marker][line] are also supported, but note that their parsing may be ambiguous. Markers   
character description   
'.' point marker  
',' pixel marker  
'o' circle marker  
'v' triangle_down marker  
'^' triangle_up marker  
'<' triangle_left marker  
'>' triangle_right marker  
'1' tri_down marker  
'2' tri_up marker  
'3' tri_left marker  
'4' tri_right marker  
'8' octagon marker  
's' square marker  
'p' pentagon marker  
'P' plus (filled) marker  
'*' star marker  
'h' hexagon1 marker  
'H' hexagon2 marker  
'+' plus marker  
'x' x marker  
'X' x (filled) marker  
'D' diamond marker  
'd' thin_diamond marker  
'|' vline marker  
'_' hline marker   Line Styles   
character description   
'-' solid line style  
'--' dashed line style  
'-.' dash-dot line style  
':' dotted line style   Example format strings: 'b'    # blue markers with default shape
'or'   # red circles
'-g'   # green solid line
'--'   # dashed line with default color
'^k:'  # black triangle_up markers connected by a dotted line
 Colors The supported color abbreviations are the single letter codes   
character color   
'b' blue  
'g' green  
'r' red  
'c' cyan  
'm' magenta  
'y' yellow  
'k' black  
'w' white   and the 'CN' colors that index into the default property cycle. If the color is the only part of the format string, you can additionally use any matplotlib.colors spec, e.g. full names ('green') or hex strings ('#008000'). 
  Examples using matplotlib.axes.Axes.plot
 
   Plotting categorical variables   

   CSD Demo   

   Curve with error band   

   EventCollection Demo   

   Fill Between and Alpha   

   Filling the area between lines   

   Fill Betweenx Demo   

   Customizing dashed line styles   

   Lines with a ticked patheffect   

   Marker reference   

   Markevery Demo   

   prop_cycle property markevery in rcParams   

   Psd Demo   

   Simple Plot   

   Using span_where   

   Creating a timeline with lines, dates, and text   

   hlines and vlines   

   Contour Corner Mask   

   Contour plot of irregularly spaced data   

   pcolormesh grids and shading   

   Streamplot   

   Spectrogram Demo   

   Watermark image   

   Aligning Labels   

   Axes box aspect   

   Axes Demo   

   Controlling view limits using margins and sticky_edges   

   Axes Props   

   axhspan Demo   

   Broken Axis   

   Resizing axes with constrained layout   

   Resizing axes with tight layout   

   Figure labels: suptitle, supxlabel, supylabel   

   Invert Axes   

   Secondary Axis   

   Sharing axis limits and views   

   Figure subfigures   

   Multiple subplots   

   Creating multiple subplots using plt.subplots   

   Plots with different scales   

   Boxplots   

   Using histograms to plot a cumulative distribution   

   Some features of the histogram (hist) function   

   Polar plot   

   Polar Legend   

   Using accented text in matplotlib   

   Scale invariant angle label   

   Annotating Plots   

   Composing Custom Legends   

   Date tick labels   

   Custom tick formatter for time series   

   AnnotationBbox demo   

   Labeling ticks using engineering notation   

   Annotation arrow style reference   

   Legend using pre-defined labels   

   Legend Demo   

   Mathtext   

   Math fontfamily   

   Multiline   

   Rendering math equations using TeX   

   Text Rotation Relative To Line   

   Title positioning   

   Text watermark   

   Annotate Transform   

   Annotating a plot   

   Annotation Polar   

   Programmatically controlling subplot adjustment   

   Dollar Ticks   

   Simple axes labels   

   Text Commands   

   Color Demo   

   Color by y-value   

   PathPatch object   

   Bezier Curve   

   Dark background style sheet   

   FiveThirtyEight style sheet   

   ggplot style sheet   

   Axes with a fixed physical size   

   Parasite Simple   

   Simple Axisline4   

   Axis line styles   

   Parasite Axes demo   

   Parasite axis demo   

   Custom spines with axisartist   

   Simple Axisline   

   Anatomy of a figure   

   Bachelor's degrees by gender   

   Integral as the area under a curve   

   XKCD   

   Decay   

   The Bayes update   

   The double pendulum problem   

   Animated 3D random walk   

   Animated line plot   

   MATPLOTLIB UNCHAINED   

   Mouse move and click events   

   Data Browser   

   Keypress event   

   Legend Picking   

   Looking Glass   

   Path Editor   

   Pick Event Demo2   

   Resampling Data   

   Timers   

   Frontpage histogram example   

   Frontpage plot example   

   Changing colors of lines intersecting a box   

   Cross hair cursor   

   Custom projection   

   Patheffect Demo   

   Pythonic Matplotlib   

   SVG Filter Line   

   TickedStroke patheffect   

   Zorder Demo   

   Plot 2D data on 3D plot   

   3D box surface plot   

   Parametric Curve   

   Lorenz Attractor   

   2D and 3D Axes in same Figure   

   Loglog Aspect   

   Scales   

   Symlog Demo   

   Anscombe's quartet   

   Radar chart (aka spider or star chart)   

   Centered spines with arrows   

   Multiple Yaxis With Spines   

   Spine Placement   

   Spines   

   Custom spine bounds   

   Centering labels between ticks   

   Formatting date ticks using ConciseDateFormatter   

   Date Demo Convert   

   Date Index Formatter   

   Date Precision and Epochs   

   Major and minor ticks   

   The default tick formatter   

   Set default y-axis tick labels on the right   

   Setting tick labels from a list of values   

   Set default x-axis tick labels on the top   

   Evans test   

   CanvasAgg demo   

   Annotate Explain   

   Connect Simple01   

   Connection styles for annotations   

   Nested GridSpecs   

   Pgf Fonts   

   Pgf Texsystem   

   Simple Annotate01   

   Simple Legend01   

   Simple Legend02   

   Annotated Cursor   

   Check Buttons   

   Cursor   

   Multicursor   

   Radio Buttons   

   Rectangle and ellipse selectors   

   Span Selector   

   Textbox   

   Basic Usage   

   Artist tutorial   

   Legend guide   

   Styling with cycler   

   Constrained Layout Guide   

   Tight Layout guide   

   Arranging multiple Axes in a Figure   

   Autoscaling   

   Faster rendering by using blitting   

   Path Tutorial   

   Transformations Tutorial   

   Specifying Colors   

   Text in Matplotlib Plots   

   plot(x, y)   

   fill_between(x, y1, y2)   

   tricontour(x, y, z)   

   tricontourf(x, y, z)   

   tripcolor(x, y, z)
pandas.plotting.bootstrap_plot   pandas.plotting.bootstrap_plot(series, fig=None, size=50, samples=500, **kwds)[source]
 
Bootstrap plot on mean, median and mid-range statistics. The bootstrap plot is used to estimate the uncertainty of a statistic by relaying on random sampling with replacement [1]. This function will generate bootstrapping plots for mean, median and mid-range statistics for the given number of samples of the given size.  1 
“Bootstrapping (statistics)” in https://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29    Parameters 
 
series:pandas.Series


Series from where to get the samplings for the bootstrapping.  
fig:matplotlib.figure.Figure, default None


If given, it will use the fig reference for plotting instead of creating a new one with default parameters.  
size:int, default 50


Number of data points to consider during each sampling. It must be less than or equal to the length of the series.  
samples:int, default 500


Number of times the bootstrap procedure is performed.  **kwds

Options to pass to matplotlib plotting method.    Returns 
 matplotlib.figure.Figure

Matplotlib figure.      See also  DataFrame.plot

Basic plotting for DataFrame objects.  Series.plot

Basic plotting for Series objects.    Examples This example draws a basic bootstrap plot for a Series. 
>>> s = pd.Series(np.random.uniform(size=100))
>>> pd.plotting.bootstrap_plot(s)
<Figure size 640x480 with 6 Axes>
def f_20865487(df):
    """plot dataframe `df` without a legend
    """
    return  
 --------------------

def f_13368659():
    """loop through the IP address range "192.168.x.x"
    """
    return  
 --------------------

def f_4065737(x):
    """Sum the corresponding decimal values for binary values of each boolean element in list `x`
    """
    return  
 --------------------

def f_8691311(line1, line2, line3, target):
    """write multiple strings `line1`, `line2` and `line3` in one line in a file `target`
    """
     
 --------------------

def f_10632111(data):
    """Convert list of lists `data` into a flat list
    """
    return  
 --------------------

def f_15392730():
    """Print new line character as `\n` in a string `foo\nbar`
    """
    return  
 --------------------

def f_1010961(s):
    """remove last comma character ',' in string `s`
    """
    return  
 --------------------

def f_23855976(x):
    """calculate the mean of each element in array `x` with the element previous to it
    """
    return  
 --------------------

def f_23855976(x):
    """get an array of the mean of each two consecutive values in numpy array `x`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
POP3.utf8()  
Try to switch to UTF-8 mode. Returns the server response if successful, raises error_proto if not. Specified in RFC 6856.  New in version 3.5.
numpy.newaxis
 
A convenient alias for None, useful for indexing arrays. Examples >>> newaxis is None
True
>>> x = np.arange(3)
>>> x
array([0, 1, 2])
>>> x[:, newaxis]
array([[0],
[1],
[2]])
>>> x[:, newaxis, newaxis]
array([[[0]],
[[1]],
[[2]]])
>>> x[:, newaxis] * x
array([[0, 0, 0],
[0, 1, 2],
[0, 2, 4]])
 Outer product, same as outer(x, y): >>> y = np.arange(3, 6)
>>> x[:, newaxis] * y
array([[ 0,  0,  0],
[ 3,  4,  5],
[ 6,  8, 10]])
 x[newaxis, :] is equivalent to x[newaxis] and x[None]: >>> x[newaxis, :].shape
(1, 3)
>>> x[newaxis].shape
(1, 3)
>>> x[None].shape
(1, 3)
>>> x[:, newaxis].shape
(3, 1)
numpy.fromregex   numpy.fromregex(file, regexp, dtype, encoding=None)[source]
 
Construct an array from a text file, using regular expression parsing. The returned array is always a structured array, and is constructed from all matches of the regular expression in the file. Groups in the regular expression are converted to fields of the structured array.  Parameters 
 
filepath or file


Filename or file object to read.  Changed in version 1.22.0: Now accepts os.PathLike implementations.   
regexpstr or regexp


Regular expression used to parse the file. Groups in the regular expression correspond to fields in the dtype.  
dtypedtype or list of dtypes


Dtype for the structured array; must be a structured datatype.  
encodingstr, optional


Encoding used to decode the inputfile. Does not apply to input streams.  New in version 1.14.0.     Returns 
 
outputndarray


The output array, containing the part of the content of file that was matched by regexp. output is always a structured array.    Raises 
 TypeError

When dtype is not a valid dtype for a structured array.      See also  
fromstring, loadtxt

  Notes Dtypes for structured arrays can be specified in several forms, but all forms specify at least the data type and field name. For details see basics.rec. Examples >>> from io import StringIO
>>> text = StringIO("1312 foo\n1534  bar\n444   qux")
 >>> regexp = r"(\d+)\s+(...)"  # match [digits, whitespace, anything]
>>> output = np.fromregex(text, regexp,
...                       [('num', np.int64), ('key', 'S3')])
>>> output
array([(1312, b'foo'), (1534, b'bar'), ( 444, b'qux')],
      dtype=[('num', '<i8'), ('key', 'S3')])
>>> output['num']
array([1312, 1534,  444])
new  
True if the session is new, False otherwise.
numpy.loadtxt   numpy.loadtxt(fname, dtype=<class 'float'>, comments='#', delimiter=None, converters=None, skiprows=0, usecols=None, unpack=False, ndmin=0, encoding='bytes', max_rows=None, *, like=None)[source]
 
Load data from a text file. Each row in the text file must have the same number of values.  Parameters 
 
fnamefile, str, pathlib.Path, list of str, generator


File, filename, list, or generator to read. If the filename extension is .gz or .bz2, the file is first decompressed. Note that generators must return bytes or strings. The strings in a list or produced by a generator are treated as lines.  
dtypedata-type, optional


Data-type of the resulting array; default: float. If this is a structured data-type, the resulting array will be 1-dimensional, and each row will be interpreted as an element of the array. In this case, the number of columns used must match the number of fields in the data-type.  
commentsstr or sequence of str, optional


The characters or list of characters used to indicate the start of a comment. None implies no comments. For backwards compatibility, byte strings will be decoded as ‘latin1’. The default is ‘#’.  
delimiterstr, optional


The string used to separate values. For backwards compatibility, byte strings will be decoded as ‘latin1’. The default is whitespace.  
convertersdict, optional


A dictionary mapping column number to a function that will parse the column string into the desired value. E.g., if column 0 is a date string: converters = {0: datestr2num}. Converters can also be used to provide a default value for missing data (but see also genfromtxt): converters = {3: lambda s: float(s.strip() or 0)}. Default: None.  
skiprowsint, optional


Skip the first skiprows lines, including comments; default: 0.  
usecolsint or sequence, optional


Which columns to read, with 0 being the first. For example, usecols = (1,4,5) will extract the 2nd, 5th and 6th columns. The default, None, results in all columns being read.  Changed in version 1.11.0: When a single column has to be read it is possible to use an integer instead of a tuple. E.g usecols = 3 reads the fourth column the same way as usecols = (3,) would.   
unpackbool, optional


If True, the returned array is transposed, so that arguments may be unpacked using x, y, z = loadtxt(...). When used with a structured data-type, arrays are returned for each field. Default is False.  
ndminint, optional


The returned array will have at least ndmin dimensions. Otherwise mono-dimensional axes will be squeezed. Legal values: 0 (default), 1 or 2.  New in version 1.6.0.   
encodingstr, optional


Encoding used to decode the inputfile. Does not apply to input streams. The special value ‘bytes’ enables backward compatibility workarounds that ensures you receive byte arrays as results if possible and passes ‘latin1’ encoded strings to converters. Override this value to receive unicode arrays and pass strings as input to converters. If set to None the system default is used. The default value is ‘bytes’.  New in version 1.14.0.   
max_rowsint, optional


Read max_rows lines of content after skiprows lines. The default is to read all the lines.  New in version 1.16.0.   
likearray_like


Reference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as like supports the __array_function__ protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.  New in version 1.20.0.     Returns 
 
outndarray


Data read from the text file.      See also  
load, fromstring, fromregex

genfromtxt

Load data with missing values handled as specified.  scipy.io.loadmat

reads MATLAB data files    Notes This function aims to be a fast reader for simply formatted files. The genfromtxt function provides more sophisticated handling of, e.g., lines with missing values.  New in version 1.10.0.  The strings produced by the Python float.hex method can be used as input for floats. Examples >>> from io import StringIO   # StringIO behaves like a file object
>>> c = StringIO("0 1\n2 3")
>>> np.loadtxt(c)
array([[0., 1.],
       [2., 3.]])
 >>> d = StringIO("M 21 72\nF 35 58")
>>> np.loadtxt(d, dtype={'names': ('gender', 'age', 'weight'),
...                      'formats': ('S1', 'i4', 'f4')})
array([(b'M', 21, 72.), (b'F', 35, 58.)],
      dtype=[('gender', 'S1'), ('age', '<i4'), ('weight', '<f4')])
 >>> c = StringIO("1,0,2\n3,0,4")
>>> x, y = np.loadtxt(c, delimiter=',', usecols=(0, 2), unpack=True)
>>> x
array([1., 3.])
>>> y
array([2., 4.])
 This example shows how converters can be used to convert a field with a trailing minus sign into a negative number. >>> s = StringIO('10.01 31.25-\n19.22 64.31\n17.57- 63.94')
>>> def conv(fld):
...     return -float(fld[:-1]) if fld.endswith(b'-') else float(fld)
...
>>> np.loadtxt(s, converters={0: conv, 1: conv})
array([[ 10.01, -31.25],
       [ 19.22,  64.31],
       [-17.57,  63.94]])
def f_6375343():
    """load data containing `utf-8` from file `new.txt` into numpy array `arr`
    """
     
 --------------------
Please refer to the following documentation to generate the code:
datetimes(field_name, kind, order='ASC', tzinfo=None, is_dst=None)
reverse_order()  
This method for the Stats class reverses the ordering of the basic list within the object. Note that by default ascending vs descending order is properly selected based on the sort key of choice.
dates(field, kind, order='ASC')
pandas.DatetimeIndex.indexer_at_time   DatetimeIndex.indexer_at_time(time, asof=False)[source]
 
Return index locations of values at particular time of day (e.g. 9:30AM).  Parameters 
 
time:datetime.time or str


Time passed in either as object (datetime.time) or as string in appropriate format (“%H:%M”, “%H%M”, “%I:%M%p”, “%I%M%p”, “%H:%M:%S”, “%H%M%S”, “%I:%M:%S%p”, “%I%M%S%p”).    Returns 
 np.ndarray[np.intp]
    See also  indexer_between_time

Get index locations of values between particular times of day.  DataFrame.at_time

Select values at particular time of day.
pandas.Series.at_time   Series.at_time(time, asof=False, axis=None)[source]
 
Select values at particular time of day (e.g., 9:30AM).  Parameters 
 
time:datetime.time or str


axis:{0 or ‘index’, 1 or ‘columns’}, default 0

  Returns 
 Series or DataFrame
  Raises 
 TypeError

If the index is not a DatetimeIndex      See also  between_time

Select values between particular times of the day.  first

Select initial periods of time series based on a date offset.  last

Select final periods of time series based on a date offset.  DatetimeIndex.indexer_at_time

Get just the index locations for values at particular time of the day.    Examples 
>>> i = pd.date_range('2018-04-09', periods=4, freq='12H')
>>> ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i)
>>> ts
                     A
2018-04-09 00:00:00  1
2018-04-09 12:00:00  2
2018-04-10 00:00:00  3
2018-04-10 12:00:00  4
  
>>> ts.at_time('12:00')
                     A
2018-04-09 12:00:00  2
2018-04-10 12:00:00  4
def f_1547733(l):
    """reverse sort list of dicts `l` by value for key `time`
    """
     
 --------------------

def f_1547733(l):
    """Sort a list of dictionary `l` based on key `time` in descending order
    """
     
 --------------------
Please refer to the following documentation to generate the code:
pandas.DataFrame.filter   DataFrame.filter(items=None, like=None, regex=None, axis=None)[source]
 
Subset the dataframe rows or columns according to the specified index labels. Note that this routine does not filter a dataframe on its contents. The filter is applied to the labels of the index.  Parameters 
 
items:list-like


Keep labels from axis which are in items.  
like:str


Keep labels from axis for which “like in label == True”.  
regex:str (regular expression)


Keep labels from axis for which re.search(regex, label) == True.  
axis:{0 or ‘index’, 1 or ‘columns’, None}, default None


The axis to filter on, expressed either as an index (int) or axis name (str). By default this is the info axis, ‘index’ for Series, ‘columns’ for DataFrame.    Returns 
 same type as input object
    See also  DataFrame.loc

Access a group of rows and columns by label(s) or a boolean array.    Notes The items, like, and regex parameters are enforced to be mutually exclusive. axis defaults to the info axis that is used when indexing with []. Examples 
>>> df = pd.DataFrame(np.array(([1, 2, 3], [4, 5, 6])),
...                   index=['mouse', 'rabbit'],
...                   columns=['one', 'two', 'three'])
>>> df
        one  two  three
mouse     1    2      3
rabbit    4    5      6
  
>>> # select columns by name
>>> df.filter(items=['one', 'three'])
         one  three
mouse     1      3
rabbit    4      6
  
>>> # select columns by regular expression
>>> df.filter(regex='e$', axis=1)
         one  three
mouse     1      3
rabbit    4      6
  
>>> # select rows containing 'bbi'
>>> df.filter(like='bbi', axis=0)
         one  two  three
rabbit    4    5      6
pandas.Series.filter   Series.filter(items=None, like=None, regex=None, axis=None)[source]
 
Subset the dataframe rows or columns according to the specified index labels. Note that this routine does not filter a dataframe on its contents. The filter is applied to the labels of the index.  Parameters 
 
items:list-like


Keep labels from axis which are in items.  
like:str


Keep labels from axis for which “like in label == True”.  
regex:str (regular expression)


Keep labels from axis for which re.search(regex, label) == True.  
axis:{0 or ‘index’, 1 or ‘columns’, None}, default None


The axis to filter on, expressed either as an index (int) or axis name (str). By default this is the info axis, ‘index’ for Series, ‘columns’ for DataFrame.    Returns 
 same type as input object
    See also  DataFrame.loc

Access a group of rows and columns by label(s) or a boolean array.    Notes The items, like, and regex parameters are enforced to be mutually exclusive. axis defaults to the info axis that is used when indexing with []. Examples 
>>> df = pd.DataFrame(np.array(([1, 2, 3], [4, 5, 6])),
...                   index=['mouse', 'rabbit'],
...                   columns=['one', 'two', 'three'])
>>> df
        one  two  three
mouse     1    2      3
rabbit    4    5      6
  
>>> # select columns by name
>>> df.filter(items=['one', 'three'])
         one  three
mouse     1      3
rabbit    4      6
  
>>> # select columns by regular expression
>>> df.filter(regex='e$', axis=1)
         one  three
mouse     1      3
rabbit    4      6
  
>>> # select rows containing 'bbi'
>>> df.filter(like='bbi', axis=0)
         one  two  three
rabbit    4    5      6
pandas.Series.str.extractall   Series.str.extractall(pat, flags=0)[source]
 
Extract capture groups in the regex pat as columns in DataFrame. For each subject string in the Series, extract groups from all matches of regular expression pat. When each subject string in the Series has exactly one match, extractall(pat).xs(0, level=’match’) is the same as extract(pat).  Parameters 
 
pat:str


Regular expression pattern with capturing groups.  
flags:int, default 0 (no flags)


A re module flag, for example re.IGNORECASE. These allow to modify regular expression matching for things like case, spaces, etc. Multiple flags can be combined with the bitwise OR operator, for example re.IGNORECASE | re.MULTILINE.    Returns 
 DataFrame

A DataFrame with one row for each match, and one column for each group. Its rows have a MultiIndex with first levels that come from the subject Series. The last level is named ‘match’ and indexes the matches in each item of the Series. Any capture group names in regular expression pat will be used for column names; otherwise capture group numbers will be used.      See also  extract

Returns first match only (not all matches).    Examples A pattern with one group will return a DataFrame with one column. Indices with no matches will not appear in the result. 
>>> s = pd.Series(["a1a2", "b1", "c1"], index=["A", "B", "C"])
>>> s.str.extractall(r"[ab](\d)")
        0
match
A 0      1
  1      2
B 0      1
  Capture group names are used for column names of the result. 
>>> s.str.extractall(r"[ab](?P<digit>\d)")
        digit
match
A 0         1
  1         2
B 0         1
  A pattern with two groups will return a DataFrame with two columns. 
>>> s.str.extractall(r"(?P<letter>[ab])(?P<digit>\d)")
        letter digit
match
A 0          a     1
  1          a     2
B 0          b     1
  Optional groups that do not match are NaN in the result. 
>>> s.str.extractall(r"(?P<letter>[ab])?(?P<digit>\d)")
        letter digit
match
A 0          a     1
  1          a     2
B 0          b     1
C 0        NaN     1
pandas.Series.str.extract   Series.str.extract(pat, flags=0, expand=True)[source]
 
Extract capture groups in the regex pat as columns in a DataFrame. For each subject string in the Series, extract groups from the first match of regular expression pat.  Parameters 
 
pat:str


Regular expression pattern with capturing groups.  
flags:int, default 0 (no flags)


Flags from the re module, e.g. re.IGNORECASE, that modify regular expression matching for things like case, spaces, etc. For more details, see re.  
expand:bool, default True


If True, return DataFrame with one column per capture group. If False, return a Series/Index if there is one capture group or DataFrame if there are multiple capture groups.    Returns 
 DataFrame or Series or Index

A DataFrame with one row for each subject string, and one column for each group. Any capture group names in regular expression pat will be used for column names; otherwise capture group numbers will be used. The dtype of each result column is always object, even when no match is found. If expand=False and pat has only one capture group, then return a Series (if subject is a Series) or Index (if subject is an Index).      See also  extractall

Returns all matches (not just the first match).    Examples A pattern with two groups will return a DataFrame with two columns. Non-matches will be NaN. 
>>> s = pd.Series(['a1', 'b2', 'c3'])
>>> s.str.extract(r'([ab])(\d)')
    0    1
0    a    1
1    b    2
2  NaN  NaN
  A pattern may contain optional groups. 
>>> s.str.extract(r'([ab])?(\d)')
    0  1
0    a  1
1    b  2
2  NaN  3
  Named groups will become column names in the result. 
>>> s.str.extract(r'(?P<letter>[ab])(?P<digit>\d)')
letter digit
0      a     1
1      b     2
2    NaN   NaN
  A pattern with one group will return a DataFrame with one column if expand=True. 
>>> s.str.extract(r'[ab](\d)', expand=True)
    0
0    1
1    2
2  NaN
  A pattern with one group will return a Series if expand=False. 
>>> s.str.extract(r'[ab](\d)', expand=False)
0      1
1      2
2    NaN
dtype: object
Cookbook This is a repository for short and sweet examples and links for useful pandas recipes. We encourage users to add to this documentation. Adding interesting links and/or inline examples to this section is a great First Pull Request. Simplified, condensed, new-user friendly, in-line examples have been inserted where possible to augment the Stack-Overflow and GitHub links. Many of the links contain expanded information, above what the in-line examples offer. pandas (pd) and NumPy (np) are the only two abbreviated imported modules. The rest are kept explicitly imported for newer users.  Idioms These are some neat pandas idioms if-then/if-then-else on one column, and assignment to another one or more columns: 
In [1]: df = pd.DataFrame(
   ...:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ...: )
   ...: 

In [2]: df
Out[2]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50
   if-then… An if-then on one column 
In [3]: df.loc[df.AAA >= 5, "BBB"] = -1

In [4]: df
Out[4]: 
   AAA  BBB  CCC
0    4   10  100
1    5   -1   50
2    6   -1  -30
3    7   -1  -50
  An if-then with assignment to 2 columns: 
In [5]: df.loc[df.AAA >= 5, ["BBB", "CCC"]] = 555

In [6]: df
Out[6]: 
   AAA  BBB  CCC
0    4   10  100
1    5  555  555
2    6  555  555
3    7  555  555
  Add another line with different logic, to do the -else 
In [7]: df.loc[df.AAA < 5, ["BBB", "CCC"]] = 2000

In [8]: df
Out[8]: 
   AAA   BBB   CCC
0    4  2000  2000
1    5   555   555
2    6   555   555
3    7   555   555
  Or use pandas where after you’ve set up a mask 
In [9]: df_mask = pd.DataFrame(
   ...:     {"AAA": [True] * 4, "BBB": [False] * 4, "CCC": [True, False] * 2}
   ...: )
   ...: 

In [10]: df.where(df_mask, -1000)
Out[10]: 
   AAA   BBB   CCC
0    4 -1000  2000
1    5 -1000 -1000
2    6 -1000   555
3    7 -1000 -1000
  if-then-else using NumPy’s where() 
In [11]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ....: )
   ....: 

In [12]: df
Out[12]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50

In [13]: df["logic"] = np.where(df["AAA"] > 5, "high", "low")

In [14]: df
Out[14]: 
   AAA  BBB  CCC logic
0    4   10  100   low
1    5   20   50   low
2    6   30  -30  high
3    7   40  -50  high
    Splitting Split a frame with a boolean criterion 
In [15]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ....: )
   ....: 

In [16]: df
Out[16]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50

In [17]: df[df.AAA <= 5]
Out[17]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50

In [18]: df[df.AAA > 5]
Out[18]: 
   AAA  BBB  CCC
2    6   30  -30
3    7   40  -50
    Building criteria Select with multi-column criteria 
In [19]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ....: )
   ....: 

In [20]: df
Out[20]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50
  …and (without assignment returns a Series) 
In [21]: df.loc[(df["BBB"] < 25) & (df["CCC"] >= -40), "AAA"]
Out[21]: 
0    4
1    5
Name: AAA, dtype: int64
  …or (without assignment returns a Series) 
In [22]: df.loc[(df["BBB"] > 25) | (df["CCC"] >= -40), "AAA"]
Out[22]: 
0    4
1    5
2    6
3    7
Name: AAA, dtype: int64
  …or (with assignment modifies the DataFrame.) 
In [23]: df.loc[(df["BBB"] > 25) | (df["CCC"] >= 75), "AAA"] = 0.1

In [24]: df
Out[24]: 
   AAA  BBB  CCC
0  0.1   10  100
1  5.0   20   50
2  0.1   30  -30
3  0.1   40  -50
  Select rows with data closest to certain value using argsort 
In [25]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ....: )
   ....: 

In [26]: df
Out[26]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50

In [27]: aValue = 43.0

In [28]: df.loc[(df.CCC - aValue).abs().argsort()]
Out[28]: 
   AAA  BBB  CCC
1    5   20   50
0    4   10  100
2    6   30  -30
3    7   40  -50
  Dynamically reduce a list of criteria using a binary operators 
In [29]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ....: )
   ....: 

In [30]: df
Out[30]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50

In [31]: Crit1 = df.AAA <= 5.5

In [32]: Crit2 = df.BBB == 10.0

In [33]: Crit3 = df.CCC > -40.0
  One could hard code: 
In [34]: AllCrit = Crit1 & Crit2 & Crit3
  …Or it can be done with a list of dynamically built criteria 
In [35]: import functools

In [36]: CritList = [Crit1, Crit2, Crit3]

In [37]: AllCrit = functools.reduce(lambda x, y: x & y, CritList)

In [38]: df[AllCrit]
Out[38]: 
   AAA  BBB  CCC
0    4   10  100
     Selection  Dataframes The indexing docs. Using both row labels and value conditionals 
In [39]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ....: )
   ....: 

In [40]: df
Out[40]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50

In [41]: df[(df.AAA <= 6) & (df.index.isin([0, 2, 4]))]
Out[41]: 
   AAA  BBB  CCC
0    4   10  100
2    6   30  -30
  Use loc for label-oriented slicing and iloc positional slicing GH2904 
In [42]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]},
   ....:     index=["foo", "bar", "boo", "kar"],
   ....: )
   ....: 
  There are 2 explicit slicing methods, with a third general case  Positional-oriented (Python slicing style : exclusive of end) Label-oriented (Non-Python slicing style : inclusive of end) General (Either slicing style : depends on if the slice contains labels or positions)  
In [43]: df.loc["bar":"kar"]  # Label
Out[43]: 
     AAA  BBB  CCC
bar    5   20   50
boo    6   30  -30
kar    7   40  -50

# Generic
In [44]: df[0:3]
Out[44]: 
     AAA  BBB  CCC
foo    4   10  100
bar    5   20   50
boo    6   30  -30

In [45]: df["bar":"kar"]
Out[45]: 
     AAA  BBB  CCC
bar    5   20   50
boo    6   30  -30
kar    7   40  -50
  Ambiguity arises when an index consists of integers with a non-zero start or non-unit increment. 
In [46]: data = {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}

In [47]: df2 = pd.DataFrame(data=data, index=[1, 2, 3, 4])  # Note index starts at 1.

In [48]: df2.iloc[1:3]  # Position-oriented
Out[48]: 
   AAA  BBB  CCC
2    5   20   50
3    6   30  -30

In [49]: df2.loc[1:3]  # Label-oriented
Out[49]: 
   AAA  BBB  CCC
1    4   10  100
2    5   20   50
3    6   30  -30
  Using inverse operator (~) to take the complement of a mask 
In [50]: df = pd.DataFrame(
   ....:     {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   ....: )
   ....: 

In [51]: df
Out[51]: 
   AAA  BBB  CCC
0    4   10  100
1    5   20   50
2    6   30  -30
3    7   40  -50

In [52]: df[~((df.AAA <= 6) & (df.index.isin([0, 2, 4])))]
Out[52]: 
   AAA  BBB  CCC
1    5   20   50
3    7   40  -50
    New columns Efficiently and dynamically creating new columns using applymap 
In [53]: df = pd.DataFrame({"AAA": [1, 2, 1, 3], "BBB": [1, 1, 2, 2], "CCC": [2, 1, 3, 1]})

In [54]: df
Out[54]: 
   AAA  BBB  CCC
0    1    1    2
1    2    1    1
2    1    2    3
3    3    2    1

In [55]: source_cols = df.columns  # Or some subset would work too

In [56]: new_cols = [str(x) + "_cat" for x in source_cols]

In [57]: categories = {1: "Alpha", 2: "Beta", 3: "Charlie"}

In [58]: df[new_cols] = df[source_cols].applymap(categories.get)

In [59]: df
Out[59]: 
   AAA  BBB  CCC  AAA_cat BBB_cat  CCC_cat
0    1    1    2    Alpha   Alpha     Beta
1    2    1    1     Beta   Alpha    Alpha
2    1    2    3    Alpha    Beta  Charlie
3    3    2    1  Charlie    Beta    Alpha
  Keep other columns when using min() with groupby 
In [60]: df = pd.DataFrame(
   ....:     {"AAA": [1, 1, 1, 2, 2, 2, 3, 3], "BBB": [2, 1, 3, 4, 5, 1, 2, 3]}
   ....: )
   ....: 

In [61]: df
Out[61]: 
   AAA  BBB
0    1    2
1    1    1
2    1    3
3    2    4
4    2    5
5    2    1
6    3    2
7    3    3
  Method 1 : idxmin() to get the index of the minimums 
In [62]: df.loc[df.groupby("AAA")["BBB"].idxmin()]
Out[62]: 
   AAA  BBB
1    1    1
5    2    1
6    3    2
  Method 2 : sort then take first of each 
In [63]: df.sort_values(by="BBB").groupby("AAA", as_index=False).first()
Out[63]: 
   AAA  BBB
0    1    1
1    2    1
2    3    2
  Notice the same results, with the exception of the index.    Multiindexing The multindexing docs. Creating a MultiIndex from a labeled frame 
In [64]: df = pd.DataFrame(
   ....:     {
   ....:         "row": [0, 1, 2],
   ....:         "One_X": [1.1, 1.1, 1.1],
   ....:         "One_Y": [1.2, 1.2, 1.2],
   ....:         "Two_X": [1.11, 1.11, 1.11],
   ....:         "Two_Y": [1.22, 1.22, 1.22],
   ....:     }
   ....: )
   ....: 

In [65]: df
Out[65]: 
   row  One_X  One_Y  Two_X  Two_Y
0    0    1.1    1.2   1.11   1.22
1    1    1.1    1.2   1.11   1.22
2    2    1.1    1.2   1.11   1.22

# As Labelled Index
In [66]: df = df.set_index("row")

In [67]: df
Out[67]: 
     One_X  One_Y  Two_X  Two_Y
row                            
0      1.1    1.2   1.11   1.22
1      1.1    1.2   1.11   1.22
2      1.1    1.2   1.11   1.22

# With Hierarchical Columns
In [68]: df.columns = pd.MultiIndex.from_tuples([tuple(c.split("_")) for c in df.columns])

In [69]: df
Out[69]: 
     One        Two      
       X    Y     X     Y
row                      
0    1.1  1.2  1.11  1.22
1    1.1  1.2  1.11  1.22
2    1.1  1.2  1.11  1.22

# Now stack & Reset
In [70]: df = df.stack(0).reset_index(1)

In [71]: df
Out[71]: 
    level_1     X     Y
row                    
0       One  1.10  1.20
0       Two  1.11  1.22
1       One  1.10  1.20
1       Two  1.11  1.22
2       One  1.10  1.20
2       Two  1.11  1.22

# And fix the labels (Notice the label 'level_1' got added automatically)
In [72]: df.columns = ["Sample", "All_X", "All_Y"]

In [73]: df
Out[73]: 
    Sample  All_X  All_Y
row                     
0      One   1.10   1.20
0      Two   1.11   1.22
1      One   1.10   1.20
1      Two   1.11   1.22
2      One   1.10   1.20
2      Two   1.11   1.22
   Arithmetic Performing arithmetic with a MultiIndex that needs broadcasting 
In [74]: cols = pd.MultiIndex.from_tuples(
   ....:     [(x, y) for x in ["A", "B", "C"] for y in ["O", "I"]]
   ....: )
   ....: 

In [75]: df = pd.DataFrame(np.random.randn(2, 6), index=["n", "m"], columns=cols)

In [76]: df
Out[76]: 
          A                   B                   C          
          O         I         O         I         O         I
n  0.469112 -0.282863 -1.509059 -1.135632  1.212112 -0.173215
m  0.119209 -1.044236 -0.861849 -2.104569 -0.494929  1.071804

In [77]: df = df.div(df["C"], level=1)

In [78]: df
Out[78]: 
          A                   B              C     
          O         I         O         I    O    I
n  0.387021  1.633022 -1.244983  6.556214  1.0  1.0
m -0.240860 -0.974279  1.741358 -1.963577  1.0  1.0
    Slicing Slicing a MultiIndex with xs 
In [79]: coords = [("AA", "one"), ("AA", "six"), ("BB", "one"), ("BB", "two"), ("BB", "six")]

In [80]: index = pd.MultiIndex.from_tuples(coords)

In [81]: df = pd.DataFrame([11, 22, 33, 44, 55], index, ["MyData"])

In [82]: df
Out[82]: 
        MyData
AA one      11
   six      22
BB one      33
   two      44
   six      55
  To take the cross section of the 1st level and 1st axis the index: 
# Note : level and axis are optional, and default to zero
In [83]: df.xs("BB", level=0, axis=0)
Out[83]: 
     MyData
one      33
two      44
six      55
  …and now the 2nd level of the 1st axis. 
In [84]: df.xs("six", level=1, axis=0)
Out[84]: 
    MyData
AA      22
BB      55
  Slicing a MultiIndex with xs, method #2 
In [85]: import itertools

In [86]: index = list(itertools.product(["Ada", "Quinn", "Violet"], ["Comp", "Math", "Sci"]))

In [87]: headr = list(itertools.product(["Exams", "Labs"], ["I", "II"]))

In [88]: indx = pd.MultiIndex.from_tuples(index, names=["Student", "Course"])

In [89]: cols = pd.MultiIndex.from_tuples(headr)  # Notice these are un-named

In [90]: data = [[70 + x + y + (x * y) % 3 for x in range(4)] for y in range(9)]

In [91]: df = pd.DataFrame(data, indx, cols)

In [92]: df
Out[92]: 
               Exams     Labs    
                   I  II    I  II
Student Course                   
Ada     Comp      70  71   72  73
        Math      71  73   75  74
        Sci       72  75   75  75
Quinn   Comp      73  74   75  76
        Math      74  76   78  77
        Sci       75  78   78  78
Violet  Comp      76  77   78  79
        Math      77  79   81  80
        Sci       78  81   81  81

In [93]: All = slice(None)

In [94]: df.loc["Violet"]
Out[94]: 
       Exams     Labs    
           I  II    I  II
Course                   
Comp      76  77   78  79
Math      77  79   81  80
Sci       78  81   81  81

In [95]: df.loc[(All, "Math"), All]
Out[95]: 
               Exams     Labs    
                   I  II    I  II
Student Course                   
Ada     Math      71  73   75  74
Quinn   Math      74  76   78  77
Violet  Math      77  79   81  80

In [96]: df.loc[(slice("Ada", "Quinn"), "Math"), All]
Out[96]: 
               Exams     Labs    
                   I  II    I  II
Student Course                   
Ada     Math      71  73   75  74
Quinn   Math      74  76   78  77

In [97]: df.loc[(All, "Math"), ("Exams")]
Out[97]: 
                 I  II
Student Course        
Ada     Math    71  73
Quinn   Math    74  76
Violet  Math    77  79

In [98]: df.loc[(All, "Math"), (All, "II")]
Out[98]: 
               Exams Labs
                  II   II
Student Course           
Ada     Math      73   74
Quinn   Math      76   77
Violet  Math      79   80
  Setting portions of a MultiIndex with xs   Sorting Sort by specific column or an ordered list of columns, with a MultiIndex 
In [99]: df.sort_values(by=("Labs", "II"), ascending=False)
Out[99]: 
               Exams     Labs    
                   I  II    I  II
Student Course                   
Violet  Sci       78  81   81  81
        Math      77  79   81  80
        Comp      76  77   78  79
Quinn   Sci       75  78   78  78
        Math      74  76   78  77
        Comp      73  74   75  76
Ada     Sci       72  75   75  75
        Math      71  73   75  74
        Comp      70  71   72  73
  Partial selection, the need for sortedness GH2995   Levels Prepending a level to a multiindex Flatten Hierarchical columns    Missing data The missing data docs. Fill forward a reversed timeseries 
In [100]: df = pd.DataFrame(
   .....:     np.random.randn(6, 1),
   .....:     index=pd.date_range("2013-08-01", periods=6, freq="B"),
   .....:     columns=list("A"),
   .....: )
   .....: 

In [101]: df.loc[df.index[3], "A"] = np.nan

In [102]: df
Out[102]: 
                   A
2013-08-01  0.721555
2013-08-02 -0.706771
2013-08-05 -1.039575
2013-08-06       NaN
2013-08-07 -0.424972
2013-08-08  0.567020

In [103]: df.reindex(df.index[::-1]).ffill()
Out[103]: 
                   A
2013-08-08  0.567020
2013-08-07 -0.424972
2013-08-06 -0.424972
2013-08-05 -1.039575
2013-08-02 -0.706771
2013-08-01  0.721555
  cumsum reset at NaN values  Replace Using replace with backrefs    Grouping The grouping docs. Basic grouping with apply Unlike agg, apply’s callable is passed a sub-DataFrame which gives you access to all the columns 
In [104]: df = pd.DataFrame(
   .....:     {
   .....:         "animal": "cat dog cat fish dog cat cat".split(),
   .....:         "size": list("SSMMMLL"),
   .....:         "weight": [8, 10, 11, 1, 20, 12, 12],
   .....:         "adult": [False] * 5 + [True] * 2,
   .....:     }
   .....: )
   .....: 

In [105]: df
Out[105]: 
  animal size  weight  adult
0    cat    S       8  False
1    dog    S      10  False
2    cat    M      11  False
3   fish    M       1  False
4    dog    M      20  False
5    cat    L      12   True
6    cat    L      12   True

# List the size of the animals with the highest weight.
In [106]: df.groupby("animal").apply(lambda subf: subf["size"][subf["weight"].idxmax()])
Out[106]: 
animal
cat     L
dog     M
fish    M
dtype: object
  Using get_group 
In [107]: gb = df.groupby(["animal"])

In [108]: gb.get_group("cat")
Out[108]: 
  animal size  weight  adult
0    cat    S       8  False
2    cat    M      11  False
5    cat    L      12   True
6    cat    L      12   True
  Apply to different items in a group 
In [109]: def GrowUp(x):
   .....:     avg_weight = sum(x[x["size"] == "S"].weight * 1.5)
   .....:     avg_weight += sum(x[x["size"] == "M"].weight * 1.25)
   .....:     avg_weight += sum(x[x["size"] == "L"].weight)
   .....:     avg_weight /= len(x)
   .....:     return pd.Series(["L", avg_weight, True], index=["size", "weight", "adult"])
   .....: 

In [110]: expected_df = gb.apply(GrowUp)

In [111]: expected_df
Out[111]: 
       size   weight  adult
animal                     
cat       L  12.4375   True
dog       L  20.0000   True
fish      L   1.2500   True
  Expanding apply 
In [112]: S = pd.Series([i / 100.0 for i in range(1, 11)])

In [113]: def cum_ret(x, y):
   .....:     return x * (1 + y)
   .....: 

In [114]: def red(x):
   .....:     return functools.reduce(cum_ret, x, 1.0)
   .....: 

In [115]: S.expanding().apply(red, raw=True)
Out[115]: 
0    1.010000
1    1.030200
2    1.061106
3    1.103550
4    1.158728
5    1.228251
6    1.314229
7    1.419367
8    1.547110
9    1.701821
dtype: float64
  Replacing some values with mean of the rest of a group 
In [116]: df = pd.DataFrame({"A": [1, 1, 2, 2], "B": [1, -1, 1, 2]})

In [117]: gb = df.groupby("A")

In [118]: def replace(g):
   .....:     mask = g < 0
   .....:     return g.where(mask, g[~mask].mean())
   .....: 

In [119]: gb.transform(replace)
Out[119]: 
     B
0  1.0
1 -1.0
2  1.5
3  1.5
  Sort groups by aggregated data 
In [120]: df = pd.DataFrame(
   .....:     {
   .....:         "code": ["foo", "bar", "baz"] * 2,
   .....:         "data": [0.16, -0.21, 0.33, 0.45, -0.59, 0.62],
   .....:         "flag": [False, True] * 3,
   .....:     }
   .....: )
   .....: 

In [121]: code_groups = df.groupby("code")

In [122]: agg_n_sort_order = code_groups[["data"]].transform(sum).sort_values(by="data")

In [123]: sorted_df = df.loc[agg_n_sort_order.index]

In [124]: sorted_df
Out[124]: 
  code  data   flag
1  bar -0.21   True
4  bar -0.59  False
0  foo  0.16  False
3  foo  0.45   True
2  baz  0.33  False
5  baz  0.62   True
  Create multiple aggregated columns 
In [125]: rng = pd.date_range(start="2014-10-07", periods=10, freq="2min")

In [126]: ts = pd.Series(data=list(range(10)), index=rng)

In [127]: def MyCust(x):
   .....:     if len(x) > 2:
   .....:         return x[1] * 1.234
   .....:     return pd.NaT
   .....: 

In [128]: mhc = {"Mean": np.mean, "Max": np.max, "Custom": MyCust}

In [129]: ts.resample("5min").apply(mhc)
Out[129]: 
                     Mean  Max Custom
2014-10-07 00:00:00   1.0    2  1.234
2014-10-07 00:05:00   3.5    4    NaT
2014-10-07 00:10:00   6.0    7  7.404
2014-10-07 00:15:00   8.5    9    NaT

In [130]: ts
Out[130]: 
2014-10-07 00:00:00    0
2014-10-07 00:02:00    1
2014-10-07 00:04:00    2
2014-10-07 00:06:00    3
2014-10-07 00:08:00    4
2014-10-07 00:10:00    5
2014-10-07 00:12:00    6
2014-10-07 00:14:00    7
2014-10-07 00:16:00    8
2014-10-07 00:18:00    9
Freq: 2T, dtype: int64
  Create a value counts column and reassign back to the DataFrame 
In [131]: df = pd.DataFrame(
   .....:     {"Color": "Red Red Red Blue".split(), "Value": [100, 150, 50, 50]}
   .....: )
   .....: 

In [132]: df
Out[132]: 
  Color  Value
0   Red    100
1   Red    150
2   Red     50
3  Blue     50

In [133]: df["Counts"] = df.groupby(["Color"]).transform(len)

In [134]: df
Out[134]: 
  Color  Value  Counts
0   Red    100       3
1   Red    150       3
2   Red     50       3
3  Blue     50       1
  Shift groups of the values in a column based on the index 
In [135]: df = pd.DataFrame(
   .....:     {"line_race": [10, 10, 8, 10, 10, 8], "beyer": [99, 102, 103, 103, 88, 100]},
   .....:     index=[
   .....:         "Last Gunfighter",
   .....:         "Last Gunfighter",
   .....:         "Last Gunfighter",
   .....:         "Paynter",
   .....:         "Paynter",
   .....:         "Paynter",
   .....:     ],
   .....: )
   .....: 

In [136]: df
Out[136]: 
                 line_race  beyer
Last Gunfighter         10     99
Last Gunfighter         10    102
Last Gunfighter          8    103
Paynter                 10    103
Paynter                 10     88
Paynter                  8    100

In [137]: df["beyer_shifted"] = df.groupby(level=0)["beyer"].shift(1)

In [138]: df
Out[138]: 
                 line_race  beyer  beyer_shifted
Last Gunfighter         10     99            NaN
Last Gunfighter         10    102           99.0
Last Gunfighter          8    103          102.0
Paynter                 10    103            NaN
Paynter                 10     88          103.0
Paynter                  8    100           88.0
  Select row with maximum value from each group 
In [139]: df = pd.DataFrame(
   .....:     {
   .....:         "host": ["other", "other", "that", "this", "this"],
   .....:         "service": ["mail", "web", "mail", "mail", "web"],
   .....:         "no": [1, 2, 1, 2, 1],
   .....:     }
   .....: ).set_index(["host", "service"])
   .....: 

In [140]: mask = df.groupby(level=0).agg("idxmax")

In [141]: df_count = df.loc[mask["no"]].reset_index()

In [142]: df_count
Out[142]: 
    host service  no
0  other     web   2
1   that    mail   1
2   this    mail   2
  Grouping like Python’s itertools.groupby 
In [143]: df = pd.DataFrame([0, 1, 0, 1, 1, 1, 0, 1, 1], columns=["A"])

In [144]: df["A"].groupby((df["A"] != df["A"].shift()).cumsum()).groups
Out[144]: {1: [0], 2: [1], 3: [2], 4: [3, 4, 5], 5: [6], 6: [7, 8]}

In [145]: df["A"].groupby((df["A"] != df["A"].shift()).cumsum()).cumsum()
Out[145]: 
0    0
1    1
2    0
3    1
4    2
5    3
6    0
7    1
8    2
Name: A, dtype: int64
   Expanding data Alignment and to-date Rolling Computation window based on values instead of counts Rolling Mean by Time Interval   Splitting Splitting a frame Create a list of dataframes, split using a delineation based on logic included in rows. 
In [146]: df = pd.DataFrame(
   .....:     data={
   .....:         "Case": ["A", "A", "A", "B", "A", "A", "B", "A", "A"],
   .....:         "Data": np.random.randn(9),
   .....:     }
   .....: )
   .....: 

In [147]: dfs = list(
   .....:     zip(
   .....:         *df.groupby(
   .....:             (1 * (df["Case"] == "B"))
   .....:             .cumsum()
   .....:             .rolling(window=3, min_periods=1)
   .....:             .median()
   .....:         )
   .....:     )
   .....: )[-1]
   .....: 

In [148]: dfs[0]
Out[148]: 
  Case      Data
0    A  0.276232
1    A -1.087401
2    A -0.673690
3    B  0.113648

In [149]: dfs[1]
Out[149]: 
  Case      Data
4    A -1.478427
5    A  0.524988
6    B  0.404705

In [150]: dfs[2]
Out[150]: 
  Case      Data
7    A  0.577046
8    A -1.715002
    Pivot The Pivot docs. Partial sums and subtotals 
In [151]: df = pd.DataFrame(
   .....:     data={
   .....:         "Province": ["ON", "QC", "BC", "AL", "AL", "MN", "ON"],
   .....:         "City": [
   .....:             "Toronto",
   .....:             "Montreal",
   .....:             "Vancouver",
   .....:             "Calgary",
   .....:             "Edmonton",
   .....:             "Winnipeg",
   .....:             "Windsor",
   .....:         ],
   .....:         "Sales": [13, 6, 16, 8, 4, 3, 1],
   .....:     }
   .....: )
   .....: 

In [152]: table = pd.pivot_table(
   .....:     df,
   .....:     values=["Sales"],
   .....:     index=["Province"],
   .....:     columns=["City"],
   .....:     aggfunc=np.sum,
   .....:     margins=True,
   .....: )
   .....: 

In [153]: table.stack("City")
Out[153]: 
                    Sales
Province City            
AL       All         12.0
         Calgary      8.0
         Edmonton     4.0
BC       All         16.0
         Vancouver   16.0
...                   ...
All      Montreal     6.0
         Toronto     13.0
         Vancouver   16.0
         Windsor      1.0
         Winnipeg     3.0

[20 rows x 1 columns]
  Frequency table like plyr in R 
In [154]: grades = [48, 99, 75, 80, 42, 80, 72, 68, 36, 78]

In [155]: df = pd.DataFrame(
   .....:     {
   .....:         "ID": ["x%d" % r for r in range(10)],
   .....:         "Gender": ["F", "M", "F", "M", "F", "M", "F", "M", "M", "M"],
   .....:         "ExamYear": [
   .....:             "2007",
   .....:             "2007",
   .....:             "2007",
   .....:             "2008",
   .....:             "2008",
   .....:             "2008",
   .....:             "2008",
   .....:             "2009",
   .....:             "2009",
   .....:             "2009",
   .....:         ],
   .....:         "Class": [
   .....:             "algebra",
   .....:             "stats",
   .....:             "bio",
   .....:             "algebra",
   .....:             "algebra",
   .....:             "stats",
   .....:             "stats",
   .....:             "algebra",
   .....:             "bio",
   .....:             "bio",
   .....:         ],
   .....:         "Participated": [
   .....:             "yes",
   .....:             "yes",
   .....:             "yes",
   .....:             "yes",
   .....:             "no",
   .....:             "yes",
   .....:             "yes",
   .....:             "yes",
   .....:             "yes",
   .....:             "yes",
   .....:         ],
   .....:         "Passed": ["yes" if x > 50 else "no" for x in grades],
   .....:         "Employed": [
   .....:             True,
   .....:             True,
   .....:             True,
   .....:             False,
   .....:             False,
   .....:             False,
   .....:             False,
   .....:             True,
   .....:             True,
   .....:             False,
   .....:         ],
   .....:         "Grade": grades,
   .....:     }
   .....: )
   .....: 

In [156]: df.groupby("ExamYear").agg(
   .....:     {
   .....:         "Participated": lambda x: x.value_counts()["yes"],
   .....:         "Passed": lambda x: sum(x == "yes"),
   .....:         "Employed": lambda x: sum(x),
   .....:         "Grade": lambda x: sum(x) / len(x),
   .....:     }
   .....: )
   .....: 
Out[156]: 
          Participated  Passed  Employed      Grade
ExamYear                                           
2007                 3       2         3  74.000000
2008                 3       3         0  68.500000
2009                 3       2         2  60.666667
  Plot pandas DataFrame with year over year data To create year and month cross tabulation: 
In [157]: df = pd.DataFrame(
   .....:     {"value": np.random.randn(36)},
   .....:     index=pd.date_range("2011-01-01", freq="M", periods=36),
   .....: )
   .....: 

In [158]: pd.pivot_table(
   .....:     df, index=df.index.month, columns=df.index.year, values="value", aggfunc="sum"
   .....: )
   .....: 
Out[158]: 
        2011      2012      2013
1  -1.039268 -0.968914  2.565646
2  -0.370647 -1.294524  1.431256
3  -1.157892  0.413738  1.340309
4  -1.344312  0.276662 -1.170299
5   0.844885 -0.472035 -0.226169
6   1.075770 -0.013960  0.410835
7  -0.109050 -0.362543  0.813850
8   1.643563 -0.006154  0.132003
9  -1.469388 -0.923061 -0.827317
10  0.357021  0.895717 -0.076467
11 -0.674600  0.805244 -1.187678
12 -1.776904 -1.206412  1.130127
    Apply Rolling apply to organize - Turning embedded lists into a MultiIndex frame 
In [159]: df = pd.DataFrame(
   .....:     data={
   .....:         "A": [[2, 4, 8, 16], [100, 200], [10, 20, 30]],
   .....:         "B": [["a", "b", "c"], ["jj", "kk"], ["ccc"]],
   .....:     },
   .....:     index=["I", "II", "III"],
   .....: )
   .....: 

In [160]: def SeriesFromSubList(aList):
   .....:     return pd.Series(aList)
   .....: 

In [161]: df_orgz = pd.concat(
   .....:     {ind: row.apply(SeriesFromSubList) for ind, row in df.iterrows()}
   .....: )
   .....: 

In [162]: df_orgz
Out[162]: 
         0     1     2     3
I   A    2     4     8  16.0
    B    a     b     c   NaN
II  A  100   200   NaN   NaN
    B   jj    kk   NaN   NaN
III A   10  20.0  30.0   NaN
    B  ccc   NaN   NaN   NaN
  Rolling apply with a DataFrame returning a Series Rolling Apply to multiple columns where function calculates a Series before a Scalar from the Series is returned 
In [163]: df = pd.DataFrame(
   .....:     data=np.random.randn(2000, 2) / 10000,
   .....:     index=pd.date_range("2001-01-01", periods=2000),
   .....:     columns=["A", "B"],
   .....: )
   .....: 

In [164]: df
Out[164]: 
                   A         B
2001-01-01 -0.000144 -0.000141
2001-01-02  0.000161  0.000102
2001-01-03  0.000057  0.000088
2001-01-04 -0.000221  0.000097
2001-01-05 -0.000201 -0.000041
...              ...       ...
2006-06-19  0.000040 -0.000235
2006-06-20 -0.000123 -0.000021
2006-06-21 -0.000113  0.000114
2006-06-22  0.000136  0.000109
2006-06-23  0.000027  0.000030

[2000 rows x 2 columns]

In [165]: def gm(df, const):
   .....:     v = ((((df["A"] + df["B"]) + 1).cumprod()) - 1) * const
   .....:     return v.iloc[-1]
   .....: 

In [166]: s = pd.Series(
   .....:     {
   .....:         df.index[i]: gm(df.iloc[i: min(i + 51, len(df) - 1)], 5)
   .....:         for i in range(len(df) - 50)
   .....:     }
   .....: )
   .....: 

In [167]: s
Out[167]: 
2001-01-01    0.000930
2001-01-02    0.002615
2001-01-03    0.001281
2001-01-04    0.001117
2001-01-05    0.002772
                ...   
2006-04-30    0.003296
2006-05-01    0.002629
2006-05-02    0.002081
2006-05-03    0.004247
2006-05-04    0.003928
Length: 1950, dtype: float64
  Rolling apply with a DataFrame returning a Scalar Rolling Apply to multiple columns where function returns a Scalar (Volume Weighted Average Price) 
In [168]: rng = pd.date_range(start="2014-01-01", periods=100)

In [169]: df = pd.DataFrame(
   .....:     {
   .....:         "Open": np.random.randn(len(rng)),
   .....:         "Close": np.random.randn(len(rng)),
   .....:         "Volume": np.random.randint(100, 2000, len(rng)),
   .....:     },
   .....:     index=rng,
   .....: )
   .....: 

In [170]: df
Out[170]: 
                Open     Close  Volume
2014-01-01 -1.611353 -0.492885    1219
2014-01-02 -3.000951  0.445794    1054
2014-01-03 -0.138359 -0.076081    1381
2014-01-04  0.301568  1.198259    1253
2014-01-05  0.276381 -0.669831    1728
...              ...       ...     ...
2014-04-06 -0.040338  0.937843    1188
2014-04-07  0.359661 -0.285908    1864
2014-04-08  0.060978  1.714814     941
2014-04-09  1.759055 -0.455942    1065
2014-04-10  0.138185 -1.147008    1453

[100 rows x 3 columns]

In [171]: def vwap(bars):
   .....:     return (bars.Close * bars.Volume).sum() / bars.Volume.sum()
   .....: 

In [172]: window = 5

In [173]: s = pd.concat(
   .....:     [
   .....:         (pd.Series(vwap(df.iloc[i: i + window]), index=[df.index[i + window]]))
   .....:         for i in range(len(df) - window)
   .....:     ]
   .....: )
   .....: 

In [174]: s.round(2)
Out[174]: 
2014-01-06    0.02
2014-01-07    0.11
2014-01-08    0.10
2014-01-09    0.07
2014-01-10   -0.29
              ... 
2014-04-06   -0.63
2014-04-07   -0.02
2014-04-08   -0.03
2014-04-09    0.34
2014-04-10    0.29
Length: 95, dtype: float64
     Timeseries Between times Using indexer between time Constructing a datetime range that excludes weekends and includes only certain times Vectorized Lookup Aggregation and plotting time series Turn a matrix with hours in columns and days in rows into a continuous row sequence in the form of a time series. How to rearrange a Python pandas DataFrame? Dealing with duplicates when reindexing a timeseries to a specified frequency Calculate the first day of the month for each entry in a DatetimeIndex 
In [175]: dates = pd.date_range("2000-01-01", periods=5)

In [176]: dates.to_period(freq="M").to_timestamp()
Out[176]: 
DatetimeIndex(['2000-01-01', '2000-01-01', '2000-01-01', '2000-01-01',
               '2000-01-01'],
              dtype='datetime64[ns]', freq=None)
   Resampling The Resample docs. Using Grouper instead of TimeGrouper for time grouping of values Time grouping with some missing values Valid frequency arguments to Grouper Timeseries Grouping using a MultiIndex Using TimeGrouper and another grouping to create subgroups, then apply a custom function GH3791 Resampling with custom periods Resample intraday frame without adding new days Resample minute data Resample with groupby    Merge The Join docs. Concatenate two dataframes with overlapping index (emulate R rbind) 
In [177]: rng = pd.date_range("2000-01-01", periods=6)

In [178]: df1 = pd.DataFrame(np.random.randn(6, 3), index=rng, columns=["A", "B", "C"])

In [179]: df2 = df1.copy()
  Depending on df construction, ignore_index may be needed 
In [180]: df = pd.concat([df1, df2], ignore_index=True)

In [181]: df
Out[181]: 
           A         B         C
0  -0.870117 -0.479265 -0.790855
1   0.144817  1.726395 -0.464535
2  -0.821906  1.597605  0.187307
3  -0.128342 -1.511638 -0.289858
4   0.399194 -1.430030 -0.639760
5   1.115116 -2.012600  1.810662
6  -0.870117 -0.479265 -0.790855
7   0.144817  1.726395 -0.464535
8  -0.821906  1.597605  0.187307
9  -0.128342 -1.511638 -0.289858
10  0.399194 -1.430030 -0.639760
11  1.115116 -2.012600  1.810662
  Self Join of a DataFrame GH2996 
In [182]: df = pd.DataFrame(
   .....:     data={
   .....:         "Area": ["A"] * 5 + ["C"] * 2,
   .....:         "Bins": [110] * 2 + [160] * 3 + [40] * 2,
   .....:         "Test_0": [0, 1, 0, 1, 2, 0, 1],
   .....:         "Data": np.random.randn(7),
   .....:     }
   .....: )
   .....: 

In [183]: df
Out[183]: 
  Area  Bins  Test_0      Data
0    A   110       0 -0.433937
1    A   110       1 -0.160552
2    A   160       0  0.744434
3    A   160       1  1.754213
4    A   160       2  0.000850
5    C    40       0  0.342243
6    C    40       1  1.070599

In [184]: df["Test_1"] = df["Test_0"] - 1

In [185]: pd.merge(
   .....:     df,
   .....:     df,
   .....:     left_on=["Bins", "Area", "Test_0"],
   .....:     right_on=["Bins", "Area", "Test_1"],
   .....:     suffixes=("_L", "_R"),
   .....: )
   .....: 
Out[185]: 
  Area  Bins  Test_0_L    Data_L  Test_1_L  Test_0_R    Data_R  Test_1_R
0    A   110         0 -0.433937        -1         1 -0.160552         0
1    A   160         0  0.744434        -1         1  1.754213         0
2    A   160         1  1.754213         0         2  0.000850         1
3    C    40         0  0.342243        -1         1  1.070599         0
  How to set the index and join KDB like asof join Join with a criteria based on the values Using searchsorted to merge based on values inside a range   Plotting The Plotting docs. Make Matplotlib look like R Setting x-axis major and minor labels Plotting multiple charts in an IPython Jupyter notebook Creating a multi-line plot Plotting a heatmap Annotate a time-series plot Annotate a time-series plot #2 Generate Embedded plots in excel files using Pandas, Vincent and xlsxwriter Boxplot for each quartile of a stratifying variable 
In [186]: df = pd.DataFrame(
   .....:     {
   .....:         "stratifying_var": np.random.uniform(0, 100, 20),
   .....:         "price": np.random.normal(100, 5, 20),
   .....:     }
   .....: )
   .....: 

In [187]: df["quartiles"] = pd.qcut(
   .....:     df["stratifying_var"], 4, labels=["0-25%", "25-50%", "50-75%", "75-100%"]
   .....: )
   .....: 

In [188]: df.boxplot(column="price", by="quartiles")
Out[188]: <AxesSubplot:title={'center':'price'}, xlabel='quartiles'>
     Data in/out Performance comparison of SQL vs HDF5  CSV The CSV docs read_csv in action appending to a csv Reading a csv chunk-by-chunk Reading only certain rows of a csv chunk-by-chunk Reading the first few lines of a frame Reading a file that is compressed but not by gzip/bz2 (the native compressed formats which read_csv understands). This example shows a WinZipped file, but is a general application of opening the file within a context manager and using that handle to read. See here Inferring dtypes from a file Dealing with bad lines GH2886 Write a multi-row index CSV without writing duplicates  Reading multiple files to create a single DataFrame The best way to combine multiple files into a single DataFrame is to read the individual frames one by one, put all of the individual frames into a list, and then combine the frames in the list using pd.concat(): 
In [189]: for i in range(3):
   .....:     data = pd.DataFrame(np.random.randn(10, 4))
   .....:     data.to_csv("file_{}.csv".format(i))
   .....: 

In [190]: files = ["file_0.csv", "file_1.csv", "file_2.csv"]

In [191]: result = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)
  You can use the same approach to read all files matching a pattern. Here is an example using glob: 
In [192]: import glob

In [193]: import os

In [194]: files = glob.glob("file_*.csv")

In [195]: result = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)
  Finally, this strategy will work with the other pd.read_*(...) functions described in the io docs.   Parsing date components in multi-columns Parsing date components in multi-columns is faster with a format 
In [196]: i = pd.date_range("20000101", periods=10000)

In [197]: df = pd.DataFrame({"year": i.year, "month": i.month, "day": i.day})

In [198]: df.head()
Out[198]: 
   year  month  day
0  2000      1    1
1  2000      1    2
2  2000      1    3
3  2000      1    4
4  2000      1    5

In [199]: %timeit pd.to_datetime(df.year * 10000 + df.month * 100 + df.day, format='%Y%m%d')
   .....: ds = df.apply(lambda x: "%04d%02d%02d" % (x["year"], x["month"], x["day"]), axis=1)
   .....: ds.head()
   .....: %timeit pd.to_datetime(ds)
   .....: 
8.7 ms +- 765 us per loop (mean +- std. dev. of 7 runs, 100 loops each)
2.1 ms +- 419 us per loop (mean +- std. dev. of 7 runs, 100 loops each)
    Skip row between header and data 
In [200]: data = """;;;;
   .....:  ;;;;
   .....:  ;;;;
   .....:  ;;;;
   .....:  ;;;;
   .....:  ;;;;
   .....: ;;;;
   .....:  ;;;;
   .....:  ;;;;
   .....: ;;;;
   .....: date;Param1;Param2;Param4;Param5
   .....:     ;m²;°C;m²;m
   .....: ;;;;
   .....: 01.01.1990 00:00;1;1;2;3
   .....: 01.01.1990 01:00;5;3;4;5
   .....: 01.01.1990 02:00;9;5;6;7
   .....: 01.01.1990 03:00;13;7;8;9
   .....: 01.01.1990 04:00;17;9;10;11
   .....: 01.01.1990 05:00;21;11;12;13
   .....: """
   .....: 
   Option 1: pass rows explicitly to skip rows 
In [201]: from io import StringIO

In [202]: pd.read_csv(
   .....:     StringIO(data),
   .....:     sep=";",
   .....:     skiprows=[11, 12],
   .....:     index_col=0,
   .....:     parse_dates=True,
   .....:     header=10,
   .....: )
   .....: 
Out[202]: 
                     Param1  Param2  Param4  Param5
date                                               
1990-01-01 00:00:00       1       1       2       3
1990-01-01 01:00:00       5       3       4       5
1990-01-01 02:00:00       9       5       6       7
1990-01-01 03:00:00      13       7       8       9
1990-01-01 04:00:00      17       9      10      11
1990-01-01 05:00:00      21      11      12      13
    Option 2: read column names and then data 
In [203]: pd.read_csv(StringIO(data), sep=";", header=10, nrows=10).columns
Out[203]: Index(['date', 'Param1', 'Param2', 'Param4', 'Param5'], dtype='object')

In [204]: columns = pd.read_csv(StringIO(data), sep=";", header=10, nrows=10).columns

In [205]: pd.read_csv(
   .....:     StringIO(data), sep=";", index_col=0, header=12, parse_dates=True, names=columns
   .....: )
   .....: 
Out[205]: 
                     Param1  Param2  Param4  Param5
date                                               
1990-01-01 00:00:00       1       1       2       3
1990-01-01 01:00:00       5       3       4       5
1990-01-01 02:00:00       9       5       6       7
1990-01-01 03:00:00      13       7       8       9
1990-01-01 04:00:00      17       9      10      11
1990-01-01 05:00:00      21      11      12      13
      SQL The SQL docs Reading from databases with SQL   Excel The Excel docs Reading from a filelike handle Modifying formatting in XlsxWriter output Loading only visible sheets GH19842#issuecomment-892150745   HTML Reading HTML tables from a server that cannot handle the default request header   HDFStore The HDFStores docs Simple queries with a Timestamp Index Managing heterogeneous data using a linked multiple table hierarchy GH3032 Merging on-disk tables with millions of rows Avoiding inconsistencies when writing to a store from multiple processes/threads De-duplicating a large store by chunks, essentially a recursive reduction operation. Shows a function for taking in data from csv file and creating a store by chunks, with date parsing as well. See here Creating a store chunk-by-chunk from a csv file Appending to a store, while creating a unique index Large Data work flows Reading in a sequence of files, then providing a global unique index to a store while appending Groupby on a HDFStore with low group density Groupby on a HDFStore with high group density Hierarchical queries on a HDFStore Counting with a HDFStore Troubleshoot HDFStore exceptions Setting min_itemsize with strings Using ptrepack to create a completely-sorted-index on a store Storing Attributes to a group node 
In [206]: df = pd.DataFrame(np.random.randn(8, 3))

In [207]: store = pd.HDFStore("test.h5")

In [208]: store.put("df", df)

# you can store an arbitrary Python object via pickle
In [209]: store.get_storer("df").attrs.my_attribute = {"A": 10}

In [210]: store.get_storer("df").attrs.my_attribute
Out[210]: {'A': 10}
  You can create or load a HDFStore in-memory by passing the driver parameter to PyTables. Changes are only written to disk when the HDFStore is closed. 
In [211]: store = pd.HDFStore("test.h5", "w", driver="H5FD_CORE")

In [212]: df = pd.DataFrame(np.random.randn(8, 3))

In [213]: store["test"] = df

# only after closing the store, data is written to disk:
In [214]: store.close()
    Binary files pandas readily accepts NumPy record arrays, if you need to read in a binary file consisting of an array of C structs. For example, given this C program in a file called main.c compiled with gcc main.c -std=gnu99 on a 64-bit machine, 
#include <stdio.h>
#include <stdint.h>

typedef struct _Data
{
    int32_t count;
    double avg;
    float scale;
} Data;

int main(int argc, const char *argv[])
{
    size_t n = 10;
    Data d[n];

    for (int i = 0; i < n; ++i)
    {
        d[i].count = i;
        d[i].avg = i + 1.0;
        d[i].scale = (float) i + 2.0f;
    }

    FILE *file = fopen("binary.dat", "wb");
    fwrite(&d, sizeof(Data), n, file);
    fclose(file);

    return 0;
}
  the following Python code will read the binary file 'binary.dat' into a pandas DataFrame, where each element of the struct corresponds to a column in the frame: 
names = "count", "avg", "scale"

# note that the offsets are larger than the size of the type because of
# struct padding
offsets = 0, 8, 16
formats = "i4", "f8", "f4"
dt = np.dtype({"names": names, "offsets": offsets, "formats": formats}, align=True)
df = pd.DataFrame(np.fromfile("binary.dat", dt))
   Note The offsets of the structure elements may be different depending on the architecture of the machine on which the file was created. Using a raw binary file format like this for general data storage is not recommended, as it is not cross platform. We recommended either HDF5 or parquet, both of which are supported by pandas’ IO facilities.     Computation Numerical integration (sample-based) of a time series  Correlation Often it’s useful to obtain the lower (or upper) triangular form of a correlation matrix calculated from DataFrame.corr(). This can be achieved by passing a boolean mask to where as follows: 
In [215]: df = pd.DataFrame(np.random.random(size=(100, 5)))

In [216]: corr_mat = df.corr()

In [217]: mask = np.tril(np.ones_like(corr_mat, dtype=np.bool_), k=-1)

In [218]: corr_mat.where(mask)
Out[218]: 
          0         1         2        3   4
0       NaN       NaN       NaN      NaN NaN
1 -0.079861       NaN       NaN      NaN NaN
2 -0.236573  0.183801       NaN      NaN NaN
3 -0.013795 -0.051975  0.037235      NaN NaN
4 -0.031974  0.118342 -0.073499 -0.02063 NaN
  The method argument within DataFrame.corr can accept a callable in addition to the named correlation types. Here we compute the distance correlation matrix for a DataFrame object. 
In [219]: def distcorr(x, y):
   .....:     n = len(x)
   .....:     a = np.zeros(shape=(n, n))
   .....:     b = np.zeros(shape=(n, n))
   .....:     for i in range(n):
   .....:         for j in range(i + 1, n):
   .....:             a[i, j] = abs(x[i] - x[j])
   .....:             b[i, j] = abs(y[i] - y[j])
   .....:     a += a.T
   .....:     b += b.T
   .....:     a_bar = np.vstack([np.nanmean(a, axis=0)] * n)
   .....:     b_bar = np.vstack([np.nanmean(b, axis=0)] * n)
   .....:     A = a - a_bar - a_bar.T + np.full(shape=(n, n), fill_value=a_bar.mean())
   .....:     B = b - b_bar - b_bar.T + np.full(shape=(n, n), fill_value=b_bar.mean())
   .....:     cov_ab = np.sqrt(np.nansum(A * B)) / n
   .....:     std_a = np.sqrt(np.sqrt(np.nansum(A ** 2)) / n)
   .....:     std_b = np.sqrt(np.sqrt(np.nansum(B ** 2)) / n)
   .....:     return cov_ab / std_a / std_b
   .....: 

In [220]: df = pd.DataFrame(np.random.normal(size=(100, 3)))

In [221]: df.corr(method=distcorr)
Out[221]: 
          0         1         2
0  1.000000  0.197613  0.216328
1  0.197613  1.000000  0.208749
2  0.216328  0.208749  1.000000
     Timedeltas The Timedeltas docs. Using timedeltas 
In [222]: import datetime

In [223]: s = pd.Series(pd.date_range("2012-1-1", periods=3, freq="D"))

In [224]: s - s.max()
Out[224]: 
0   -2 days
1   -1 days
2    0 days
dtype: timedelta64[ns]

In [225]: s.max() - s
Out[225]: 
0   2 days
1   1 days
2   0 days
dtype: timedelta64[ns]

In [226]: s - datetime.datetime(2011, 1, 1, 3, 5)
Out[226]: 
0   364 days 20:55:00
1   365 days 20:55:00
2   366 days 20:55:00
dtype: timedelta64[ns]

In [227]: s + datetime.timedelta(minutes=5)
Out[227]: 
0   2012-01-01 00:05:00
1   2012-01-02 00:05:00
2   2012-01-03 00:05:00
dtype: datetime64[ns]

In [228]: datetime.datetime(2011, 1, 1, 3, 5) - s
Out[228]: 
0   -365 days +03:05:00
1   -366 days +03:05:00
2   -367 days +03:05:00
dtype: timedelta64[ns]

In [229]: datetime.timedelta(minutes=5) + s
Out[229]: 
0   2012-01-01 00:05:00
1   2012-01-02 00:05:00
2   2012-01-03 00:05:00
dtype: datetime64[ns]
  Adding and subtracting deltas and dates 
In [230]: deltas = pd.Series([datetime.timedelta(days=i) for i in range(3)])

In [231]: df = pd.DataFrame({"A": s, "B": deltas})

In [232]: df
Out[232]: 
           A      B
0 2012-01-01 0 days
1 2012-01-02 1 days
2 2012-01-03 2 days

In [233]: df["New Dates"] = df["A"] + df["B"]

In [234]: df["Delta"] = df["A"] - df["New Dates"]

In [235]: df
Out[235]: 
           A      B  New Dates   Delta
0 2012-01-01 0 days 2012-01-01  0 days
1 2012-01-02 1 days 2012-01-03 -1 days
2 2012-01-03 2 days 2012-01-05 -2 days

In [236]: df.dtypes
Out[236]: 
A             datetime64[ns]
B            timedelta64[ns]
New Dates     datetime64[ns]
Delta        timedelta64[ns]
dtype: object
  Another example Values can be set to NaT using np.nan, similar to datetime 
In [237]: y = s - s.shift()

In [238]: y
Out[238]: 
0      NaT
1   1 days
2   1 days
dtype: timedelta64[ns]

In [239]: y[1] = np.nan

In [240]: y
Out[240]: 
0      NaT
1      NaT
2   1 days
dtype: timedelta64[ns]
    Creating example data To create a dataframe from every combination of some given values, like R’s expand.grid() function, we can create a dict where the keys are column names and the values are lists of the data values: 
In [241]: def expand_grid(data_dict):
   .....:     rows = itertools.product(*data_dict.values())
   .....:     return pd.DataFrame.from_records(rows, columns=data_dict.keys())
   .....: 

In [242]: df = expand_grid(
   .....:     {"height": [60, 70], "weight": [100, 140, 180], "sex": ["Male", "Female"]}
   .....: )
   .....: 

In [243]: df
Out[243]: 
    height  weight     sex
0       60     100    Male
1       60     100  Female
2       60     140    Male
3       60     140  Female
4       60     180    Male
5       60     180  Female
6       70     100    Male
7       70     100  Female
8       70     140    Male
9       70     140  Female
10      70     180    Male
11      70     180  Female
def f_37080612(df):
    """get rows of dataframe `df` that match regex '(Hel|Just)'
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
email.utils.unquote(str)  
Return a new string which is an unquoted version of str. If str ends and begins with double quotes, they are stripped off. Likewise if str ends and begins with angle brackets, they are stripped off.
token.LSQB  
Token value for "[".
token.RSQB  
Token value for "]".
difflib.restore(sequence, which)  
Return one of the two sequences that generated a delta. Given a sequence produced by Differ.compare() or ndiff(), extract lines originating from file 1 or 2 (parameter which), stripping off line prefixes. Example: >>> diff = ndiff('one\ntwo\nthree\n'.splitlines(keepends=True),
...              'ore\ntree\nemu\n'.splitlines(keepends=True))
>>> diff = list(diff) # materialize the generated delta into a list
>>> print(''.join(restore(diff, 1)), end="")
one
two
three
>>> print(''.join(restore(diff, 2)), end="")
ore
tree
emu
get_topmost_subplotspec()[source]
 
Return the topmost SubplotSpec instance associated with the subplot.
def f_14716342(your_string):
    """find the string in `your_string` between two special characters "[" and "]"
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
pandas.Series.tolist   Series.tolist()[source]
 
Return a list of the values. These are each a scalar type, which is a Python scalar (for str, int, float) or a pandas scalar (for Timestamp/Timedelta/Interval/Period)  Returns 
 list
    See also  numpy.ndarray.tolist

Return the array as an a.ndim-levels deep nested list of Python scalars.
pandas.StringDtype   classpandas.StringDtype(storage=None)[source]
 
Extension dtype for string data.  New in version 1.0.0.   Warning StringDtype is considered experimental. The implementation and parts of the API may change without warning. In particular, StringDtype.na_value may change to no longer be numpy.nan.   Parameters 
 
storage:{“python”, “pyarrow”}, optional


If not given, the value of pd.options.mode.string_storage.     Examples 
>>> pd.StringDtype()
string[python]
  
>>> pd.StringDtype(storage="pyarrow")
string[pyarrow]
  Attributes       
None     Methods       
None
pandas.Series.to_list   Series.to_list()[source]
 
Return a list of the values. These are each a scalar type, which is a Python scalar (for str, int, float) or a pandas scalar (for Timestamp/Timedelta/Interval/Period)  Returns 
 list
    See also  numpy.ndarray.tolist

Return the array as an a.ndim-levels deep nested list of Python scalars.
matplotlib.dates.drange(dstart, dend, delta)[source]
 
Return a sequence of equally spaced Matplotlib dates. The dates start at dstart and reach up to, but not including dend. They are spaced by delta.  Parameters 
 
dstart, denddatetime


The date limits.  
deltadatetime.timedelta


Spacing of the dates.    Returns 
 numpy.array

A list floats representing Matplotlib dates.
pandas.Index.to_list   Index.to_list()[source]
 
Return a list of the values. These are each a scalar type, which is a Python scalar (for str, int, float) or a pandas scalar (for Timestamp/Timedelta/Interval/Period)  Returns 
 list
    See also  numpy.ndarray.tolist

Return the array as an a.ndim-levels deep nested list of Python scalars.
def f_18684076():
    """create a list of date string in 'yyyymmdd' format with Python Pandas from '20130226' to '20130302'
    """
    return  
 --------------------

def f_1666700():
    """count number of times string 'brown' occurred in string 'The big brown fox is brown'
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
json_decoder  
alias of flask.json.JSONDecoder
HttpRequest.body  
The raw HTTP request body as a bytestring. This is useful for processing data in different ways than conventional HTML forms: binary images, XML payload etc. For processing conventional form data, use HttpRequest.POST. You can also read from an HttpRequest using a file-like interface with HttpRequest.read() or HttpRequest.readline(). Accessing the body attribute after reading the request with either of these I/O stream methods will produce a RawPostDataException.
tf.compat.v1.estimator.classifier_parse_example_spec Generates parsing spec for tf.parse_example to be used with classifiers. 
tf.compat.v1.estimator.classifier_parse_example_spec(
    feature_columns, label_key, label_dtype=tf.dtypes.int64, label_default=None,
    weight_column=None
)
 If users keep data in tf.Example format, they need to call tf.parse_example with a proper feature spec. There are two main things that this utility helps:  Users need to combine parsing spec of features with labels and weights (if any) since they are all parsed from same tf.Example instance. This utility combines these specs. It is difficult to map expected label by a classifier such as DNNClassifier to corresponding tf.parse_example spec. This utility encodes it by getting related information from users (key, dtype).  Example output of parsing spec: # Define features and transformations
feature_b = tf.feature_column.numeric_column(...)
feature_c_bucketized = tf.feature_column.bucketized_column(
  tf.feature_column.numeric_column("feature_c"), ...)
feature_a_x_feature_c = tf.feature_column.crossed_column(
    columns=["feature_a", feature_c_bucketized], ...)

feature_columns = [feature_b, feature_c_bucketized, feature_a_x_feature_c]
parsing_spec = tf.estimator.classifier_parse_example_spec(
    feature_columns, label_key='my-label', label_dtype=tf.string)

# For the above example, classifier_parse_example_spec would return the dict:
assert parsing_spec == {
  "feature_a": parsing_ops.VarLenFeature(tf.string),
  "feature_b": parsing_ops.FixedLenFeature([1], dtype=tf.float32),
  "feature_c": parsing_ops.FixedLenFeature([1], dtype=tf.float32)
  "my-label" : parsing_ops.FixedLenFeature([1], dtype=tf.string)
}
 Example usage with a classifier: feature_columns = # define features via tf.feature_column
estimator = DNNClassifier(
    n_classes=1000,
    feature_columns=feature_columns,
    weight_column='example-weight',
    label_vocabulary=['photos', 'keep', ...],
    hidden_units=[256, 64, 16])
# This label configuration tells the classifier the following:
# * weights are retrieved with key 'example-weight'
# * label is string and can be one of the following ['photos', 'keep', ...]
# * integer id for label 'photos' is 0, 'keep' is 1, ...


# Input builders
def input_fn_train():  # Returns a tuple of features and labels.
  features = tf.contrib.learn.read_keyed_batch_features(
      file_pattern=train_files,
      batch_size=batch_size,
      # creates parsing configuration for tf.parse_example
      features=tf.estimator.classifier_parse_example_spec(
          feature_columns,
          label_key='my-label',
          label_dtype=tf.string,
          weight_column='example-weight'),
      reader=tf.RecordIOReader)
   labels = features.pop('my-label')
   return features, labels

estimator.train(input_fn=input_fn_train)

 


 Args
  feature_columns   An iterable containing all feature columns. All items should be instances of classes derived from FeatureColumn.  
  label_key   A string identifying the label. It means tf.Example stores labels with this key.  
  label_dtype   A tf.dtype identifies the type of labels. By default it is tf.int64. If user defines a label_vocabulary, this should be set as tf.string. tf.float32 labels are only supported for binary classification.  
  label_default   used as label if label_key does not exist in given tf.Example. An example usage: let's say label_key is 'clicked' and tf.Example contains clicked data only for positive examples in following format key:clicked, value:1. This means that if there is no data with key 'clicked' it should count as negative example by setting label_deafault=0. Type of this value should be compatible with label_dtype.  
  weight_column   A string or a NumericColumn created by tf.feature_column.numeric_column defining feature column representing weights. It is used to down weight or boost examples during training. It will be multiplied by the loss of the example. If it is a string, it is used as a key to fetch weight tensor from the features. If it is a NumericColumn, raw tensor is fetched by key weight_column.key, then weight_column.normalizer_fn is applied on it to get weight tensor.   
 


 Returns   A dict mapping each feature key to a FixedLenFeature or VarLenFeature value.  

 


 Raises
  ValueError   If label is used in feature_columns.  
  ValueError   If weight_column is used in feature_columns.  
  ValueError   If any of the given feature_columns is not a _FeatureColumn instance.  
  ValueError   If weight_column is not a NumericColumn instance.  
  ValueError   if label_key is None.
tf.compat.v1.estimator.regressor_parse_example_spec Generates parsing spec for tf.parse_example to be used with regressors. 
tf.compat.v1.estimator.regressor_parse_example_spec(
    feature_columns, label_key, label_dtype=tf.dtypes.float32, label_default=None,
    label_dimension=1, weight_column=None
)
 If users keep data in tf.Example format, they need to call tf.parse_example with a proper feature spec. There are two main things that this utility helps:  Users need to combine parsing spec of features with labels and weights (if any) since they are all parsed from same tf.Example instance. This utility combines these specs. It is difficult to map expected label by a regressor such as DNNRegressor to corresponding tf.parse_example spec. This utility encodes it by getting related information from users (key, dtype).  Example output of parsing spec: # Define features and transformations
feature_b = tf.feature_column.numeric_column(...)
feature_c_bucketized = tf.feature_column.bucketized_column(
  tf.feature_column.numeric_column("feature_c"), ...)
feature_a_x_feature_c = tf.feature_column.crossed_column(
    columns=["feature_a", feature_c_bucketized], ...)

feature_columns = [feature_b, feature_c_bucketized, feature_a_x_feature_c]
parsing_spec = tf.estimator.regressor_parse_example_spec(
    feature_columns, label_key='my-label')

# For the above example, regressor_parse_example_spec would return the dict:
assert parsing_spec == {
  "feature_a": parsing_ops.VarLenFeature(tf.string),
  "feature_b": parsing_ops.FixedLenFeature([1], dtype=tf.float32),
  "feature_c": parsing_ops.FixedLenFeature([1], dtype=tf.float32)
  "my-label" : parsing_ops.FixedLenFeature([1], dtype=tf.float32)
}
 Example usage with a regressor: feature_columns = # define features via tf.feature_column
estimator = DNNRegressor(
    hidden_units=[256, 64, 16],
    feature_columns=feature_columns,
    weight_column='example-weight',
    label_dimension=3)
# This label configuration tells the regressor the following:
# * weights are retrieved with key 'example-weight'
# * label is a 3 dimension tensor with float32 dtype.


# Input builders
def input_fn_train():  # Returns a tuple of features and labels.
  features = tf.contrib.learn.read_keyed_batch_features(
      file_pattern=train_files,
      batch_size=batch_size,
      # creates parsing configuration for tf.parse_example
      features=tf.estimator.classifier_parse_example_spec(
          feature_columns,
          label_key='my-label',
          label_dimension=3,
          weight_column='example-weight'),
      reader=tf.RecordIOReader)
   labels = features.pop('my-label')
   return features, labels

estimator.train(input_fn=input_fn_train)

 


 Args
  feature_columns   An iterable containing all feature columns. All items should be instances of classes derived from _FeatureColumn.  
  label_key   A string identifying the label. It means tf.Example stores labels with this key.  
  label_dtype   A tf.dtype identifies the type of labels. By default it is tf.float32.  
  label_default   used as label if label_key does not exist in given tf.Example. By default default_value is none, which means tf.parse_example will error out if there is any missing label.  
  label_dimension   Number of regression targets per example. This is the size of the last dimension of the labels and logits Tensor objects (typically, these have shape [batch_size, label_dimension]).  
  weight_column   A string or a NumericColumn created by tf.feature_column.numeric_column defining feature column representing weights. It is used to down weight or boost examples during training. It will be multiplied by the loss of the example. If it is a string, it is used as a key to fetch weight tensor from the features. If it is a NumericColumn, raw tensor is fetched by key weight_column.key, then weight_column.normalizer_fn is applied on it to get weight tensor.   
 


 Returns   A dict mapping each feature key to a FixedLenFeature or VarLenFeature value.  

 


 Raises
  ValueError   If label is used in feature_columns.  
  ValueError   If weight_column is used in feature_columns.  
  ValueError   If any of the given feature_columns is not a _FeatureColumn instance.  
  ValueError   If weight_column is not a NumericColumn instance.  
  ValueError   if label_key is None.
CGIXMLRPCRequestHandler.handle_request(request_text=None)  
Handle an XML-RPC request. If request_text is given, it should be the POST data provided by the HTTP server, otherwise the contents of stdin will be used.
def f_18979111(request_body):
    """decode json string `request_body` to python dict
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
winreg.SaveKey(key, file_name)  
Saves the specified key, and all its subkeys to the specified file. key is an already open key, or one of the predefined HKEY_* constants. file_name is the name of the file to save registry data to. This file cannot already exist. If this filename includes an extension, it cannot be used on file allocation table (FAT) file systems by the LoadKey() method. If key represents a key on a remote computer, the path described by file_name is relative to the remote computer. The caller of this method must possess the SeBackupPrivilege security privilege. Note that privileges are different than permissions – see the Conflicts Between User Rights and Permissions documentation for more details. This function passes NULL for security_attributes to the API. Raises an auditing event winreg.SaveKey with arguments key, file_name.
torch.hub.download_url_to_file(url, dst, hash_prefix=None, progress=True) [source]
 
Download object at the given URL to a local path.  Parameters 
 
url (string) – URL of the object to download 
dst (string) – Full path where object will be saved, e.g. /tmp/temporary_file
 
hash_prefix (string, optional) – If not None, the SHA256 downloaded file should start with hash_prefix. Default: None 
progress (bool, optional) – whether or not to display a progress bar to stderr Default: True    Example >>> torch.hub.download_url_to_file('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth', '/tmp/temporary_file')
Engine.select_template(template_name_list)  
Like get_template(), except it takes a list of names and returns the first template that was found.
select_template(template_name_list, using=None)  
select_template() is just like get_template(), except it takes a list of template names. It tries each name in order and returns the first template that exists.
tf.raw_ops.BatchMatrixInverse  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.raw_ops.BatchMatrixInverse  
tf.raw_ops.BatchMatrixInverse(
    input, adjoint=False, name=None
)

 


 Args
  input   A Tensor. Must be one of the following types: float64, float32.  
  adjoint   An optional bool. Defaults to False.  
  name   A name for the operation (optional).   
 


 Returns   A Tensor. Has the same type as input.
def f_7243750(url, file_name):
    """download the file from url `url` and save it under file `file_name`
    """
    return  
 --------------------

def f_743806(text):
    """split string `text` by space
    """
    return  
 --------------------

def f_743806(text):
    """split string `text` by ","
    """
    return  
 --------------------

def f_743806(line):
    """Split string `line` into a list by whitespace
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
re.S  
re.DOTALL  
Make the '.' special character match any character at all, including a newline; without this flag, '.' will match anything except a newline. Corresponds to the inline flag (?s).
re.S  
re.DOTALL  
Make the '.' special character match any character at all, including a newline; without this flag, '.' will match anything except a newline. Corresponds to the inline flag (?s).
token.DOT  
Token value for ".".
DefaultCookiePolicy.DomainStrictNoDots  
When setting cookies, the ‘host prefix’ must not contain a dot (eg. www.foo.bar.com can’t set a cookie for .bar.com, because www.foo contains a dot).
tf.keras.layers.Dot     View source on GitHub    Layer that computes a dot product between samples in two tensors. Inherits From: Layer, Module  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.keras.layers.Dot  
tf.keras.layers.Dot(
    axes, normalize=False, **kwargs
)
 E.g. if applied to a list of two tensors a and b of shape (batch_size, n), the output will be a tensor of shape (batch_size, 1) where each entry i will be the dot product between a[i] and b[i]. 
x = np.arange(10).reshape(1, 5, 2)
print(x)
[[[0 1]
  [2 3]
  [4 5]
  [6 7]
  [8 9]]]
y = np.arange(10, 20).reshape(1, 2, 5)
print(y)
[[[10 11 12 13 14]
  [15 16 17 18 19]]]
tf.keras.layers.Dot(axes=(1, 2))([x, y])
<tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy=
array([[[260, 360],
        [320, 445]]])>
 
x1 = tf.keras.layers.Dense(8)(np.arange(10).reshape(5, 2))
x2 = tf.keras.layers.Dense(8)(np.arange(10, 20).reshape(5, 2))
dotted = tf.keras.layers.Dot(axes=1)([x1, x2])
dotted.shape
TensorShape([5, 1])

 


 Arguments
  axes   Integer or tuple of integers, axis or axes along which to take the dot product. If a tuple, should be two integers corresponding to the desired axis from the first input and the desired axis from the second input, respectively. Note that the size of the two selected axes must match.  
  normalize   Whether to L2-normalize samples along the dot product axis before taking the dot product. If set to True, then the output of the dot product is the cosine proximity between the two samples.  
  **kwargs   Standard layer keyword arguments.
def f_35044115(s):
    """replace dot characters  '.' associated with ascii letters in list `s` with space ' '
    """
    return  
 --------------------

def f_38388799(list_of_strings):
    """sort list `list_of_strings` based on second index of each string `s`
    """
    return  
 --------------------

def f_37004138(lst):
    """eliminate non-integer items from list `lst`
    """
    return  
 --------------------

def f_37004138(lst):
    """get all the elements except strings from the list 'lst'.
    """
    return  
 --------------------

def f_72899(list_to_be_sorted):
    """Sort a list of dictionaries `list_to_be_sorted` by the value of the dictionary key `name`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
numpy.distutils.ccompiler_opt.CCompilerOpt.feature_sorted method   distutils.ccompiler_opt.CCompilerOpt.feature_sorted(names, reverse=False)[source]
 
Sort a list of CPU features ordered by the lowest interest.  Parameters 
 ‘names’: sequence

sequence of supported feature names in uppercase.  ‘reverse’: bool, optional

If true, the sorted features is reversed. (highest interest)    Returns 
 list, sorted CPU features
get_all(name)  
Return a list of all the values for the named header. The returned list will be sorted in the order they appeared in the original header list or were added to this instance, and may contain duplicates. Any fields deleted and re-inserted are always appended to the header list. If no fields exist with the given name, returns an empty list.
sort(*, key=None, reverse=False)  
This method sorts the list in place, using only < comparisons between items. Exceptions are not suppressed - if any comparison operations fail, the entire sort operation will fail (and the list will likely be left in a partially modified state). sort() accepts two arguments that can only be passed by keyword (keyword-only arguments): key specifies a function of one argument that is used to extract a comparison key from each list element (for example, key=str.lower). The key corresponding to each item in the list is calculated once and then used for the entire sorting process. The default value of None means that list items are sorted directly without calculating a separate key value. The functools.cmp_to_key() utility is available to convert a 2.x style cmp function to a key function. reverse is a boolean value. If set to True, then the list elements are sorted as if each comparison were reversed. This method modifies the sequence in place for economy of space when sorting a large sequence. To remind users that it operates by side effect, it does not return the sorted sequence (use sorted() to explicitly request a new sorted list instance). The sort() method is guaranteed to be stable. A sort is stable if it guarantees not to change the relative order of elements that compare equal — this is helpful for sorting in multiple passes (for example, sort by department, then by salary grade). For sorting examples and a brief sorting tutorial, see Sorting HOW TO.  CPython implementation detail: While a list is being sorted, the effect of attempting to mutate, or even inspect, the list is undefined. The C implementation of Python makes the list appear empty for the duration, and raises ValueError if it can detect that the list has been mutated during a sort.
get_all(name, failobj=None)  
Return a list of all the values for the field named name. If there are no such named headers in the message, failobj is returned (defaults to None).
get_all(name, failobj=None)  
Return a list of all the values for the field named name. If there are no such named headers in the message, failobj is returned (defaults to None).
def f_72899(l):
    """sort a list of dictionaries `l` by values in key `name` in descending order
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
sortTestMethodsUsing  
Function to be used to compare method names when sorting them in getTestCaseNames() and all the loadTestsFrom*() methods.
JSON_SORT_KEYS  
Sort the keys of JSON objects alphabetically. This is useful for caching because it ensures the data is serialized the same way no matter what Python’s hash seed is. While not recommended, you can disable this for a possible performance improvement at the cost of caching. Default: True
class werkzeug.datastructures.CombinedMultiDict(dicts=None)  
A read only MultiDict that you can pass multiple MultiDict instances as sequence and it will combine the return values of all wrapped dicts: >>> from werkzeug.datastructures import CombinedMultiDict, MultiDict
>>> post = MultiDict([('foo', 'bar')])
>>> get = MultiDict([('blub', 'blah')])
>>> combined = CombinedMultiDict([get, post])
>>> combined['foo']
'bar'
>>> combined['blub']
'blah'
 This works for all read operations and will raise a TypeError for methods that usually change data which isn’t possible. From Werkzeug 0.3 onwards, the KeyError raised by this class is also a subclass of the BadRequest HTTP exception and will render a page for a 400 BAD REQUEST if caught in a catch-all for HTTP exceptions.
loadTestsFromNames(names, module=None)  
Similar to loadTestsFromName(), but takes a sequence of names rather than a single name. The return value is a test suite which supports all the tests defined for each name.
pprint.pp(object, *args, sort_dicts=False, **kwargs)  
Prints the formatted representation of object followed by a newline. If sort_dicts is false (the default), dictionaries will be displayed with their keys in insertion order, otherwise the dict keys will be sorted. args and kwargs will be passed to pprint() as formatting parameters.  New in version 3.8.
def f_72899(list_of_dicts):
    """sort a list of dictionaries `list_of_dicts` by `name` values of the dictionary
    """
     
 --------------------
Please refer to the following documentation to generate the code:
sort_stats(*keys)  
This method modifies the Stats object by sorting it according to the supplied criteria. The argument can be either a string or a SortKey enum identifying the basis of a sort (example: 'time', 'name', SortKey.TIME or SortKey.NAME). The SortKey enums argument have advantage over the string argument in that it is more robust and less error prone. When more than one key is provided, then additional keys are used as secondary criteria when there is equality in all keys selected before them. For example, sort_stats(SortKey.NAME, SortKey.FILE) will sort all the entries according to their function name, and resolve all ties (identical function names) by sorting by file name. For the string argument, abbreviations can be used for any key names, as long as the abbreviation is unambiguous. The following are the valid string and SortKey:   
Valid String Arg Valid enum Arg Meaning   
'calls' SortKey.CALLS call count  
'cumulative' SortKey.CUMULATIVE cumulative time  
'cumtime' N/A cumulative time  
'file' N/A file name  
'filename' SortKey.FILENAME file name  
'module' N/A file name  
'ncalls' N/A call count  
'pcalls' SortKey.PCALLS primitive call count  
'line' SortKey.LINE line number  
'name' SortKey.NAME function name  
'nfl' SortKey.NFL name/file/line  
'stdname' SortKey.STDNAME standard name  
'time' SortKey.TIME internal time  
'tottime' N/A internal time   Note that all sorts on statistics are in descending order (placing most time consuming items first), where as name, file, and line number searches are in ascending order (alphabetical). The subtle distinction between SortKey.NFL and SortKey.STDNAME is that the standard name is a sort of the name as printed, which means that the embedded line numbers get compared in an odd way. For example, lines 3, 20, and 40 would (if the file names were the same) appear in the string order 20, 3 and 40. In contrast, SortKey.NFL does a numeric compare of the line numbers. In fact, sort_stats(SortKey.NFL) is the same as sort_stats(SortKey.NAME, SortKey.FILENAME, SortKey.LINE). For backward-compatibility reasons, the numeric arguments -1, 0, 1, and 2 are permitted. They are interpreted as 'stdname', 'calls', 'time', and 'cumulative' respectively. If this old style format (numeric) is used, only one sort key (the numeric key) will be used, and additional arguments will be silently ignored.  New in version 3.7: Added the SortKey enum.
set_3d_properties(path, zs=0, zdir='z')[source]
numpy.sign   numpy.sign(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj]) = <ufunc 'sign'>
 
Returns an element-wise indication of the sign of a number. The sign function returns -1 if x < 0, 0 if x==0, 1 if x > 0. nan is returned for nan inputs. For complex inputs, the sign function returns sign(x.real) + 0j if x.real != 0 else sign(x.imag) + 0j. complex(nan, 0) is returned for complex nan inputs.  Parameters 
 
xarray_like


Input values.  
outndarray, None, or tuple of ndarray and None, optional


A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.  
wherearray_like, optional


This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.  **kwargs

For other keyword-only arguments, see the ufunc docs.    Returns 
 
yndarray


The sign of x. This is a scalar if x is a scalar.     Notes There is more than one definition of sign in common use for complex numbers. The definition used here is equivalent to \(x/\sqrt{x*x}\) which is different from a common alternative, \(x/|x|\). Examples >>> np.sign([-5., 4.5])
array([-1.,  1.])
>>> np.sign(0)
0
>>> np.sign(5-2j)
(1+0j)
class werkzeug.datastructures.CombinedMultiDict(dicts=None)  
A read only MultiDict that you can pass multiple MultiDict instances as sequence and it will combine the return values of all wrapped dicts: >>> from werkzeug.datastructures import CombinedMultiDict, MultiDict
>>> post = MultiDict([('foo', 'bar')])
>>> get = MultiDict([('blub', 'blah')])
>>> combined = CombinedMultiDict([get, post])
>>> combined['foo']
'bar'
>>> combined['blub']
'blah'
 This works for all read operations and will raise a TypeError for methods that usually change data which isn’t possible. From Werkzeug 0.3 onwards, the KeyError raised by this class is also a subclass of the BadRequest HTTP exception and will render a page for a 400 BAD REQUEST if caught in a catch-all for HTTP exceptions.
pygame.image
  
 pygame module for image transfer  The image module contains functions for loading and saving pictures, as well as transferring Surfaces to formats usable by other packages. Note that there is no Image class; an image is loaded as a Surface object. The Surface class allows manipulation (drawing lines, setting pixels, capturing regions, etc.). The image module is a required dependency of pygame, but it only optionally supports any extended file formats. By default it can only load uncompressed BMP images. When built with full image support, the pygame.image.load() function can support the following formats.  
 JPG PNG 
GIF (non-animated) BMP PCX 
TGA (uncompressed) TIF 
LBM (and PBM) 
PBM (and PGM, PPM) XPM  
 Saving images only supports a limited set of formats. You can save to the following formats.  
 BMP TGA PNG JPEG  
 JPEG and JPG refer to the same file format  New in pygame 1.8: Saving PNG and JPEG files.    pygame.image.load_basic() 
 load new BMP image from a file (or file-like object) load_basic(file) -> Surface  Load an image from a file source. You can pass either a filename or a Python file-like object. This function only supports loading "basic" image format, ie BMP format. This function is always available, no matter how pygame was built. 
   pygame.image.load() 
 load new image from a file (or file-like object) load(filename) -> Surface load(fileobj, namehint="") -> Surface  Load an image from a file source. You can pass either a filename or a Python file-like object. Pygame will automatically determine the image type (e.g., GIF or bitmap) and create a new Surface object from the data. In some cases it will need to know the file extension (e.g., GIF images should end in ".gif"). If you pass a raw file-like object, you may also want to pass the original filename as the namehint argument. The returned Surface will contain the same color format, colorkey and alpha transparency as the file it came from. You will often want to call Surface.convert() with no arguments, to create a copy that will draw more quickly on the screen. For alpha transparency, like in .png images, use the convert_alpha() method after loading so that the image has per pixel transparency. pygame may not always be built to support all image formats. At minimum it will support uncompressed BMP. If pygame.image.get_extended() returns 'True', you should be able to load most images (including PNG, JPG and GIF). You should use os.path.join() for compatibility. eg. asurf = pygame.image.load(os.path.join('data', 'bla.png')) 
   pygame.image.load_extended() 
 load an image from a file (or file-like object) load_extended(filename) -> Surface load_extended(fileobj, namehint="") -> Surface  This function is similar to pygame.image.load(), except that this function can only be used if pygame was built with extended image format support. From version 2.0.1, this function is always available, but raises an error if extended image formats are not supported. Previously, this function may or may not be available, depending on the state of extended image format support.  Changed in pygame 2.0.1.  
   pygame.image.save() 
 save an image to file (or file-like object) save(Surface, filename) -> None save(Surface, fileobj, namehint="") -> None  This will save your Surface as either a BMP, TGA, PNG, or JPEG image. If the filename extension is unrecognized it will default to TGA. Both TGA, and BMP file formats create uncompressed files. You can pass a filename or a Python file-like object. For file-like object, the image is saved to TGA format unless a namehint with a recognizable extension is passed in.  Note To be able to save the JPEG file format to a file-like object, SDL2_Image version 2.0.2 or newer is needed.   Note When saving to a file-like object, it seems that for most formats, the object needs to be flushed after saving to it to make loading from it possible.   Changed in pygame 1.8: Saving PNG and JPEG files.   Changed in pygame 2.0.0.dev11: The namehint parameter was added to make it possible to save other formats than TGA to a file-like object.  
   pygame.image.save_extended() 
 save a png/jpg image to file (or file-like object) save_extended(Surface, filename) -> None save_extended(Surface, fileobj, namehint="") -> None  This will save your Surface as either a PNG or JPEG image. Incase the image is being saved to a file-like object, this function uses the namehint argument to determine the format of the file being saved. Saves to JPEG incase the namehint was not specified while saving to file-like object. From version 2.0.1, this function is always available, but raises an error if extended image formats are not supported. Previously, this function may or may not be available, depending on the state of extended image format support.  Changed in pygame 2.0.1.  
   pygame.image.get_sdl_image_version() 
 get version number of the SDL_Image library being used get_sdl_image_version() -> None get_sdl_image_version() -> (major, minor, patch)  If pygame is built with extended image formats, then this function will return the SDL_Image library's version number as a tuple of 3 integers (major, minor, patch). If not, then it will return None.  New in pygame 2.0.0.dev11.  
   pygame.image.get_extended() 
 test if extended image formats can be loaded get_extended() -> bool  If pygame is built with extended image formats this function will return True. It is still not possible to determine which formats will be available, but generally you will be able to load them all. 
   pygame.image.tostring() 
 transfer image to string buffer tostring(Surface, format, flipped=False) -> string  Creates a string that can be transferred with the 'fromstring' method in other Python imaging packages. Some Python image packages prefer their images in bottom-to-top format (PyOpenGL for example). If you pass True for the flipped argument, the string buffer will be vertically flipped. The format argument is a string of one of the following values. Note that only 8-bit Surfaces can use the "P" format. The other formats will work for any Surface. Also note that other Python image packages support more formats than pygame.  
 
P, 8-bit palettized Surfaces 
RGB, 24-bit image 
RGBX, 32-bit image with unused space 
RGBA, 32-bit image with an alpha channel 
ARGB, 32-bit image with alpha channel first 
RGBA_PREMULT, 32-bit image with colors scaled by alpha channel 
ARGB_PREMULT, 32-bit image with colors scaled by alpha channel, alpha channel first  
 
   pygame.image.fromstring() 
 create new Surface from a string buffer fromstring(string, size, format, flipped=False) -> Surface  This function takes arguments similar to pygame.image.tostring(). The size argument is a pair of numbers representing the width and height. Once the new Surface is created you can destroy the string buffer. The size and format image must compute the exact same size as the passed string buffer. Otherwise an exception will be raised. See the pygame.image.frombuffer() method for a potentially faster way to transfer images into pygame. 
   pygame.image.frombuffer() 
 create a new Surface that shares data inside a bytes buffer frombuffer(bytes, size, format) -> Surface  Create a new Surface that shares pixel data directly from a bytes buffer. This method takes similar arguments to pygame.image.fromstring(), but is unable to vertically flip the source data. This will run much faster than pygame.image.fromstring(), since no pixel data must be allocated and copied. It accepts the following 'format' arguments:  
 
P, 8-bit palettized Surfaces 
RGB, 24-bit image 
BGR, 24-bit image, red and blue channels swapped. 
RGBX, 32-bit image with unused space 
RGBA, 32-bit image with an alpha channel 
ARGB, 32-bit image with alpha channel first
def f_72899(list_of_dicts):
    """sort a list of dictionaries `list_of_dicts` by `age` values of the dictionary
    """
     
 --------------------
Please refer to the following documentation to generate the code:
pandas.core.resample.Resampler.prod   Resampler.prod(_method='prod', min_count=0, *args, **kwargs)[source]
 
Compute prod of group values.  Parameters 
 
numeric_only:bool, default True


Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data.  
min_count:int, default 0


The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA.    Returns 
 Series or DataFrame

Computed prod of values within each group.
pandas.core.groupby.GroupBy.prod   finalGroupBy.prod(numeric_only=NoDefault.no_default, min_count=0)[source]
 
Compute prod of group values.  Parameters 
 
numeric_only:bool, default True


Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data.  
min_count:int, default 0


The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA.    Returns 
 Series or DataFrame

Computed prod of values within each group.
tf.compat.v1.train.RMSPropOptimizer Optimizer that implements the RMSProp algorithm (Tielemans et al. Inherits From: Optimizer 
tf.compat.v1.train.RMSPropOptimizer(
    learning_rate, decay=0.9, momentum=0.0, epsilon=1e-10, use_locking=False,
    centered=False, name='RMSProp'
)
 2012). References: Coursera slide 29: Hinton, 2012 (pdf)
 


 Args
  learning_rate   A Tensor or a floating point value. The learning rate.  
  decay   Discounting factor for the history/coming gradient  
  momentum   A scalar tensor.  
  epsilon   Small value to avoid zero denominator.  
  use_locking   If True use locks for update operation.  
  centered   If True, gradients are normalized by the estimated variance of the gradient; if False, by the uncentered second moment. Setting this to True may help with training, but is slightly more expensive in terms of computation and memory. Defaults to False.  
  name   Optional name prefix for the operations created when applying gradients. Defaults to "RMSProp".    Methods apply_gradients View source 
apply_gradients(
    grads_and_vars, global_step=None, name=None
)
 Apply gradients to variables. This is the second part of minimize(). It returns an Operation that applies gradients.
 


 Args
  grads_and_vars   List of (gradient, variable) pairs as returned by compute_gradients().  
  global_step   Optional Variable to increment by one after the variables have been updated.  
  name   Optional name for the returned operation. Default to the name passed to the Optimizer constructor.   
 


 Returns   An Operation that applies the specified gradients. If global_step was not None, that operation also increments global_step.  

 


 Raises
  TypeError   If grads_and_vars is malformed.  
  ValueError   If none of the variables have gradients.  
  RuntimeError   If you should use _distributed_apply() instead.    compute_gradients View source 
compute_gradients(
    loss, var_list=None, gate_gradients=GATE_OP, aggregation_method=None,
    colocate_gradients_with_ops=False, grad_loss=None
)
 Compute gradients of loss for the variables in var_list. This is the first part of minimize(). It returns a list of (gradient, variable) pairs where "gradient" is the gradient for "variable". Note that "gradient" can be a Tensor, an IndexedSlices, or None if there is no gradient for the given variable.
 


 Args
  loss   A Tensor containing the value to minimize or a callable taking no arguments which returns the value to minimize. When eager execution is enabled it must be a callable.  
  var_list   Optional list or tuple of tf.Variable to update to minimize loss. Defaults to the list of variables collected in the graph under the key GraphKeys.TRAINABLE_VARIABLES.  
  gate_gradients   How to gate the computation of gradients. Can be GATE_NONE, GATE_OP, or GATE_GRAPH.  
  aggregation_method   Specifies the method used to combine gradient terms. Valid values are defined in the class AggregationMethod.  
  colocate_gradients_with_ops   If True, try colocating gradients with the corresponding op.  
  grad_loss   Optional. A Tensor holding the gradient computed for loss.   
 


 Returns   A list of (gradient, variable) pairs. Variable is always present, but gradient can be None.  

 


 Raises
  TypeError   If var_list contains anything else than Variable objects.  
  ValueError   If some arguments are invalid.  
  RuntimeError   If called with eager execution enabled and loss is not callable.    Eager Compatibility When eager execution is enabled, gate_gradients, aggregation_method, and colocate_gradients_with_ops are ignored. get_name View source 
get_name()
 get_slot View source 
get_slot(
    var, name
)
 Return a slot named name created for var by the Optimizer. Some Optimizer subclasses use additional variables. For example Momentum and Adagrad use variables to accumulate updates. This method gives access to these Variable objects if for some reason you need them. Use get_slot_names() to get the list of slot names created by the Optimizer.
 


 Args
  var   A variable passed to minimize() or apply_gradients().  
  name   A string.   
 


 Returns   The Variable for the slot if it was created, None otherwise.  
 get_slot_names View source 
get_slot_names()
 Return a list of the names of slots created by the Optimizer. See get_slot().
 


 Returns   A list of strings.  
 minimize View source 
minimize(
    loss, global_step=None, var_list=None, gate_gradients=GATE_OP,
    aggregation_method=None, colocate_gradients_with_ops=False, name=None,
    grad_loss=None
)
 Add operations to minimize loss by updating var_list. This method simply combines calls compute_gradients() and apply_gradients(). If you want to process the gradient before applying them call compute_gradients() and apply_gradients() explicitly instead of using this function.
 


 Args
  loss   A Tensor containing the value to minimize.  
  global_step   Optional Variable to increment by one after the variables have been updated.  
  var_list   Optional list or tuple of Variable objects to update to minimize loss. Defaults to the list of variables collected in the graph under the key GraphKeys.TRAINABLE_VARIABLES.  
  gate_gradients   How to gate the computation of gradients. Can be GATE_NONE, GATE_OP, or GATE_GRAPH.  
  aggregation_method   Specifies the method used to combine gradient terms. Valid values are defined in the class AggregationMethod.  
  colocate_gradients_with_ops   If True, try colocating gradients with the corresponding op.  
  name   Optional name for the returned operation.  
  grad_loss   Optional. A Tensor holding the gradient computed for loss.   
 


 Returns   An Operation that updates the variables in var_list. If global_step was not None, that operation also increments global_step.  

 


 Raises
  ValueError   If some of the variables are not Variable objects.    Eager Compatibility When eager execution is enabled, loss should be a Python function that takes no arguments and computes the value to be minimized. Minimization (and gradient computation) is done with respect to the elements of var_list if not None, else with respect to any trainable variables created during the execution of the loss function. gate_gradients, aggregation_method, colocate_gradients_with_ops and grad_loss are ignored when eager execution is enabled. variables View source 
variables()
 A list of variables which encode the current state of Optimizer. Includes slot variables and additional global variables created by the optimizer in the current default graph.
 


 Returns   A list of variables.  

 


 Class Variables
  GATE_GRAPH   2  
  GATE_NONE   0  
  GATE_OP   1
mpl_toolkits.mplot3d.proj3d.proj_points   mpl_toolkits.mplot3d.proj3d.proj_points(points, M)[source]
class flask.sessions.NullSession(initial=None)  
Class used to generate nicer error messages if sessions are not available. Will still allow read-only access to the empty session but fail on setting.  Parameters 
initial (Any) –   Return type 
None
def f_36402748(df):
    """sort a Dataframe `df` by the total ocurrences in a column 'scores' group by 'prots'
    """
    return  
 --------------------

def f_29881993(trans):
    """join together with "," elements inside a list indexed with 'category' within a dictionary `trans`
    """
    return  
 --------------------

def f_34158494():
    """concatenate array of strings `['A', 'B', 'C', 'D']` into a string
    """
    return  
 --------------------

def f_12666897(sents):
    """Remove all strings from a list a strings `sents` where the values starts with `@$\t` or `#`
    """
    return  
 --------------------

def f_5944630(list):
    """sort a list of dictionary `list` first by key `points` and then by `time`
    """
     
 --------------------
Please refer to the following documentation to generate the code:
datetime.timestamp()  
Return POSIX timestamp corresponding to the datetime instance. The return value is a float similar to that returned by time.time(). Naive datetime instances are assumed to represent local time and this method relies on the platform C mktime() function to perform the conversion. Since datetime supports wider range of values than mktime() on many platforms, this method may raise OverflowError for times far in the past or far in the future. For aware datetime instances, the return value is computed as: (dt - datetime(1970, 1, 1, tzinfo=timezone.utc)).total_seconds()
  New in version 3.3.   Changed in version 3.6: The timestamp() method uses the fold attribute to disambiguate the times during a repeated interval.   Note There is no method to obtain the POSIX timestamp directly from a naive datetime instance representing UTC time. If your application uses this convention and your system timezone is not set to UTC, you can obtain the POSIX timestamp by supplying tzinfo=timezone.utc: timestamp = dt.replace(tzinfo=timezone.utc).timestamp()
 or by calculating the timestamp directly: timestamp = (dt - datetime(1970, 1, 1)) / timedelta(seconds=1)
matplotlib.dates.num2epoch(d)[source]
 
[Deprecated] Convert days since Matplotlib epoch to UNIX time.  Parameters 
 
dlist of floats


Time in days since Matplotlib epoch (see get_epoch()).    Returns 
 numpy.array

Time in seconds since 1970-01-01.     Notes  Deprecated since version 3.5.
timedelta.total_seconds()  
Return the total number of seconds contained in the duration. Equivalent to td / timedelta(seconds=1). For interval units other than seconds, use the division form directly (e.g. td / timedelta(microseconds=1)). Note that for very large time intervals (greater than 270 years on most platforms) this method will lose microsecond accuracy.  New in version 3.2.
pandas.Timedelta.seconds   Timedelta.seconds
 
Number of seconds (>= 0 and less than 1 day).
pandas.Timedelta.total_seconds   Timedelta.total_seconds()
 
Total seconds in the duration.
def f_7852855():
    """convert datetime object `(1970, 1, 1)` to seconds
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
suffix_map  
Dictionary mapping suffixes to suffixes. This is used to allow recognition of encoded files for which the encoding and the type are indicated by the same extension. For example, the .tgz extension is mapped to .tar.gz to allow the encoding and type to be recognized separately. This is initially a copy of the global suffix_map defined in the module.
mimetypes.suffix_map  
Dictionary mapping suffixes to suffixes. This is used to allow recognition of encoded files for which the encoding and the type are indicated by the same extension. For example, the .tgz extension is mapped to .tar.gz to allow the encoding and type to be recognized separately.
pandas.DataFrame.add_suffix   DataFrame.add_suffix(suffix)[source]
 
Suffix labels with string suffix. For Series, the row labels are suffixed. For DataFrame, the column labels are suffixed.  Parameters 
 
suffix:str


The string to add after each label.    Returns 
 Series or DataFrame

New Series or DataFrame with updated labels.      See also  Series.add_prefix

Prefix row labels with string prefix.  DataFrame.add_prefix

Prefix column labels with string prefix.    Examples 
>>> s = pd.Series([1, 2, 3, 4])
>>> s
0    1
1    2
2    3
3    4
dtype: int64
  
>>> s.add_suffix('_item')
0_item    1
1_item    2
2_item    3
3_item    4
dtype: int64
  
>>> df = pd.DataFrame({'A': [1, 2, 3, 4], 'B': [3, 4, 5, 6]})
>>> df
   A  B
0  1  3
1  2  4
2  3  5
3  4  6
  
>>> df.add_suffix('_col')
     A_col  B_col
0       1       3
1       2       4
2       3       5
3       4       6
get_alternative_name(file_root, file_ext)  
Returns an alternative filename based on the file_root and file_ext parameters, an underscore plus a random 7 character alphanumeric string is appended to the filename before the extension.
stat.UF_APPEND  
The file may only be appended to.
def f_2763750():
    """insert `_suff` before the file extension in `long.file.name.jpg` or replace `_a` with `suff` if it precedes the extension.
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
importlib.reload(module)  
Reload a previously imported module. The argument must be a module object, so it must have been successfully imported before. This is useful if you have edited the module source file using an external editor and want to try out the new version without leaving the Python interpreter. The return value is the module object (which can be different if re-importing causes a different object to be placed in sys.modules). When reload() is executed:  Python module’s code is recompiled and the module-level code re-executed, defining a new set of objects which are bound to names in the module’s dictionary by reusing the loader which originally loaded the module. The init function of extension modules is not called a second time. As with all other objects in Python the old objects are only reclaimed after their reference counts drop to zero. The names in the module namespace are updated to point to any new or changed objects. Other references to the old objects (such as names external to the module) are not rebound to refer to the new objects and must be updated in each namespace where they occur if that is desired.  There are a number of other caveats: When a module is reloaded, its dictionary (containing the module’s global variables) is retained. Redefinitions of names will override the old definitions, so this is generally not a problem. If the new version of a module does not define a name that was defined by the old version, the old definition remains. This feature can be used to the module’s advantage if it maintains a global table or cache of objects — with a try statement it can test for the table’s presence and skip its initialization if desired: try:
    cache
except NameError:
    cache = {}
 It is generally not very useful to reload built-in or dynamically loaded modules. Reloading sys, __main__, builtins and other key modules is not recommended. In many cases extension modules are not designed to be initialized more than once, and may fail in arbitrary ways when reloaded. If a module imports objects from another module using from … import …, calling reload() for the other module does not redefine the objects imported from it — one way around this is to re-execute the from statement, another is to use import and qualified names (module.name) instead. If a module instantiates instances of a class, reloading the module that defines the class does not affect the method definitions of the instances — they continue to use the old class definition. The same is true for derived classes.  New in version 3.4.   Changed in version 3.7: ModuleNotFoundError is raised when the module being reloaded lacks a ModuleSpec.
exec_module(module)  
Concrete implementation of Loader.exec_module().  New in version 3.4.
exec_module(module)  
Implementation of Loader.exec_module().  New in version 3.4.
exec_module(module)  
An abstract method that executes the module in its own namespace when a module is imported or reloaded. The module should already be initialized when exec_module() is called. When this method exists, create_module() must be defined.  New in version 3.4.   Changed in version 3.6: create_module() must also be defined.
test.support.forget(module_name)  
Remove the module named module_name from sys.modules and delete any byte-compiled files of the module.
def f_6420361(module):
    """reload a module `module`
    """
     
 --------------------
Please refer to the following documentation to generate the code:
operator.inv(obj)  
operator.invert(obj)  
operator.__inv__(obj)  
operator.__invert__(obj)  
Return the bitwise inverse of the number obj. This is equivalent to ~obj.
operator.inv(obj)  
operator.invert(obj)  
operator.__inv__(obj)  
operator.__invert__(obj)  
Return the bitwise inverse of the number obj. This is equivalent to ~obj.
operator.inv(obj)  
operator.invert(obj)  
operator.__inv__(obj)  
operator.__invert__(obj)  
Return the bitwise inverse of the number obj. This is equivalent to ~obj.
operator.inv(obj)  
operator.invert(obj)  
operator.__inv__(obj)  
operator.__invert__(obj)  
Return the bitwise inverse of the number obj. This is equivalent to ~obj.
numpy.number.__class_getitem__ method   number.__class_getitem__(item, /)
 
Return a parametrized wrapper around the number type.  New in version 1.22.   Returns 
 
aliastypes.GenericAlias


A parametrized number type.      See also  PEP 585

Type hinting generics in standard collections.    Notes This method is only available for python 3.9 and later. Examples >>> from typing import Any
>>> import numpy as np
 >>> np.signedinteger[Any]
numpy.signedinteger[typing.Any]
def f_19546911(number):
    """Convert integer `number` into an unassigned integer
    """
    return  
 --------------------

def f_9746522(numlist):
    """convert int values in list `numlist` to float
    """
     
 --------------------
Please refer to the following documentation to generate the code:
pandas.io.stata.StataWriter.write_file   StataWriter.write_file()[source]
 
Export DataFrame object to Stata dta format.
pandas.errors.DtypeWarning   exceptionpandas.errors.DtypeWarning[source]
 
Warning raised when reading different dtypes in a column from a file. Raised for a dtype incompatibility. This can happen whenever read_csv or read_table encounter non-uniform dtypes in a column(s) of a given CSV file.  See also  read_csv

Read CSV (comma-separated) file into a DataFrame.  read_table

Read general delimited file into a DataFrame.    Notes This warning is issued when dealing with larger files because the dtype checking happens per chunk read. Despite the warning, the CSV file is read with mixed types in a single column which will be an object type. See the examples below to better understand this issue. Examples This example creates and reads a large CSV file with a column that contains int and str. 
>>> df = pd.DataFrame({'a': (['1'] * 100000 + ['X'] * 100000 +
...                          ['1'] * 100000),
...                    'b': ['b'] * 300000})  
>>> df.to_csv('test.csv', index=False)  
>>> df2 = pd.read_csv('test.csv')  
... # DtypeWarning: Columns (0) have mixed types
  Important to notice that df2 will contain both str and int for the same input, ‘1’. 
>>> df2.iloc[262140, 0]  
'1'
>>> type(df2.iloc[262140, 0])  
<class 'str'>
>>> df2.iloc[262150, 0]  
1
>>> type(df2.iloc[262150, 0])  
<class 'int'>
  One way to solve this issue is using the dtype parameter in the read_csv and read_table functions to explicit the conversion: 
>>> df2 = pd.read_csv('test.csv', sep=',', dtype={'a': str})  
  No warning was issued.
class werkzeug.middleware.shared_data.SharedDataMiddleware(app, exports, disallow=None, cache=True, cache_timeout=43200, fallback_mimetype='application/octet-stream')  
A WSGI middleware which provides static content for development environments or simple server setups. Its usage is quite simple: import os
from werkzeug.middleware.shared_data import SharedDataMiddleware

app = SharedDataMiddleware(app, {
    '/shared': os.path.join(os.path.dirname(__file__), 'shared')
})
 The contents of the folder ./shared will now be available on http://example.com/shared/. This is pretty useful during development because a standalone media server is not required. Files can also be mounted on the root folder and still continue to use the application because the shared data middleware forwards all unhandled requests to the application, even if the requests are below one of the shared folders. If pkg_resources is available you can also tell the middleware to serve files from package data: app = SharedDataMiddleware(app, {
    '/static': ('myapplication', 'static')
})
 This will then serve the static folder in the myapplication Python package. The optional disallow parameter can be a list of fnmatch() rules for files that are not accessible from the web. If cache is set to False no caching headers are sent. Currently the middleware does not support non-ASCII filenames. If the encoding on the file system happens to match the encoding of the URI it may work but this could also be by accident. We strongly suggest using ASCII only file names for static files. The middleware will guess the mimetype using the Python mimetype module. If it’s unable to figure out the charset it will fall back to fallback_mimetype.  Parameters 
 
app (WSGIApplication) – the application to wrap. If you don’t want to wrap an application you can pass it NotFound. 
exports (Union[Dict[str, Union[str, Tuple[str, str]]], Iterable[Tuple[str, Union[str, Tuple[str, str]]]]]) – a list or dict of exported files and folders. 
disallow (None) – a list of fnmatch() rules. 
cache (bool) – enable or disable caching headers. 
cache_timeout (int) – the cache timeout in seconds for the headers. 
fallback_mimetype (str) – The fallback mimetype for unknown files.   Return type 
None    Changelog Changed in version 1.0: The default fallback_mimetype is application/octet-stream. If a filename looks like a text mimetype, the utf-8 charset is added to it.   New in version 0.6: Added fallback_mimetype.   Changed in version 0.5: Added cache_timeout.   
is_allowed(filename)  
Subclasses can override this method to disallow the access to certain files. However by providing disallow in the constructor this method is overwritten.  Parameters 
filename (str) –   Return type 
bool
tf.train.SequenceExample     View source on GitHub    A ProtocolMessage  View aliases  Compat aliases for migration 
See Migration guide for more details. tf.compat.v1.train.SequenceExample 
 


 Attributes
  context   Features context  
  feature_lists   FeatureLists feature_lists
pandas.DataFrame.to_csv   DataFrame.to_csv(path_or_buf=None, sep=',', na_rep='', float_format=None, columns=None, header=True, index=True, index_label=None, mode='w', encoding=None, compression='infer', quoting=None, quotechar='"', line_terminator=None, chunksize=None, date_format=None, doublequote=True, escapechar=None, decimal='.', errors='strict', storage_options=None)[source]
 
Write object to a comma-separated values (csv) file.  Parameters 
 
path_or_buf:str, path object, file-like object, or None, default None


String, path object (implementing os.PathLike[str]), or file-like object implementing a write() function. If None, the result is returned as a string. If a non-binary file object is passed, it should be opened with newline=’’, disabling universal newlines. If a binary file object is passed, mode might need to contain a ‘b’.  Changed in version 1.2.0: Support for binary file objects was introduced.   
sep:str, default ‘,’


String of length 1. Field delimiter for the output file.  
na_rep:str, default ‘’


Missing data representation.  
float_format:str, default None


Format string for floating point numbers.  
columns:sequence, optional


Columns to write.  
header:bool or list of str, default True


Write out the column names. If a list of strings is given it is assumed to be aliases for the column names.  
index:bool, default True


Write row names (index).  
index_label:str or sequence, or False, default None


Column label for index column(s) if desired. If None is given, and header and index are True, then the index names are used. A sequence should be given if the object uses MultiIndex. If False do not print fields for index names. Use index_label=False for easier importing in R.  
mode:str


Python write mode, default ‘w’.  
encoding:str, optional


A string representing the encoding to use in the output file, defaults to ‘utf-8’. encoding is not supported if path_or_buf is a non-binary file object.  
compression:str or dict, default ‘infer’


For on-the-fly compression of the output data. If ‘infer’ and ‘%s’ path-like, then detect compression from the following extensions: ‘.gz’, ‘.bz2’, ‘.zip’, ‘.xz’, or ‘.zst’ (otherwise no compression). Set to None for no compression. Can also be a dict with key 'method' set to one of {'zip', 'gzip', 'bz2', 'zstd'} and other key-value pairs are forwarded to zipfile.ZipFile, gzip.GzipFile, bz2.BZ2File, or zstandard.ZstdDecompressor, respectively. As an example, the following could be passed for faster compression and to create a reproducible gzip archive: compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}.  Changed in version 1.0.0: May now be a dict with key ‘method’ as compression mode and other entries as additional compression options if compression mode is ‘zip’.   Changed in version 1.1.0: Passing compression options as keys in dict is supported for compression modes ‘gzip’, ‘bz2’, ‘zstd’, and ‘zip’.   Changed in version 1.2.0: Compression is supported for binary file objects.   Changed in version 1.2.0: Previous versions forwarded dict entries for ‘gzip’ to gzip.open instead of gzip.GzipFile which prevented setting mtime.   
quoting:optional constant from csv module


Defaults to csv.QUOTE_MINIMAL. If you have set a float_format then floats are converted to strings and thus csv.QUOTE_NONNUMERIC will treat them as non-numeric.  
quotechar:str, default ‘"’


String of length 1. Character used to quote fields.  
line_terminator:str, optional


The newline character or character sequence to use in the output file. Defaults to os.linesep, which depends on the OS in which this method is called (’\n’ for linux, ‘\r\n’ for Windows, i.e.).  
chunksize:int or None


Rows to write at a time.  
date_format:str, default None


Format string for datetime objects.  
doublequote:bool, default True


Control quoting of quotechar inside a field.  
escapechar:str, default None


String of length 1. Character used to escape sep and quotechar when appropriate.  
decimal:str, default ‘.’


Character recognized as decimal separator. E.g. use ‘,’ for European data.  
errors:str, default ‘strict’


Specifies how encoding and decoding errors are to be handled. See the errors argument for open() for a full list of options.  New in version 1.1.0.   
storage_options:dict, optional


Extra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib as header options. For other URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are forwarded to fsspec. Please see fsspec and urllib for more details.  New in version 1.2.0.     Returns 
 None or str

If path_or_buf is None, returns the resulting csv format as a string. Otherwise returns None.      See also  read_csv

Load a CSV file into a DataFrame.  to_excel

Write DataFrame to an Excel file.    Examples 
>>> df = pd.DataFrame({'name': ['Raphael', 'Donatello'],
...                    'mask': ['red', 'purple'],
...                    'weapon': ['sai', 'bo staff']})
>>> df.to_csv(index=False)
'name,mask,weapon\nRaphael,red,sai\nDonatello,purple,bo staff\n'
  Create ‘out.zip’ containing ‘out.csv’ 
>>> compression_opts = dict(method='zip',
...                         archive_name='out.csv')  
>>> df.to_csv('out.zip', index=False,
...           compression=compression_opts)  
  To write a csv file to a new folder or nested folder you will first need to create it using either Pathlib or os: 
>>> from pathlib import Path  
>>> filepath = Path('folder/subfolder/out.csv')  
>>> filepath.parent.mkdir(parents=True, exist_ok=True)  
>>> df.to_csv(filepath)  
  
>>> import os  
>>> os.makedirs('folder/subfolder', exist_ok=True)  
>>> df.to_csv('folder/subfolder/out.csv')
def f_20107570(df, filename):
    """write dataframe `df`, excluding index, to a csv file `filename`
    """
     
 --------------------
Please refer to the following documentation to generate the code:
decode(s)  
Return the Python representation of s (a str instance containing a JSON document). JSONDecodeError will be raised if the given JSON document is not valid.
urllib.parse.unquote_to_bytes(string)  
Replace %xx escapes with their single-octet equivalent, and return a bytes object. string may be either a str or a bytes object. If it is a str, unescaped non-ASCII characters in string are encoded into UTF-8 bytes. Example: unquote_to_bytes('a%26%EF') yields b'a&\xef'.
urllib.parse.unquote(string, encoding='utf-8', errors='replace')  
Replace %xx escapes with their single-character equivalent. The optional encoding and errors parameters specify how to decode percent-encoded sequences into Unicode characters, as accepted by the bytes.decode() method. string may be either a str or a bytes object. encoding defaults to 'utf-8'. errors defaults to 'replace', meaning invalid sequences are replaced by a placeholder character. Example: unquote('/El%20Ni%C3%B1o/') yields '/El Niño/'.  Changed in version 3.9: string parameter supports bytes and str objects (previously only str).
werkzeug.urls.url_unquote(s, charset='utf-8', errors='replace', unsafe='')  
URL decode a single string with a given encoding. If the charset is set to None no decoding is performed and raw bytes are returned.  Parameters 
 
s (Union[str, bytes]) – the string to unquote. 
charset (str) – the charset of the query string. If set to None no decoding will take place. 
errors (str) – the error handling for the charset decoding. 
unsafe (str) –    Return type 
str
xml.sax.saxutils.unescape(data, entities={})  
Unescape '&amp;', '&lt;', and '&gt;' in a string of data. You can unescape other strings of data by passing a dictionary as the optional entities parameter. The keys and values must all be strings; each key will be replaced with its corresponding value. '&amp', '&lt;', and '&gt;' are always unescaped, even if entities is provided.
def f_8740353(unescaped):
    """convert a urllib unquoted string `unescaped` to a json data `json_data`
    """
     
 --------------------

def f_5891453():
    """Create a list containing all ascii characters as its elements
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
Path.write_bytes(data)  
Open the file pointed to in bytes mode, write data to it, and close the file: >>> p = Path('my_binary_file')
>>> p.write_bytes(b'Binary file contents')
20
>>> p.read_bytes()
b'Binary file contents'
 An existing file of the same name is overwritten.  New in version 3.5.
Path.read_bytes()  
Return the binary contents of the pointed-to file as a bytes object: >>> p = Path('my_binary_file')
>>> p.write_bytes(b'Binary file contents')
20
>>> p.read_bytes()
b'Binary file contents'
  New in version 3.5.
set_data(path, data)  
Optional abstract method which writes the specified bytes to a file path. Any intermediate directories which do not exist are to be created automatically. When writing to the path fails because the path is read-only (errno.EACCES/PermissionError), do not propagate the exception.  Changed in version 3.4: No longer raises NotImplementedError when called.
class urllib.parse.SplitResultBytes(scheme, netloc, path, query, fragment)  
Concrete class for urlsplit() results containing bytes data. The decode() method returns a SplitResult instance.  New in version 3.2.
Path.read_bytes()  
Read the current file as bytes.
def f_18367007(newFileBytes, newFile):
    """write `newFileBytes` to a binary file `newFile`
    """
     
 --------------------
Please refer to the following documentation to generate the code:
str.istitle()  
Return True if the string is a titlecased string and there is at least one character, for example uppercase characters may only follow uncased characters and lowercase characters only cased ones. Return False otherwise.
str.islower()  
Return True if all cased characters 4 in the string are lowercase and there is at least one cased character, False otherwise.
str.isupper()  
Return True if all cased characters 4 in the string are uppercase and there is at least one cased character, False otherwise. >>> 'BANANA'.isupper()
True
>>> 'banana'.isupper()
False
>>> 'baNana'.isupper()
False
>>> ' '.isupper()
False
numpy.chararray.isupper method   chararray.isupper()[source]
 
Returns true for each element if all cased characters in the string are uppercase and there is at least one character, false otherwise.  See also  char.isupper
bytes.islower()  
bytearray.islower()  
Return True if there is at least one lowercase ASCII character in the sequence and no uppercase ASCII characters, False otherwise. For example: >>> b'hello world'.islower()
True
>>> b'Hello world'.islower()
False
 Lowercase ASCII characters are those byte values in the sequence b'abcdefghijklmnopqrstuvwxyz'. Uppercase ASCII characters are those byte values in the sequence b'ABCDEFGHIJKLMNOPQRSTUVWXYZ'.
def f_21805490(string):
    """python regex - check for a capital letter with a following lowercase in string `string`
    """
    return  
 --------------------

def f_16125229(dict):
    """get the last key of dictionary `dict`
    """
    return  
 --------------------

def f_6159900(f):
    """write line "hi there" to file `f`
    """
    return  
 --------------------

def f_6159900(myfile):
    """write line "hi there" to file `myfile`
    """
     
 --------------------

def f_6159900():
    """write line "Hello" to file `somefile.txt`
    """
     
 --------------------

def f_19527279(s):
    """convert unicode string `s` to ascii
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
re.S  
re.DOTALL  
Make the '.' special character match any character at all, including a newline; without this flag, '.' will match anything except a newline. Corresponds to the inline flag (?s).
re.S  
re.DOTALL  
Make the '.' special character match any character at all, including a newline; without this flag, '.' will match anything except a newline. Corresponds to the inline flag (?s).
locale.LC_NUMERIC  
Locale category for formatting numbers. The functions format(), atoi(), atof() and str() of the locale module are affected by that category. All other numeric formatting operations are not affected.
Match.span([group])  
For a match m, return the 2-tuple (m.start(group), m.end(group)). Note that if group did not contribute to the match, this is (-1, -1). group defaults to zero, the entire match.
set_fontsize(fontsize)[source]
 
Set the font size.  Parameters 
 
fontsizefloat or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}


If float, the fontsize in points. The string values denote sizes relative to the default font size.      See also  font_manager.FontProperties.set_size
def f_356483(text):
    """Find all numbers and dots from a string `text` using regex
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
psname
 
Alias for field number 1
classmatplotlib.dviread.PsFont(texname, psname, effects, encoding, filename)[source]
 
Bases: tuple Create new instance of PsFont(texname, psname, effects, encoding, filename)   effects
 
Alias for field number 2 
   encoding
 
Alias for field number 3 
   filename
 
Alias for field number 4 
   psname
 
Alias for field number 1 
   texname
 
Alias for field number 0
msvcrt — Useful routines from the MS VC++ runtime These functions provide access to some useful capabilities on Windows platforms. Some higher-level modules use these functions to build the Windows implementations of their services. For example, the getpass module uses this in the implementation of the getpass() function. Further documentation on these functions can be found in the Platform API documentation. The module implements both the normal and wide char variants of the console I/O api. The normal API deals only with ASCII characters and is of limited use for internationalized applications. The wide char API should be used where ever possible.  Changed in version 3.3: Operations in this module now raise OSError where IOError was raised.  File Operations  
msvcrt.locking(fd, mode, nbytes)  
Lock part of a file based on file descriptor fd from the C runtime. Raises OSError on failure. The locked region of the file extends from the current file position for nbytes bytes, and may continue beyond the end of the file. mode must be one of the LK_* constants listed below. Multiple regions in a file may be locked at the same time, but may not overlap. Adjacent regions are not merged; they must be unlocked individually. Raises an auditing event msvcrt.locking with arguments fd, mode, nbytes. 
  
msvcrt.LK_LOCK  
msvcrt.LK_RLCK  
Locks the specified bytes. If the bytes cannot be locked, the program immediately tries again after 1 second. If, after 10 attempts, the bytes cannot be locked, OSError is raised. 
  
msvcrt.LK_NBLCK  
msvcrt.LK_NBRLCK  
Locks the specified bytes. If the bytes cannot be locked, OSError is raised. 
  
msvcrt.LK_UNLCK  
Unlocks the specified bytes, which must have been previously locked. 
  
msvcrt.setmode(fd, flags)  
Set the line-end translation mode for the file descriptor fd. To set it to text mode, flags should be os.O_TEXT; for binary, it should be os.O_BINARY. 
  
msvcrt.open_osfhandle(handle, flags)  
Create a C runtime file descriptor from the file handle handle. The flags parameter should be a bitwise OR of os.O_APPEND, os.O_RDONLY, and os.O_TEXT. The returned file descriptor may be used as a parameter to os.fdopen() to create a file object. Raises an auditing event msvcrt.open_osfhandle with arguments handle, flags. 
  
msvcrt.get_osfhandle(fd)  
Return the file handle for the file descriptor fd. Raises OSError if fd is not recognized. Raises an auditing event msvcrt.get_osfhandle with argument fd. 
 Console I/O  
msvcrt.kbhit()  
Return True if a keypress is waiting to be read. 
  
msvcrt.getch()  
Read a keypress and return the resulting character as a byte string. Nothing is echoed to the console. This call will block if a keypress is not already available, but will not wait for Enter to be pressed. If the pressed key was a special function key, this will return '\000' or '\xe0'; the next call will return the keycode. The Control-C keypress cannot be read with this function. 
  
msvcrt.getwch()  
Wide char variant of getch(), returning a Unicode value. 
  
msvcrt.getche()  
Similar to getch(), but the keypress will be echoed if it represents a printable character. 
  
msvcrt.getwche()  
Wide char variant of getche(), returning a Unicode value. 
  
msvcrt.putch(char)  
Print the byte string char to the console without buffering. 
  
msvcrt.putwch(unicode_char)  
Wide char variant of putch(), accepting a Unicode value. 
  
msvcrt.ungetch(char)  
Cause the byte string char to be “pushed back” into the console buffer; it will be the next character read by getch() or getche(). 
  
msvcrt.ungetwch(unicode_char)  
Wide char variant of ungetch(), accepting a Unicode value. 
 Other Functions  
msvcrt.heapmin()  
Force the malloc() heap to clean itself up and return unused blocks to the operating system. On failure, this raises OSError.
executescript(sql_script)  
This is a nonstandard shortcut that creates a cursor object by calling the cursor() method, calls the cursor’s executescript() method with the given sql_script, and returns the cursor.
winreg — Windows registry access These functions expose the Windows registry API to Python. Instead of using an integer as the registry handle, a handle object is used to ensure that the handles are closed correctly, even if the programmer neglects to explicitly close them.  Changed in version 3.3: Several functions in this module used to raise a WindowsError, which is now an alias of OSError.  Functions This module offers the following functions:  
winreg.CloseKey(hkey)  
Closes a previously opened registry key. The hkey argument specifies a previously opened key.  Note If hkey is not closed using this method (or via hkey.Close()), it is closed when the hkey object is destroyed by Python.  
  
winreg.ConnectRegistry(computer_name, key)  
Establishes a connection to a predefined registry handle on another computer, and returns a handle object. computer_name is the name of the remote computer, of the form r"\\computername". If None, the local computer is used. key is the predefined handle to connect to. The return value is the handle of the opened key. If the function fails, an OSError exception is raised. Raises an auditing event winreg.ConnectRegistry with arguments computer_name, key.  Changed in version 3.3: See above.  
  
winreg.CreateKey(key, sub_key)  
Creates or opens the specified key, returning a handle object. key is an already open key, or one of the predefined HKEY_* constants. sub_key is a string that names the key this method opens or creates. If key is one of the predefined keys, sub_key may be None. In that case, the handle returned is the same key handle passed in to the function. If the key already exists, this function opens the existing key. The return value is the handle of the opened key. If the function fails, an OSError exception is raised. Raises an auditing event winreg.CreateKey with arguments key, sub_key, access. Raises an auditing event winreg.OpenKey/result with argument key.  Changed in version 3.3: See above.  
  
winreg.CreateKeyEx(key, sub_key, reserved=0, access=KEY_WRITE)  
Creates or opens the specified key, returning a handle object. key is an already open key, or one of the predefined HKEY_* constants. sub_key is a string that names the key this method opens or creates. reserved is a reserved integer, and must be zero. The default is zero. access is an integer that specifies an access mask that describes the desired security access for the key. Default is KEY_WRITE. See Access Rights for other allowed values. If key is one of the predefined keys, sub_key may be None. In that case, the handle returned is the same key handle passed in to the function. If the key already exists, this function opens the existing key. The return value is the handle of the opened key. If the function fails, an OSError exception is raised. Raises an auditing event winreg.CreateKey with arguments key, sub_key, access. Raises an auditing event winreg.OpenKey/result with argument key.  New in version 3.2.   Changed in version 3.3: See above.  
  
winreg.DeleteKey(key, sub_key)  
Deletes the specified key. key is an already open key, or one of the predefined HKEY_* constants. sub_key is a string that must be a subkey of the key identified by the key parameter. This value must not be None, and the key may not have subkeys. This method can not delete keys with subkeys. If the method succeeds, the entire key, including all of its values, is removed. If the method fails, an OSError exception is raised. Raises an auditing event winreg.DeleteKey with arguments key, sub_key, access.  Changed in version 3.3: See above.  
  
winreg.DeleteKeyEx(key, sub_key, access=KEY_WOW64_64KEY, reserved=0)  
Deletes the specified key.  Note The DeleteKeyEx() function is implemented with the RegDeleteKeyEx Windows API function, which is specific to 64-bit versions of Windows. See the RegDeleteKeyEx documentation.  key is an already open key, or one of the predefined HKEY_* constants. sub_key is a string that must be a subkey of the key identified by the key parameter. This value must not be None, and the key may not have subkeys. reserved is a reserved integer, and must be zero. The default is zero. access is an integer that specifies an access mask that describes the desired security access for the key. Default is KEY_WOW64_64KEY. See Access Rights for other allowed values. This method can not delete keys with subkeys. If the method succeeds, the entire key, including all of its values, is removed. If the method fails, an OSError exception is raised. On unsupported Windows versions, NotImplementedError is raised. Raises an auditing event winreg.DeleteKey with arguments key, sub_key, access.  New in version 3.2.   Changed in version 3.3: See above.  
  
winreg.DeleteValue(key, value)  
Removes a named value from a registry key. key is an already open key, or one of the predefined HKEY_* constants. value is a string that identifies the value to remove. Raises an auditing event winreg.DeleteValue with arguments key, value. 
  
winreg.EnumKey(key, index)  
Enumerates subkeys of an open registry key, returning a string. key is an already open key, or one of the predefined HKEY_* constants. index is an integer that identifies the index of the key to retrieve. The function retrieves the name of one subkey each time it is called. It is typically called repeatedly until an OSError exception is raised, indicating, no more values are available. Raises an auditing event winreg.EnumKey with arguments key, index.  Changed in version 3.3: See above.  
  
winreg.EnumValue(key, index)  
Enumerates values of an open registry key, returning a tuple. key is an already open key, or one of the predefined HKEY_* constants. index is an integer that identifies the index of the value to retrieve. The function retrieves the name of one subkey each time it is called. It is typically called repeatedly, until an OSError exception is raised, indicating no more values. The result is a tuple of 3 items:   
Index Meaning   
0 A string that identifies the value name  
1 An object that holds the value data, and whose type depends on the underlying registry type  
2 An integer that identifies the type of the value data (see table in docs for SetValueEx())   Raises an auditing event winreg.EnumValue with arguments key, index.  Changed in version 3.3: See above.  
  
winreg.ExpandEnvironmentStrings(str)  
Expands environment variable placeholders %NAME% in strings like REG_EXPAND_SZ: >>> ExpandEnvironmentStrings('%windir%')
'C:\\Windows'
 Raises an auditing event winreg.ExpandEnvironmentStrings with argument str. 
  
winreg.FlushKey(key)  
Writes all the attributes of a key to the registry. key is an already open key, or one of the predefined HKEY_* constants. It is not necessary to call FlushKey() to change a key. Registry changes are flushed to disk by the registry using its lazy flusher. Registry changes are also flushed to disk at system shutdown. Unlike CloseKey(), the FlushKey() method returns only when all the data has been written to the registry. An application should only call FlushKey() if it requires absolute certainty that registry changes are on disk.  Note If you don’t know whether a FlushKey() call is required, it probably isn’t.  
  
winreg.LoadKey(key, sub_key, file_name)  
Creates a subkey under the specified key and stores registration information from a specified file into that subkey. key is a handle returned by ConnectRegistry() or one of the constants HKEY_USERS or HKEY_LOCAL_MACHINE. sub_key is a string that identifies the subkey to load. file_name is the name of the file to load registry data from. This file must have been created with the SaveKey() function. Under the file allocation table (FAT) file system, the filename may not have an extension. A call to LoadKey() fails if the calling process does not have the SE_RESTORE_PRIVILEGE privilege. Note that privileges are different from permissions – see the RegLoadKey documentation for more details. If key is a handle returned by ConnectRegistry(), then the path specified in file_name is relative to the remote computer. Raises an auditing event winreg.LoadKey with arguments key, sub_key, file_name. 
  
winreg.OpenKey(key, sub_key, reserved=0, access=KEY_READ)  
winreg.OpenKeyEx(key, sub_key, reserved=0, access=KEY_READ)  
Opens the specified key, returning a handle object. key is an already open key, or one of the predefined HKEY_* constants. sub_key is a string that identifies the sub_key to open. reserved is a reserved integer, and must be zero. The default is zero. access is an integer that specifies an access mask that describes the desired security access for the key. Default is KEY_READ. See Access Rights for other allowed values. The result is a new handle to the specified key. If the function fails, OSError is raised. Raises an auditing event winreg.OpenKey with arguments key, sub_key, access. Raises an auditing event winreg.OpenKey/result with argument key.  Changed in version 3.2: Allow the use of named arguments.   Changed in version 3.3: See above.  
  
winreg.QueryInfoKey(key)  
Returns information about a key, as a tuple. key is an already open key, or one of the predefined HKEY_* constants. The result is a tuple of 3 items:   
Index Meaning   
0 An integer giving the number of sub keys this key has.  
1 An integer giving the number of values this key has.  
2 An integer giving when the key was last modified (if available) as 100’s of nanoseconds since Jan 1, 1601.   Raises an auditing event winreg.QueryInfoKey with argument key. 
  
winreg.QueryValue(key, sub_key)  
Retrieves the unnamed value for a key, as a string. key is an already open key, or one of the predefined HKEY_* constants. sub_key is a string that holds the name of the subkey with which the value is associated. If this parameter is None or empty, the function retrieves the value set by the SetValue() method for the key identified by key. Values in the registry have name, type, and data components. This method retrieves the data for a key’s first value that has a NULL name. But the underlying API call doesn’t return the type, so always use QueryValueEx() if possible. Raises an auditing event winreg.QueryValue with arguments key, sub_key, value_name. 
  
winreg.QueryValueEx(key, value_name)  
Retrieves the type and data for a specified value name associated with an open registry key. key is an already open key, or one of the predefined HKEY_* constants. value_name is a string indicating the value to query. The result is a tuple of 2 items:   
Index Meaning   
0 The value of the registry item.  
1 An integer giving the registry type for this value (see table in docs for SetValueEx())   Raises an auditing event winreg.QueryValue with arguments key, sub_key, value_name. 
  
winreg.SaveKey(key, file_name)  
Saves the specified key, and all its subkeys to the specified file. key is an already open key, or one of the predefined HKEY_* constants. file_name is the name of the file to save registry data to. This file cannot already exist. If this filename includes an extension, it cannot be used on file allocation table (FAT) file systems by the LoadKey() method. If key represents a key on a remote computer, the path described by file_name is relative to the remote computer. The caller of this method must possess the SeBackupPrivilege security privilege. Note that privileges are different than permissions – see the Conflicts Between User Rights and Permissions documentation for more details. This function passes NULL for security_attributes to the API. Raises an auditing event winreg.SaveKey with arguments key, file_name. 
  
winreg.SetValue(key, sub_key, type, value)  
Associates a value with a specified key. key is an already open key, or one of the predefined HKEY_* constants. sub_key is a string that names the subkey with which the value is associated. type is an integer that specifies the type of the data. Currently this must be REG_SZ, meaning only strings are supported. Use the SetValueEx() function for support for other data types. value is a string that specifies the new value. If the key specified by the sub_key parameter does not exist, the SetValue function creates it. Value lengths are limited by available memory. Long values (more than 2048 bytes) should be stored as files with the filenames stored in the configuration registry. This helps the registry perform efficiently. The key identified by the key parameter must have been opened with KEY_SET_VALUE access. Raises an auditing event winreg.SetValue with arguments key, sub_key, type, value. 
  
winreg.SetValueEx(key, value_name, reserved, type, value)  
Stores data in the value field of an open registry key. key is an already open key, or one of the predefined HKEY_* constants. value_name is a string that names the subkey with which the value is associated. reserved can be anything – zero is always passed to the API. type is an integer that specifies the type of the data. See Value Types for the available types. value is a string that specifies the new value. This method can also set additional value and type information for the specified key. The key identified by the key parameter must have been opened with KEY_SET_VALUE access. To open the key, use the CreateKey() or OpenKey() methods. Value lengths are limited by available memory. Long values (more than 2048 bytes) should be stored as files with the filenames stored in the configuration registry. This helps the registry perform efficiently. Raises an auditing event winreg.SetValue with arguments key, sub_key, type, value. 
  
winreg.DisableReflectionKey(key)  
Disables registry reflection for 32-bit processes running on a 64-bit operating system. key is an already open key, or one of the predefined HKEY_* constants. Will generally raise NotImplementedError if executed on a 32-bit operating system. If the key is not on the reflection list, the function succeeds but has no effect. Disabling reflection for a key does not affect reflection of any subkeys. Raises an auditing event winreg.DisableReflectionKey with argument key. 
  
winreg.EnableReflectionKey(key)  
Restores registry reflection for the specified disabled key. key is an already open key, or one of the predefined HKEY_* constants. Will generally raise NotImplementedError if executed on a 32-bit operating system. Restoring reflection for a key does not affect reflection of any subkeys. Raises an auditing event winreg.EnableReflectionKey with argument key. 
  
winreg.QueryReflectionKey(key)  
Determines the reflection state for the specified key. key is an already open key, or one of the predefined HKEY_* constants. Returns True if reflection is disabled. Will generally raise NotImplementedError if executed on a 32-bit operating system. Raises an auditing event winreg.QueryReflectionKey with argument key. 
 Constants The following constants are defined for use in many _winreg functions. HKEY_* Constants  
winreg.HKEY_CLASSES_ROOT  
Registry entries subordinate to this key define types (or classes) of documents and the properties associated with those types. Shell and COM applications use the information stored under this key. 
  
winreg.HKEY_CURRENT_USER  
Registry entries subordinate to this key define the preferences of the current user. These preferences include the settings of environment variables, data about program groups, colors, printers, network connections, and application preferences. 
  
winreg.HKEY_LOCAL_MACHINE  
Registry entries subordinate to this key define the physical state of the computer, including data about the bus type, system memory, and installed hardware and software. 
  
winreg.HKEY_USERS  
Registry entries subordinate to this key define the default user configuration for new users on the local computer and the user configuration for the current user. 
  
winreg.HKEY_PERFORMANCE_DATA  
Registry entries subordinate to this key allow you to access performance data. The data is not actually stored in the registry; the registry functions cause the system to collect the data from its source. 
  
winreg.HKEY_CURRENT_CONFIG  
Contains information about the current hardware profile of the local computer system. 
  
winreg.HKEY_DYN_DATA  
This key is not used in versions of Windows after 98. 
 Access Rights For more information, see Registry Key Security and Access.  
winreg.KEY_ALL_ACCESS  
Combines the STANDARD_RIGHTS_REQUIRED, KEY_QUERY_VALUE, KEY_SET_VALUE, KEY_CREATE_SUB_KEY, KEY_ENUMERATE_SUB_KEYS, KEY_NOTIFY, and KEY_CREATE_LINK access rights. 
  
winreg.KEY_WRITE  
Combines the STANDARD_RIGHTS_WRITE, KEY_SET_VALUE, and KEY_CREATE_SUB_KEY access rights. 
  
winreg.KEY_READ  
Combines the STANDARD_RIGHTS_READ, KEY_QUERY_VALUE, KEY_ENUMERATE_SUB_KEYS, and KEY_NOTIFY values. 
  
winreg.KEY_EXECUTE  
Equivalent to KEY_READ. 
  
winreg.KEY_QUERY_VALUE  
Required to query the values of a registry key. 
  
winreg.KEY_SET_VALUE  
Required to create, delete, or set a registry value. 
  
winreg.KEY_CREATE_SUB_KEY  
Required to create a subkey of a registry key. 
  
winreg.KEY_ENUMERATE_SUB_KEYS  
Required to enumerate the subkeys of a registry key. 
  
winreg.KEY_NOTIFY  
Required to request change notifications for a registry key or for subkeys of a registry key. 
  
winreg.KEY_CREATE_LINK  
Reserved for system use. 
 64-bit Specific For more information, see Accessing an Alternate Registry View.  
winreg.KEY_WOW64_64KEY  
Indicates that an application on 64-bit Windows should operate on the 64-bit registry view. 
  
winreg.KEY_WOW64_32KEY  
Indicates that an application on 64-bit Windows should operate on the 32-bit registry view. 
 Value Types For more information, see Registry Value Types.  
winreg.REG_BINARY  
Binary data in any form. 
  
winreg.REG_DWORD  
32-bit number. 
  
winreg.REG_DWORD_LITTLE_ENDIAN  
A 32-bit number in little-endian format. Equivalent to REG_DWORD. 
  
winreg.REG_DWORD_BIG_ENDIAN  
A 32-bit number in big-endian format. 
  
winreg.REG_EXPAND_SZ  
Null-terminated string containing references to environment variables (%PATH%). 
  
winreg.REG_LINK  
A Unicode symbolic link. 
  
winreg.REG_MULTI_SZ  
A sequence of null-terminated strings, terminated by two null characters. (Python handles this termination automatically.) 
  
winreg.REG_NONE  
No defined value type. 
  
winreg.REG_QWORD  
A 64-bit number.  New in version 3.6.  
  
winreg.REG_QWORD_LITTLE_ENDIAN  
A 64-bit number in little-endian format. Equivalent to REG_QWORD.  New in version 3.6.  
  
winreg.REG_RESOURCE_LIST  
A device-driver resource list. 
  
winreg.REG_FULL_RESOURCE_DESCRIPTOR  
A hardware setting. 
  
winreg.REG_RESOURCE_REQUIREMENTS_LIST  
A hardware resource list. 
  
winreg.REG_SZ  
A null-terminated string. 
 Registry Handle Objects This object wraps a Windows HKEY object, automatically closing it when the object is destroyed. To guarantee cleanup, you can call either the Close() method on the object, or the CloseKey() function. All registry functions in this module return one of these objects. All registry functions in this module which accept a handle object also accept an integer, however, use of the handle object is encouraged. Handle objects provide semantics for __bool__() – thus if handle:
    print("Yes")
 will print Yes if the handle is currently valid (has not been closed or detached). The object also support comparison semantics, so handle objects will compare true if they both reference the same underlying Windows handle value. Handle objects can be converted to an integer (e.g., using the built-in int() function), in which case the underlying Windows handle value is returned. You can also use the Detach() method to return the integer handle, and also disconnect the Windows handle from the handle object.  
PyHKEY.Close()  
Closes the underlying Windows handle. If the handle is already closed, no error is raised. 
  
PyHKEY.Detach()  
Detaches the Windows handle from the handle object. The result is an integer that holds the value of the handle before it is detached. If the handle is already detached or closed, this will return zero. After calling this function, the handle is effectively invalidated, but the handle is not closed. You would call this function when you need the underlying Win32 handle to exist beyond the lifetime of the handle object. Raises an auditing event winreg.PyHKEY.Detach with argument key. 
  
PyHKEY.__enter__()  
PyHKEY.__exit__(*exc_info)  
The HKEY object implements __enter__() and __exit__() and thus supports the context protocol for the with statement: with OpenKey(HKEY_LOCAL_MACHINE, "foo") as key:
    ...  # work with key
 will automatically close key when control leaves the with block.
def f_38081866():
    """execute script 'script.ps1' using 'powershell.exe' shell
    """
    return  
 --------------------

def f_7349646(b):
    """Sort a list of tuples `b` by third item in the tuple
    """
     
 --------------------
Please refer to the following documentation to generate the code:
classmethod datetime.utcnow()  
Return the current UTC date and time, with tzinfo None. This is like now(), but returns the current UTC date and time, as a naive datetime object. An aware current UTC datetime can be obtained by calling datetime.now(timezone.utc). See also now().  Warning Because naive datetime objects are treated by many datetime methods as local times, it is preferred to use aware datetimes to represent times in UTC. As such, the recommended way to create an object representing the current time in UTC is by calling datetime.now(timezone.utc).
classmethod datetime.today()  
Return the current local datetime, with tzinfo None. Equivalent to: datetime.fromtimestamp(time.time())
 See also now(), fromtimestamp(). This method is functionally equivalent to now(), but without a tz parameter.
classmethod datetime.now(tz=None)  
Return the current local date and time. If optional argument tz is None or not specified, this is like today(), but, if possible, supplies more precision than can be gotten from going through a time.time() timestamp (for example, this may be possible on platforms supplying the C gettimeofday() function). If tz is not None, it must be an instance of a tzinfo subclass, and the current date and time are converted to tz’s time zone. This function is preferred over today() and utcnow().
now()  
Returns a datetime that represents the current point in time. Exactly what’s returned depends on the value of USE_TZ:  If USE_TZ is False, this will be a naive datetime (i.e. a datetime without an associated timezone) that represents the current time in the system’s local timezone. If USE_TZ is True, this will be an aware datetime representing the current time in UTC. Note that now() will always return times in UTC regardless of the value of TIME_ZONE; you can use localtime() to get the time in the current time zone.
classmethod date.today()  
Return the current local date. This is equivalent to date.fromtimestamp(time.time()).
def f_10607688():
    """create a datetime with the current date & time
    """
    return  
 --------------------

def f_30843103(lst):
    """get the index of an integer `1` from a list `lst` if the list also contains boolean items
    """
    return  
 --------------------

def f_4918425(a):
    """subtract 13 from every number in a list `a`
    """
     
 --------------------
Please refer to the following documentation to generate the code:
numpy.matrix.argmax method   matrix.argmax(axis=None, out=None)[source]
 
Indexes of the maximum values along an axis. Return the indexes of the first occurrences of the maximum values along the specified axis. If axis is None, the index is for the flattened matrix.  Parameters 
 See `numpy.argmax` for complete descriptions
    See also  numpy.argmax
  Notes This is the same as ndarray.argmax, but returns a matrix object where ndarray.argmax would return an ndarray. Examples >>> x = np.matrix(np.arange(12).reshape((3,4))); x
matrix([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11]])
>>> x.argmax()
11
>>> x.argmax(0)
matrix([[2, 2, 2, 2]])
>>> x.argmax(1)
matrix([[3],
        [3],
        [3]])
numpy.matrix.max method   matrix.max(axis=None, out=None)[source]
 
Return the maximum value along an axis.  Parameters 
 See `amax` for complete descriptions
    See also  
amax, ndarray.max

  Notes This is the same as ndarray.max, but returns a matrix object where ndarray.max would return an ndarray. Examples >>> x = np.matrix(np.arange(12).reshape((3,4))); x
matrix([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11]])
>>> x.max()
11
>>> x.max(0)
matrix([[ 8,  9, 10, 11]])
>>> x.max(1)
matrix([[ 3],
        [ 7],
        [11]])
numpy.ndarray.max method   ndarray.max(axis=None, out=None, keepdims=False, initial=<no value>, where=True)
 
Return the maximum along a given axis. Refer to numpy.amax for full documentation.  See also  numpy.amax

equivalent function
numpy.record.argmax method   record.argmax()
 
Scalar method identical to the corresponding array attribute. Please see ndarray.argmax.
abs(x)  
Return the absolute value of a number. The argument may be an integer, a floating point number, or an object implementing __abs__(). If the argument is a complex number, its magnitude is returned.
def f_17794266(x):
    """get the highest element in absolute value in a numpy matrix `x`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
get_urls()[source]
 
Return a list of URLs, one for each element of the collection. The list contains None for elements without a URL. See Hyperlinks for an example.
get_glyphs_tex(prop, s, glyph_map=None, return_new_glyphs_only=False)[source]
 
Convert the string s to vertices and codes using usetex mode.
get_glyphs_with_font(font, s, glyph_map=None, return_new_glyphs_only=False)[source]
 
Convert string s to vertices and codes using the provided ttf font.
get_urls()[source]
 
Return a list of URLs, one for each element of the collection. The list contains None for elements without a URL. See Hyperlinks for an example.
get_urls()[source]
 
Return a list of URLs, one for each element of the collection. The list contains None for elements without a URL. See Hyperlinks for an example.
def f_30551576(s):
    """Get all urls within text `s`
    """
    return  
 --------------------

def f_113534(mystring):
    """split a string `mystring` considering the spaces ' '
    """
    return  
 --------------------

def f_5838735(path):
    """open file `path` with mode 'r'
    """
    return  
 --------------------

def f_36003967(data):
    """sum elements at the same index in list `data`
    """
    return  
 --------------------
Please refer to the following documentation to generate the code:
matplotlib.axes.Axes.add_child_axes   Axes.add_child_axes(ax)[source]
 
Add an AxesBase to the Axes' children; return the child Axes. This is the lowlevel version. See axes.Axes.inset_axes.
numpy.newaxis
 
A convenient alias for None, useful for indexing arrays. Examples >>> newaxis is None
True
>>> x = np.arange(3)
>>> x
array([0, 1, 2])
>>> x[:, newaxis]
array([[0],
[1],
[2]])
>>> x[:, newaxis, newaxis]
array([[[0]],
[[1]],
[[2]]])
>>> x[:, newaxis] * x
array([[0, 0, 0],
[0, 1, 2],
[0, 2, 4]])
 Outer product, same as outer(x, y): >>> y = np.arange(3, 6)
>>> x[:, newaxis] * y
array([[ 0,  0,  0],
[ 3,  4,  5],
[ 6,  8, 10]])
 x[newaxis, :] is equivalent to x[newaxis] and x[None]: >>> x[newaxis, :].shape
(1, 3)
>>> x[newaxis].shape
(1, 3)
>>> x[None].shape
(1, 3)
>>> x[:, newaxis].shape
(3, 1)
register_axis(axis)[source]
 
Register an axis. An axis should be registered with its corresponding spine from the Axes instance. This allows the spine to clear any axis properties when needed.
set_axis(axis)[source]
append_axes(position, size, pad=None, add_to_figure=<deprecated parameter>, **kwargs)[source]
 
Create an axes at the given position with the same height (or width) of the main axes.  position

["left"|"right"|"bottom"|"top"]   size and pad should be axes_grid.axes_size compatible.
def f_7635237(a):
    """add a new axis to array `a`
    """
    return  
 --------------------
  0%|          | 1/439 [00:01<09:57,  1.36s/it]  0%|          | 2/439 [00:01<06:07,  1.19it/s]  1%|          | 3/439 [00:01<03:49,  1.90it/s]  1%|          | 4/439 [00:02<03:40,  1.97it/s]  1%|          | 5/439 [00:03<03:54,  1.85it/s]  1%|▏         | 6/439 [00:03<03:58,  1.82it/s]  2%|▏         | 7/439 [00:03<03:03,  2.36it/s]  2%|▏         | 8/439 [00:03<02:22,  3.03it/s]  2%|▏         | 9/439 [00:04<02:37,  2.73it/s]  2%|▏         | 10/439 [01:37<3:27:12, 28.98s/it]  3%|▎         | 11/439 [01:38<2:25:17, 20.37s/it]  3%|▎         | 12/439 [01:38<1:41:27, 14.26s/it]  3%|▎         | 13/439 [01:38<1:10:50,  9.98s/it]  3%|▎         | 14/439 [01:38<49:55,  7.05s/it]    3%|▎         | 15/439 [01:39<35:53,  5.08s/it]  4%|▎         | 16/439 [02:54<3:03:24, 26.02s/it]  4%|▍         | 17/439 [02:54<2:08:29, 18.27s/it]  4%|▍         | 18/439 [02:55<1:31:25, 13.03s/it]  4%|▍         | 19/439 [02:56<1:05:42,  9.39s/it]  5%|▍         | 20/439 [02:56<46:36,  6.67s/it]    5%|▍         | 21/439 [02:57<34:52,  5.01s/it]  5%|▌         | 22/439 [02:58<25:15,  3.64s/it]  5%|▌         | 23/439 [02:58<18:16,  2.64s/it]  5%|▌         | 24/439 [03:58<2:17:29, 19.88s/it]  6%|▌         | 25/439 [04:15<2:12:16, 19.17s/it]  6%|▌         | 26/439 [04:16<1:32:35, 13.45s/it]  6%|▌         | 27/439 [04:17<1:08:06,  9.92s/it]  6%|▋         | 28/439 [04:18<49:14,  7.19s/it]    7%|▋         | 29/439 [04:18<35:00,  5.12s/it]  7%|▋         | 30/439 [04:19<25:34,  3.75s/it]  7%|▋         | 31/439 [04:19<18:07,  2.67s/it]  7%|▋         | 32/439 [05:47<3:11:26, 28.22s/it]  8%|▊         | 33/439 [05:48<2:16:28, 20.17s/it]  8%|▊         | 34/439 [05:49<1:36:34, 14.31s/it]  8%|▊         | 35/439 [06:38<2:47:21, 24.86s/it]  8%|▊         | 36/439 [06:39<1:57:28, 17.49s/it]  8%|▊         | 37/439 [06:40<1:23:59, 12.54s/it]  9%|▊         | 38/439 [06:40<1:00:09,  9.00s/it]  9%|▉         | 39/439 [06:41<42:37,  6.39s/it]    9%|▉         | 40/439 [06:41<31:07,  4.68s/it]  9%|▉         | 41/439 [06:42<22:54,  3.45s/it] 10%|▉         | 42/439 [06:42<16:30,  2.49s/it] 10%|▉         | 43/439 [06:43<12:34,  1.90s/it] 10%|█         | 44/439 [06:44<10:24,  1.58s/it] 10%|█         | 45/439 [06:44<07:31,  1.15s/it] 10%|█         | 46/439 [06:44<06:05,  1.07it/s] 11%|█         | 47/439 [06:45<06:41,  1.02s/it] 11%|█         | 48/439 [06:46<05:52,  1.11it/s] 11%|█         | 49/439 [06:46<04:48,  1.35it/s] 11%|█▏        | 50/439 [06:47<04:40,  1.39it/s] 12%|█▏        | 51/439 [06:47<03:31,  1.84it/s] 12%|█▏        | 52/439 [08:17<2:55:53, 27.27s/it] 12%|█▏        | 53/439 [08:17<2:04:04, 19.29s/it] 12%|█▏        | 54/439 [08:18<1:28:04, 13.73s/it] 13%|█▎        | 55/439 [08:19<1:02:04,  9.70s/it] 13%|█▎        | 56/439 [09:59<3:55:16, 36.86s/it] 13%|█▎        | 57/439 [09:59<2:44:31, 25.84s/it] 13%|█▎        | 58/439 [09:59<1:55:03, 18.12s/it] 13%|█▎        | 59/439 [10:00<1:21:31, 12.87s/it] 14%|█▎        | 60/439 [10:00<57:32,  9.11s/it]   14%|█▍        | 61/439 [10:00<40:44,  6.47s/it] 14%|█▍        | 62/439 [10:01<30:13,  4.81s/it] 14%|█▍        | 63/439 [11:27<3:01:49, 29.02s/it] 15%|█▍        | 64/439 [11:27<2:07:28, 20.40s/it] 15%|█▍        | 65/439 [13:03<4:28:05, 43.01s/it] 15%|█▌        | 66/439 [13:03<3:07:27, 30.15s/it] 15%|█▌        | 67/439 [13:03<2:11:58, 21.29s/it] 16%|█▌        | 69/439 [13:05<1:12:31, 11.76s/it] 16%|█▌        | 70/439 [13:05<54:51,  8.92s/it]   16%|█▌        | 71/439 [13:05<40:36,  6.62s/it] 16%|█▋        | 72/439 [13:06<30:01,  4.91s/it] 17%|█▋        | 73/439 [13:06<22:17,  3.66s/it] 17%|█▋        | 74/439 [13:07<17:11,  2.83s/it] 17%|█▋        | 75/439 [13:08<13:39,  2.25s/it] 17%|█▋        | 76/439 [13:08<10:57,  1.81s/it] 18%|█▊        | 77/439 [13:08<07:56,  1.32s/it] 18%|█▊        | 78/439 [13:09<05:48,  1.04it/s] 18%|█▊        | 79/439 [13:09<05:04,  1.18it/s] 18%|█▊        | 80/439 [13:09<03:46,  1.58it/s] 18%|█▊        | 81/439 [13:10<04:31,  1.32it/s] 19%|█▊        | 82/439 [13:10<03:23,  1.75it/s] 19%|█▉        | 83/439 [14:42<2:44:54, 27.79s/it] 19%|█▉        | 84/439 [15:15<2:54:37, 29.52s/it] 19%|█▉        | 85/439 [15:16<2:02:35, 20.78s/it] 20%|█▉        | 86/439 [15:16<1:26:22, 14.68s/it] 20%|█▉        | 87/439 [15:16<1:00:32, 10.32s/it] 20%|██        | 88/439 [15:17<43:42,  7.47s/it]   20%|██        | 89/439 [15:35<1:01:08, 10.48s/it] 21%|██        | 90/439 [15:35<42:54,  7.38s/it]   21%|██        | 91/439 [15:35<30:10,  5.20s/it] 21%|██        | 92/439 [15:36<22:37,  3.91s/it] 21%|██        | 93/439 [15:37<17:02,  2.95s/it] 21%|██▏       | 94/439 [17:10<2:53:13, 30.12s/it] 22%|██▏       | 95/439 [18:26<4:11:06, 43.80s/it] 22%|██▏       | 96/439 [18:27<2:56:36, 30.89s/it] 22%|██▏       | 97/439 [18:27<2:03:29, 21.66s/it] 22%|██▏       | 98/439 [18:27<1:26:37, 15.24s/it] 23%|██▎       | 99/439 [18:28<1:01:50, 10.91s/it] 23%|██▎       | 100/439 [18:29<44:49,  7.93s/it]  23%|██▎       | 101/439 [18:29<31:32,  5.60s/it] 23%|██▎       | 102/439 [18:29<22:14,  3.96s/it] 23%|██▎       | 103/439 [18:29<15:55,  2.84s/it] 24%|██▎       | 104/439 [18:30<11:31,  2.06s/it] 24%|██▍       | 105/439 [18:30<08:15,  1.48s/it] 24%|██▍       | 106/439 [18:30<05:58,  1.08s/it] 24%|██▍       | 107/439 [18:32<07:34,  1.37s/it] 25%|██▍       | 108/439 [18:32<05:30,  1.00it/s] 25%|██▍       | 109/439 [18:32<04:20,  1.27it/s] 25%|██▌       | 110/439 [18:34<05:01,  1.09it/s] 25%|██▌       | 111/439 [18:34<04:19,  1.26it/s] 26%|██▌       | 112/439 [18:34<03:13,  1.69it/s] 26%|██▌       | 113/439 [18:34<02:42,  2.00it/s] 26%|██▌       | 114/439 [18:36<04:15,  1.27it/s] 26%|██▌       | 115/439 [18:38<05:58,  1.11s/it] 26%|██▋       | 116/439 [18:38<04:44,  1.13it/s] 27%|██▋       | 117/439 [18:38<03:47,  1.41it/s] 27%|██▋       | 118/439 [18:39<03:00,  1.78it/s] 27%|██▋       | 119/439 [18:39<02:57,  1.80it/s] 27%|██▋       | 120/439 [18:40<02:34,  2.07it/s] 28%|██▊       | 121/439 [18:41<03:27,  1.53it/s] 28%|██▊       | 122/439 [18:41<03:52,  1.36it/s] 28%|██▊       | 123/439 [18:43<04:58,  1.06it/s] 28%|██▊       | 124/439 [18:45<06:42,  1.28s/it] 28%|██▊       | 125/439 [18:45<04:53,  1.07it/s] 29%|██▊       | 126/439 [18:45<03:50,  1.36it/s] 29%|██▉       | 127/439 [18:46<03:18,  1.57it/s] 29%|██▉       | 128/439 [20:03<2:01:52, 23.51s/it] 29%|██▉       | 129/439 [20:03<1:25:53, 16.63s/it] 30%|██▉       | 130/439 [20:04<1:00:51, 11.82s/it] 30%|██▉       | 131/439 [20:04<43:04,  8.39s/it]   30%|███       | 132/439 [20:05<31:18,  6.12s/it] 30%|███       | 133/439 [20:06<22:49,  4.48s/it] 31%|███       | 134/439 [20:07<17:31,  3.45s/it] 31%|███       | 135/439 [20:07<13:14,  2.61s/it] 31%|███       | 136/439 [20:08<09:35,  1.90s/it] 31%|███       | 137/439 [21:23<2:00:40, 23.97s/it] 31%|███▏      | 138/439 [21:24<1:25:08, 16.97s/it] 32%|███▏      | 139/439 [21:24<1:00:11, 12.04s/it] 32%|███▏      | 140/439 [21:24<42:11,  8.47s/it]   32%|███▏      | 141/439 [21:25<29:35,  5.96s/it] 32%|███▏      | 142/439 [21:25<20:50,  4.21s/it] 33%|███▎      | 143/439 [21:25<15:10,  3.08s/it] 33%|███▎      | 144/439 [22:25<1:38:31, 20.04s/it] 33%|███▎      | 145/439 [22:25<1:08:55, 14.07s/it] 33%|███▎      | 146/439 [22:25<49:02, 10.04s/it]   33%|███▎      | 147/439 [22:27<35:56,  7.38s/it] 34%|███▎      | 148/439 [22:28<26:58,  5.56s/it] 34%|███▍      | 149/439 [22:29<19:39,  4.07s/it] 34%|███▍      | 150/439 [22:29<14:41,  3.05s/it] 34%|███▍      | 151/439 [22:31<13:25,  2.80s/it] 35%|███▍      | 152/439 [23:51<2:03:37, 25.85s/it] 35%|███▍      | 153/439 [23:52<1:27:13, 18.30s/it] 35%|███▌      | 154/439 [24:28<1:52:35, 23.70s/it] 35%|███▌      | 155/439 [24:29<1:19:38, 16.83s/it] 36%|███▌      | 156/439 [24:29<55:58, 11.87s/it]   36%|███▌      | 157/439 [24:29<39:24,  8.38s/it] 36%|███▌      | 158/439 [24:30<28:11,  6.02s/it] 36%|███▌      | 159/439 [24:30<20:26,  4.38s/it] 36%|███▋      | 160/439 [24:31<14:26,  3.10s/it] 37%|███▋      | 161/439 [24:31<11:03,  2.39s/it] 37%|███▋      | 162/439 [24:32<08:24,  1.82s/it] 37%|███▋      | 163/439 [24:32<06:02,  1.31s/it] 37%|███▋      | 164/439 [24:32<04:23,  1.04it/s] 38%|███▊      | 165/439 [24:32<03:15,  1.40it/s] 38%|███▊      | 166/439 [24:33<03:18,  1.38it/s] 38%|███▊      | 167/439 [24:34<03:34,  1.27it/s] 38%|███▊      | 168/439 [24:34<03:04,  1.47it/s] 38%|███▊      | 169/439 [24:35<02:53,  1.56it/s] 39%|███▊      | 170/439 [24:35<02:45,  1.62it/s] 39%|███▉      | 171/439 [24:36<02:23,  1.86it/s] 39%|███▉      | 172/439 [24:36<02:02,  2.18it/s] 39%|███▉      | 173/439 [24:37<02:11,  2.02it/s] 40%|███▉      | 174/439 [24:38<03:51,  1.14it/s] 40%|███▉      | 175/439 [24:39<03:25,  1.29it/s] 40%|████      | 176/439 [26:21<2:16:29, 31.14s/it] 40%|████      | 177/439 [26:22<1:36:05, 22.01s/it] 41%|████      | 178/439 [26:22<1:07:10, 15.44s/it] 41%|████      | 179/439 [26:22<47:14, 10.90s/it]   41%|████      | 180/439 [26:22<33:07,  7.67s/it] 41%|████      | 181/439 [27:27<1:46:05, 24.67s/it] 41%|████▏     | 182/439 [27:27<1:14:40, 17.43s/it] 42%|████▏     | 183/439 [27:28<53:07, 12.45s/it]   42%|████▏     | 184/439 [27:28<37:29,  8.82s/it] 42%|████▏     | 185/439 [27:29<26:33,  6.27s/it] 42%|████▏     | 186/439 [27:29<19:40,  4.67s/it] 43%|████▎     | 187/439 [27:30<14:28,  3.45s/it] 43%|████▎     | 188/439 [27:30<10:15,  2.45s/it] 43%|████▎     | 189/439 [27:30<07:26,  1.79s/it] 43%|████▎     | 190/439 [27:31<05:46,  1.39s/it] 44%|████▎     | 191/439 [27:31<04:11,  1.01s/it] 44%|████▎     | 192/439 [27:31<03:04,  1.34it/s] 44%|████▍     | 193/439 [28:50<1:38:29, 24.02s/it] 44%|████▍     | 194/439 [30:27<3:08:29, 46.16s/it] 44%|████▍     | 195/439 [30:29<2:12:50, 32.67s/it] 45%|████▍     | 196/439 [30:29<1:33:17, 23.04s/it] 45%|████▍     | 197/439 [30:29<1:05:11, 16.16s/it] 45%|████▌     | 198/439 [30:47<1:06:31, 16.56s/it] 45%|████▌     | 199/439 [30:47<46:31, 11.63s/it]   46%|████▌     | 200/439 [30:47<33:11,  8.33s/it] 46%|████▌     | 201/439 [30:48<23:17,  5.87s/it] 46%|████▌     | 202/439 [30:48<16:26,  4.16s/it] 46%|████▌     | 203/439 [30:48<12:08,  3.09s/it] 46%|████▋     | 204/439 [32:23<1:59:16, 30.45s/it] 47%|████▋     | 205/439 [32:23<1:23:37, 21.44s/it] 47%|████▋     | 206/439 [32:24<59:00, 15.20s/it]   47%|████▋     | 207/439 [32:24<41:40, 10.78s/it] 47%|████▋     | 208/439 [32:25<29:33,  7.68s/it] 48%|████▊     | 209/439 [32:25<21:01,  5.49s/it] 48%|████▊     | 210/439 [32:26<15:54,  4.17s/it] 48%|████▊     | 211/439 [33:29<1:22:17, 21.66s/it] 48%|████▊     | 212/439 [33:29<58:01, 15.34s/it]   49%|████▊     | 213/439 [33:29<40:34, 10.77s/it] 49%|████▊     | 214/439 [33:30<28:35,  7.63s/it] 49%|████▉     | 215/439 [33:30<20:04,  5.38s/it] 49%|████▉     | 216/439 [33:30<14:27,  3.89s/it] 49%|████▉     | 217/439 [33:30<10:14,  2.77s/it] 50%|████▉     | 218/439 [33:31<07:52,  2.14s/it] 50%|████▉     | 219/439 [33:32<06:15,  1.71s/it] 50%|█████     | 220/439 [33:32<05:04,  1.39s/it] 50%|█████     | 221/439 [33:34<05:04,  1.40s/it] 51%|█████     | 222/439 [33:34<03:50,  1.06s/it] 51%|█████     | 223/439 [33:34<02:48,  1.28it/s] 51%|█████     | 224/439 [33:34<02:05,  1.71it/s] 51%|█████▏    | 225/439 [33:52<20:10,  5.66s/it] 51%|█████▏    | 226/439 [33:52<14:22,  4.05s/it] 52%|█████▏    | 227/439 [33:52<10:33,  2.99s/it] 52%|█████▏    | 228/439 [33:53<07:53,  2.24s/it] 52%|█████▏    | 229/439 [33:54<06:17,  1.80s/it] 52%|█████▏    | 230/439 [33:54<04:42,  1.35s/it] 53%|█████▎    | 231/439 [33:55<03:48,  1.10s/it] 53%|█████▎    | 232/439 [33:56<04:04,  1.18s/it] 53%|█████▎    | 233/439 [33:56<03:00,  1.14it/s] 53%|█████▎    | 234/439 [33:56<02:21,  1.45it/s] 54%|█████▎    | 235/439 [33:56<01:46,  1.92it/s] 54%|█████▍    | 236/439 [33:57<01:21,  2.48it/s] 54%|█████▍    | 237/439 [33:57<01:04,  3.11it/s] 54%|█████▍    | 238/439 [33:57<01:04,  3.09it/s] 54%|█████▍    | 239/439 [33:58<01:15,  2.65it/s] 55%|█████▍    | 240/439 [33:58<01:29,  2.22it/s] 55%|█████▍    | 241/439 [34:00<03:14,  1.02it/s] 55%|█████▌    | 242/439 [34:01<02:35,  1.27it/s] 55%|█████▌    | 243/439 [35:45<1:44:22, 31.95s/it] 56%|█████▌    | 244/439 [35:46<1:13:10, 22.52s/it] 56%|█████▌    | 245/439 [35:47<51:31, 15.93s/it]   56%|█████▌    | 246/439 [35:47<36:32, 11.36s/it] 56%|█████▋    | 247/439 [35:47<25:33,  7.99s/it] 56%|█████▋    | 248/439 [35:47<17:55,  5.63s/it] 57%|█████▋    | 249/439 [35:48<12:44,  4.02s/it] 57%|█████▋    | 250/439 [35:48<08:59,  2.85s/it] 57%|█████▋    | 251/439 [36:05<22:42,  7.25s/it] 57%|█████▋    | 252/439 [36:06<16:01,  5.14s/it] 58%|█████▊    | 253/439 [36:06<11:22,  3.67s/it] 58%|█████▊    | 254/439 [36:06<08:09,  2.64s/it] 58%|█████▊    | 255/439 [36:06<05:53,  1.92s/it] 58%|█████▊    | 256/439 [36:07<04:58,  1.63s/it] 59%|█████▊    | 257/439 [36:08<04:05,  1.35s/it] 59%|█████▉    | 258/439 [36:08<03:15,  1.08s/it] 59%|█████▉    | 259/439 [36:09<02:33,  1.17it/s] 59%|█████▉    | 260/439 [36:09<02:11,  1.36it/s] 59%|█████▉    | 261/439 [36:10<01:49,  1.63it/s] 60%|█████▉    | 262/439 [36:10<01:22,  2.14it/s] 60%|█████▉    | 263/439 [36:10<01:16,  2.31it/s] 60%|██████    | 264/439 [37:43<1:22:36, 28.33s/it] 60%|██████    | 265/439 [37:44<58:18, 20.10s/it]   61%|██████    | 266/439 [37:45<40:59, 14.22s/it] 61%|██████    | 267/439 [37:46<29:18, 10.22s/it] 61%|██████    | 268/439 [38:03<35:21, 12.41s/it] 61%|██████▏   | 269/439 [38:04<25:08,  8.87s/it] 62%|██████▏   | 270/439 [38:05<18:51,  6.70s/it] 62%|██████▏   | 271/439 [39:38<1:30:57, 32.48s/it] 62%|██████▏   | 272/439 [39:39<1:04:02, 23.01s/it] 62%|██████▏   | 273/439 [39:40<45:20, 16.39s/it]   62%|██████▏   | 274/439 [40:39<1:20:20, 29.21s/it] 63%|██████▎   | 275/439 [40:40<56:21, 20.62s/it]   63%|██████▎   | 276/439 [40:40<39:50, 14.66s/it] 63%|██████▎   | 277/439 [42:13<1:42:48, 38.08s/it] 63%|██████▎   | 278/439 [42:13<1:11:49, 26.77s/it] 64%|██████▎   | 279/439 [43:25<1:46:49, 40.06s/it] 64%|██████▍   | 280/439 [43:25<1:14:43, 28.20s/it] 64%|██████▍   | 281/439 [43:26<52:24, 19.90s/it]   64%|██████▍   | 282/439 [43:26<36:45, 14.05s/it] 64%|██████▍   | 283/439 [43:26<25:39,  9.87s/it] 65%|██████▍   | 284/439 [43:27<18:10,  7.04s/it] 65%|██████▍   | 285/439 [43:27<13:03,  5.09s/it] 65%|██████▌   | 286/439 [43:28<09:23,  3.68s/it] 65%|██████▌   | 287/439 [43:45<19:49,  7.83s/it] 66%|██████▌   | 288/439 [43:45<14:07,  5.62s/it] 66%|██████▌   | 289/439 [43:46<10:22,  4.15s/it] 66%|██████▌   | 290/439 [43:46<07:18,  2.94s/it] 66%|██████▋   | 291/439 [43:46<05:09,  2.09s/it] 67%|██████▋   | 292/439 [43:47<03:53,  1.59s/it] 67%|██████▋   | 293/439 [43:47<03:04,  1.26s/it] 67%|██████▋   | 294/439 [43:48<02:24,  1.00it/s] 67%|██████▋   | 295/439 [43:48<02:01,  1.19it/s] 67%|██████▋   | 296/439 [43:49<01:38,  1.46it/s] 68%|██████▊   | 297/439 [43:49<01:37,  1.46it/s] 68%|██████▊   | 298/439 [43:50<01:35,  1.48it/s] 68%|██████▊   | 299/439 [43:51<01:38,  1.43it/s] 68%|██████▊   | 300/439 [43:51<01:32,  1.50it/s] 69%|██████▊   | 301/439 [43:51<01:09,  1.98it/s] 69%|██████▉   | 302/439 [43:52<00:55,  2.47it/s] 69%|██████▉   | 303/439 [43:52<01:03,  2.13it/s] 69%|██████▉   | 304/439 [45:29<1:06:06, 29.38s/it] 69%|██████▉   | 305/439 [45:30<46:31, 20.83s/it]   70%|██████▉   | 306/439 [45:30<32:29, 14.66s/it] 70%|██████▉   | 307/439 [45:30<22:46, 10.35s/it] 70%|███████   | 308/439 [45:31<15:54,  7.28s/it] 70%|███████   | 309/439 [45:31<11:17,  5.21s/it] 71%|███████   | 310/439 [45:31<08:01,  3.74s/it] 71%|███████   | 311/439 [45:32<05:52,  2.75s/it] 71%|███████   | 312/439 [46:56<57:54, 27.36s/it] 71%|███████▏  | 313/439 [48:09<1:26:12, 41.05s/it] 72%|███████▏  | 314/439 [48:10<1:00:10, 28.88s/it] 72%|███████▏  | 315/439 [48:10<41:57, 20.30s/it]   72%|███████▏  | 316/439 [48:10<29:12, 14.25s/it] 72%|███████▏  | 317/439 [49:37<1:13:25, 36.11s/it] 72%|███████▏  | 318/439 [49:55<1:01:33, 30.53s/it] 73%|███████▎  | 319/439 [49:56<43:14, 21.62s/it]   73%|███████▎  | 320/439 [49:56<30:05, 15.17s/it] 73%|███████▎  | 321/439 [49:56<21:04, 10.71s/it] 73%|███████▎  | 322/439 [50:14<24:52, 12.76s/it] 74%|███████▎  | 323/439 [50:14<17:35,  9.10s/it] 74%|███████▍  | 324/439 [51:49<1:06:38, 34.77s/it] 74%|███████▍  | 325/439 [51:49<46:19, 24.38s/it]   74%|███████▍  | 326/439 [51:49<32:20, 17.17s/it] 74%|███████▍  | 327/439 [51:50<22:38, 12.13s/it] 75%|███████▍  | 328/439 [51:50<15:50,  8.56s/it] 75%|███████▍  | 329/439 [51:50<11:03,  6.03s/it] 75%|███████▌  | 330/439 [51:50<07:50,  4.31s/it] 75%|███████▌  | 331/439 [51:51<05:30,  3.06s/it] 76%|███████▌  | 332/439 [53:24<53:50, 30.19s/it] 76%|███████▌  | 333/439 [53:29<39:39, 22.45s/it] 76%|███████▌  | 334/439 [53:29<27:39, 15.81s/it] 76%|███████▋  | 335/439 [53:29<19:16, 11.12s/it] 77%|███████▋  | 336/439 [53:30<13:38,  7.95s/it] 77%|███████▋  | 337/439 [54:17<33:37, 19.78s/it] 77%|███████▋  | 338/439 [55:04<47:14, 28.06s/it] 77%|███████▋  | 339/439 [55:52<56:26, 33.86s/it] 77%|███████▋  | 340/439 [55:53<39:32, 23.96s/it] 78%|███████▊  | 341/439 [55:53<27:42, 16.97s/it] 78%|███████▊  | 342/439 [57:22<1:02:12, 38.48s/it] 78%|███████▊  | 343/439 [57:23<43:24, 27.13s/it]   78%|███████▊  | 344/439 [57:23<30:10, 19.05s/it] 79%|███████▊  | 345/439 [57:23<21:07, 13.48s/it] 79%|███████▉  | 346/439 [57:24<14:58,  9.66s/it] 79%|███████▉  | 347/439 [57:25<10:50,  7.07s/it] 79%|███████▉  | 348/439 [57:25<07:34,  4.99s/it] 79%|███████▉  | 349/439 [59:03<49:09, 32.77s/it] 80%|███████▉  | 350/439 [1:00:00<59:41, 40.24s/it] 80%|███████▉  | 351/439 [1:00:01<41:33, 28.34s/it] 80%|████████  | 352/439 [1:00:01<28:58, 19.98s/it] 80%|████████  | 353/439 [1:00:02<20:15, 14.13s/it] 81%|████████  | 354/439 [1:00:02<14:12, 10.03s/it] 81%|████████  | 355/439 [1:00:03<09:57,  7.11s/it] 81%|████████  | 356/439 [1:01:29<42:41, 30.86s/it] 81%|████████▏ | 357/439 [1:01:29<29:34, 21.64s/it] 82%|████████▏ | 358/439 [1:02:39<48:33, 35.97s/it] 82%|████████▏ | 359/439 [1:02:39<33:37, 25.22s/it] 82%|████████▏ | 360/439 [1:02:39<23:17, 17.69s/it] 82%|████████▏ | 361/439 [1:02:39<16:09, 12.43s/it] 82%|████████▏ | 362/439 [1:02:39<11:12,  8.74s/it] 83%|████████▎ | 363/439 [1:02:39<07:53,  6.23s/it] 83%|████████▎ | 364/439 [1:02:40<05:34,  4.46s/it] 83%|████████▎ | 365/439 [1:02:40<03:56,  3.19s/it] 83%|████████▎ | 366/439 [1:02:41<03:03,  2.51s/it] 84%|████████▎ | 367/439 [1:02:42<02:21,  1.96s/it] 84%|████████▍ | 368/439 [1:02:42<01:40,  1.41s/it] 84%|████████▍ | 369/439 [1:04:14<33:29, 28.71s/it] 84%|████████▍ | 370/439 [1:04:14<23:09, 20.14s/it] 85%|████████▍ | 371/439 [1:04:15<16:21, 14.44s/it] 85%|████████▍ | 372/439 [1:04:16<11:27, 10.27s/it] 85%|████████▍ | 373/439 [1:04:17<08:11,  7.44s/it] 85%|████████▌ | 374/439 [1:04:17<05:52,  5.42s/it] 85%|████████▌ | 375/439 [1:04:18<04:06,  3.85s/it] 86%|████████▌ | 376/439 [1:04:19<03:14,  3.08s/it] 86%|████████▌ | 377/439 [1:05:33<25:05, 24.28s/it] 86%|████████▌ | 378/439 [1:05:33<17:21, 17.08s/it] 86%|████████▋ | 379/439 [1:05:33<12:06, 12.11s/it] 87%|████████▋ | 380/439 [1:05:34<08:22,  8.52s/it] 87%|████████▋ | 381/439 [1:05:34<05:59,  6.20s/it] 87%|████████▋ | 382/439 [1:05:36<04:29,  4.73s/it] 87%|████████▋ | 383/439 [1:05:36<03:12,  3.43s/it] 87%|████████▋ | 384/439 [1:05:36<02:14,  2.44s/it] 88%|████████▊ | 385/439 [1:05:37<01:39,  1.85s/it] 88%|████████▊ | 386/439 [1:05:37<01:16,  1.43s/it] 88%|████████▊ | 387/439 [1:05:38<00:59,  1.15s/it] 88%|████████▊ | 388/439 [1:05:39<01:06,  1.30s/it] 89%|████████▊ | 389/439 [1:05:40<00:57,  1.15s/it] 89%|████████▉ | 390/439 [1:05:41<00:49,  1.00s/it] 89%|████████▉ | 391/439 [1:05:58<04:45,  5.95s/it] 89%|████████▉ | 392/439 [1:07:36<26:08, 33.38s/it] 90%|████████▉ | 393/439 [1:07:36<17:57, 23.42s/it] 90%|████████▉ | 394/439 [1:07:36<12:19, 16.44s/it] 90%|████████▉ | 395/439 [1:07:37<08:36, 11.74s/it] 90%|█████████ | 396/439 [1:07:37<05:56,  8.29s/it] 90%|█████████ | 397/439 [1:07:37<04:06,  5.86s/it] 91%|█████████ | 398/439 [1:07:37<02:50,  4.16s/it] 91%|█████████ | 399/439 [1:07:38<01:59,  2.98s/it] 91%|█████████ | 400/439 [1:09:02<17:47, 27.38s/it] 91%|█████████▏| 401/439 [1:09:02<12:14, 19.32s/it] 92%|█████████▏| 402/439 [1:09:03<08:26, 13.68s/it] 92%|█████████▏| 403/439 [1:09:03<05:49,  9.72s/it] 92%|█████████▏| 404/439 [1:09:04<04:04,  6.98s/it] 92%|█████████▏| 405/439 [1:09:05<02:52,  5.08s/it] 92%|█████████▏| 406/439 [1:09:05<02:04,  3.77s/it] 93%|█████████▎| 407/439 [1:09:07<01:36,  3.01s/it] 93%|█████████▎| 408/439 [1:09:49<07:37, 14.75s/it] 93%|█████████▎| 409/439 [1:09:49<05:12, 10.43s/it] 93%|█████████▎| 410/439 [1:09:49<03:32,  7.34s/it] 94%|█████████▎| 411/439 [1:09:50<02:30,  5.37s/it] 94%|█████████▍| 412/439 [1:09:51<01:46,  3.93s/it] 94%|█████████▍| 413/439 [1:09:51<01:13,  2.82s/it] 94%|█████████▍| 414/439 [1:09:51<00:50,  2.04s/it] 95%|█████████▍| 415/439 [1:09:54<00:54,  2.28s/it] 95%|█████████▍| 416/439 [1:11:30<11:40, 30.44s/it] 95%|█████████▍| 417/439 [1:11:31<07:54, 21.58s/it] 95%|█████████▌| 418/439 [1:11:48<07:07, 20.34s/it] 95%|█████████▌| 419/439 [1:13:20<13:53, 41.69s/it] 96%|█████████▌| 420/439 [1:13:20<09:16, 29.31s/it] 96%|█████████▌| 421/439 [1:13:21<06:13, 20.76s/it] 96%|█████████▌| 422/439 [1:13:22<04:10, 14.75s/it] 96%|█████████▋| 423/439 [1:13:22<02:46, 10.42s/it] 97%|█████████▋| 424/439 [1:13:22<01:49,  7.33s/it] 97%|█████████▋| 425/439 [1:13:23<01:14,  5.34s/it] 97%|█████████▋| 426/439 [1:13:24<00:51,  3.94s/it] 97%|█████████▋| 427/439 [1:13:24<00:34,  2.86s/it] 97%|█████████▋| 428/439 [1:13:25<00:24,  2.19s/it] 98%|█████████▊| 429/439 [1:13:25<00:17,  1.71s/it] 98%|█████████▊| 430/439 [1:13:26<00:12,  1.37s/it] 98%|█████████▊| 431/439 [1:13:26<00:08,  1.03s/it] 98%|█████████▊| 432/439 [1:13:26<00:05,  1.22it/s] 99%|█████████▊| 433/439 [1:13:27<00:04,  1.44it/s] 99%|█████████▉| 434/439 [1:13:29<00:05,  1.15s/it] 99%|█████████▉| 435/439 [1:13:29<00:03,  1.19it/s] 99%|█████████▉| 436/439 [1:13:29<00:02,  1.50it/s]100%|█████████▉| 437/439 [1:13:29<00:01,  1.98it/s]100%|█████████▉| 438/439 [1:13:30<00:00,  1.75it/s]100%|██████████| 439/439 [1:13:31<00:00,  1.95it/s]100%|██████████| 439/439 [1:13:31<00:00, 10.05s/it]
Evaluating generations...
Process Process-85:
Traceback (most recent call last):
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 58, in unsafe_execute
    with create_tempdir():
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 113, in create_tempdir
    with tempfile.TemporaryDirectory() as dirname:
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 878, in __exit__
    self.cleanup()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 882, in cleanup
    self._rmtree(self.name, ignore_errors=self._ignore_cleanup_errors)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 864, in _rmtree
    _shutil.rmtree(name, onerror=onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 725, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 679, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
TypeError: 'NoneType' object is not callable
Process Process-121:
Traceback (most recent call last):
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 58, in unsafe_execute
    with create_tempdir():
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 113, in create_tempdir
    with tempfile.TemporaryDirectory() as dirname:
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 878, in __exit__
    self.cleanup()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 882, in cleanup
    self._rmtree(self.name, ignore_errors=self._ignore_cleanup_errors)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 864, in _rmtree
    _shutil.rmtree(name, onerror=onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 725, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 679, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
TypeError: 'NoneType' object is not callable
Process Process-129:
Traceback (most recent call last):
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 58, in unsafe_execute
    with create_tempdir():
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 113, in create_tempdir
    with tempfile.TemporaryDirectory() as dirname:
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 878, in __exit__
    self.cleanup()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 882, in cleanup
    self._rmtree(self.name, ignore_errors=self._ignore_cleanup_errors)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 864, in _rmtree
    _shutil.rmtree(name, onerror=onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 725, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 679, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
TypeError: 'NoneType' object is not callable
Process Process-148:
Traceback (most recent call last):
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 58, in unsafe_execute
    with create_tempdir():
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 113, in create_tempdir
    with tempfile.TemporaryDirectory() as dirname:
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 878, in __exit__
    self.cleanup()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 882, in cleanup
    self._rmtree(self.name, ignore_errors=self._ignore_cleanup_errors)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 864, in _rmtree
    _shutil.rmtree(name, onerror=onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 725, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 679, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
TypeError: 'NoneType' object is not callable
Process Process-168:
Traceback (most recent call last):
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 58, in unsafe_execute
    with create_tempdir():
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 113, in create_tempdir
    with tempfile.TemporaryDirectory() as dirname:
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 878, in __exit__
    self.cleanup()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 882, in cleanup
    self._rmtree(self.name, ignore_errors=self._ignore_cleanup_errors)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 864, in _rmtree
    _shutil.rmtree(name, onerror=onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 725, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 679, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
TypeError: 'NoneType' object is not callable
Process Process-202:
Traceback (most recent call last):
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 58, in unsafe_execute
    with create_tempdir():
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 113, in create_tempdir
    with tempfile.TemporaryDirectory() as dirname:
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 878, in __exit__
    self.cleanup()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 882, in cleanup
    self._rmtree(self.name, ignore_errors=self._ignore_cleanup_errors)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 864, in _rmtree
    _shutil.rmtree(name, onerror=onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 725, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 679, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
TypeError: 'NoneType' object is not callable
Process Process-222:
Traceback (most recent call last):
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 58, in unsafe_execute
    with create_tempdir():
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 113, in create_tempdir
    with tempfile.TemporaryDirectory() as dirname:
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 878, in __exit__
    self.cleanup()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 882, in cleanup
    self._rmtree(self.name, ignore_errors=self._ignore_cleanup_errors)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 864, in _rmtree
    _shutil.rmtree(name, onerror=onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 725, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 679, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
TypeError: 'NoneType' object is not callable
Process Process-241:
Traceback (most recent call last):
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 58, in unsafe_execute
    with create_tempdir():
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 113, in create_tempdir
    with tempfile.TemporaryDirectory() as dirname:
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 878, in __exit__
    self.cleanup()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 882, in cleanup
    self._rmtree(self.name, ignore_errors=self._ignore_cleanup_errors)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 864, in _rmtree
    _shutil.rmtree(name, onerror=onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 725, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 679, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
TypeError: 'NoneType' object is not callable
Process Process-255:
Traceback (most recent call last):
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 58, in unsafe_execute
    with create_tempdir():
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 113, in create_tempdir
    with tempfile.TemporaryDirectory() as dirname:
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 878, in __exit__
    self.cleanup()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 882, in cleanup
    self._rmtree(self.name, ignore_errors=self._ignore_cleanup_errors)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 864, in _rmtree
    _shutil.rmtree(name, onerror=onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 725, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 679, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
TypeError: 'NoneType' object is not callable
Process Process-347:
Traceback (most recent call last):
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 58, in unsafe_execute
    with create_tempdir():
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 113, in create_tempdir
    with tempfile.TemporaryDirectory() as dirname:
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 878, in __exit__
    self.cleanup()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 882, in cleanup
    self._rmtree(self.name, ignore_errors=self._ignore_cleanup_errors)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 864, in _rmtree
    _shutil.rmtree(name, onerror=onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 725, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 679, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
TypeError: 'NoneType' object is not callable
Process Process-354:
Traceback (most recent call last):
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 58, in unsafe_execute
    with create_tempdir():
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 113, in create_tempdir
    with tempfile.TemporaryDirectory() as dirname:
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 878, in __exit__
    self.cleanup()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 882, in cleanup
    self._rmtree(self.name, ignore_errors=self._ignore_cleanup_errors)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 864, in _rmtree
    _shutil.rmtree(name, onerror=onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 725, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 679, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
TypeError: 'NoneType' object is not callable
Process Process-402:
Traceback (most recent call last):
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 58, in unsafe_execute
    with create_tempdir():
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 113, in create_tempdir
    with tempfile.TemporaryDirectory() as dirname:
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 878, in __exit__
    self.cleanup()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 882, in cleanup
    self._rmtree(self.name, ignore_errors=self._ignore_cleanup_errors)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 864, in _rmtree
    _shutil.rmtree(name, onerror=onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 725, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 679, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
TypeError: 'NoneType' object is not callable
Process Process-437:
Traceback (most recent call last):
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 58, in unsafe_execute
    with create_tempdir():
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 113, in create_tempdir
    with tempfile.TemporaryDirectory() as dirname:
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 878, in __exit__
    self.cleanup()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 882, in cleanup
    self._rmtree(self.name, ignore_errors=self._ignore_cleanup_errors)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 864, in _rmtree
    _shutil.rmtree(name, onerror=onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 725, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 679, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
TypeError: 'NoneType' object is not callable
Process Process-479:
Traceback (most recent call last):
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 58, in unsafe_execute
    with create_tempdir():
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 113, in create_tempdir
    with tempfile.TemporaryDirectory() as dirname:
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 878, in __exit__
    self.cleanup()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 882, in cleanup
    self._rmtree(self.name, ignore_errors=self._ignore_cleanup_errors)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 864, in _rmtree
    _shutil.rmtree(name, onerror=onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 725, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 679, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
TypeError: 'NoneType' object is not callable
Process Process-533:
Traceback (most recent call last):
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 58, in unsafe_execute
    with create_tempdir():
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 113, in create_tempdir
    with tempfile.TemporaryDirectory() as dirname:
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 878, in __exit__
    self.cleanup()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 882, in cleanup
    self._rmtree(self.name, ignore_errors=self._ignore_cleanup_errors)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 864, in _rmtree
    _shutil.rmtree(name, onerror=onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 725, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 679, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
TypeError: 'NoneType' object is not callable
Process Process-536:
Traceback (most recent call last):
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 58, in unsafe_execute
    with create_tempdir():
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 113, in create_tempdir
    with tempfile.TemporaryDirectory() as dirname:
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 878, in __exit__
    self.cleanup()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 882, in cleanup
    self._rmtree(self.name, ignore_errors=self._ignore_cleanup_errors)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 864, in _rmtree
    _shutil.rmtree(name, onerror=onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 725, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 679, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
TypeError: 'NoneType' object is not callable
Process Process-639:
Traceback (most recent call last):
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 58, in unsafe_execute
    with create_tempdir():
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 113, in create_tempdir
    with tempfile.TemporaryDirectory() as dirname:
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 878, in __exit__
    self.cleanup()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 882, in cleanup
    self._rmtree(self.name, ignore_errors=self._ignore_cleanup_errors)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 864, in _rmtree
    _shutil.rmtree(name, onerror=onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 725, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 679, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
TypeError: 'NoneType' object is not callable
Process Process-741:
Traceback (most recent call last):
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 58, in unsafe_execute
    with create_tempdir():
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 113, in create_tempdir
    with tempfile.TemporaryDirectory() as dirname:
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 878, in __exit__
    self.cleanup()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 882, in cleanup
    self._rmtree(self.name, ignore_errors=self._ignore_cleanup_errors)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 864, in _rmtree
    _shutil.rmtree(name, onerror=onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 725, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 679, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
TypeError: 'NoneType' object is not callable
Process Process-763:
Traceback (most recent call last):
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 58, in unsafe_execute
    with create_tempdir():
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 113, in create_tempdir
    with tempfile.TemporaryDirectory() as dirname:
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 878, in __exit__
    self.cleanup()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 882, in cleanup
    self._rmtree(self.name, ignore_errors=self._ignore_cleanup_errors)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 864, in _rmtree
    _shutil.rmtree(name, onerror=onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 725, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 679, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
TypeError: 'NoneType' object is not callable
Process Process-777:
Traceback (most recent call last):
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 58, in unsafe_execute
    with create_tempdir():
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 113, in create_tempdir
    with tempfile.TemporaryDirectory() as dirname:
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 878, in __exit__
    self.cleanup()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 882, in cleanup
    self._rmtree(self.name, ignore_errors=self._ignore_cleanup_errors)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 864, in _rmtree
    _shutil.rmtree(name, onerror=onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 725, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 679, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
TypeError: 'NoneType' object is not callable
Process Process-846:
Traceback (most recent call last):
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 58, in unsafe_execute
    with create_tempdir():
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 113, in create_tempdir
    with tempfile.TemporaryDirectory() as dirname:
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 878, in __exit__
    self.cleanup()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 882, in cleanup
    self._rmtree(self.name, ignore_errors=self._ignore_cleanup_errors)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 864, in _rmtree
    _shutil.rmtree(name, onerror=onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 725, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 679, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
TypeError: 'NoneType' object is not callable
Process Process-851:
Traceback (most recent call last):
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 58, in unsafe_execute
    with create_tempdir():
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 113, in create_tempdir
    with tempfile.TemporaryDirectory() as dirname:
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 878, in __exit__
    self.cleanup()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 882, in cleanup
    self._rmtree(self.name, ignore_errors=self._ignore_cleanup_errors)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 864, in _rmtree
    _shutil.rmtree(name, onerror=onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 725, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 679, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
TypeError: 'NoneType' object is not callable
Process Process-853:
Traceback (most recent call last):
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 58, in unsafe_execute
    with create_tempdir():
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 113, in create_tempdir
    with tempfile.TemporaryDirectory() as dirname:
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 878, in __exit__
    self.cleanup()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 882, in cleanup
    self._rmtree(self.name, ignore_errors=self._ignore_cleanup_errors)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 864, in _rmtree
    _shutil.rmtree(name, onerror=onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 725, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 679, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
TypeError: 'NoneType' object is not callable
Process Process-854:
Traceback (most recent call last):
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 58, in unsafe_execute
    with create_tempdir():
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 113, in create_tempdir
    with tempfile.TemporaryDirectory() as dirname:
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 878, in __exit__
    self.cleanup()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 882, in cleanup
    self._rmtree(self.name, ignore_errors=self._ignore_cleanup_errors)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 864, in _rmtree
    _shutil.rmtree(name, onerror=onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 725, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 679, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
TypeError: 'NoneType' object is not callable
Process Process-876:
Traceback (most recent call last):
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 58, in unsafe_execute
    with create_tempdir():
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/home/avisingh/CodeRagBench/code-rag-bench-main/generation/eval/tasks/custom_metrics/execute.py", line 113, in create_tempdir
    with tempfile.TemporaryDirectory() as dirname:
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 878, in __exit__
    self.cleanup()
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 882, in cleanup
    self._rmtree(self.name, ignore_errors=self._ignore_cleanup_errors)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/tempfile.py", line 864, in _rmtree
    _shutil.rmtree(name, onerror=onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 725, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/home/avisingh/miniconda3/envs/crag/lib/python3.10/shutil.py", line 679, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
TypeError: 'NoneType' object is not callable
{
  "odex-en": {
    "pass@1": 0.27107061503416857
  },
  "config": {
    "prefix": "",
    "do_sample": true,
    "temperature": 0.2,
    "top_k": 0,
    "top_p": 0.95,
    "n_samples": 1,
    "eos": "<|endoftext|>",
    "ignore_eos": false,
    "seed": 0,
    "remove_linebreak": false,
    "model_backend": "hf",
    "model": "unsloth/codellama-7b",
    "modeltype": "causal",
    "peft_model": null,
    "revision": null,
    "token": null,
    "trust_remote_code": false,
    "tasks": "odex-en",
    "instruction_tokens": null,
    "batch_size": 1,
    "max_length_input": 3072,
    "max_length_generation": 3584,
    "topk_docs": 5,
    "precision": "bf16",
    "load_in_8bit": false,
    "load_in_4bit": false,
    "left_padding": false,
    "limit": null,
    "limit_start": 0,
    "save_every_k_tasks": -1,
    "postprocess": true,
    "allow_code_execution": true,
    "generation_only": false,
    "load_generations_path": null,
    "load_data_path": null,
    "metric_output_path": "evaluation_results.json",
    "save_generations": false,
    "load_generations_intermediate_paths": null,
    "save_generations_path": "generations.json",
    "save_references": false,
    "save_references_path": "references.json",
    "prompt": "prompt",
    "new_tokens_only": false,
    "max_memory_per_gpu": null,
    "check_references": false,
    "dataset_path": "json",
    "dataset_name": null,
    "data_files_test": "/home/avisingh/CodeRagBench/code-rag-bench-main/retrieval/odex_codesage_generation.json",
    "cache_dir": null,
    "model_cache_dir": null,
    "setup_repoeval": false,
    "repoeval_input_repo_dir": "../retrieval/output/repoeval/repositories/function_level",
    "repoeval_cache_dir": "scripts/repoeval",
    "data_files": {
      "test": "/home/avisingh/CodeRagBench/code-rag-bench-main/retrieval/odex_codesage_generation.json"
    },
    "tokenizer": "unsloth/codellama-7b"
  }
}
