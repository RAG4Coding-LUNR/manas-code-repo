{"task_id":3283984,"prompt":"def f_3283984():\n\treturn ","suffix":"","canonical_solution":"bytes.fromhex('4a4b4c').decode('utf-8')","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == \"JKL\"\n"],"entry_point":"f_3283984","intent":"decode a hex string '4a4b4c' to UTF-8.","library":[],"docs":[]}
{"task_id":3844801,"prompt":"def f_3844801(myList):\n\treturn ","suffix":"","canonical_solution":"all(x == myList[0] for x in myList)","test_start":"\ndef check(candidate):","test":["\n    assert candidate([1,2,3]) == False\n","\n    assert candidate([1,1,1,1,1,1]) == True\n","\n    assert candidate([1]) == True\n","\n    assert candidate(['k','k','k','k','k']) == True\n","\n    assert candidate([None,'%$#ga',3]) == False\n"],"entry_point":"f_3844801","intent":"check if all elements in list `myList` are identical","library":[],"docs":[]}
{"task_id":4302166,"prompt":"def f_4302166():\n\treturn ","suffix":"","canonical_solution":"'%*s : %*s' % (20, 'Python', 20, 'Very Good')","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == '              Python :            Very Good'\n"],"entry_point":"f_4302166","intent":"format number of spaces between strings `Python`, `:` and `Very Good` to be `20`","library":[],"docs":[]}
{"task_id":7555335,"prompt":"def f_7555335(d):\n\treturn ","suffix":"","canonical_solution":"d.decode('cp1251').encode('utf8')","test_start":"\ndef check(candidate):","test":["\n    assert candidate('hello world!'.encode('cp1251')) == b'hello world!'\n","\n    assert candidate('%*(^O*'.encode('cp1251')) == b'%*(^O*'\n","\n    assert candidate(''.encode('cp1251')) == b''\n","\n    assert candidate('hello world!'.encode('cp1251')) != 'hello world!'\n"],"entry_point":"f_7555335","intent":"convert a string `d` from CP-1251 to UTF-8","library":[],"docs":[]}
{"task_id":2544710,"prompt":"def f_2544710(kwargs):\n\treturn ","suffix":"","canonical_solution":"{k: v for k, v in list(kwargs.items()) if v is not None}","test_start":"\ndef check(candidate):","test":["\n    assert candidate({i: None for i in range(10)}) == {}\n","\n    assert candidate({i: min(i,4) for i in range(6)}) == {0:0,1:1,2:2,3:3,4:4,5:4}\n","\n    assert candidate({'abc': 'abc'})['abc'] == 'abc'\n","\n    assert candidate({'x': None, 'yy': 234}) == {'yy': 234}\n"],"entry_point":"f_2544710","intent":"get rid of None values in dictionary `kwargs`","library":[],"docs":[]}
{"task_id":2544710,"prompt":"def f_2544710(kwargs):\n\treturn ","suffix":"","canonical_solution":"dict((k, v) for k, v in kwargs.items() if v is not None)","test_start":"\ndef check(candidate):","test":["\n    assert candidate({i: None for i in range(10)}) == {}\n","\n    assert candidate({i: min(i,4) for i in range(6)}) == {0:0,1:1,2:2,3:3,4:4,5:4}\n","\n    assert candidate({'abc': 'abc'})['abc'] == 'abc'\n","\n    assert candidate({'x': None, 'yy': 234}) == {'yy': 234}\n"],"entry_point":"f_2544710","intent":"get rid of None values in dictionary `kwargs`","library":[],"docs":[]}
{"task_id":14971373,"prompt":"def f_14971373():\n\treturn ","suffix":"","canonical_solution":"subprocess.check_output('ps -ef | grep something | wc -l', shell=True)","test_start":"\nimport subprocess\nfrom unittest.mock import Mock\n\ndef check(candidate):","test":["\n    output = b'  PID TTY          TIME CMD\\n  226 pts\/1    00:00:00 bash\\n  285 pts\/1    00:00:00 python3\\n  352 pts\/1    00:00:00 ps\\n'\n    subprocess.check_output = Mock(return_value = output)\n    assert candidate() == output\n"],"entry_point":"f_14971373","intent":"capture final output of a chain of system commands `ps -ef | grep something | wc -l`","library":["subprocess"],"docs":[{"text":"pwd.getpwall()  \nReturn a list of all available password database entries, in arbitrary order.","title":"python.library.pwd#pwd.getpwall"},{"text":"subprocess.getoutput(cmd)  \nReturn output (stdout and stderr) of executing cmd in a shell. Like getstatusoutput(), except the exit code is ignored and the return value is a string containing the command\u2019s output. Example: >>> subprocess.getoutput('ls \/bin\/ls')\n'\/bin\/ls'\n Availability: POSIX & Windows.  Changed in version 3.3.4: Windows support added","title":"python.library.subprocess#subprocess.getoutput"},{"text":"test.support.script_helper.kill_python(p)  \nRun the given subprocess.Popen process until completion and return stdout.","title":"python.library.test#test.support.script_helper.kill_python"},{"text":"classmatplotlib.dviread.PsFont(texname, psname, effects, encoding, filename)[source]\n \nBases: tuple Create new instance of PsFont(texname, psname, effects, encoding, filename)   effects\n \nAlias for field number 2 \n   encoding\n \nAlias for field number 3 \n   filename\n \nAlias for field number 4 \n   psname\n \nAlias for field number 1 \n   texname\n \nAlias for field number 0","title":"matplotlib.dviread#matplotlib.dviread.PsFont"},{"text":"numpy.distutils.exec_command.filepath_from_subprocess_output   distutils.exec_command.filepath_from_subprocess_output(output)[source]\n \nConvert bytes in the encoding used by a subprocess into a filesystem-appropriate str. Inherited from exec_command, and possibly incorrect.","title":"numpy.reference.generated.numpy.distutils.exec_command.filepath_from_subprocess_output"},{"text":"modulefinder \u2014 Find modules used by a script Source code: Lib\/modulefinder.py This module provides a ModuleFinder class that can be used to determine the set of modules imported by a script. modulefinder.py can also be run as a script, giving the filename of a Python script as its argument, after which a report of the imported modules will be printed.  \nmodulefinder.AddPackagePath(pkg_name, path)  \nRecord that the package named pkg_name can be found in the specified path. \n  \nmodulefinder.ReplacePackage(oldname, newname)  \nAllows specifying that the module named oldname is in fact the package named newname. \n  \nclass modulefinder.ModuleFinder(path=None, debug=0, excludes=[], replace_paths=[])  \nThis class provides run_script() and report() methods to determine the set of modules imported by a script. path can be a list of directories to search for modules; if not specified, sys.path is used. debug sets the debugging level; higher values make the class print debugging messages about what it\u2019s doing. excludes is a list of module names to exclude from the analysis. replace_paths is a list of (oldpath, newpath) tuples that will be replaced in module paths.  \nreport()  \nPrint a report to standard output that lists the modules imported by the script and their paths, as well as modules that are missing or seem to be missing. \n  \nrun_script(pathname)  \nAnalyze the contents of the pathname file, which must contain Python code. \n  \nmodules  \nA dictionary mapping module names to modules. See Example usage of ModuleFinder. \n \n Example usage of ModuleFinder The script that is going to get analyzed later on (bacon.py): import re, itertools\n\ntry:\n    import baconhameggs\nexcept ImportError:\n    pass\n\ntry:\n    import guido.python.ham\nexcept ImportError:\n    pass\n The script that will output the report of bacon.py: from modulefinder import ModuleFinder\n\nfinder = ModuleFinder()\nfinder.run_script('bacon.py')\n\nprint('Loaded modules:')\nfor name, mod in finder.modules.items():\n    print('%s: ' % name, end='')\n    print(','.join(list(mod.globalnames.keys())[:3]))\n\nprint('-'*50)\nprint('Modules not imported:')\nprint('\\n'.join(finder.badmodules.keys()))\n Sample output (may vary depending on the architecture): Loaded modules:\n_types:\ncopyreg:  _inverted_registry,_slotnames,__all__\nsre_compile:  isstring,_sre,_optimize_unicode\n_sre:\nsre_constants:  REPEAT_ONE,makedict,AT_END_LINE\nsys:\nre:  __module__,finditer,_expand\nitertools:\n__main__:  re,itertools,baconhameggs\nsre_parse:  _PATTERNENDERS,SRE_FLAG_UNICODE\narray:\ntypes:  __module__,IntType,TypeType\n---------------------------------------------------\nModules not imported:\nguido.python.ham\nbaconhameggs","title":"python.library.modulefinder"},{"text":"torch.cuda.list_gpu_processes(device=None) [source]\n \nReturns a human-readable printout of the running processes and their GPU memory use for a given device. This can be useful to display periodically during training, or when handling out-of-memory exceptions.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns printout for the current device, given by current_device(), if device is None (default).","title":"torch.cuda#torch.cuda.list_gpu_processes"},{"text":"pwd \u2014 The password database This module provides access to the Unix user account and password database. It is available on all Unix versions. Password database entries are reported as a tuple-like object, whose attributes correspond to the members of the passwd structure (Attribute field below, see <pwd.h>):   \nIndex Attribute Meaning   \n0 pw_name Login name  \n1 pw_passwd Optional encrypted password  \n2 pw_uid Numerical user ID  \n3 pw_gid Numerical group ID  \n4 pw_gecos User name or comment field  \n5 pw_dir User home directory  \n6 pw_shell User command interpreter   The uid and gid items are integers, all others are strings. KeyError is raised if the entry asked for cannot be found.  Note In traditional Unix the field pw_passwd usually contains a password encrypted with a DES derived algorithm (see module crypt). However most modern unices use a so-called shadow password system. On those unices the pw_passwd field only contains an asterisk ('*') or the letter 'x' where the encrypted password is stored in a file \/etc\/shadow which is not world readable. Whether the pw_passwd field contains anything useful is system-dependent. If available, the spwd module should be used where access to the encrypted password is required.  It defines the following items:  \npwd.getpwuid(uid)  \nReturn the password database entry for the given numeric user ID. \n  \npwd.getpwnam(name)  \nReturn the password database entry for the given user name. \n  \npwd.getpwall()  \nReturn a list of all available password database entries, in arbitrary order. \n  See also  \nModule grp\n\n\nAn interface to the group database, similar to this.  \nModule spwd\n\n\nAn interface to the shadow password database, similar to this.","title":"python.library.pwd"},{"text":"class asyncio.FastChildWatcher  \nThis implementation reaps every terminated processes by calling os.waitpid(-1) directly, possibly breaking other code spawning processes and waiting for their termination. There is no noticeable overhead when handling a big number of children (O(1) each time a child terminates). This solution requires a running event loop in the main thread to work, as SafeChildWatcher.","title":"python.library.asyncio-policy#asyncio.FastChildWatcher"},{"text":"collect_callgrind(number=100, collect_baseline=True) [source]\n \nCollect instruction counts using Callgrind. Unlike wall times, instruction counts are deterministic (modulo non-determinism in the program itself and small amounts of jitter from the Python interpreter.) This makes them ideal for detailed performance analysis. This method runs stmt in a separate process so that Valgrind can instrument the program. Performance is severely degraded due to the instrumentation, howevever this is ameliorated by the fact that a small number of iterations is generally sufficient to obtain good measurements. In order to to use this method valgrind, callgrind_control, and callgrind_annotate must be installed. Because there is a process boundary between the caller (this process) and the stmt execution, globals cannot contain arbitrary in-memory data structures. (Unlike timing methods) Instead, globals are restricted to builtins, nn.Modules\u2019s, and TorchScripted functions\/modules to reduce the surprise factor from serialization and subsequent deserialization. The GlobalsBridge class provides more detail on this subject. Take particular care with nn.Modules: they rely on pickle and you may need to add an import to setup for them to transfer properly. By default, a profile for an empty statement will be collected and cached to indicate how many instructions are from the Python loop which drives stmt.  Returns \nA CallgrindStats object which provides instruction counts and some basic facilities for analyzing and manipulating results.","title":"torch.benchmark_utils#torch.utils.benchmark.Timer.collect_callgrind"}]}
{"task_id":6726636,"prompt":"def f_6726636():\n\treturn ","suffix":"","canonical_solution":"\"\"\"\"\"\".join(['a', 'b', 'c'])","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == \"abc\"\n","\n    assert candidate() == 'a' + 'b' + 'c'\n"],"entry_point":"f_6726636","intent":"concatenate a list of strings `['a', 'b', 'c']`","library":[],"docs":[]}
{"task_id":18079563,"prompt":"def f_18079563(s1, s2):\n\treturn ","suffix":"","canonical_solution":"pd.Series(list(set(s1).intersection(set(s2))))","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    x1, x2 = pd.Series([1,2]), pd.Series([1,3])\n    assert candidate(x1, x2).equals(pd.Series([1]))\n","\n    x1, x2 = pd.Series([1,2]), pd.Series([1,3, 10, 4, 5, 9])\n    assert candidate(x1, x2).equals(pd.Series([1]))\n","\n    x1, x2 = pd.Series([1,2]), pd.Series([1,2, 10])\n    assert candidate(x1, x2).equals(pd.Series([1, 2]))\n"],"entry_point":"f_18079563","intent":"find intersection data between series `s1` and series `s2`","library":["pandas"],"docs":[{"text":"skimage.segmentation.join_segmentations(s1, s2) [source]\n \nReturn the join of the two input segmentations. The join J of S1 and S2 is defined as the segmentation in which two voxels are in the same segment if and only if they are in the same segment in both S1 and S2.  Parameters \n \ns1, s2numpy arrays \n\ns1 and s2 are label fields of the same shape.    Returns \n \njnumpy array \n\nThe join segmentation of s1 and s2.     Examples >>> from skimage.segmentation import join_segmentations\n>>> s1 = np.array([[0, 0, 1, 1],\n...                [0, 2, 1, 1],\n...                [2, 2, 2, 1]])\n>>> s2 = np.array([[0, 1, 1, 0],\n...                [0, 1, 1, 0],\n...                [0, 1, 1, 1]])\n>>> join_segmentations(s1, s2)\narray([[0, 1, 3, 2],\n       [0, 5, 3, 2],\n       [4, 5, 5, 3]])","title":"skimage.api.skimage.segmentation#skimage.segmentation.join_segmentations"},{"text":"numpy.intersect1d   numpy.intersect1d(ar1, ar2, assume_unique=False, return_indices=False)[source]\n \nFind the intersection of two arrays. Return the sorted, unique values that are in both of the input arrays.  Parameters \n \nar1, ar2array_like\n\n\nInput arrays. Will be flattened if not already 1D.  \nassume_uniquebool\n\n\nIf True, the input arrays are both assumed to be unique, which can speed up the calculation. If True but ar1 or ar2 are not unique, incorrect results and out-of-bounds indices could result. Default is False.  \nreturn_indicesbool\n\n\nIf True, the indices which correspond to the intersection of the two arrays are returned. The first instance of a value is used if there are multiple. Default is False.  New in version 1.15.0.     Returns \n \nintersect1dndarray\n\n\nSorted 1D array of common and unique elements.  \ncomm1ndarray\n\n\nThe indices of the first occurrences of the common values in ar1. Only provided if return_indices is True.  \ncomm2ndarray\n\n\nThe indices of the first occurrences of the common values in ar2. Only provided if return_indices is True.      See also  numpy.lib.arraysetops\n\nModule with a number of other functions for performing set operations on arrays.    Examples >>> np.intersect1d([1, 3, 4, 3], [3, 1, 2, 1])\narray([1, 3])\n To intersect more than two arrays, use functools.reduce: >>> from functools import reduce\n>>> reduce(np.intersect1d, ([1, 3, 4, 3], [3, 1, 2, 1], [6, 3, 4, 2]))\narray([3])\n To return the indices of the values common to the input arrays along with the intersected values: >>> x = np.array([1, 1, 2, 3, 4])\n>>> y = np.array([2, 1, 4, 6])\n>>> xy, x_ind, y_ind = np.intersect1d(x, y, return_indices=True)\n>>> x_ind, y_ind\n(array([0, 2, 4]), array([1, 0, 2]))\n>>> xy, x[x_ind], y[y_ind]\n(array([1, 2, 4]), array([1, 2, 4]), array([1, 2, 4]))","title":"numpy.reference.generated.numpy.intersect1d"},{"text":"pandas.Series.combine_first   Series.combine_first(other)[source]\n \nUpdate null elements with value in the same location in \u2018other\u2019. Combine two Series objects by filling null values in one Series with non-null values from the other Series. Result index will be the union of the two indexes.  Parameters \n \nother:Series\n\n\nThe value(s) to be used for filling null values.    Returns \n Series\n\nThe result of combining the provided Series with the other object.      See also  Series.combine\n\nPerform element-wise operation on two Series using a given function.    Examples \n>>> s1 = pd.Series([1, np.nan])\n>>> s2 = pd.Series([3, 4, 5])\n>>> s1.combine_first(s2)\n0    1.0\n1    4.0\n2    5.0\ndtype: float64\n  Null values still persist if the location of that null value does not exist in other \n>>> s1 = pd.Series({'falcon': np.nan, 'eagle': 160.0})\n>>> s2 = pd.Series({'eagle': 200.0, 'duck': 30.0})\n>>> s1.combine_first(s2)\nduck       30.0\neagle     160.0\nfalcon      NaN\ndtype: float64","title":"pandas.reference.api.pandas.series.combine_first"},{"text":"pandas.Series.combine   Series.combine(other, func, fill_value=None)[source]\n \nCombine the Series with a Series or scalar according to func. Combine the Series and other using func to perform elementwise selection for combined Series. fill_value is assumed when value is missing at some index from one of the two objects being combined.  Parameters \n \nother:Series or scalar\n\n\nThe value(s) to be combined with the Series.  \nfunc:function\n\n\nFunction that takes two scalars as inputs and returns an element.  \nfill_value:scalar, optional\n\n\nThe value to assume when an index is missing from one Series or the other. The default specifies to use the appropriate NaN value for the underlying dtype of the Series.    Returns \n Series\n\nThe result of combining the Series with the other object.      See also  Series.combine_first\n\nCombine Series values, choosing the calling Series\u2019 values first.    Examples Consider 2 Datasets s1 and s2 containing highest clocked speeds of different birds. \n>>> s1 = pd.Series({'falcon': 330.0, 'eagle': 160.0})\n>>> s1\nfalcon    330.0\neagle     160.0\ndtype: float64\n>>> s2 = pd.Series({'falcon': 345.0, 'eagle': 200.0, 'duck': 30.0})\n>>> s2\nfalcon    345.0\neagle     200.0\nduck       30.0\ndtype: float64\n  Now, to combine the two datasets and view the highest speeds of the birds across the two datasets \n>>> s1.combine(s2, max)\nduck        NaN\neagle     200.0\nfalcon    345.0\ndtype: float64\n  In the previous example, the resulting value for duck is missing, because the maximum of a NaN and a float is a NaN. So, in the example, we set fill_value=0, so the maximum value returned will be the value from some dataset. \n>>> s1.combine(s2, max, fill_value=0)\nduck       30.0\neagle     200.0\nfalcon    345.0\ndtype: float64","title":"pandas.reference.api.pandas.series.combine"},{"text":"numpy.in1d   numpy.in1d(ar1, ar2, assume_unique=False, invert=False)[source]\n \nTest whether each element of a 1-D array is also present in a second array. Returns a boolean array the same length as ar1 that is True where an element of ar1 is in ar2 and False otherwise. We recommend using isin instead of in1d for new code.  Parameters \n \nar1(M,) array_like\n\n\nInput array.  \nar2array_like\n\n\nThe values against which to test each value of ar1.  \nassume_uniquebool, optional\n\n\nIf True, the input arrays are both assumed to be unique, which can speed up the calculation. Default is False.  \ninvertbool, optional\n\n\nIf True, the values in the returned array are inverted (that is, False where an element of ar1 is in ar2 and True otherwise). Default is False. np.in1d(a, b, invert=True) is equivalent to (but is faster than) np.invert(in1d(a, b)).  New in version 1.8.0.     Returns \n \nin1d(M,) ndarray, bool\n\n\nThe values ar1[in1d] are in ar2.      See also  isin\n\nVersion of this function that preserves the shape of ar1.  numpy.lib.arraysetops\n\nModule with a number of other functions for performing set operations on arrays.    Notes in1d can be considered as an element-wise function version of the python keyword in, for 1-D sequences. in1d(a, b) is roughly equivalent to np.array([item in b for item in a]). However, this idea fails if ar2 is a set, or similar (non-sequence) container: As ar2 is converted to an array, in those cases asarray(ar2) is an object array rather than the expected array of contained values.  New in version 1.4.0.  Examples >>> test = np.array([0, 1, 2, 5, 0])\n>>> states = [0, 2]\n>>> mask = np.in1d(test, states)\n>>> mask\narray([ True, False,  True, False,  True])\n>>> test[mask]\narray([0, 2, 0])\n>>> mask = np.in1d(test, states, invert=True)\n>>> mask\narray([False,  True, False,  True, False])\n>>> test[mask]\narray([1, 5])","title":"numpy.reference.generated.numpy.in1d"},{"text":"pandas.Series.align   Series.align(other, join='outer', axis=None, level=None, copy=True, fill_value=None, method=None, limit=None, fill_axis=0, broadcast_axis=None)[source]\n \nAlign two objects on their axes with the specified join method. Join method is specified for each axis Index.  Parameters \n \nother:DataFrame or Series\n\n\njoin:{\u2018outer\u2019, \u2018inner\u2019, \u2018left\u2019, \u2018right\u2019}, default \u2018outer\u2019\n\n\naxis:allowed axis of the other object, default None\n\n\nAlign on index (0), columns (1), or both (None).  \nlevel:int or level name, default None\n\n\nBroadcast across a level, matching Index values on the passed MultiIndex level.  \ncopy:bool, default True\n\n\nAlways returns new objects. If copy=False and no reindexing is required then original objects are returned.  \nfill_value:scalar, default np.NaN\n\n\nValue to use for missing values. Defaults to NaN, but can be any \u201ccompatible\u201d value.  \nmethod:{\u2018backfill\u2019, \u2018bfill\u2019, \u2018pad\u2019, \u2018ffill\u2019, None}, default None\n\n\nMethod to use for filling holes in reindexed Series:  pad \/ ffill: propagate last valid observation forward to next valid. backfill \/ bfill: use NEXT valid observation to fill gap.   \nlimit:int, default None\n\n\nIf method is specified, this is the maximum number of consecutive NaN values to forward\/backward fill. In other words, if there is a gap with more than this number of consecutive NaNs, it will only be partially filled. If method is not specified, this is the maximum number of entries along the entire axis where NaNs will be filled. Must be greater than 0 if not None.  \nfill_axis:{0 or \u2018index\u2019}, default 0\n\n\nFilling axis, method and limit.  \nbroadcast_axis:{0 or \u2018index\u2019}, default None\n\n\nBroadcast values along this axis, if aligning two objects of different dimensions.    Returns \n \n(left, right):(Series, type of other)\n\n\nAligned objects.     Examples \n>>> df = pd.DataFrame(\n...     [[1, 2, 3, 4], [6, 7, 8, 9]], columns=[\"D\", \"B\", \"E\", \"A\"], index=[1, 2]\n... )\n>>> other = pd.DataFrame(\n...     [[10, 20, 30, 40], [60, 70, 80, 90], [600, 700, 800, 900]],\n...     columns=[\"A\", \"B\", \"C\", \"D\"],\n...     index=[2, 3, 4],\n... )\n>>> df\n   D  B  E  A\n1  1  2  3  4\n2  6  7  8  9\n>>> other\n    A    B    C    D\n2   10   20   30   40\n3   60   70   80   90\n4  600  700  800  900\n  Align on columns: \n>>> left, right = df.align(other, join=\"outer\", axis=1)\n>>> left\n   A  B   C  D  E\n1  4  2 NaN  1  3\n2  9  7 NaN  6  8\n>>> right\n    A    B    C    D   E\n2   10   20   30   40 NaN\n3   60   70   80   90 NaN\n4  600  700  800  900 NaN\n  We can also align on the index: \n>>> left, right = df.align(other, join=\"outer\", axis=0)\n>>> left\n    D    B    E    A\n1  1.0  2.0  3.0  4.0\n2  6.0  7.0  8.0  9.0\n3  NaN  NaN  NaN  NaN\n4  NaN  NaN  NaN  NaN\n>>> right\n    A      B      C      D\n1    NaN    NaN    NaN    NaN\n2   10.0   20.0   30.0   40.0\n3   60.0   70.0   80.0   90.0\n4  600.0  700.0  800.0  900.0\n  Finally, the default axis=None will align on both index and columns: \n>>> left, right = df.align(other, join=\"outer\", axis=None)\n>>> left\n     A    B   C    D    E\n1  4.0  2.0 NaN  1.0  3.0\n2  9.0  7.0 NaN  6.0  8.0\n3  NaN  NaN NaN  NaN  NaN\n4  NaN  NaN NaN  NaN  NaN\n>>> right\n       A      B      C      D   E\n1    NaN    NaN    NaN    NaN NaN\n2   10.0   20.0   30.0   40.0 NaN\n3   60.0   70.0   80.0   90.0 NaN\n4  600.0  700.0  800.0  900.0 NaN","title":"pandas.reference.api.pandas.series.align"},{"text":"pandas.Series.compare   Series.compare(other, align_axis=1, keep_shape=False, keep_equal=False)[source]\n \nCompare to another Series and show the differences.  New in version 1.1.0.   Parameters \n \nother:Series\n\n\nObject to compare with.  \nalign_axis:{0 or \u2018index\u2019, 1 or \u2018columns\u2019}, default 1\n\n\nDetermine which axis to align the comparison on.  \n 0, or \u2018index\u2019:Resulting differences are stacked vertically\n\n\nwith rows drawn alternately from self and other.    \n 1, or \u2018columns\u2019:Resulting differences are aligned horizontally\n\n\nwith columns drawn alternately from self and other.      \nkeep_shape:bool, default False\n\n\nIf true, all rows and columns are kept. Otherwise, only the ones with different values are kept.  \nkeep_equal:bool, default False\n\n\nIf true, the result keeps values that are equal. Otherwise, equal values are shown as NaNs.    Returns \n Series or DataFrame\n\nIf axis is 0 or \u2018index\u2019 the result will be a Series. The resulting index will be a MultiIndex with \u2018self\u2019 and \u2018other\u2019 stacked alternately at the inner level. If axis is 1 or \u2018columns\u2019 the result will be a DataFrame. It will have two columns namely \u2018self\u2019 and \u2018other\u2019.      See also  DataFrame.compare\n\nCompare with another DataFrame and show differences.    Notes Matching NaNs will not appear as a difference. Examples \n>>> s1 = pd.Series([\"a\", \"b\", \"c\", \"d\", \"e\"])\n>>> s2 = pd.Series([\"a\", \"a\", \"c\", \"b\", \"e\"])\n  Align the differences on columns \n>>> s1.compare(s2)\n  self other\n1    b     a\n3    d     b\n  Stack the differences on indices \n>>> s1.compare(s2, align_axis=0)\n1  self     b\n   other    a\n3  self     d\n   other    b\ndtype: object\n  Keep all original rows \n>>> s1.compare(s2, keep_shape=True)\n  self other\n0  NaN   NaN\n1    b     a\n2  NaN   NaN\n3    d     b\n4  NaN   NaN\n  Keep all original rows and also all original values \n>>> s1.compare(s2, keep_shape=True, keep_equal=True)\n  self other\n0    a     a\n1    b     a\n2    c     c\n3    d     b\n4    e     e","title":"pandas.reference.api.pandas.series.compare"},{"text":"pandas.testing.assert_series_equal   pandas.testing.assert_series_equal(left, right, check_dtype=True, check_index_type='equiv', check_series_type=True, check_less_precise=NoDefault.no_default, check_names=True, check_exact=False, check_datetimelike_compat=False, check_categorical=True, check_category_order=True, check_freq=True, check_flags=True, rtol=1e-05, atol=1e-08, obj='Series', *, check_index=True)[source]\n \nCheck that left and right Series are equal.  Parameters \n \nleft:Series\n\n\nright:Series\n\n\ncheck_dtype:bool, default True\n\n\nWhether to check the Series dtype is identical.  \ncheck_index_type:bool or {\u2018equiv\u2019}, default \u2018equiv\u2019\n\n\nWhether to check the Index class, dtype and inferred_type are identical.  \ncheck_series_type:bool, default True\n\n\nWhether to check the Series class is identical.  \ncheck_less_precise:bool or int, default False\n\n\nSpecify comparison precision. Only used when check_exact is False. 5 digits (False) or 3 digits (True) after decimal points are compared. If int, then specify the digits to compare. When comparing two numbers, if the first number has magnitude less than 1e-5, we compare the two numbers directly and check whether they are equivalent within the specified precision. Otherwise, we compare the ratio of the second number to the first number and check whether it is equivalent to 1 within the specified precision.  Deprecated since version 1.1.0: Use rtol and atol instead to define relative\/absolute tolerance, respectively. Similar to math.isclose().   \ncheck_names:bool, default True\n\n\nWhether to check the Series and Index names attribute.  \ncheck_exact:bool, default False\n\n\nWhether to compare number exactly.  \ncheck_datetimelike_compat:bool, default False\n\n\nCompare datetime-like which is comparable ignoring dtype.  \ncheck_categorical:bool, default True\n\n\nWhether to compare internal Categorical exactly.  \ncheck_category_order:bool, default True\n\n\nWhether to compare category order of internal Categoricals.  New in version 1.0.2.   \ncheck_freq:bool, default True\n\n\nWhether to check the freq attribute on a DatetimeIndex or TimedeltaIndex.  New in version 1.1.0.   \ncheck_flags:bool, default True\n\n\nWhether to check the flags attribute.  New in version 1.2.0.   \nrtol:float, default 1e-5\n\n\nRelative tolerance. Only used when check_exact is False.  New in version 1.1.0.   \natol:float, default 1e-8\n\n\nAbsolute tolerance. Only used when check_exact is False.  New in version 1.1.0.   \nobj:str, default \u2018Series\u2019\n\n\nSpecify object name being compared, internally used to show appropriate assertion message.  \ncheck_index:bool, default True\n\n\nWhether to check index equivalence. If False, then compare only values.  New in version 1.3.0.      Examples \n>>> from pandas import testing as tm\n>>> a = pd.Series([1, 2, 3, 4])\n>>> b = pd.Series([1, 2, 3, 4])\n>>> tm.assert_series_equal(a, b)","title":"pandas.reference.api.pandas.testing.assert_series_equal"},{"text":"pandas.Index.intersection   finalIndex.intersection(other, sort=False)[source]\n \nForm the intersection of two Index objects. This returns a new Index with elements common to the index and other.  Parameters \n \nother:Index or array-like\n\n\nsort:False or None, default False\n\n\nWhether to sort the resulting index.  False : do not sort the result. None : sort the result, except when self and other are equal or when the values cannot be compared.     Returns \n \nintersection:Index\n\n   Examples \n>>> idx1 = pd.Index([1, 2, 3, 4])\n>>> idx2 = pd.Index([3, 4, 5, 6])\n>>> idx1.intersection(idx2)\nInt64Index([3, 4], dtype='int64')","title":"pandas.reference.api.pandas.index.intersection"},{"text":"pandas.Series.multiply   Series.multiply(other, level=None, fill_value=None, axis=0)[source]\n \nReturn Multiplication of series and other, element-wise (binary operator mul). Equivalent to series * other, but with support to substitute a fill_value for missing data in either one of the inputs.  Parameters \n \nother:Series or scalar value\n\n\nfill_value:None or float value, default None (NaN)\n\n\nFill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing.  \nlevel:int or name\n\n\nBroadcast across a level, matching Index values on the passed MultiIndex level.    Returns \n Series\n\nThe result of the operation.      See also  Series.rmul\n\nReverse of the Multiplication operator, see Python documentation for more details.    Examples \n>>> a = pd.Series([1, 1, 1, np.nan], index=['a', 'b', 'c', 'd'])\n>>> a\na    1.0\nb    1.0\nc    1.0\nd    NaN\ndtype: float64\n>>> b = pd.Series([1, np.nan, 1, np.nan], index=['a', 'b', 'd', 'e'])\n>>> b\na    1.0\nb    NaN\nd    1.0\ne    NaN\ndtype: float64\n>>> a.multiply(b, fill_value=0)\na    1.0\nb    0.0\nc    0.0\nd    0.0\ne    NaN\ndtype: float64","title":"pandas.reference.api.pandas.series.multiply"}]}
{"task_id":8315209,"prompt":"def f_8315209(client):\n\t","suffix":"\n\treturn ","canonical_solution":"client.send('HTTP\/1.0 200 OK\\r\\n')","test_start":"\nimport socket\nfrom unittest.mock import Mock\nimport mock\n\ndef check(candidate):","test":["\n    with mock.patch('socket.socket') as mock_socket:\n        mock_socket.return_value.recv.return_value = ''\n        mock_socket.bind(('', 8080))\n        mock_socket.listen(5)\n        mock_socket.accept = Mock(return_value = mock_socket)\n        mock_socket.send = Mock()\n        try:\n            candidate(mock_socket)\n        except:\n            assert False\n"],"entry_point":"f_8315209","intent":"sending http headers to `client`","library":["socket"],"docs":[{"text":"flush_headers()  \nFinally send the headers to the output stream and flush the internal headers buffer.  New in version 3.3.","title":"python.library.http.server#http.server.BaseHTTPRequestHandler.flush_headers"},{"text":"FileResponse.set_headers(open_file)  \nThis method is automatically called during the response initialization and set various headers (Content-Length, Content-Type, and Content-Disposition) depending on open_file.","title":"django.ref.request-response#django.http.FileResponse.set_headers"},{"text":"end_headers()  \nAdds a blank line (indicating the end of the HTTP headers in the response) to the headers buffer and calls flush_headers().  Changed in version 3.2: The buffered headers are written to the output stream.","title":"python.library.http.server#http.server.BaseHTTPRequestHandler.end_headers"},{"text":"HTTPConnection.endheaders(message_body=None, *, encode_chunked=False)  \nSend a blank line to the server, signalling the end of the headers. The optional message_body argument can be used to pass a message body associated with the request. If encode_chunked is True, the result of each iteration of message_body will be chunk-encoded as specified in RFC 7230, Section 3.3.1. How the data is encoded is dependent on the type of message_body. If message_body implements the buffer interface the encoding will result in a single chunk. If message_body is a collections.abc.Iterable, each iteration of message_body will result in a chunk. If message_body is a file object, each call to .read() will result in a chunk. The method automatically signals the end of the chunk-encoded data immediately after message_body.  Note Due to the chunked encoding specification, empty chunks yielded by an iterator body will be ignored by the chunk-encoder. This is to avoid premature termination of the read of the request by the target server due to malformed encoding.   New in version 3.6: Chunked encoding support. The encode_chunked parameter was added.","title":"python.library.http.client#http.client.HTTPConnection.endheaders"},{"text":"do_HEAD()  \nThis method serves the 'HEAD' request type: it sends the headers it would send for the equivalent GET request. See the do_GET() method for a more complete explanation of the possible headers.","title":"python.library.http.server#http.server.SimpleHTTPRequestHandler.do_HEAD"},{"text":"HTTPConnection.putheader(header, argument[, ...])  \nSend an RFC 822-style header to the server. It sends a line to the server consisting of the header, a colon and a space, and the first argument. If more arguments are given, continuation lines are sent, each consisting of a tab and an argument.","title":"python.library.http.client#http.client.HTTPConnection.putheader"},{"text":"send_response(code, message=None)  \nAdds a response header to the headers buffer and logs the accepted request. The HTTP response line is written to the internal buffer, followed by Server and Date headers. The values for these two headers are picked up from the version_string() and date_time_string() methods, respectively. If the server does not intend to send any other headers using the send_header() method, then send_response() should be followed by an end_headers() call.  Changed in version 3.3: Headers are stored to an internal buffer and end_headers() needs to be called explicitly.","title":"python.library.http.server#http.server.BaseHTTPRequestHandler.send_response"},{"text":"headers  \nHolds an instance of the class specified by the MessageClass class variable. This instance parses and manages the headers in the HTTP request. The parse_headers() function from http.client is used to parse the headers and it requires that the HTTP request provide a valid RFC 2822 style header.","title":"python.library.http.server#http.server.BaseHTTPRequestHandler.headers"},{"text":"send_header(keyword, value)  \nAdds the HTTP header to an internal buffer which will be written to the output stream when either end_headers() or flush_headers() is invoked. keyword should specify the header keyword, with value specifying its value. Note that, after the send_header calls are done, end_headers() MUST BE called in order to complete the operation.  Changed in version 3.2: Headers are stored in an internal buffer.","title":"python.library.http.server#http.server.BaseHTTPRequestHandler.send_header"},{"text":"HTTPHandler.http_open(req)  \nSend an HTTP request, which can be either GET or POST, depending on req.has_data().","title":"python.library.urllib.request#urllib.request.HTTPHandler.http_open"}]}
{"task_id":26153795,"prompt":"def f_26153795(when):\n\treturn ","suffix":"","canonical_solution":"datetime.datetime.strptime(when, '%Y-%m-%d').date()","test_start":"\nimport datetime\n\ndef check(candidate):","test":["\n    assert candidate('2013-05-07') == datetime.date(2013, 5, 7)\n","\n    assert candidate('2000-02-29') == datetime.date(2000, 2, 29)\n","\n    assert candidate('1990-01-08') == datetime.date(1990, 1, 8)\n","\n    assert candidate('1990-1-08') == datetime.date(1990, 1, 8)\n","\n    assert candidate('1990-1-8') == datetime.date(1990, 1, 8)\n","\n    assert candidate('1990-01-8') == datetime.date(1990, 1, 8)\n"],"entry_point":"f_26153795","intent":"Format a datetime string `when` to extract date only","library":["datetime"],"docs":[{"text":"class When(condition=None, then=None, **lookups)","title":"django.ref.models.conditional-expressions#django.db.models.expressions.When"},{"text":"tty.setraw(fd, when=termios.TCSAFLUSH)  \nChange the mode of the file descriptor fd to raw. If when is omitted, it defaults to termios.TCSAFLUSH, and is passed to termios.tcsetattr().","title":"python.library.tty#tty.setraw"},{"text":"write_sys_ex() \n writes a timestamped system-exclusive midi message. write_sys_ex(when, msg) -> None  Writes a timestamped system-exclusive midi message.     \nParameters:\n\n \nmsg (list[int] or str) -- midi message \nwhen -- timestamp in milliseconds      Example: midi_output.write_sys_ex(0, '\\xF0\\x7D\\x10\\x11\\x12\\x13\\xF7')\n\n# is equivalent to\n\nmidi_output.write_sys_ex(pygame.midi.time(),\n                         [0xF0, 0x7D, 0x10, 0x11, 0x12, 0x13, 0xF7])","title":"pygame.ref.midi#pygame.midi.Output.write_sys_ex"},{"text":"pandas.DataFrame.asof   DataFrame.asof(where, subset=None)[source]\n \nReturn the last row(s) without any NaNs before where. The last row (for each element in where, if list) without any NaN is taken. In case of a DataFrame, the last row without NaN considering only the subset of columns (if not None) If there is no good value, NaN is returned for a Series or a Series of NaN values for a DataFrame  Parameters \n \nwhere:date or array-like of dates\n\n\nDate(s) before which the last row(s) are returned.  \nsubset:str or array-like of str, default None\n\n\nFor DataFrame, if not None, only use these columns to check for NaNs.    Returns \n scalar, Series, or DataFrame\n\nThe return can be:  scalar : when self is a Series and where is a scalar Series: when self is a Series and where is an array-like, or when self is a DataFrame and where is a scalar DataFrame : when self is a DataFrame and where is an array-like  Return scalar, Series, or DataFrame.      See also  merge_asof\n\nPerform an asof merge. Similar to left join.    Notes Dates are assumed to be sorted. Raises if this is not the case. Examples A Series and a scalar where. \n>>> s = pd.Series([1, 2, np.nan, 4], index=[10, 20, 30, 40])\n>>> s\n10    1.0\n20    2.0\n30    NaN\n40    4.0\ndtype: float64\n  \n>>> s.asof(20)\n2.0\n  For a sequence where, a Series is returned. The first value is NaN, because the first element of where is before the first index value. \n>>> s.asof([5, 20])\n5     NaN\n20    2.0\ndtype: float64\n  Missing values are not considered. The following is 2.0, not NaN, even though NaN is at the index location for 30. \n>>> s.asof(30)\n2.0\n  Take all columns into consideration \n>>> df = pd.DataFrame({'a': [10, 20, 30, 40, 50],\n...                    'b': [None, None, None, None, 500]},\n...                   index=pd.DatetimeIndex(['2018-02-27 09:01:00',\n...                                           '2018-02-27 09:02:00',\n...                                           '2018-02-27 09:03:00',\n...                                           '2018-02-27 09:04:00',\n...                                           '2018-02-27 09:05:00']))\n>>> df.asof(pd.DatetimeIndex(['2018-02-27 09:03:30',\n...                           '2018-02-27 09:04:30']))\n                      a   b\n2018-02-27 09:03:30 NaN NaN\n2018-02-27 09:04:30 NaN NaN\n  Take a single column into consideration \n>>> df.asof(pd.DatetimeIndex(['2018-02-27 09:03:30',\n...                           '2018-02-27 09:04:30']),\n...         subset=['a'])\n                         a   b\n2018-02-27 09:03:30   30.0 NaN\n2018-02-27 09:04:30   40.0 NaN","title":"pandas.reference.api.pandas.dataframe.asof"},{"text":"pandas.Series.asof   Series.asof(where, subset=None)[source]\n \nReturn the last row(s) without any NaNs before where. The last row (for each element in where, if list) without any NaN is taken. In case of a DataFrame, the last row without NaN considering only the subset of columns (if not None) If there is no good value, NaN is returned for a Series or a Series of NaN values for a DataFrame  Parameters \n \nwhere:date or array-like of dates\n\n\nDate(s) before which the last row(s) are returned.  \nsubset:str or array-like of str, default None\n\n\nFor DataFrame, if not None, only use these columns to check for NaNs.    Returns \n scalar, Series, or DataFrame\n\nThe return can be:  scalar : when self is a Series and where is a scalar Series: when self is a Series and where is an array-like, or when self is a DataFrame and where is a scalar DataFrame : when self is a DataFrame and where is an array-like  Return scalar, Series, or DataFrame.      See also  merge_asof\n\nPerform an asof merge. Similar to left join.    Notes Dates are assumed to be sorted. Raises if this is not the case. Examples A Series and a scalar where. \n>>> s = pd.Series([1, 2, np.nan, 4], index=[10, 20, 30, 40])\n>>> s\n10    1.0\n20    2.0\n30    NaN\n40    4.0\ndtype: float64\n  \n>>> s.asof(20)\n2.0\n  For a sequence where, a Series is returned. The first value is NaN, because the first element of where is before the first index value. \n>>> s.asof([5, 20])\n5     NaN\n20    2.0\ndtype: float64\n  Missing values are not considered. The following is 2.0, not NaN, even though NaN is at the index location for 30. \n>>> s.asof(30)\n2.0\n  Take all columns into consideration \n>>> df = pd.DataFrame({'a': [10, 20, 30, 40, 50],\n...                    'b': [None, None, None, None, 500]},\n...                   index=pd.DatetimeIndex(['2018-02-27 09:01:00',\n...                                           '2018-02-27 09:02:00',\n...                                           '2018-02-27 09:03:00',\n...                                           '2018-02-27 09:04:00',\n...                                           '2018-02-27 09:05:00']))\n>>> df.asof(pd.DatetimeIndex(['2018-02-27 09:03:30',\n...                           '2018-02-27 09:04:30']))\n                      a   b\n2018-02-27 09:03:30 NaN NaN\n2018-02-27 09:04:30 NaN NaN\n  Take a single column into consideration \n>>> df.asof(pd.DatetimeIndex(['2018-02-27 09:03:30',\n...                           '2018-02-27 09:04:30']),\n...         subset=['a'])\n                         a   b\n2018-02-27 09:03:30   30.0 NaN\n2018-02-27 09:04:30   40.0 NaN","title":"pandas.reference.api.pandas.series.asof"},{"text":"tty.setcbreak(fd, when=termios.TCSAFLUSH)  \nChange the mode of file descriptor fd to cbreak. If when is omitted, it defaults to termios.TCSAFLUSH, and is passed to termios.tcsetattr().","title":"python.library.tty#tty.setcbreak"},{"text":"pandas.core.window.ewm.ExponentialMovingWindow.std   ExponentialMovingWindow.std(bias=False, *args, **kwargs)[source]\n \nCalculate the ewm (exponential weighted moment) standard deviation.  Parameters \n \nbias:bool, default False\n\n\nUse a standard estimation bias correction.  *args\n\nFor NumPy compatibility and will not have an effect on the result.  **kwargs\n\nFor NumPy compatibility and will not have an effect on the result.    Returns \n Series or DataFrame\n\nReturn type is the same as the original object with np.float64 dtype.      See also  pandas.Series.ewm\n\nCalling ewm with Series data.  pandas.DataFrame.ewm\n\nCalling ewm with DataFrames.  pandas.Series.std\n\nAggregating std for Series.  pandas.DataFrame.std\n\nAggregating std for DataFrame.","title":"pandas.reference.api.pandas.core.window.ewm.exponentialmovingwindow.std"},{"text":"set_date(date)  \nSet the delivery date of the message to date, a floating-point number representing seconds since the epoch.","title":"python.library.mailbox#mailbox.MaildirMessage.set_date"},{"text":"pandas.core.resample.Resampler.std   Resampler.std(ddof=1, *args, **kwargs)[source]\n \nCompute standard deviation of groups, excluding missing values.  Parameters \n \nddof:int, default 1\n\n\nDegrees of freedom.    Returns \n DataFrame or Series\n\nStandard deviation of values within each group.","title":"pandas.reference.api.pandas.core.resample.resampler.std"},{"text":"termios.tcsetattr(fd, when, attributes)  \nSet the tty attributes for file descriptor fd from the attributes, which is a list like the one returned by tcgetattr(). The when argument determines when the attributes are changed: TCSANOW to change immediately, TCSADRAIN to change after transmitting all queued output, or TCSAFLUSH to change after transmitting all queued output and discarding all queued input.","title":"python.library.termios#termios.tcsetattr"}]}
{"task_id":172439,"prompt":"def f_172439(inputString):\n\treturn ","suffix":"","canonical_solution":"inputString.split('\\n')","test_start":"\ndef check(candidate):","test":["\n    assert candidate('line a\\nfollows by line b\t...bye\\n') ==         ['line a', 'follows by line b\t...bye', '']\n","\n    assert candidate('no new line in this sentence. ') == ['no new line in this sentence. ']\n","\n    assert candidate('a\tbfs hhhdf\tsfdas') == ['a\tbfs hhhdf\tsfdas']\n","\n    assert candidate('') == ['']\n"],"entry_point":"f_172439","intent":"split a multi-line string `inputString` into separate strings","library":[],"docs":[]}
{"task_id":172439,"prompt":"def f_172439():\n\treturn ","suffix":"","canonical_solution":"' a \\n b \\r\\n c '.split('\\n')","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == [' a ', ' b \\r', ' c ']\n"],"entry_point":"f_172439","intent":"Split a multi-line string ` a \\n b \\r\\n c ` by new line character `\\n`","library":[],"docs":[]}
{"task_id":13954222,"prompt":"def f_13954222(b):\n\treturn ","suffix":"","canonical_solution":"\"\"\":\"\"\".join(str(x) for x in b)","test_start":"\ndef check(candidate):","test":["\n    assert candidate(['x','y','zzz']) == 'x:y:zzz'\n","\n    assert candidate(['111','22','3']) == '111:22:3'\n","\n    assert candidate(['']) == ''\n","\n    assert candidate([':',':']) == ':::'\n","\n    assert candidate([',','#','#$%']) == ',:#:#$%'\n","\n    assert candidate(['a','b','c']) != 'abc'\n"],"entry_point":"f_13954222","intent":"concatenate elements of list `b` by a colon \":\"","library":[],"docs":[]}
{"task_id":13567345,"prompt":"def f_13567345(a):\n\treturn ","suffix":"","canonical_solution":"a.sum(axis=1)","test_start":"\nimport numpy as np \n\ndef check(candidate):","test":["\n    a1 = np.array([[i for i in range(3)] for j in range(5)])\n    assert np.array_equal(candidate(a1), np.array([3, 3, 3, 3, 3]))\n","\n    a2 = np.array([[i+j for i in range(3)] for j in range(5)])\n    assert np.array_equal(candidate(a2), np.array([ 3,  6,  9, 12, 15]))\n","\n    a3 = np.array([[i*j for i in range(3)] for j in range(5)])\n    assert np.array_equal(candidate(a3), np.array([ 0,  3,  6,  9, 12]))\n"],"entry_point":"f_13567345","intent":"Calculate sum over all rows of 2D numpy array `a`","library":["numpy"],"docs":[{"text":"numpy.matrix.sum method   matrix.sum(axis=None, dtype=None, out=None)[source]\n \nReturns the sum of the matrix elements, along the given axis. Refer to numpy.sum for full documentation.  See also  numpy.sum\n  Notes This is the same as ndarray.sum, except that where an ndarray would be returned, a matrix object is returned instead. Examples >>> x = np.matrix([[1, 2], [4, 3]])\n>>> x.sum()\n10\n>>> x.sum(axis=1)\nmatrix([[3],\n        [7]])\n>>> x.sum(axis=1, dtype='float')\nmatrix([[3.],\n        [7.]])\n>>> out = np.zeros((2, 1), dtype='float')\n>>> x.sum(axis=1, dtype='float', out=np.asmatrix(out))\nmatrix([[3.],\n        [7.]])","title":"numpy.reference.generated.numpy.matrix.sum"},{"text":"numpy.ndarray.sum method   ndarray.sum(axis=None, dtype=None, out=None, keepdims=False, initial=0, where=True)\n \nReturn the sum of the array elements over the given axis. Refer to numpy.sum for full documentation.  See also  numpy.sum\n\nequivalent function","title":"numpy.reference.generated.numpy.ndarray.sum"},{"text":"numpy.trace   numpy.trace(a, offset=0, axis1=0, axis2=1, dtype=None, out=None)[source]\n \nReturn the sum along diagonals of the array. If a is 2-D, the sum along its diagonal with the given offset is returned, i.e., the sum of elements a[i,i+offset] for all i. If a has more than two dimensions, then the axes specified by axis1 and axis2 are used to determine the 2-D sub-arrays whose traces are returned. The shape of the resulting array is the same as that of a with axis1 and axis2 removed.  Parameters \n \naarray_like\n\n\nInput array, from which the diagonals are taken.  \noffsetint, optional\n\n\nOffset of the diagonal from the main diagonal. Can be both positive and negative. Defaults to 0.  \naxis1, axis2int, optional\n\n\nAxes to be used as the first and second axis of the 2-D sub-arrays from which the diagonals should be taken. Defaults are the first two axes of a.  \ndtypedtype, optional\n\n\nDetermines the data-type of the returned array and of the accumulator where the elements are summed. If dtype has the value None and a is of integer type of precision less than the default integer precision, then the default integer precision is used. Otherwise, the precision is the same as that of a.  \noutndarray, optional\n\n\nArray into which the output is placed. Its type is preserved and it must be of the right shape to hold the output.    Returns \n \nsum_along_diagonalsndarray\n\n\nIf a is 2-D, the sum along the diagonal is returned. If a has larger dimensions, then an array of sums along diagonals is returned.      See also  \ndiag, diagonal, diagflat\n\n  Examples >>> np.trace(np.eye(3))\n3.0\n>>> a = np.arange(8).reshape((2,2,2))\n>>> np.trace(a)\narray([6, 8])\n >>> a = np.arange(24).reshape((2,2,2,3))\n>>> np.trace(a).shape\n(2, 3)","title":"numpy.reference.generated.numpy.trace"},{"text":"numpy.recarray.sum method   recarray.sum(axis=None, dtype=None, out=None, keepdims=False, initial=0, where=True)\n \nReturn the sum of the array elements over the given axis. Refer to numpy.sum for full documentation.  See also  numpy.sum\n\nequivalent function","title":"numpy.reference.generated.numpy.recarray.sum"},{"text":"numpy.ma.sum   ma.sum(self, axis=None, dtype=None, out=None, keepdims=<no value>) = <numpy.ma.core._frommethod object>\n \nReturn the sum of the array elements over the given axis. Masked elements are set to 0 internally. Refer to numpy.sum for full documentation.  See also  numpy.ndarray.sum\n\ncorresponding function for ndarrays  numpy.sum\n\nequivalent function    Examples >>> x = np.ma.array([[1,2,3],[4,5,6],[7,8,9]], mask=[0] + [1,0]*4)\n>>> x\nmasked_array(\n  data=[[1, --, 3],\n        [--, 5, --],\n        [7, --, 9]],\n  mask=[[False,  True, False],\n        [ True, False,  True],\n        [False,  True, False]],\n  fill_value=999999)\n>>> x.sum()\n25\n>>> x.sum(axis=1)\nmasked_array(data=[4, 5, 16],\n             mask=[False, False, False],\n       fill_value=999999)\n>>> x.sum(axis=0)\nmasked_array(data=[8, 5, 12],\n             mask=[False, False, False],\n       fill_value=999999)\n>>> print(type(x.sum(axis=0, dtype=np.int64)[0]))\n<class 'numpy.int64'>","title":"numpy.reference.generated.numpy.ma.sum"},{"text":"numpy.ndarray.trace method   ndarray.trace(offset=0, axis1=0, axis2=1, dtype=None, out=None)\n \nReturn the sum along diagonals of the array. Refer to numpy.trace for full documentation.  See also  numpy.trace\n\nequivalent function","title":"numpy.reference.generated.numpy.ndarray.trace"},{"text":"tf.experimental.numpy.sum TensorFlow variant of NumPy's sum. \ntf.experimental.numpy.sum(\n    a, axis=None, dtype=None, keepdims=None\n)\n Unsupported arguments: out, initial, where. See the NumPy documentation for numpy.sum.","title":"tensorflow.experimental.numpy.sum"},{"text":"numpy.sum   numpy.sum(a, axis=None, dtype=None, out=None, keepdims=<no value>, initial=<no value>, where=<no value>)[source]\n \nSum of array elements over a given axis.  Parameters \n \naarray_like\n\n\nElements to sum.  \naxisNone or int or tuple of ints, optional\n\n\nAxis or axes along which a sum is performed. The default, axis=None, will sum all of the elements of the input array. If axis is negative it counts from the last to the first axis.  New in version 1.7.0.  If axis is a tuple of ints, a sum is performed on all of the axes specified in the tuple instead of a single axis or all the axes as before.  \ndtypedtype, optional\n\n\nThe type of the returned array and of the accumulator in which the elements are summed. The dtype of a is used by default unless a has an integer dtype of less precision than the default platform integer. In that case, if a is signed then the platform integer is used while if a is unsigned then an unsigned integer of the same precision as the platform integer is used.  \noutndarray, optional\n\n\nAlternative output array in which to place the result. It must have the same shape as the expected output, but the type of the output values will be cast if necessary.  \nkeepdimsbool, optional\n\n\nIf this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array. If the default value is passed, then keepdims will not be passed through to the sum method of sub-classes of ndarray, however any non-default value will be. If the sub-class\u2019 method does not implement keepdims any exceptions will be raised.  \ninitialscalar, optional\n\n\nStarting value for the sum. See reduce for details.  New in version 1.15.0.   \nwherearray_like of bool, optional\n\n\nElements to include in the sum. See reduce for details.  New in version 1.17.0.     Returns \n \nsum_along_axisndarray\n\n\nAn array with the same shape as a, with the specified axis removed. If a is a 0-d array, or if axis is None, a scalar is returned. If an output array is specified, a reference to out is returned.      See also  ndarray.sum\n\nEquivalent method.  add.reduce\n\nEquivalent functionality of add.  cumsum\n\nCumulative sum of array elements.  trapz\n\nIntegration of array values using the composite trapezoidal rule.  \nmean, average\n\n  Notes Arithmetic is modular when using integer types, and no error is raised on overflow. The sum of an empty array is the neutral element 0: >>> np.sum([])\n0.0\n For floating point numbers the numerical precision of sum (and np.add.reduce) is in general limited by directly adding each number individually to the result causing rounding errors in every step. However, often numpy will use a numerically better approach (partial pairwise summation) leading to improved precision in many use-cases. This improved precision is always provided when no axis is given. When axis is given, it will depend on which axis is summed. Technically, to provide the best speed possible, the improved precision is only used when the summation is along the fast axis in memory. Note that the exact precision may vary depending on other parameters. In contrast to NumPy, Python\u2019s math.fsum function uses a slower but more precise approach to summation. Especially when summing a large number of lower precision floating point numbers, such as float32, numerical errors can become significant. In such cases it can be advisable to use dtype=\u201dfloat64\u201d to use a higher precision for the output. Examples >>> np.sum([0.5, 1.5])\n2.0\n>>> np.sum([0.5, 0.7, 0.2, 1.5], dtype=np.int32)\n1\n>>> np.sum([[0, 1], [0, 5]])\n6\n>>> np.sum([[0, 1], [0, 5]], axis=0)\narray([0, 6])\n>>> np.sum([[0, 1], [0, 5]], axis=1)\narray([1, 5])\n>>> np.sum([[0, 1], [np.nan, 5]], where=[False, True], axis=1)\narray([1., 5.])\n If the accumulator is too small, overflow occurs: >>> np.ones(128, dtype=np.int8).sum(dtype=np.int8)\n-128\n You can also start the sum with a value other than zero: >>> np.sum([10], initial=5)\n15","title":"numpy.reference.generated.numpy.sum"},{"text":"numpy.matrix.trace method   matrix.trace(offset=0, axis1=0, axis2=1, dtype=None, out=None)\n \nReturn the sum along diagonals of the array. Refer to numpy.trace for full documentation.  See also  numpy.trace\n\nequivalent function","title":"numpy.reference.generated.numpy.matrix.trace"},{"text":"numpy.ma.MaskedArray.sum method   ma.MaskedArray.sum(axis=None, dtype=None, out=None, keepdims=<no value>)[source]\n \nReturn the sum of the array elements over the given axis. Masked elements are set to 0 internally. Refer to numpy.sum for full documentation.  See also  numpy.ndarray.sum\n\ncorresponding function for ndarrays  numpy.sum\n\nequivalent function    Examples >>> x = np.ma.array([[1,2,3],[4,5,6],[7,8,9]], mask=[0] + [1,0]*4)\n>>> x\nmasked_array(\n  data=[[1, --, 3],\n        [--, 5, --],\n        [7, --, 9]],\n  mask=[[False,  True, False],\n        [ True, False,  True],\n        [False,  True, False]],\n  fill_value=999999)\n>>> x.sum()\n25\n>>> x.sum(axis=1)\nmasked_array(data=[4, 5, 16],\n             mask=[False, False, False],\n       fill_value=999999)\n>>> x.sum(axis=0)\nmasked_array(data=[8, 5, 12],\n             mask=[False, False, False],\n       fill_value=999999)\n>>> print(type(x.sum(axis=0, dtype=np.int64)[0]))\n<class 'numpy.int64'>","title":"numpy.reference.generated.numpy.ma.maskedarray.sum"}]}
{"task_id":29784889,"prompt":"def f_29784889():\n\t","suffix":"\n\treturn ","canonical_solution":"warnings.simplefilter('always')","test_start":"\nimport warnings \n\ndef check(candidate):","test":["\n    candidate() \n    assert any([(wf[0] == 'always') for wf in warnings.filters])\n"],"entry_point":"f_29784889","intent":"enable warnings using action 'always'","library":["warnings"],"docs":[{"text":"warnings.resetwarnings()  \nReset the warnings filter. This discards the effect of all previous calls to filterwarnings(), including that of the -W command line options and calls to simplefilter().","title":"python.library.warnings#warnings.resetwarnings"},{"text":"logging.captureWarnings(capture)  \nThis function is used to turn the capture of warnings by logging on and off. If capture is True, warnings issued by the warnings module will be redirected to the logging system. Specifically, a warning will be formatted using warnings.formatwarning() and the resulting string logged to a logger named 'py.warnings' with a severity of WARNING. If capture is False, the redirection of warnings to the logging system will stop, and warnings will be redirected to their original destinations (i.e. those in effect before captureWarnings(True) was called).","title":"python.library.logging#logging.captureWarnings"},{"text":"numpy.testing.suppress_warnings   class numpy.testing.suppress_warnings(forwarding_rule='always')[source]\n \nContext manager and decorator doing much the same as warnings.catch_warnings. However, it also provides a filter mechanism to work around https:\/\/bugs.python.org\/issue4180. This bug causes Python before 3.4 to not reliably show warnings again after they have been ignored once (even within catch_warnings). It means that no \u201cignore\u201d filter can be used easily, since following tests might need to see the warning. Additionally it allows easier specificity for testing warnings and can be nested.  Parameters \n \nforwarding_rulestr, optional\n\n\nOne of \u201calways\u201d, \u201conce\u201d, \u201cmodule\u201d, or \u201clocation\u201d. Analogous to the usual warnings module filter mode, it is useful to reduce noise mostly on the outmost level. Unsuppressed and unrecorded warnings will be forwarded based on this rule. Defaults to \u201calways\u201d. \u201clocation\u201d is equivalent to the warnings \u201cdefault\u201d, match by exact location the warning warning originated from.     Notes Filters added inside the context manager will be discarded again when leaving it. Upon entering all filters defined outside a context will be applied automatically. When a recording filter is added, matching warnings are stored in the log attribute as well as in the list returned by record. If filters are added and the module keyword is given, the warning registry of this module will additionally be cleared when applying it, entering the context, or exiting it. This could cause warnings to appear a second time after leaving the context if they were configured to be printed once (default) and were already printed before the context was entered. Nesting this context manager will work as expected when the forwarding rule is \u201calways\u201d (default). Unfiltered and unrecorded warnings will be passed out and be matched by the outer level. On the outmost level they will be printed (or caught by another warnings context). The forwarding rule argument can modify this behaviour. Like catch_warnings this context manager is not threadsafe. Examples With a context manager: with np.testing.suppress_warnings() as sup:\n    sup.filter(DeprecationWarning, \"Some text\")\n    sup.filter(module=np.ma.core)\n    log = sup.record(FutureWarning, \"Does this occur?\")\n    command_giving_warnings()\n    # The FutureWarning was given once, the filtered warnings were\n    # ignored. All other warnings abide outside settings (may be\n    # printed\/error)\n    assert_(len(log) == 1)\n    assert_(len(sup.log) == 1)  # also stored in log attribute\n Or as a decorator: sup = np.testing.suppress_warnings()\nsup.filter(module=np.ma.core)  # module must match exactly\n@sup\ndef some_function():\n    # do something which causes a warning in np.ma.core\n    pass\n Methods  \n__call__(func) Function decorator to apply certain suppressions to a whole function.  \nfilter([category, message, module]) Add a new suppressing filter or apply it if the state is entered.  \nrecord([category, message, module]) Append a new recording filter or apply it if the state is entered.","title":"numpy.reference.generated.numpy.testing.suppress_warnings"},{"text":"sys.warnoptions  \nThis is an implementation detail of the warnings framework; do not modify this value. Refer to the warnings module for more information on the warnings framework.","title":"python.library.sys#sys.warnoptions"},{"text":"class warnings.catch_warnings(*, record=False, module=None)  \nA context manager that copies and, upon exit, restores the warnings filter and the showwarning() function. If the record argument is False (the default) the context manager returns None on entry. If record is True, a list is returned that is progressively populated with objects as seen by a custom showwarning() function (which also suppresses output to sys.stdout). Each object in the list has attributes with the same names as the arguments to showwarning(). The module argument takes a module that will be used instead of the module returned when you import warnings whose filter will be protected. This argument exists primarily for testing the warnings module itself.  Note The catch_warnings manager works by replacing and then later restoring the module\u2019s showwarning() function and internal list of filter specifications. This means the context manager is modifying global state and therefore is not thread-safe.","title":"python.library.warnings#warnings.catch_warnings"},{"text":"warnings \u2014 Warning control Source code: Lib\/warnings.py Warning messages are typically issued in situations where it is useful to alert the user of some condition in a program, where that condition (normally) doesn\u2019t warrant raising an exception and terminating the program. For example, one might want to issue a warning when a program uses an obsolete module. Python programmers issue warnings by calling the warn() function defined in this module. (C programmers use PyErr_WarnEx(); see Exception Handling for details). Warning messages are normally written to sys.stderr, but their disposition can be changed flexibly, from ignoring all warnings to turning them into exceptions. The disposition of warnings can vary based on the warning category, the text of the warning message, and the source location where it is issued. Repetitions of a particular warning for the same source location are typically suppressed. There are two stages in warning control: first, each time a warning is issued, a determination is made whether a message should be issued or not; next, if a message is to be issued, it is formatted and printed using a user-settable hook. The determination whether to issue a warning message is controlled by the warning filter, which is a sequence of matching rules and actions. Rules can be added to the filter by calling filterwarnings() and reset to its default state by calling resetwarnings(). The printing of warning messages is done by calling showwarning(), which may be overridden; the default implementation of this function formats the message by calling formatwarning(), which is also available for use by custom implementations.  See also logging.captureWarnings() allows you to handle all warnings with the standard logging infrastructure.  Warning Categories There are a number of built-in exceptions that represent warning categories. This categorization is useful to be able to filter out groups of warnings. While these are technically built-in exceptions, they are documented here, because conceptually they belong to the warnings mechanism. User code can define additional warning categories by subclassing one of the standard warning categories. A warning category must always be a subclass of the Warning class. The following warnings category classes are currently defined:   \nClass Description   \nWarning This is the base class of all warning category classes. It is a subclass of Exception.  \nUserWarning The default category for warn().  \nDeprecationWarning Base category for warnings about deprecated features when those warnings are intended for other Python developers (ignored by default, unless triggered by code in __main__).  \nSyntaxWarning Base category for warnings about dubious syntactic features.  \nRuntimeWarning Base category for warnings about dubious runtime features.  \nFutureWarning Base category for warnings about deprecated features when those warnings are intended for end users of applications that are written in Python.  \nPendingDeprecationWarning Base category for warnings about features that will be deprecated in the future (ignored by default).  \nImportWarning Base category for warnings triggered during the process of importing a module (ignored by default).  \nUnicodeWarning Base category for warnings related to Unicode.  \nBytesWarning Base category for warnings related to bytes and bytearray.  \nResourceWarning Base category for warnings related to resource usage.    Changed in version 3.7: Previously DeprecationWarning and FutureWarning were distinguished based on whether a feature was being removed entirely or changing its behaviour. They are now distinguished based on their intended audience and the way they\u2019re handled by the default warnings filters.  The Warnings Filter The warnings filter controls whether warnings are ignored, displayed, or turned into errors (raising an exception). Conceptually, the warnings filter maintains an ordered list of filter specifications; any specific warning is matched against each filter specification in the list in turn until a match is found; the filter determines the disposition of the match. Each entry is a tuple of the form (action, message, category, module, lineno), where:  \naction is one of the following strings:   \nValue Disposition   \n\"default\" print the first occurrence of matching warnings for each location (module + line number) where the warning is issued  \n\"error\" turn matching warnings into exceptions  \n\"ignore\" never print matching warnings  \n\"always\" always print matching warnings  \n\"module\" print the first occurrence of matching warnings for each module where the warning is issued (regardless of line number)  \n\"once\" print only the first occurrence of matching warnings, regardless of location    \nmessage is a string containing a regular expression that the start of the warning message must match. The expression is compiled to always be case-insensitive. \ncategory is a class (a subclass of Warning) of which the warning category must be a subclass in order to match. \nmodule is a string containing a regular expression that the module name must match. The expression is compiled to be case-sensitive. \nlineno is an integer that the line number where the warning occurred must match, or 0 to match all line numbers.  Since the Warning class is derived from the built-in Exception class, to turn a warning into an error we simply raise category(message). If a warning is reported and doesn\u2019t match any registered filter then the \u201cdefault\u201d action is applied (hence its name). Describing Warning Filters The warnings filter is initialized by -W options passed to the Python interpreter command line and the PYTHONWARNINGS environment variable. The interpreter saves the arguments for all supplied entries without interpretation in sys.warnoptions; the warnings module parses these when it is first imported (invalid options are ignored, after printing a message to sys.stderr). Individual warnings filters are specified as a sequence of fields separated by colons: action:message:category:module:line\n The meaning of each of these fields is as described in The Warnings Filter. When listing multiple filters on a single line (as for PYTHONWARNINGS), the individual filters are separated by commas and the filters listed later take precedence over those listed before them (as they\u2019re applied left-to-right, and the most recently applied filters take precedence over earlier ones). Commonly used warning filters apply to either all warnings, warnings in a particular category, or warnings raised by particular modules or packages. Some examples: default                      # Show all warnings (even those ignored by default)\nignore                       # Ignore all warnings\nerror                        # Convert all warnings to errors\nerror::ResourceWarning       # Treat ResourceWarning messages as errors\ndefault::DeprecationWarning  # Show DeprecationWarning messages\nignore,default:::mymodule    # Only report warnings triggered by \"mymodule\"\nerror:::mymodule[.*]         # Convert warnings to errors in \"mymodule\"\n                             # and any subpackages of \"mymodule\"\n Default Warning Filter By default, Python installs several warning filters, which can be overridden by the -W command-line option, the PYTHONWARNINGS environment variable and calls to filterwarnings(). In regular release builds, the default warning filter has the following entries (in order of precedence): default::DeprecationWarning:__main__\nignore::DeprecationWarning\nignore::PendingDeprecationWarning\nignore::ImportWarning\nignore::ResourceWarning\n In debug builds, the list of default warning filters is empty.  Changed in version 3.2: DeprecationWarning is now ignored by default in addition to PendingDeprecationWarning.   Changed in version 3.7: DeprecationWarning is once again shown by default when triggered directly by code in __main__.   Changed in version 3.7: BytesWarning no longer appears in the default filter list and is instead configured via sys.warnoptions when -b is specified twice.  Overriding the default filter Developers of applications written in Python may wish to hide all Python level warnings from their users by default, and only display them when running tests or otherwise working on the application. The sys.warnoptions attribute used to pass filter configurations to the interpreter can be used as a marker to indicate whether or not warnings should be disabled: import sys\n\nif not sys.warnoptions:\n    import warnings\n    warnings.simplefilter(\"ignore\")\n Developers of test runners for Python code are advised to instead ensure that all warnings are displayed by default for the code under test, using code like: import sys\n\nif not sys.warnoptions:\n    import os, warnings\n    warnings.simplefilter(\"default\") # Change the filter in this process\n    os.environ[\"PYTHONWARNINGS\"] = \"default\" # Also affect subprocesses\n Finally, developers of interactive shells that run user code in a namespace other than __main__ are advised to ensure that DeprecationWarning messages are made visible by default, using code like the following (where user_ns is the module used to execute code entered interactively): import warnings\nwarnings.filterwarnings(\"default\", category=DeprecationWarning,\n                                   module=user_ns.get(\"__name__\"))\n Temporarily Suppressing Warnings If you are using code that you know will raise a warning, such as a deprecated function, but do not want to see the warning (even when warnings have been explicitly configured via the command line), then it is possible to suppress the warning using the catch_warnings context manager: import warnings\n\ndef fxn():\n    warnings.warn(\"deprecated\", DeprecationWarning)\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    fxn()\n While within the context manager all warnings will simply be ignored. This allows you to use known-deprecated code without having to see the warning while not suppressing the warning for other code that might not be aware of its use of deprecated code. Note: this can only be guaranteed in a single-threaded application. If two or more threads use the catch_warnings context manager at the same time, the behavior is undefined. Testing Warnings To test warnings raised by code, use the catch_warnings context manager. With it you can temporarily mutate the warnings filter to facilitate your testing. For instance, do the following to capture all raised warnings to check: import warnings\n\ndef fxn():\n    warnings.warn(\"deprecated\", DeprecationWarning)\n\nwith warnings.catch_warnings(record=True) as w:\n    # Cause all warnings to always be triggered.\n    warnings.simplefilter(\"always\")\n    # Trigger a warning.\n    fxn()\n    # Verify some things\n    assert len(w) == 1\n    assert issubclass(w[-1].category, DeprecationWarning)\n    assert \"deprecated\" in str(w[-1].message)\n One can also cause all warnings to be exceptions by using error instead of always. One thing to be aware of is that if a warning has already been raised because of a once\/default rule, then no matter what filters are set the warning will not be seen again unless the warnings registry related to the warning has been cleared. Once the context manager exits, the warnings filter is restored to its state when the context was entered. This prevents tests from changing the warnings filter in unexpected ways between tests and leading to indeterminate test results. The showwarning() function in the module is also restored to its original value. Note: this can only be guaranteed in a single-threaded application. If two or more threads use the catch_warnings context manager at the same time, the behavior is undefined. When testing multiple operations that raise the same kind of warning, it is important to test them in a manner that confirms each operation is raising a new warning (e.g. set warnings to be raised as exceptions and check the operations raise exceptions, check that the length of the warning list continues to increase after each operation, or else delete the previous entries from the warnings list before each new operation). Updating Code For New Versions of Dependencies Warning categories that are primarily of interest to Python developers (rather than end users of applications written in Python) are ignored by default. Notably, this \u201cignored by default\u201d list includes DeprecationWarning (for every module except __main__), which means developers should make sure to test their code with typically ignored warnings made visible in order to receive timely notifications of future breaking API changes (whether in the standard library or third party packages). In the ideal case, the code will have a suitable test suite, and the test runner will take care of implicitly enabling all warnings when running tests (the test runner provided by the unittest module does this). In less ideal cases, applications can be checked for use of deprecated interfaces by passing -Wd to the Python interpreter (this is shorthand for -W default) or setting PYTHONWARNINGS=default in the environment. This enables default handling for all warnings, including those that are ignored by default. To change what action is taken for encountered warnings you can change what argument is passed to -W (e.g. -W error). See the -W flag for more details on what is possible. Available Functions  \nwarnings.warn(message, category=None, stacklevel=1, source=None)  \nIssue a warning, or maybe ignore it or raise an exception. The category argument, if given, must be a warning category class; it defaults to UserWarning. Alternatively, message can be a Warning instance, in which case category will be ignored and message.__class__ will be used. In this case, the message text will be str(message). This function raises an exception if the particular warning issued is changed into an error by the warnings filter. The stacklevel argument can be used by wrapper functions written in Python, like this: def deprecation(message):\n    warnings.warn(message, DeprecationWarning, stacklevel=2)\n This makes the warning refer to deprecation()\u2019s caller, rather than to the source of deprecation() itself (since the latter would defeat the purpose of the warning message). source, if supplied, is the destroyed object which emitted a ResourceWarning.  Changed in version 3.6: Added source parameter.  \n  \nwarnings.warn_explicit(message, category, filename, lineno, module=None, registry=None, module_globals=None, source=None)  \nThis is a low-level interface to the functionality of warn(), passing in explicitly the message, category, filename and line number, and optionally the module name and the registry (which should be the __warningregistry__ dictionary of the module). The module name defaults to the filename with .py stripped; if no registry is passed, the warning is never suppressed. message must be a string and category a subclass of Warning or message may be a Warning instance, in which case category will be ignored. module_globals, if supplied, should be the global namespace in use by the code for which the warning is issued. (This argument is used to support displaying source for modules found in zipfiles or other non-filesystem import sources). source, if supplied, is the destroyed object which emitted a ResourceWarning.  Changed in version 3.6: Add the source parameter.  \n  \nwarnings.showwarning(message, category, filename, lineno, file=None, line=None)  \nWrite a warning to a file. The default implementation calls formatwarning(message, category, filename, lineno, line) and writes the resulting string to file, which defaults to sys.stderr. You may replace this function with any callable by assigning to warnings.showwarning. line is a line of source code to be included in the warning message; if line is not supplied, showwarning() will try to read the line specified by filename and lineno. \n  \nwarnings.formatwarning(message, category, filename, lineno, line=None)  \nFormat a warning the standard way. This returns a string which may contain embedded newlines and ends in a newline. line is a line of source code to be included in the warning message; if line is not supplied, formatwarning() will try to read the line specified by filename and lineno. \n  \nwarnings.filterwarnings(action, message='', category=Warning, module='', lineno=0, append=False)  \nInsert an entry into the list of warnings filter specifications. The entry is inserted at the front by default; if append is true, it is inserted at the end. This checks the types of the arguments, compiles the message and module regular expressions, and inserts them as a tuple in the list of warnings filters. Entries closer to the front of the list override entries later in the list, if both match a particular warning. Omitted arguments default to a value that matches everything. \n  \nwarnings.simplefilter(action, category=Warning, lineno=0, append=False)  \nInsert a simple entry into the list of warnings filter specifications. The meaning of the function parameters is as for filterwarnings(), but regular expressions are not needed as the filter inserted always matches any message in any module as long as the category and line number match. \n  \nwarnings.resetwarnings()  \nReset the warnings filter. This discards the effect of all previous calls to filterwarnings(), including that of the -W command line options and calls to simplefilter(). \n Available Context Managers  \nclass warnings.catch_warnings(*, record=False, module=None)  \nA context manager that copies and, upon exit, restores the warnings filter and the showwarning() function. If the record argument is False (the default) the context manager returns None on entry. If record is True, a list is returned that is progressively populated with objects as seen by a custom showwarning() function (which also suppresses output to sys.stdout). Each object in the list has attributes with the same names as the arguments to showwarning(). The module argument takes a module that will be used instead of the module returned when you import warnings whose filter will be protected. This argument exists primarily for testing the warnings module itself.  Note The catch_warnings manager works by replacing and then later restoring the module\u2019s showwarning() function and internal list of filter specifications. This means the context manager is modifying global state and therefore is not thread-safe.","title":"python.library.warnings"},{"text":"class test.support.WarningsRecorder  \nClass used to record warnings for unit tests. See documentation of check_warnings() above for more details.","title":"python.library.test#test.support.WarningsRecorder"},{"text":"warnings.simplefilter(action, category=Warning, lineno=0, append=False)  \nInsert a simple entry into the list of warnings filter specifications. The meaning of the function parameters is as for filterwarnings(), but regular expressions are not needed as the filter inserted always matches any message in any module as long as the category and line number match.","title":"python.library.warnings#warnings.simplefilter"},{"text":"numpy.testing.suppress_warnings.__call__ method   testing.suppress_warnings.__call__(func)[source]\n \nFunction decorator to apply certain suppressions to a whole function.","title":"numpy.reference.generated.numpy.testing.suppress_warnings.__call__"},{"text":"warnings.filterwarnings(action, message='', category=Warning, module='', lineno=0, append=False)  \nInsert an entry into the list of warnings filter specifications. The entry is inserted at the front by default; if append is true, it is inserted at the end. This checks the types of the arguments, compiles the message and module regular expressions, and inserts them as a tuple in the list of warnings filters. Entries closer to the front of the list override entries later in the list, if both match a particular warning. Omitted arguments default to a value that matches everything.","title":"python.library.warnings#warnings.filterwarnings"}]}
{"task_id":13550423,"prompt":"def f_13550423(l):\n\treturn ","suffix":"","canonical_solution":"' '.join(map(str, l))","test_start":"\ndef check(candidate):","test":["\n    assert candidate(['x','y','zzz']) == 'x y zzz'\n","\n    assert candidate(['111','22','3']) == '111 22 3'\n","\n    assert candidate(['']) == ''\n","\n    assert candidate([':',':']) == ': :'\n","\n    assert candidate([',','#','#$%']) == ', # #$%'\n","\n    assert candidate(['a','b','c']) != 'abc'\n"],"entry_point":"f_13550423","intent":"concatenate items of list `l` with a space ' '","library":[],"docs":[]}
{"task_id":698223,"prompt":"def f_698223():\n\treturn ","suffix":"","canonical_solution":"time.strptime('30\/03\/09 16:31:32.123', '%d\/%m\/%y %H:%M:%S.%f')","test_start":"\nimport time \n\ndef check(candidate):","test":["\n    answer = time.strptime('30\/03\/09 16:31:32.123', '%d\/%m\/%y %H:%M:%S.%f')\n    assert candidate() == answer\n    false_1 = time.strptime('30\/03\/09 17:31:32.123', '%d\/%m\/%y %H:%M:%S.%f')\n    assert candidate() != false_1\n    false_2 = time.strptime('20\/03\/09 17:31:32.123', '%d\/%m\/%y %H:%M:%S.%f')\n    assert candidate() != false_2\n"],"entry_point":"f_698223","intent":"parse a time string '30\/03\/09 16:31:32.123' containing milliseconds in it","library":["time"],"docs":[{"text":"email.utils.parsedate(date)  \nAttempts to parse a date according to the rules in RFC 2822. however, some mailers don\u2019t follow that format as specified, so parsedate() tries to guess correctly in such cases. date is a string containing an RFC 2822 date, such as \"Mon, 20 Nov 1995 19:12:08 -0500\". If it succeeds in parsing the date, parsedate() returns a 9-tuple that can be passed directly to time.mktime(); otherwise None will be returned. Note that indexes 6, 7, and 8 of the result tuple are not usable.","title":"python.library.email.utils#email.utils.parsedate"},{"text":"imaplib.Internaldate2tuple(datestr)  \nParse an IMAP4 INTERNALDATE string and return corresponding local time. The return value is a time.struct_time tuple or None if the string has wrong format.","title":"python.library.imaplib#imaplib.Internaldate2tuple"},{"text":"classmethod time.fromisoformat(time_string)  \nReturn a time corresponding to a time_string in one of the formats emitted by time.isoformat(). Specifically, this function supports strings in the format: HH[:MM[:SS[.fff[fff]]]][+HH:MM[:SS[.ffffff]]]\n  Caution This does not support parsing arbitrary ISO 8601 strings. It is only intended as the inverse operation of time.isoformat().  Examples: >>> from datetime import time\n>>> time.fromisoformat('04:23:01')\ndatetime.time(4, 23, 1)\n>>> time.fromisoformat('04:23:01.000384')\ndatetime.time(4, 23, 1, 384)\n>>> time.fromisoformat('04:23:01+04:00')\ndatetime.time(4, 23, 1, tzinfo=datetime.timezone(datetime.timedelta(seconds=14400)))\n  New in version 3.7.","title":"python.library.datetime#datetime.time.fromisoformat"},{"text":"email.utils.parsedate_tz(date)  \nPerforms the same function as parsedate(), but returns either None or a 10-tuple; the first 9 elements make up a tuple that can be passed directly to time.mktime(), and the tenth is the offset of the date\u2019s timezone from UTC (which is the official term for Greenwich Mean Time) 1. If the input string has no timezone, the last element of the tuple returned is 0, which represents UTC. Note that indexes 6, 7, and 8 of the result tuple are not usable.","title":"python.library.email.utils#email.utils.parsedate_tz"},{"text":"parse_time(value)  \nParses a string and returns a datetime.time. UTC offsets aren\u2019t supported; if value describes one, the result is None.","title":"django.ref.utils#django.utils.dateparse.parse_time"},{"text":"parse_duration(value)  \nParses a string and returns a datetime.timedelta. Expects data in the format \"DD HH:MM:SS.uuuuuu\", \"DD HH:MM:SS,uuuuuu\", or as specified by ISO 8601 (e.g. P4DT1H15M20S which is equivalent to 4 1:15:20) or PostgreSQL\u2019s day-time interval format (e.g. 3 days 04:05:06).","title":"django.ref.utils#django.utils.dateparse.parse_duration"},{"text":"time.strptime(string[, format])  \nParse a string representing a time according to a format. The return value is a struct_time as returned by gmtime() or localtime(). The format parameter uses the same directives as those used by strftime(); it defaults to \"%a %b %d %H:%M:%S %Y\" which matches the formatting returned by ctime(). If string cannot be parsed according to format, or if it has excess data after parsing, ValueError is raised. The default values used to fill in any missing data when more accurate values cannot be inferred are (1900, 1, 1, 0, 0, 0, 0, 1, -1). Both string and format must be strings. For example: >>> import time\n>>> time.strptime(\"30 Nov 00\", \"%d %b %y\")   \ntime.struct_time(tm_year=2000, tm_mon=11, tm_mday=30, tm_hour=0, tm_min=0,\n                 tm_sec=0, tm_wday=3, tm_yday=335, tm_isdst=-1)\n Support for the %Z directive is based on the values contained in tzname and whether daylight is true. Because of this, it is platform-specific except for recognizing UTC and GMT which are always known (and are considered to be non-daylight savings timezones). Only the directives specified in the documentation are supported. Because strftime() is implemented per platform it can sometimes offer more directives than those listed. But strptime() is independent of any platform and thus does not necessarily support all directives available that are not documented as supported.","title":"python.library.time#time.strptime"},{"text":"classmethod datetime.strptime(date_string, format)  \nReturn a datetime corresponding to date_string, parsed according to format. This is equivalent to: datetime(*(time.strptime(date_string, format)[0:6]))\n ValueError is raised if the date_string and format can\u2019t be parsed by time.strptime() or if it returns a value which isn\u2019t a time tuple. For a complete list of formatting directives, see strftime() and strptime() Behavior.","title":"python.library.datetime#datetime.datetime.strptime"},{"text":"classmethod datetime.fromisoformat(date_string)  \nReturn a datetime corresponding to a date_string in one of the formats emitted by date.isoformat() and datetime.isoformat(). Specifically, this function supports strings in the format: YYYY-MM-DD[*HH[:MM[:SS[.fff[fff]]]][+HH:MM[:SS[.ffffff]]]]\n where * can match any single character.  Caution This does not support parsing arbitrary ISO 8601 strings - it is only intended as the inverse operation of datetime.isoformat(). A more full-featured ISO 8601 parser, dateutil.parser.isoparse is available in the third-party package dateutil.  Examples: >>> from datetime import datetime\n>>> datetime.fromisoformat('2011-11-04')\ndatetime.datetime(2011, 11, 4, 0, 0)\n>>> datetime.fromisoformat('2011-11-04T00:05:23')\ndatetime.datetime(2011, 11, 4, 0, 5, 23)\n>>> datetime.fromisoformat('2011-11-04 00:05:23.283')\ndatetime.datetime(2011, 11, 4, 0, 5, 23, 283000)\n>>> datetime.fromisoformat('2011-11-04 00:05:23.283+00:00')\ndatetime.datetime(2011, 11, 4, 0, 5, 23, 283000, tzinfo=datetime.timezone.utc)\n>>> datetime.fromisoformat('2011-11-04T00:05:23+04:00')   \ndatetime.datetime(2011, 11, 4, 0, 5, 23,\n    tzinfo=datetime.timezone(datetime.timedelta(seconds=14400)))\n  New in version 3.7.","title":"python.library.datetime#datetime.datetime.fromisoformat"},{"text":"decode(string)  \nAccept a string as the instance\u2019s new time value.","title":"python.library.xmlrpc.client#xmlrpc.client.DateTime.decode"}]}
{"task_id":6633523,"prompt":"def f_6633523(my_string):\n\t","suffix":"\n\treturn my_float","canonical_solution":"my_float = float(my_string.replace(',', ''))","test_start":"\ndef check(candidate):","test":["\n    assert (candidate('1,234.00') - 1234.0) < 1e-6\n","\n    assert (candidate('0.00') - 0.00) < 1e-6\n","\n    assert (candidate('1,000,000.00') - 1000000.00) < 1e-6\n","\n    assert (candidate('1,000,000.00') - 999999.98) > 1e-6\n","\n    assert (candidate('1') - 1.00) < 1e-6\n"],"entry_point":"f_6633523","intent":"convert a string `my_string` with dot and comma into a float number `my_float`","library":[],"docs":[]}
{"task_id":6633523,"prompt":"def f_6633523():\n\treturn ","suffix":"","canonical_solution":"float('123,456.908'.replace(',', ''))","test_start":"\ndef check(candidate):","test":["\n    assert (candidate() - 123456.908) < 1e-6\n    assert (candidate() - 123456.9) > 1e-6\n    assert (candidate() - 1234.908) > 1e-6\n    assert type(candidate()) == float\n    assert int(candidate()) == 123456\n"],"entry_point":"f_6633523","intent":"convert a string `123,456.908` with dot and comma into a floating number","library":[],"docs":[]}
{"task_id":3108285,"prompt":"def f_3108285():\n\t","suffix":"\n\treturn ","canonical_solution":"sys.path.append('\/path\/to\/whatever')","test_start":"\nimport sys \n\ndef check(candidate):","test":["\n    original_paths = [sp for sp in sys.path]\n    candidate()\n    assert '\/path\/to\/whatever' in sys.path\n"],"entry_point":"f_3108285","intent":"set python path '\/path\/to\/whatever' in python script","library":["sys"],"docs":[{"text":"sys.path  \nA list of strings that specifies the search path for modules. Initialized from the environment variable PYTHONPATH, plus an installation-dependent default. As initialized upon program startup, the first item of this list, path[0], is the directory containing the script that was used to invoke the Python interpreter. If the script directory is not available (e.g. if the interpreter is invoked interactively or if the script is read from standard input), path[0] is the empty string, which directs Python to search modules in the current directory first. Notice that the script directory is inserted before the entries inserted as a result of PYTHONPATH. A program is free to modify this list for its own purposes. Only strings and bytes should be added to sys.path; all other data types are ignored during import.  See also Module site This describes how to use .pth files to extend sys.path.","title":"python.library.sys#sys.path"},{"text":"multiprocessing.set_executable()  \nSets the path of the Python interpreter to use when starting a child process. (By default sys.executable is used). Embedders will probably need to do some thing like set_executable(os.path.join(sys.exec_prefix, 'pythonw.exe'))\n before they can create child processes.  Changed in version 3.4: Now supported on Unix when the 'spawn' start method is used.","title":"python.library.multiprocessing#multiprocessing.set_executable"},{"text":"modulefinder.AddPackagePath(pkg_name, path)  \nRecord that the package named pkg_name can be found in the specified path.","title":"python.library.modulefinder#modulefinder.AddPackagePath"},{"text":"site.main()  \nAdds all the standard site-specific directories to the module search path. This function is called automatically when this module is imported, unless the Python interpreter was started with the -S flag.  Changed in version 3.3: This function used to be called unconditionally.","title":"python.library.site#site.main"},{"text":"pkgutil.extend_path(path, name)  \nExtend the search path for the modules which comprise a package. Intended use is to place the following code in a package\u2019s __init__.py: from pkgutil import extend_path\n__path__ = extend_path(__path__, __name__)\n This will add to the package\u2019s __path__ all subdirectories of directories on sys.path named after the package. This is useful if one wants to distribute different parts of a single logical package as multiple directories. It also looks for *.pkg files beginning where * matches the name argument. This feature is similar to *.pth files (see the site module for more information), except that it doesn\u2019t special-case lines starting with import. A *.pkg file is trusted at face value: apart from checking for duplicates, all entries found in a *.pkg file are added to the path, regardless of whether they exist on the filesystem. (This is a feature.) If the input path is not a list (as is the case for frozen packages) it is returned unchanged. The input path is not modified; an extended copy is returned. Items are only appended to the copy at the end. It is assumed that sys.path is a sequence. Items of sys.path that are not strings referring to existing directories are ignored. Unicode items on sys.path that cause errors when used as filenames may cause this function to raise an exception (in line with os.path.isdir() behavior).","title":"python.library.pkgutil#pkgutil.extend_path"},{"text":"setup_python(context)  \nCreates a copy or symlink to the Python executable in the environment. On POSIX systems, if a specific executable python3.x was used, symlinks to python and python3 will be created pointing to that executable, unless files with those names already exist.","title":"python.library.venv#venv.EnvBuilder.setup_python"},{"text":"sys.pycache_prefix  \nIf this is set (not None), Python will write bytecode-cache .pyc files to (and read them from) a parallel directory tree rooted at this directory, rather than from __pycache__ directories in the source code tree. Any __pycache__ directories in the source code tree will be ignored and new .pyc files written within the pycache prefix. Thus if you use compileall as a pre-build step, you must ensure you run it with the same pycache prefix (if any) that you will use at runtime. A relative path is interpreted relative to the current working directory. This value is initially set based on the value of the -X pycache_prefix=PATH command-line option or the PYTHONPYCACHEPREFIX environment variable (command-line takes precedence). If neither are set, it is None.  New in version 3.8.","title":"python.library.sys#sys.pycache_prefix"},{"text":"sys.base_exec_prefix  \nSet during Python startup, before site.py is run, to the same value as exec_prefix. If not running in a virtual environment, the values will stay the same; if site.py finds that a virtual environment is in use, the values of prefix and exec_prefix will be changed to point to the virtual environment, whereas base_prefix and base_exec_prefix will remain pointing to the base Python installation (the one which the virtual environment was created from).  New in version 3.3.","title":"python.library.sys#sys.base_exec_prefix"},{"text":"sys.base_prefix  \nSet during Python startup, before site.py is run, to the same value as prefix. If not running in a virtual environment, the values will stay the same; if site.py finds that a virtual environment is in use, the values of prefix and exec_prefix will be changed to point to the virtual environment, whereas base_prefix and base_exec_prefix will remain pointing to the base Python installation (the one which the virtual environment was created from).  New in version 3.3.","title":"python.library.sys#sys.base_prefix"},{"text":"app_import_path  \nOptionally the import path for the Flask application.","title":"flask.api.index#flask.cli.ScriptInfo.app_import_path"}]}
{"task_id":2195340,"prompt":"def f_2195340():\n\treturn ","suffix":"","canonical_solution":"re.split('(\\\\W+)', 'Words, words, words.')","test_start":"\nimport re\n\ndef check(candidate):","test":["\n    assert candidate() == ['Words', ', ', 'words', ', ', 'words', '.', '']\n    assert candidate() == ['Words', ', '] + ['words', ', ', 'words', '.', '']\n"],"entry_point":"f_2195340","intent":"split string 'Words, words, words.' using a regex '(\\\\W+)'","library":["re"],"docs":[{"text":"Pattern.split(string, maxsplit=0)  \nIdentical to the split() function, using the compiled pattern.","title":"python.library.re#re.Pattern.split"},{"text":"str.split(sep=None, maxsplit=-1)  \nReturn a list of the words in the string, using sep as the delimiter string. If maxsplit is given, at most maxsplit splits are done (thus, the list will have at most maxsplit+1 elements). If maxsplit is not specified or -1, then there is no limit on the number of splits (all possible splits are made). If sep is given, consecutive delimiters are not grouped together and are deemed to delimit empty strings (for example, '1,,2'.split(',') returns ['1', '', '2']). The sep argument may consist of multiple characters (for example, '1<>2<>3'.split('<>') returns ['1', '2', '3']). Splitting an empty string with a specified separator returns ['']. For example: >>> '1,2,3'.split(',')\n['1', '2', '3']\n>>> '1,2,3'.split(',', maxsplit=1)\n['1', '2,3']\n>>> '1,2,,3,'.split(',')\n['1', '2', '', '3', '']\n If sep is not specified or is None, a different splitting algorithm is applied: runs of consecutive whitespace are regarded as a single separator, and the result will contain no empty strings at the start or end if the string has leading or trailing whitespace. Consequently, splitting an empty string or a string consisting of just whitespace with a None separator returns []. For example: >>> '1 2 3'.split()\n['1', '2', '3']\n>>> '1 2 3'.split(maxsplit=1)\n['1', '2 3']\n>>> '   1   2   3   '.split()\n['1', '2', '3']","title":"python.library.stdtypes#str.split"},{"text":"str.rsplit(sep=None, maxsplit=-1)  \nReturn a list of the words in the string, using sep as the delimiter string. If maxsplit is given, at most maxsplit splits are done, the rightmost ones. If sep is not specified or None, any whitespace string is a separator. Except for splitting from the right, rsplit() behaves like split() which is described in detail below.","title":"python.library.stdtypes#str.rsplit"},{"text":"pandas.DataFrame.clip   DataFrame.clip(lower=None, upper=None, axis=None, inplace=False, *args, **kwargs)[source]\n \nTrim values at input threshold(s). Assigns values outside boundary to boundary values. Thresholds can be singular values or array like, and in the latter case the clipping is performed element-wise in the specified axis.  Parameters \n \nlower:float or array-like, default None\n\n\nMinimum threshold value. All values below this threshold will be set to it. A missing threshold (e.g NA) will not clip the value.  \nupper:float or array-like, default None\n\n\nMaximum threshold value. All values above this threshold will be set to it. A missing threshold (e.g NA) will not clip the value.  \naxis:int or str axis name, optional\n\n\nAlign object with lower and upper along the given axis.  \ninplace:bool, default False\n\n\nWhether to perform the operation in place on the data.  *args, **kwargs\n\nAdditional keywords have no effect but might be accepted for compatibility with numpy.    Returns \n Series or DataFrame or None\n\nSame type as calling object with the values outside the clip boundaries replaced or None if inplace=True.      See also  Series.clip\n\nTrim values at input threshold in series.  DataFrame.clip\n\nTrim values at input threshold in dataframe.  numpy.clip\n\nClip (limit) the values in an array.    Examples \n>>> data = {'col_0': [9, -3, 0, -1, 5], 'col_1': [-2, -7, 6, 8, -5]}\n>>> df = pd.DataFrame(data)\n>>> df\n   col_0  col_1\n0      9     -2\n1     -3     -7\n2      0      6\n3     -1      8\n4      5     -5\n  Clips per column using lower and upper thresholds: \n>>> df.clip(-4, 6)\n   col_0  col_1\n0      6     -2\n1     -3     -4\n2      0      6\n3     -1      6\n4      5     -4\n  Clips using specific lower and upper thresholds per column element: \n>>> t = pd.Series([2, -4, -1, 6, 3])\n>>> t\n0    2\n1   -4\n2   -1\n3    6\n4    3\ndtype: int64\n  \n>>> df.clip(t, t + 4, axis=0)\n   col_0  col_1\n0      6      2\n1     -3     -4\n2      0      3\n3      6      8\n4      5      3\n  Clips using specific lower threshold per column element, with missing values: \n>>> t = pd.Series([2, -4, np.NaN, 6, 3])\n>>> t\n0    2.0\n1   -4.0\n2    NaN\n3    6.0\n4    3.0\ndtype: float64\n  \n>>> df.clip(t, axis=0)\ncol_0  col_1\n0      9      2\n1     -3     -4\n2      0      6\n3      6      8\n4      5      3","title":"pandas.reference.api.pandas.dataframe.clip"},{"text":"pandas.Series.clip   Series.clip(lower=None, upper=None, axis=None, inplace=False, *args, **kwargs)[source]\n \nTrim values at input threshold(s). Assigns values outside boundary to boundary values. Thresholds can be singular values or array like, and in the latter case the clipping is performed element-wise in the specified axis.  Parameters \n \nlower:float or array-like, default None\n\n\nMinimum threshold value. All values below this threshold will be set to it. A missing threshold (e.g NA) will not clip the value.  \nupper:float or array-like, default None\n\n\nMaximum threshold value. All values above this threshold will be set to it. A missing threshold (e.g NA) will not clip the value.  \naxis:int or str axis name, optional\n\n\nAlign object with lower and upper along the given axis.  \ninplace:bool, default False\n\n\nWhether to perform the operation in place on the data.  *args, **kwargs\n\nAdditional keywords have no effect but might be accepted for compatibility with numpy.    Returns \n Series or DataFrame or None\n\nSame type as calling object with the values outside the clip boundaries replaced or None if inplace=True.      See also  Series.clip\n\nTrim values at input threshold in series.  DataFrame.clip\n\nTrim values at input threshold in dataframe.  numpy.clip\n\nClip (limit) the values in an array.    Examples \n>>> data = {'col_0': [9, -3, 0, -1, 5], 'col_1': [-2, -7, 6, 8, -5]}\n>>> df = pd.DataFrame(data)\n>>> df\n   col_0  col_1\n0      9     -2\n1     -3     -7\n2      0      6\n3     -1      8\n4      5     -5\n  Clips per column using lower and upper thresholds: \n>>> df.clip(-4, 6)\n   col_0  col_1\n0      6     -2\n1     -3     -4\n2      0      6\n3     -1      6\n4      5     -4\n  Clips using specific lower and upper thresholds per column element: \n>>> t = pd.Series([2, -4, -1, 6, 3])\n>>> t\n0    2\n1   -4\n2   -1\n3    6\n4    3\ndtype: int64\n  \n>>> df.clip(t, t + 4, axis=0)\n   col_0  col_1\n0      6      2\n1     -3     -4\n2      0      3\n3      6      8\n4      5      3\n  Clips using specific lower threshold per column element, with missing values: \n>>> t = pd.Series([2, -4, np.NaN, 6, 3])\n>>> t\n0    2.0\n1   -4.0\n2    NaN\n3    6.0\n4    3.0\ndtype: float64\n  \n>>> df.clip(t, axis=0)\ncol_0  col_1\n0      9      2\n1     -3     -4\n2      0      6\n3      6      8\n4      5      3","title":"pandas.reference.api.pandas.series.clip"},{"text":"numpy.chararray.rsplit method   chararray.rsplit(sep=None, maxsplit=None)[source]\n \nFor each element in self, return a list of the words in the string, using sep as the delimiter string.  See also  char.rsplit","title":"numpy.reference.generated.numpy.chararray.rsplit"},{"text":"re.split(pattern, string, maxsplit=0, flags=0)  \nSplit string by the occurrences of pattern. If capturing parentheses are used in pattern, then the text of all groups in the pattern are also returned as part of the resulting list. If maxsplit is nonzero, at most maxsplit splits occur, and the remainder of the string is returned as the final element of the list. >>> re.split(r'\\W+', 'Words, words, words.')\n['Words', 'words', 'words', '']\n>>> re.split(r'(\\W+)', 'Words, words, words.')\n['Words', ', ', 'words', ', ', 'words', '.', '']\n>>> re.split(r'\\W+', 'Words, words, words.', 1)\n['Words', 'words, words.']\n>>> re.split('[a-f]+', '0a3B9', flags=re.IGNORECASE)\n['0', '3', '9']\n If there are capturing groups in the separator and it matches at the start of the string, the result will start with an empty string. The same holds for the end of the string: >>> re.split(r'(\\W+)', '...words, words...')\n['', '...', 'words', ', ', 'words', '...', '']\n That way, separator components are always found at the same relative indices within the result list. Empty matches for the pattern split the string only when not adjacent to a previous empty match. >>> re.split(r'\\b', 'Words, words, words.')\n['', 'Words', ', ', 'words', ', ', 'words', '.']\n>>> re.split(r'\\W*', '...words...')\n['', '', 'w', 'o', 'r', 'd', 's', '', '']\n>>> re.split(r'(\\W*)', '...words...')\n['', '...', '', '', 'w', '', 'o', '', 'r', '', 'd', '', 's', '...', '', '', '']\n  Changed in version 3.1: Added the optional flags argument.   Changed in version 3.7: Added support of splitting on a pattern that could match an empty string.","title":"python.library.re#re.split"},{"text":"re.A  \nre.ASCII  \nMake \\w, \\W, \\b, \\B, \\d, \\D, \\s and \\S perform ASCII-only matching instead of full Unicode matching. This is only meaningful for Unicode patterns, and is ignored for byte patterns. Corresponds to the inline flag (?a). Note that for backward compatibility, the re.U flag still exists (as well as its synonym re.UNICODE and its embedded counterpart (?u)), but these are redundant in Python 3 since matches are Unicode by default for strings (and Unicode matching isn\u2019t allowed for bytes).","title":"python.library.re#re.ASCII"},{"text":"numpy.char.chararray.rsplit method   char.chararray.rsplit(sep=None, maxsplit=None)[source]\n \nFor each element in self, return a list of the words in the string, using sep as the delimiter string.  See also  char.rsplit","title":"numpy.reference.generated.numpy.char.chararray.rsplit"},{"text":"set_annotation_clip(b)[source]\n \nSet the clipping behavior.  Parameters \n \nbbool or None\n\n\n \nFalse: The annotation will always be drawn regardless of its position. \nTrue: The annotation will only be drawn if self.xy is inside the axes. \nNone: The annotation will only be drawn if self.xy is inside the axes and self.xycoords == \"data\".","title":"matplotlib._as_gen.matplotlib.patches.connectionpatch#matplotlib.patches.ConnectionPatch.set_annotation_clip"}]}
{"task_id":17977584,"prompt":"def f_17977584():\n\treturn ","suffix":"","canonical_solution":"open('Output.txt', 'a')","test_start":"\ndef check(candidate):","test":["\n    f = candidate()\n    assert str(f.__class__) == \"<class '_io.TextIOWrapper'>\"\n    assert f.name == 'Output.txt'\n    assert f.mode == 'a'\n"],"entry_point":"f_17977584","intent":"open a file `Output.txt` in append mode","library":[],"docs":[]}
{"task_id":22676,"prompt":"def f_22676():\n\treturn ","suffix":"","canonical_solution":"urllib.request.urlretrieve('https:\/\/github.com\/zorazrw\/multilingual-conala\/blob\/master\/dataset\/test\/es_test.json', 'mp3.mp3')","test_start":"\nimport urllib \n\ndef check(candidate):","test":["\n    results = candidate()\n    assert len(results) == 2\n    assert results[0] == \"mp3.mp3\"\n    assert results[1].values()[0] == \"GitHub.com\"\n"],"entry_point":"f_22676","intent":"download a file \"http:\/\/www.example.com\/songs\/mp3.mp3\" over HTTP and save to \"mp3.mp3\"","library":["urllib"],"docs":[{"text":"mpl_toolkits.mplot3d The mplot3d toolkit adds simple 3D plotting capabilities (scatter, surface, line, mesh, etc.) to Matplotlib by supplying an Axes object that can create a 2D projection of a 3D scene. The resulting graph will have the same look and feel as regular 2D plots. Not the fastest or most feature complete 3D library out there, but it ships with Matplotlib and thus may be a lighter weight solution for some use cases. See the mplot3d tutorial for more information.  The interactive backends also provide the ability to rotate and zoom the 3D scene. One can rotate the 3D scene by simply clicking-and-dragging the scene. Zooming is done by right-clicking the scene and dragging the mouse up and down (unlike 2D plots, the toolbar zoom button is not used).  \nmplot3d FAQ How is mplot3d different from Mayavi? My 3D plot doesn't look right at certain viewing angles I don't like how the 3D plot is laid out, how do I change that?     Note pyplot cannot be used to add content to 3D plots, because its function signatures are strictly 2D and cannot handle the additional information needed for 3D. Instead, use the explicit API by calling the respective methods on the Axes3D object.   axes3d  Note 3D plotting in Matplotlib is still not as mature as the 2D case. Please report any functions that do not behave as expected as a bug. In addition, help and patches would be greatly appreciated!   \naxes3d.Axes3D(fig[, rect, azim, elev, ...]) 3D axes object.     axis3d  Note See mpl_toolkits.mplot3d.axis3d._axinfo for a dictionary containing constants that may be modified for controlling the look and feel of mplot3d axes (e.g., label spacing, font colors and panel colors). Historically, axis3d has suffered from having hard-coded constants that precluded user adjustments, and this dictionary was implemented in version 1.1 as a stop-gap measure.   \naxis3d.Axis(adir, v_intervalx, d_intervalx, ...) An Axis class for the 3D plots.     art3d  \nart3d.Line3D(xs, ys, zs, *args, **kwargs) 3D line object.  \nart3d.Line3DCollection(segments, *args[, zorder]) A collection of 3D lines.  \nart3d.Patch3D(*args[, zs, zdir]) 3D patch object.  \nart3d.Patch3DCollection(*args[, zs, zdir, ...]) A collection of 3D patches.  \nart3d.Path3DCollection(*args[, zs, zdir, ...]) A collection of 3D paths.  \nart3d.PathPatch3D(path, *[, zs, zdir]) 3D PathPatch object.  \nart3d.Poly3DCollection(verts, *args[, zsort]) A collection of 3D polygons.  \nart3d.Text3D([x, y, z, text, zdir]) Text object with 3D position and direction.  \nart3d.get_dir_vector(zdir) Return a direction vector.  \nart3d.juggle_axes(xs, ys, zs, zdir) Reorder coordinates so that 2D xs, ys can be plotted in the plane orthogonal to zdir.  \nart3d.line_2d_to_3d(line[, zs, zdir]) Convert a 2D line to 3D.  \nart3d.line_collection_2d_to_3d(col[, zs, zdir]) Convert a LineCollection to a Line3DCollection object.  \nart3d.patch_2d_to_3d(patch[, z, zdir]) Convert a Patch to a Patch3D object.  \nart3d.patch_collection_2d_to_3d(col[, zs, ...]) Convert a PatchCollection into a Patch3DCollection object (or a PathCollection into a Path3DCollection object).  \nart3d.pathpatch_2d_to_3d(pathpatch[, z, zdir]) Convert a PathPatch to a PathPatch3D object.  \nart3d.poly_collection_2d_to_3d(col[, zs, zdir]) Convert a PolyCollection to a Poly3DCollection object.  \nart3d.rotate_axes(xs, ys, zs, zdir) Reorder coordinates so that the axes are rotated with zdir along the original z axis.  \nart3d.text_2d_to_3d(obj[, z, zdir]) Convert a Text to a Text3D object.     proj3d  \nproj3d.inv_transform(xs, ys, zs, M)   \nproj3d.persp_transformation(zfront, zback)   \nproj3d.proj_points(points, M)   \nproj3d.proj_trans_points(points, M)   \nproj3d.proj_transform(xs, ys, zs, M) Transform the points by the projection matrix  \nproj3d.proj_transform_clip(xs, ys, zs, M) Transform the points by the projection matrix and return the clipping result returns txs, tys, tzs, tis  \nproj3d.rot_x(V, alpha)   \nproj3d.transform(xs, ys, zs, M) Transform the points by the projection matrix  \nproj3d.view_transformation(E, R, V)   \nproj3d.world_transformation(xmin, xmax, ...) Produce a matrix that scales homogeneous coords in the specified ranges to [0, 1], or [0, pb_aspect[i]] if the plotbox aspect ratio is specified.","title":"matplotlib.toolkits.mplot3d"},{"text":"stringprep.map_table_b3(code)  \nReturn the mapped value for code according to tableB.3 (Mapping for case-folding used with no normalization).","title":"python.library.stringprep#stringprep.map_table_b3"},{"text":"tf.raw_ops.BatchFFT3D  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.raw_ops.BatchFFT3D  \ntf.raw_ops.BatchFFT3D(\n    input, name=None\n)\n\n \n\n\n Args\n  input   A Tensor of type complex64.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor of type complex64.","title":"tensorflow.raw_ops.batchfft3d"},{"text":"handler403","title":"django.ref.urls#django.conf.urls.handler403"},{"text":"HTTPRedirectHandler.http_error_303(req, fp, code, msg, hdrs)  \nThe same as http_error_301(), but called for the \u2018see other\u2019 response.","title":"python.library.urllib.request#urllib.request.HTTPRedirectHandler.http_error_303"},{"text":"tf.signal.fft3d 3D fast Fourier transform.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.fft3d, tf.compat.v1.signal.fft3d, tf.compat.v1.spectral.fft3d  \ntf.signal.fft3d(\n    input, name=None\n)\n Computes the 3-dimensional discrete Fourier transform over the inner-most 3 dimensions of input.\n \n\n\n Args\n  input   A Tensor. Must be one of the following types: complex64, complex128. A complex tensor.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor. Has the same type as input.","title":"tensorflow.signal.fft3d"},{"text":"tf.raw_ops.BatchIFFT3D  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.raw_ops.BatchIFFT3D  \ntf.raw_ops.BatchIFFT3D(\n    input, name=None\n)\n\n \n\n\n Args\n  input   A Tensor of type complex64.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor of type complex64.","title":"tensorflow.raw_ops.batchifft3d"},{"text":"tf.raw_ops.FFT3D 3D fast Fourier transform.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.raw_ops.FFT3D  \ntf.raw_ops.FFT3D(\n    input, name=None\n)\n Computes the 3-dimensional discrete Fourier transform over the inner-most 3 dimensions of input.\n \n\n\n Args\n  input   A Tensor. Must be one of the following types: complex64, complex128. A complex tensor.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor. Has the same type as input.","title":"tensorflow.raw_ops.fft3d"},{"text":"base64.b32decode(s, casefold=False, map01=None)  \nDecode the Base32 encoded bytes-like object or ASCII string s and return the decoded bytes. Optional casefold is a flag specifying whether a lowercase alphabet is acceptable as input. For security purposes, the default is False. RFC 3548 allows for optional mapping of the digit 0 (zero) to the letter O (oh), and for optional mapping of the digit 1 (one) to either the letter I (eye) or letter L (el). The optional argument map01 when not None, specifies which letter the digit 1 should be mapped to (when map01 is not None, the digit 0 is always mapped to the letter O). For security purposes the default is None, so that 0 and 1 are not allowed in the input. A binascii.Error is raised if s is incorrectly padded or if there are non-alphabet characters present in the input.","title":"python.library.base64#base64.b32decode"},{"text":"class HttpResponseForbidden  \nActs just like HttpResponse but uses a 403 status code.","title":"django.ref.request-response#django.http.HttpResponseForbidden"}]}
{"task_id":22676,"prompt":"def f_22676(url):\n\t","suffix":"\n\treturn html","canonical_solution":"html = urllib.request.urlopen(url).read()","test_start":"\nimport urllib \n\ndef check(candidate):","test":["\n    html = candidate(\"https:\/\/github.com\/zorazrw\/multilingual-conala\/blob\/master\/dataset\/test\/es_test.json\")\n    assert b\"zorazrw\/multilingual-conala\" in html\n"],"entry_point":"f_22676","intent":"download a file 'http:\/\/www.example.com\/' over HTTP","library":["urllib"],"docs":[{"text":"http.client \u2014 HTTP protocol client Source code: Lib\/http\/client.py This module defines classes which implement the client side of the HTTP and HTTPS protocols. It is normally not used directly \u2014 the module urllib.request uses it to handle URLs that use HTTP and HTTPS.  See also The Requests package is recommended for a higher-level HTTP client interface.   Note HTTPS support is only available if Python was compiled with SSL support (through the ssl module).  The module provides the following classes:  \nclass http.client.HTTPConnection(host, port=None, [timeout, ]source_address=None, blocksize=8192)  \nAn HTTPConnection instance represents one transaction with an HTTP server. It should be instantiated passing it a host and optional port number. If no port number is passed, the port is extracted from the host string if it has the form host:port, else the default HTTP port (80) is used. If the optional timeout parameter is given, blocking operations (like connection attempts) will timeout after that many seconds (if it is not given, the global default timeout setting is used). The optional source_address parameter may be a tuple of a (host, port) to use as the source address the HTTP connection is made from. The optional blocksize parameter sets the buffer size in bytes for sending a file-like message body. For example, the following calls all create instances that connect to the server at the same host and port: >>> h1 = http.client.HTTPConnection('www.python.org')\n>>> h2 = http.client.HTTPConnection('www.python.org:80')\n>>> h3 = http.client.HTTPConnection('www.python.org', 80)\n>>> h4 = http.client.HTTPConnection('www.python.org', 80, timeout=10)\n  Changed in version 3.2: source_address was added.   Changed in version 3.4: The strict parameter was removed. HTTP 0.9-style \u201cSimple Responses\u201d are not longer supported.   Changed in version 3.7: blocksize parameter was added.  \n  \nclass http.client.HTTPSConnection(host, port=None, key_file=None, cert_file=None, [timeout, ]source_address=None, *, context=None, check_hostname=None, blocksize=8192)  \nA subclass of HTTPConnection that uses SSL for communication with secure servers. Default port is 443. If context is specified, it must be a ssl.SSLContext instance describing the various SSL options. Please read Security considerations for more information on best practices.  Changed in version 3.2: source_address, context and check_hostname were added.   Changed in version 3.2: This class now supports HTTPS virtual hosts if possible (that is, if ssl.HAS_SNI is true).   Changed in version 3.4: The strict parameter was removed. HTTP 0.9-style \u201cSimple Responses\u201d are no longer supported.   Changed in version 3.4.3: This class now performs all the necessary certificate and hostname checks by default. To revert to the previous, unverified, behavior ssl._create_unverified_context() can be passed to the context parameter.   Changed in version 3.8: This class now enables TLS 1.3 ssl.SSLContext.post_handshake_auth for the default context or when cert_file is passed with a custom context.   Deprecated since version 3.6: key_file and cert_file are deprecated in favor of context. Please use ssl.SSLContext.load_cert_chain() instead, or let ssl.create_default_context() select the system\u2019s trusted CA certificates for you. The check_hostname parameter is also deprecated; the ssl.SSLContext.check_hostname attribute of context should be used instead.  \n  \nclass http.client.HTTPResponse(sock, debuglevel=0, method=None, url=None)  \nClass whose instances are returned upon successful connection. Not instantiated directly by user.  Changed in version 3.4: The strict parameter was removed. HTTP 0.9 style \u201cSimple Responses\u201d are no longer supported.  \n This module provides the following function:  \nhttp.client.parse_headers(fp)  \nParse the headers from a file pointer fp representing a HTTP request\/response. The file has to be a BufferedIOBase reader (i.e. not text) and must provide a valid RFC 2822 style header. This function returns an instance of http.client.HTTPMessage that holds the header fields, but no payload (the same as HTTPResponse.msg and http.server.BaseHTTPRequestHandler.headers). After returning, the file pointer fp is ready to read the HTTP body.  Note parse_headers() does not parse the start-line of a HTTP message; it only parses the Name: value lines. The file has to be ready to read these field lines, so the first line should already be consumed before calling the function.  \n The following exceptions are raised as appropriate:  \nexception http.client.HTTPException  \nThe base class of the other exceptions in this module. It is a subclass of Exception. \n  \nexception http.client.NotConnected  \nA subclass of HTTPException. \n  \nexception http.client.InvalidURL  \nA subclass of HTTPException, raised if a port is given and is either non-numeric or empty. \n  \nexception http.client.UnknownProtocol  \nA subclass of HTTPException. \n  \nexception http.client.UnknownTransferEncoding  \nA subclass of HTTPException. \n  \nexception http.client.UnimplementedFileMode  \nA subclass of HTTPException. \n  \nexception http.client.IncompleteRead  \nA subclass of HTTPException. \n  \nexception http.client.ImproperConnectionState  \nA subclass of HTTPException. \n  \nexception http.client.CannotSendRequest  \nA subclass of ImproperConnectionState. \n  \nexception http.client.CannotSendHeader  \nA subclass of ImproperConnectionState. \n  \nexception http.client.ResponseNotReady  \nA subclass of ImproperConnectionState. \n  \nexception http.client.BadStatusLine  \nA subclass of HTTPException. Raised if a server responds with a HTTP status code that we don\u2019t understand. \n  \nexception http.client.LineTooLong  \nA subclass of HTTPException. Raised if an excessively long line is received in the HTTP protocol from the server. \n  \nexception http.client.RemoteDisconnected  \nA subclass of ConnectionResetError and BadStatusLine. Raised by HTTPConnection.getresponse() when the attempt to read the response results in no data read from the connection, indicating that the remote end has closed the connection.  New in version 3.5: Previously, BadStatusLine('') was raised.  \n The constants defined in this module are:  \nhttp.client.HTTP_PORT  \nThe default port for the HTTP protocol (always 80). \n  \nhttp.client.HTTPS_PORT  \nThe default port for the HTTPS protocol (always 443). \n  \nhttp.client.responses  \nThis dictionary maps the HTTP 1.1 status codes to the W3C names. Example: http.client.responses[http.client.NOT_FOUND] is 'Not Found'. \n See HTTP status codes for a list of HTTP status codes that are available in this module as constants. HTTPConnection Objects HTTPConnection instances have the following methods:  \nHTTPConnection.request(method, url, body=None, headers={}, *, encode_chunked=False)  \nThis will send a request to the server using the HTTP request method method and the selector url. If body is specified, the specified data is sent after the headers are finished. It may be a str, a bytes-like object, an open file object, or an iterable of bytes. If body is a string, it is encoded as ISO-8859-1, the default for HTTP. If it is a bytes-like object, the bytes are sent as is. If it is a file object, the contents of the file is sent; this file object should support at least the read() method. If the file object is an instance of io.TextIOBase, the data returned by the read() method will be encoded as ISO-8859-1, otherwise the data returned by read() is sent as is. If body is an iterable, the elements of the iterable are sent as is until the iterable is exhausted. The headers argument should be a mapping of extra HTTP headers to send with the request. If headers contains neither Content-Length nor Transfer-Encoding, but there is a request body, one of those header fields will be added automatically. If body is None, the Content-Length header is set to 0 for methods that expect a body (PUT, POST, and PATCH). If body is a string or a bytes-like object that is not also a file, the Content-Length header is set to its length. Any other type of body (files and iterables in general) will be chunk-encoded, and the Transfer-Encoding header will automatically be set instead of Content-Length. The encode_chunked argument is only relevant if Transfer-Encoding is specified in headers. If encode_chunked is False, the HTTPConnection object assumes that all encoding is handled by the calling code. If it is True, the body will be chunk-encoded.  Note Chunked transfer encoding has been added to the HTTP protocol version 1.1. Unless the HTTP server is known to handle HTTP 1.1, the caller must either specify the Content-Length, or must pass a str or bytes-like object that is not also a file as the body representation.   New in version 3.2: body can now be an iterable.   Changed in version 3.6: If neither Content-Length nor Transfer-Encoding are set in headers, file and iterable body objects are now chunk-encoded. The encode_chunked argument was added. No attempt is made to determine the Content-Length for file objects.  \n  \nHTTPConnection.getresponse()  \nShould be called after a request is sent to get the response from the server. Returns an HTTPResponse instance.  Note Note that you must have read the whole response before you can send a new request to the server.   Changed in version 3.5: If a ConnectionError or subclass is raised, the HTTPConnection object will be ready to reconnect when a new request is sent.  \n  \nHTTPConnection.set_debuglevel(level)  \nSet the debugging level. The default debug level is 0, meaning no debugging output is printed. Any value greater than 0 will cause all currently defined debug output to be printed to stdout. The debuglevel is passed to any new HTTPResponse objects that are created.  New in version 3.1.  \n  \nHTTPConnection.set_tunnel(host, port=None, headers=None)  \nSet the host and the port for HTTP Connect Tunnelling. This allows running the connection through a proxy server. The host and port arguments specify the endpoint of the tunneled connection (i.e. the address included in the CONNECT request, not the address of the proxy server). The headers argument should be a mapping of extra HTTP headers to send with the CONNECT request. For example, to tunnel through a HTTPS proxy server running locally on port 8080, we would pass the address of the proxy to the HTTPSConnection constructor, and the address of the host that we eventually want to reach to the set_tunnel() method: >>> import http.client\n>>> conn = http.client.HTTPSConnection(\"localhost\", 8080)\n>>> conn.set_tunnel(\"www.python.org\")\n>>> conn.request(\"HEAD\",\"\/index.html\")\n  New in version 3.2.  \n  \nHTTPConnection.connect()  \nConnect to the server specified when the object was created. By default, this is called automatically when making a request if the client does not already have a connection. \n  \nHTTPConnection.close()  \nClose the connection to the server. \n  \nHTTPConnection.blocksize  \nBuffer size in bytes for sending a file-like message body.  New in version 3.7.  \n As an alternative to using the request() method described above, you can also send your request step by step, by using the four functions below.  \nHTTPConnection.putrequest(method, url, skip_host=False, skip_accept_encoding=False)  \nThis should be the first call after the connection to the server has been made. It sends a line to the server consisting of the method string, the url string, and the HTTP version (HTTP\/1.1). To disable automatic sending of Host: or Accept-Encoding: headers (for example to accept additional content encodings), specify skip_host or skip_accept_encoding with non-False values. \n  \nHTTPConnection.putheader(header, argument[, ...])  \nSend an RFC 822-style header to the server. It sends a line to the server consisting of the header, a colon and a space, and the first argument. If more arguments are given, continuation lines are sent, each consisting of a tab and an argument. \n  \nHTTPConnection.endheaders(message_body=None, *, encode_chunked=False)  \nSend a blank line to the server, signalling the end of the headers. The optional message_body argument can be used to pass a message body associated with the request. If encode_chunked is True, the result of each iteration of message_body will be chunk-encoded as specified in RFC 7230, Section 3.3.1. How the data is encoded is dependent on the type of message_body. If message_body implements the buffer interface the encoding will result in a single chunk. If message_body is a collections.abc.Iterable, each iteration of message_body will result in a chunk. If message_body is a file object, each call to .read() will result in a chunk. The method automatically signals the end of the chunk-encoded data immediately after message_body.  Note Due to the chunked encoding specification, empty chunks yielded by an iterator body will be ignored by the chunk-encoder. This is to avoid premature termination of the read of the request by the target server due to malformed encoding.   New in version 3.6: Chunked encoding support. The encode_chunked parameter was added.  \n  \nHTTPConnection.send(data)  \nSend data to the server. This should be used directly only after the endheaders() method has been called and before getresponse() is called. \n HTTPResponse Objects An HTTPResponse instance wraps the HTTP response from the server. It provides access to the request headers and the entity body. The response is an iterable object and can be used in a with statement.  Changed in version 3.5: The io.BufferedIOBase interface is now implemented and all of its reader operations are supported.   \nHTTPResponse.read([amt])  \nReads and returns the response body, or up to the next amt bytes. \n  \nHTTPResponse.readinto(b)  \nReads up to the next len(b) bytes of the response body into the buffer b. Returns the number of bytes read.  New in version 3.3.  \n  \nHTTPResponse.getheader(name, default=None)  \nReturn the value of the header name, or default if there is no header matching name. If there is more than one header with the name name, return all of the values joined by \u2018, \u2018. If \u2018default\u2019 is any iterable other than a single string, its elements are similarly returned joined by commas. \n  \nHTTPResponse.getheaders()  \nReturn a list of (header, value) tuples. \n  \nHTTPResponse.fileno()  \nReturn the fileno of the underlying socket. \n  \nHTTPResponse.msg  \nA http.client.HTTPMessage instance containing the response headers. http.client.HTTPMessage is a subclass of email.message.Message. \n  \nHTTPResponse.version  \nHTTP protocol version used by server. 10 for HTTP\/1.0, 11 for HTTP\/1.1. \n  \nHTTPResponse.url  \nURL of the resource retrieved, commonly used to determine if a redirect was followed. \n  \nHTTPResponse.headers  \nHeaders of the response in the form of an email.message.EmailMessage instance. \n  \nHTTPResponse.status  \nStatus code returned by server. \n  \nHTTPResponse.reason  \nReason phrase returned by server. \n  \nHTTPResponse.debuglevel  \nA debugging hook. If debuglevel is greater than zero, messages will be printed to stdout as the response is read and parsed. \n  \nHTTPResponse.closed  \nIs True if the stream is closed. \n  \nHTTPResponse.geturl()  \n Deprecated since version 3.9: Deprecated in favor of url.  \n  \nHTTPResponse.info()  \n Deprecated since version 3.9: Deprecated in favor of headers.  \n  \nHTTPResponse.getstatus()  \n Deprecated since version 3.9: Deprecated in favor of status.  \n Examples Here is an example session that uses the GET method: >>> import http.client\n>>> conn = http.client.HTTPSConnection(\"www.python.org\")\n>>> conn.request(\"GET\", \"\/\")\n>>> r1 = conn.getresponse()\n>>> print(r1.status, r1.reason)\n200 OK\n>>> data1 = r1.read()  # This will return entire content.\n>>> # The following example demonstrates reading data in chunks.\n>>> conn.request(\"GET\", \"\/\")\n>>> r1 = conn.getresponse()\n>>> while chunk := r1.read(200):\n...     print(repr(chunk))\nb'<!doctype html>\\n<!--[if\"...\n...\n>>> # Example of an invalid request\n>>> conn = http.client.HTTPSConnection(\"docs.python.org\")\n>>> conn.request(\"GET\", \"\/parrot.spam\")\n>>> r2 = conn.getresponse()\n>>> print(r2.status, r2.reason)\n404 Not Found\n>>> data2 = r2.read()\n>>> conn.close()\n Here is an example session that uses the HEAD method. Note that the HEAD method never returns any data. >>> import http.client\n>>> conn = http.client.HTTPSConnection(\"www.python.org\")\n>>> conn.request(\"HEAD\", \"\/\")\n>>> res = conn.getresponse()\n>>> print(res.status, res.reason)\n200 OK\n>>> data = res.read()\n>>> print(len(data))\n0\n>>> data == b''\nTrue\n Here is an example session that shows how to POST requests: >>> import http.client, urllib.parse\n>>> params = urllib.parse.urlencode({'@number': 12524, '@type': 'issue', '@action': 'show'})\n>>> headers = {\"Content-type\": \"application\/x-www-form-urlencoded\",\n...            \"Accept\": \"text\/plain\"}\n>>> conn = http.client.HTTPConnection(\"bugs.python.org\")\n>>> conn.request(\"POST\", \"\", params, headers)\n>>> response = conn.getresponse()\n>>> print(response.status, response.reason)\n302 Found\n>>> data = response.read()\n>>> data\nb'Redirecting to <a href=\"http:\/\/bugs.python.org\/issue12524\">http:\/\/bugs.python.org\/issue12524<\/a>'\n>>> conn.close()\n Client side HTTP PUT requests are very similar to POST requests. The difference lies only the server side where HTTP server will allow resources to be created via PUT request. It should be noted that custom HTTP methods are also handled in urllib.request.Request by setting the appropriate method attribute. Here is an example session that shows how to send a PUT request using http.client: >>> # This creates an HTTP message\n>>> # with the content of BODY as the enclosed representation\n>>> # for the resource http:\/\/localhost:8080\/file\n...\n>>> import http.client\n>>> BODY = \"***filecontents***\"\n>>> conn = http.client.HTTPConnection(\"localhost\", 8080)\n>>> conn.request(\"PUT\", \"\/file\", BODY)\n>>> response = conn.getresponse()\n>>> print(response.status, response.reason)\n200, OK\n HTTPMessage Objects An http.client.HTTPMessage instance holds the headers from an HTTP response. It is implemented using the email.message.Message class.","title":"python.library.http.client"},{"text":"class urllib.request.FileHandler  \nOpen local files.","title":"python.library.urllib.request#urllib.request.FileHandler"},{"text":"cgi \u2014 Common Gateway Interface support Source code: Lib\/cgi.py Support module for Common Gateway Interface (CGI) scripts. This module defines a number of utilities for use by CGI scripts written in Python. Introduction A CGI script is invoked by an HTTP server, usually to process user input submitted through an HTML <FORM> or <ISINDEX> element. Most often, CGI scripts live in the server\u2019s special cgi-bin directory. The HTTP server places all sorts of information about the request (such as the client\u2019s hostname, the requested URL, the query string, and lots of other goodies) in the script\u2019s shell environment, executes the script, and sends the script\u2019s output back to the client. The script\u2019s input is connected to the client too, and sometimes the form data is read this way; at other times the form data is passed via the \u201cquery string\u201d part of the URL. This module is intended to take care of the different cases and provide a simpler interface to the Python script. It also provides a number of utilities that help in debugging scripts, and the latest addition is support for file uploads from a form (if your browser supports it). The output of a CGI script should consist of two sections, separated by a blank line. The first section contains a number of headers, telling the client what kind of data is following. Python code to generate a minimal header section looks like this: print(\"Content-Type: text\/html\")    # HTML is following\nprint()                             # blank line, end of headers\n The second section is usually HTML, which allows the client software to display nicely formatted text with header, in-line images, etc. Here\u2019s Python code that prints a simple piece of HTML: print(\"<TITLE>CGI script output<\/TITLE>\")\nprint(\"<H1>This is my first CGI script<\/H1>\")\nprint(\"Hello, world!\")\n Using the cgi module Begin by writing import cgi. When you write a new script, consider adding these lines: import cgitb\ncgitb.enable()\n This activates a special exception handler that will display detailed reports in the Web browser if any errors occur. If you\u2019d rather not show the guts of your program to users of your script, you can have the reports saved to files instead, with code like this: import cgitb\ncgitb.enable(display=0, logdir=\"\/path\/to\/logdir\")\n It\u2019s very helpful to use this feature during script development. The reports produced by cgitb provide information that can save you a lot of time in tracking down bugs. You can always remove the cgitb line later when you have tested your script and are confident that it works correctly. To get at submitted form data, use the FieldStorage class. If the form contains non-ASCII characters, use the encoding keyword parameter set to the value of the encoding defined for the document. It is usually contained in the META tag in the HEAD section of the HTML document or by the Content-Type header). This reads the form contents from the standard input or the environment (depending on the value of various environment variables set according to the CGI standard). Since it may consume standard input, it should be instantiated only once. The FieldStorage instance can be indexed like a Python dictionary. It allows membership testing with the in operator, and also supports the standard dictionary method keys() and the built-in function len(). Form fields containing empty strings are ignored and do not appear in the dictionary; to keep such values, provide a true value for the optional keep_blank_values keyword parameter when creating the FieldStorage instance. For instance, the following code (which assumes that the Content-Type header and blank line have already been printed) checks that the fields name and addr are both set to a non-empty string: form = cgi.FieldStorage()\nif \"name\" not in form or \"addr\" not in form:\n    print(\"<H1>Error<\/H1>\")\n    print(\"Please fill in the name and addr fields.\")\n    return\nprint(\"<p>name:\", form[\"name\"].value)\nprint(\"<p>addr:\", form[\"addr\"].value)\n...further form processing here...\n Here the fields, accessed through form[key], are themselves instances of FieldStorage (or MiniFieldStorage, depending on the form encoding). The value attribute of the instance yields the string value of the field. The getvalue() method returns this string value directly; it also accepts an optional second argument as a default to return if the requested key is not present. If the submitted form data contains more than one field with the same name, the object retrieved by form[key] is not a FieldStorage or MiniFieldStorage instance but a list of such instances. Similarly, in this situation, form.getvalue(key) would return a list of strings. If you expect this possibility (when your HTML form contains multiple fields with the same name), use the getlist() method, which always returns a list of values (so that you do not need to special-case the single item case). For example, this code concatenates any number of username fields, separated by commas: value = form.getlist(\"username\")\nusernames = \",\".join(value)\n If a field represents an uploaded file, accessing the value via the value attribute or the getvalue() method reads the entire file in memory as bytes. This may not be what you want. You can test for an uploaded file by testing either the filename attribute or the file attribute. You can then read the data from the file attribute before it is automatically closed as part of the garbage collection of the FieldStorage instance (the read() and readline() methods will return bytes): fileitem = form[\"userfile\"]\nif fileitem.file:\n    # It's an uploaded file; count lines\n    linecount = 0\n    while True:\n        line = fileitem.file.readline()\n        if not line: break\n        linecount = linecount + 1\n FieldStorage objects also support being used in a with statement, which will automatically close them when done. If an error is encountered when obtaining the contents of an uploaded file (for example, when the user interrupts the form submission by clicking on a Back or Cancel button) the done attribute of the object for the field will be set to the value -1. The file upload draft standard entertains the possibility of uploading multiple files from one field (using a recursive multipart\/* encoding). When this occurs, the item will be a dictionary-like FieldStorage item. This can be determined by testing its type attribute, which should be multipart\/form-data (or perhaps another MIME type matching multipart\/*). In this case, it can be iterated over recursively just like the top-level form object. When a form is submitted in the \u201cold\u201d format (as the query string or as a single data part of type application\/x-www-form-urlencoded), the items will actually be instances of the class MiniFieldStorage. In this case, the list, file, and filename attributes are always None. A form submitted via POST that also has a query string will contain both FieldStorage and MiniFieldStorage items.  Changed in version 3.4: The file attribute is automatically closed upon the garbage collection of the creating FieldStorage instance.   Changed in version 3.5: Added support for the context management protocol to the FieldStorage class.  Higher Level Interface The previous section explains how to read CGI form data using the FieldStorage class. This section describes a higher level interface which was added to this class to allow one to do it in a more readable and intuitive way. The interface doesn\u2019t make the techniques described in previous sections obsolete \u2014 they are still useful to process file uploads efficiently, for example. The interface consists of two simple methods. Using the methods you can process form data in a generic way, without the need to worry whether only one or more values were posted under one name. In the previous section, you learned to write following code anytime you expected a user to post more than one value under one name: item = form.getvalue(\"item\")\nif isinstance(item, list):\n    # The user is requesting more than one item.\nelse:\n    # The user is requesting only one item.\n This situation is common for example when a form contains a group of multiple checkboxes with the same name: <input type=\"checkbox\" name=\"item\" value=\"1\" \/>\n<input type=\"checkbox\" name=\"item\" value=\"2\" \/>\n In most situations, however, there\u2019s only one form control with a particular name in a form and then you expect and need only one value associated with this name. So you write a script containing for example this code: user = form.getvalue(\"user\").upper()\n The problem with the code is that you should never expect that a client will provide valid input to your scripts. For example, if a curious user appends another user=foo pair to the query string, then the script would crash, because in this situation the getvalue(\"user\") method call returns a list instead of a string. Calling the upper() method on a list is not valid (since lists do not have a method of this name) and results in an AttributeError exception. Therefore, the appropriate way to read form data values was to always use the code which checks whether the obtained value is a single value or a list of values. That\u2019s annoying and leads to less readable scripts. A more convenient approach is to use the methods getfirst() and getlist() provided by this higher level interface.  \nFieldStorage.getfirst(name, default=None)  \nThis method always returns only one value associated with form field name. The method returns only the first value in case that more values were posted under such name. Please note that the order in which the values are received may vary from browser to browser and should not be counted on. 1 If no such form field or value exists then the method returns the value specified by the optional parameter default. This parameter defaults to None if not specified. \n  \nFieldStorage.getlist(name)  \nThis method always returns a list of values associated with form field name. The method returns an empty list if no such form field or value exists for name. It returns a list consisting of one item if only one such value exists. \n Using these methods you can write nice compact code: import cgi\nform = cgi.FieldStorage()\nuser = form.getfirst(\"user\", \"\").upper()    # This way it's safe.\nfor item in form.getlist(\"item\"):\n    do_something(item)\n Functions These are useful if you want more control, or if you want to employ some of the algorithms implemented in this module in other circumstances.  \ncgi.parse(fp=None, environ=os.environ, keep_blank_values=False, strict_parsing=False, separator=\"&\")  \nParse a query in the environment or from a file (the file defaults to sys.stdin). The keep_blank_values, strict_parsing and separator parameters are passed to urllib.parse.parse_qs() unchanged. \n  \ncgi.parse_multipart(fp, pdict, encoding=\"utf-8\", errors=\"replace\", separator=\"&\")  \nParse input of type multipart\/form-data (for file uploads). Arguments are fp for the input file, pdict for a dictionary containing other parameters in the Content-Type header, and encoding, the request encoding. Returns a dictionary just like urllib.parse.parse_qs(): keys are the field names, each value is a list of values for that field. For non-file fields, the value is a list of strings. This is easy to use but not much good if you are expecting megabytes to be uploaded \u2014 in that case, use the FieldStorage class instead which is much more flexible.  Changed in version 3.7: Added the encoding and errors parameters. For non-file fields, the value is now a list of strings, not bytes.   Changed in version 3.9.2: Added the separator parameter.  \n  \ncgi.parse_header(string)  \nParse a MIME header (such as Content-Type) into a main value and a dictionary of parameters. \n  \ncgi.test()  \nRobust test CGI script, usable as main program. Writes minimal HTTP headers and formats all information provided to the script in HTML form. \n  \ncgi.print_environ()  \nFormat the shell environment in HTML. \n  \ncgi.print_form(form)  \nFormat a form in HTML. \n  \ncgi.print_directory()  \nFormat the current directory in HTML. \n  \ncgi.print_environ_usage()  \nPrint a list of useful (used by CGI) environment variables in HTML. \n Caring about security There\u2019s one important rule: if you invoke an external program (via the os.system() or os.popen() functions. or others with similar functionality), make very sure you don\u2019t pass arbitrary strings received from the client to the shell. This is a well-known security hole whereby clever hackers anywhere on the Web can exploit a gullible CGI script to invoke arbitrary shell commands. Even parts of the URL or field names cannot be trusted, since the request doesn\u2019t have to come from your form! To be on the safe side, if you must pass a string gotten from a form to a shell command, you should make sure the string contains only alphanumeric characters, dashes, underscores, and periods. Installing your CGI script on a Unix system Read the documentation for your HTTP server and check with your local system administrator to find the directory where CGI scripts should be installed; usually this is in a directory cgi-bin in the server tree. Make sure that your script is readable and executable by \u201cothers\u201d; the Unix file mode should be 0o755 octal (use chmod 0755 filename). Make sure that the first line of the script contains #! starting in column 1 followed by the pathname of the Python interpreter, for instance: #!\/usr\/local\/bin\/python\n Make sure the Python interpreter exists and is executable by \u201cothers\u201d. Make sure that any files your script needs to read or write are readable or writable, respectively, by \u201cothers\u201d \u2014 their mode should be 0o644 for readable and 0o666 for writable. This is because, for security reasons, the HTTP server executes your script as user \u201cnobody\u201d, without any special privileges. It can only read (write, execute) files that everybody can read (write, execute). The current directory at execution time is also different (it is usually the server\u2019s cgi-bin directory) and the set of environment variables is also different from what you get when you log in. In particular, don\u2019t count on the shell\u2019s search path for executables (PATH) or the Python module search path (PYTHONPATH) to be set to anything interesting. If you need to load modules from a directory which is not on Python\u2019s default module search path, you can change the path in your script, before importing other modules. For example: import sys\nsys.path.insert(0, \"\/usr\/home\/joe\/lib\/python\")\nsys.path.insert(0, \"\/usr\/local\/lib\/python\")\n (This way, the directory inserted last will be searched first!) Instructions for non-Unix systems will vary; check your HTTP server\u2019s documentation (it will usually have a section on CGI scripts). Testing your CGI script Unfortunately, a CGI script will generally not run when you try it from the command line, and a script that works perfectly from the command line may fail mysteriously when run from the server. There\u2019s one reason why you should still test your script from the command line: if it contains a syntax error, the Python interpreter won\u2019t execute it at all, and the HTTP server will most likely send a cryptic error to the client. Assuming your script has no syntax errors, yet it does not work, you have no choice but to read the next section. Debugging CGI scripts First of all, check for trivial installation errors \u2014 reading the section above on installing your CGI script carefully can save you a lot of time. If you wonder whether you have understood the installation procedure correctly, try installing a copy of this module file (cgi.py) as a CGI script. When invoked as a script, the file will dump its environment and the contents of the form in HTML form. Give it the right mode etc, and send it a request. If it\u2019s installed in the standard cgi-bin directory, it should be possible to send it a request by entering a URL into your browser of the form: http:\/\/yourhostname\/cgi-bin\/cgi.py?name=Joe+Blow&addr=At+Home\n If this gives an error of type 404, the server cannot find the script \u2013 perhaps you need to install it in a different directory. If it gives another error, there\u2019s an installation problem that you should fix before trying to go any further. If you get a nicely formatted listing of the environment and form content (in this example, the fields should be listed as \u201caddr\u201d with value \u201cAt Home\u201d and \u201cname\u201d with value \u201cJoe Blow\u201d), the cgi.py script has been installed correctly. If you follow the same procedure for your own script, you should now be able to debug it. The next step could be to call the cgi module\u2019s test() function from your script: replace its main code with the single statement cgi.test()\n This should produce the same results as those gotten from installing the cgi.py file itself. When an ordinary Python script raises an unhandled exception (for whatever reason: of a typo in a module name, a file that can\u2019t be opened, etc.), the Python interpreter prints a nice traceback and exits. While the Python interpreter will still do this when your CGI script raises an exception, most likely the traceback will end up in one of the HTTP server\u2019s log files, or be discarded altogether. Fortunately, once you have managed to get your script to execute some code, you can easily send tracebacks to the Web browser using the cgitb module. If you haven\u2019t done so already, just add the lines: import cgitb\ncgitb.enable()\n to the top of your script. Then try running it again; when a problem occurs, you should see a detailed report that will likely make apparent the cause of the crash. If you suspect that there may be a problem in importing the cgitb module, you can use an even more robust approach (which only uses built-in modules): import sys\nsys.stderr = sys.stdout\nprint(\"Content-Type: text\/plain\")\nprint()\n...your code here...\n This relies on the Python interpreter to print the traceback. The content type of the output is set to plain text, which disables all HTML processing. If your script works, the raw HTML will be displayed by your client. If it raises an exception, most likely after the first two lines have been printed, a traceback will be displayed. Because no HTML interpretation is going on, the traceback will be readable. Common problems and solutions  Most HTTP servers buffer the output from CGI scripts until the script is completed. This means that it is not possible to display a progress report on the client\u2019s display while the script is running. Check the installation instructions above. Check the HTTP server\u2019s log files. (tail -f logfile in a separate window may be useful!) Always check a script for syntax errors first, by doing something like python script.py. If your script does not have any syntax errors, try adding import cgitb;\ncgitb.enable() to the top of the script. When invoking external programs, make sure they can be found. Usually, this means using absolute path names \u2014 PATH is usually not set to a very useful value in a CGI script. When reading or writing external files, make sure they can be read or written by the userid under which your CGI script will be running: this is typically the userid under which the web server is running, or some explicitly specified userid for a web server\u2019s suexec feature. Don\u2019t try to give a CGI script a set-uid mode. This doesn\u2019t work on most systems, and is a security liability as well.  Footnotes  \n1  \nNote that some recent versions of the HTML specification do state what order the field values should be supplied in, but knowing whether a request was received from a conforming browser, or even from a browser at all, is tedious and error-prone.","title":"python.library.cgi"},{"text":"http.client.HTTP_PORT  \nThe default port for the HTTP protocol (always 80).","title":"python.library.http.client#http.client.HTTP_PORT"},{"text":"urllib \u2014 URL handling modules Source code: Lib\/urllib\/ urllib is a package that collects several modules for working with URLs:  \nurllib.request for opening and reading URLs \nurllib.error containing the exceptions raised by urllib.request\n \nurllib.parse for parsing URLs \nurllib.robotparser for parsing robots.txt files","title":"python.library.urllib"},{"text":"http \u2014 HTTP modules Source code: Lib\/http\/__init__.py http is a package that collects several modules for working with the HyperText Transfer Protocol:  \nhttp.client is a low-level HTTP protocol client; for high-level URL opening use urllib.request\n \nhttp.server contains basic HTTP server classes based on socketserver\n \nhttp.cookies has utilities for implementing state management with cookies \nhttp.cookiejar provides persistence of cookies  http is also a module that defines a number of HTTP status codes and associated messages through the http.HTTPStatus enum:  \nclass http.HTTPStatus  \n New in version 3.5.  A subclass of enum.IntEnum that defines a set of HTTP status codes, reason phrases and long descriptions written in English. Usage: >>> from http import HTTPStatus\n>>> HTTPStatus.OK\n<HTTPStatus.OK: 200>\n>>> HTTPStatus.OK == 200\nTrue\n>>> HTTPStatus.OK.value\n200\n>>> HTTPStatus.OK.phrase\n'OK'\n>>> HTTPStatus.OK.description\n'Request fulfilled, document follows'\n>>> list(HTTPStatus)\n[<HTTPStatus.CONTINUE: 100>, <HTTPStatus.SWITCHING_PROTOCOLS: 101>, ...]\n \n HTTP status codes Supported, IANA-registered status codes available in http.HTTPStatus are:   \nCode Enum Name Details   \n100 CONTINUE HTTP\/1.1 RFC 7231, Section 6.2.1  \n101 SWITCHING_PROTOCOLS HTTP\/1.1 RFC 7231, Section 6.2.2  \n102 PROCESSING WebDAV RFC 2518, Section 10.1  \n103 EARLY_HINTS An HTTP Status Code for Indicating Hints RFC 8297  \n200 OK HTTP\/1.1 RFC 7231, Section 6.3.1  \n201 CREATED HTTP\/1.1 RFC 7231, Section 6.3.2  \n202 ACCEPTED HTTP\/1.1 RFC 7231, Section 6.3.3  \n203 NON_AUTHORITATIVE_INFORMATION HTTP\/1.1 RFC 7231, Section 6.3.4  \n204 NO_CONTENT HTTP\/1.1 RFC 7231, Section 6.3.5  \n205 RESET_CONTENT HTTP\/1.1 RFC 7231, Section 6.3.6  \n206 PARTIAL_CONTENT HTTP\/1.1 RFC 7233, Section 4.1  \n207 MULTI_STATUS WebDAV RFC 4918, Section 11.1  \n208 ALREADY_REPORTED WebDAV Binding Extensions RFC 5842, Section 7.1 (Experimental)  \n226 IM_USED Delta Encoding in HTTP RFC 3229, Section 10.4.1  \n300 MULTIPLE_CHOICES HTTP\/1.1 RFC 7231, Section 6.4.1  \n301 MOVED_PERMANENTLY HTTP\/1.1 RFC 7231, Section 6.4.2  \n302 FOUND HTTP\/1.1 RFC 7231, Section 6.4.3  \n303 SEE_OTHER HTTP\/1.1 RFC 7231, Section 6.4.4  \n304 NOT_MODIFIED HTTP\/1.1 RFC 7232, Section 4.1  \n305 USE_PROXY HTTP\/1.1 RFC 7231, Section 6.4.5  \n307 TEMPORARY_REDIRECT HTTP\/1.1 RFC 7231, Section 6.4.7  \n308 PERMANENT_REDIRECT Permanent Redirect RFC 7238, Section 3 (Experimental)  \n400 BAD_REQUEST HTTP\/1.1 RFC 7231, Section 6.5.1  \n401 UNAUTHORIZED HTTP\/1.1 Authentication RFC 7235, Section 3.1  \n402 PAYMENT_REQUIRED HTTP\/1.1 RFC 7231, Section 6.5.2  \n403 FORBIDDEN HTTP\/1.1 RFC 7231, Section 6.5.3  \n404 NOT_FOUND HTTP\/1.1 RFC 7231, Section 6.5.4  \n405 METHOD_NOT_ALLOWED HTTP\/1.1 RFC 7231, Section 6.5.5  \n406 NOT_ACCEPTABLE HTTP\/1.1 RFC 7231, Section 6.5.6  \n407 PROXY_AUTHENTICATION_REQUIRED HTTP\/1.1 Authentication RFC 7235, Section 3.2  \n408 REQUEST_TIMEOUT HTTP\/1.1 RFC 7231, Section 6.5.7  \n409 CONFLICT HTTP\/1.1 RFC 7231, Section 6.5.8  \n410 GONE HTTP\/1.1 RFC 7231, Section 6.5.9  \n411 LENGTH_REQUIRED HTTP\/1.1 RFC 7231, Section 6.5.10  \n412 PRECONDITION_FAILED HTTP\/1.1 RFC 7232, Section 4.2  \n413 REQUEST_ENTITY_TOO_LARGE HTTP\/1.1 RFC 7231, Section 6.5.11  \n414 REQUEST_URI_TOO_LONG HTTP\/1.1 RFC 7231, Section 6.5.12  \n415 UNSUPPORTED_MEDIA_TYPE HTTP\/1.1 RFC 7231, Section 6.5.13  \n416 REQUESTED_RANGE_NOT_SATISFIABLE HTTP\/1.1 Range Requests RFC 7233, Section 4.4  \n417 EXPECTATION_FAILED HTTP\/1.1 RFC 7231, Section 6.5.14  \n418 IM_A_TEAPOT HTCPCP\/1.0 RFC 2324, Section 2.3.2  \n421 MISDIRECTED_REQUEST HTTP\/2 RFC 7540, Section 9.1.2  \n422 UNPROCESSABLE_ENTITY WebDAV RFC 4918, Section 11.2  \n423 LOCKED WebDAV RFC 4918, Section 11.3  \n424 FAILED_DEPENDENCY WebDAV RFC 4918, Section 11.4  \n425 TOO_EARLY Using Early Data in HTTP RFC 8470  \n426 UPGRADE_REQUIRED HTTP\/1.1 RFC 7231, Section 6.5.15  \n428 PRECONDITION_REQUIRED Additional HTTP Status Codes RFC 6585  \n429 TOO_MANY_REQUESTS Additional HTTP Status Codes RFC 6585  \n431 REQUEST_HEADER_FIELDS_TOO_LARGE Additional HTTP Status Codes RFC 6585  \n451 UNAVAILABLE_FOR_LEGAL_REASONS An HTTP Status Code to Report Legal Obstacles RFC 7725  \n500 INTERNAL_SERVER_ERROR HTTP\/1.1 RFC 7231, Section 6.6.1  \n501 NOT_IMPLEMENTED HTTP\/1.1 RFC 7231, Section 6.6.2  \n502 BAD_GATEWAY HTTP\/1.1 RFC 7231, Section 6.6.3  \n503 SERVICE_UNAVAILABLE HTTP\/1.1 RFC 7231, Section 6.6.4  \n504 GATEWAY_TIMEOUT HTTP\/1.1 RFC 7231, Section 6.6.5  \n505 HTTP_VERSION_NOT_SUPPORTED HTTP\/1.1 RFC 7231, Section 6.6.6  \n506 VARIANT_ALSO_NEGOTIATES Transparent Content Negotiation in HTTP RFC 2295, Section 8.1 (Experimental)  \n507 INSUFFICIENT_STORAGE WebDAV RFC 4918, Section 11.5  \n508 LOOP_DETECTED WebDAV Binding Extensions RFC 5842, Section 7.2 (Experimental)  \n510 NOT_EXTENDED An HTTP Extension Framework RFC 2774, Section 7 (Experimental)  \n511 NETWORK_AUTHENTICATION_REQUIRED Additional HTTP Status Codes RFC 6585, Section 6   In order to preserve backwards compatibility, enum values are also present in the http.client module in the form of constants. The enum name is equal to the constant name (i.e. http.HTTPStatus.OK is also available as http.client.OK).  Changed in version 3.7: Added 421 MISDIRECTED_REQUEST status code.   New in version 3.8: Added 451 UNAVAILABLE_FOR_LEGAL_REASONS status code.   New in version 3.9: Added 103 EARLY_HINTS, 418 IM_A_TEAPOT and 425 TOO_EARLY status codes.","title":"python.library.http"},{"text":"classDownload(application:tornado.web.Application, request:tornado.httputil.HTTPServerRequest, **kwargs:Any)[source]\n \nBases: tornado.web.RequestHandler   get(fignum, fmt)[source]","title":"matplotlib.backend_webagg_api#matplotlib.backends.backend_webagg.WebAggApplication.Download"},{"text":"test.support.TEST_HTTP_URL  \nDefine the URL of a dedicated HTTP server for the network tests.","title":"python.library.test#test.support.TEST_HTTP_URL"},{"text":"email.policy.HTTP  \nSuitable for serializing headers with for use in HTTP traffic. Like SMTP except that max_line_length is set to None (unlimited).","title":"python.library.email.policy#email.policy.HTTP"},{"text":"urllib.error \u2014 Exception classes raised by urllib.request Source code: Lib\/urllib\/error.py The urllib.error module defines the exception classes for exceptions raised by urllib.request. The base exception class is URLError. The following exceptions are raised by urllib.error as appropriate:  \nexception urllib.error.URLError  \nThe handlers raise this exception (or derived exceptions) when they run into a problem. It is a subclass of OSError.  \nreason  \nThe reason for this error. It can be a message string or another exception instance. \n  Changed in version 3.3: URLError has been made a subclass of OSError instead of IOError.  \n  \nexception urllib.error.HTTPError  \nThough being an exception (a subclass of URLError), an HTTPError can also function as a non-exceptional file-like return value (the same thing that urlopen() returns). This is useful when handling exotic HTTP errors, such as requests for authentication.  \ncode  \nAn HTTP status code as defined in RFC 2616. This numeric value corresponds to a value found in the dictionary of codes as found in http.server.BaseHTTPRequestHandler.responses. \n  \nreason  \nThis is usually a string explaining the reason for this error. \n  \nheaders  \nThe HTTP response headers for the HTTP request that caused the HTTPError.  New in version 3.4.  \n \n  \nexception urllib.error.ContentTooShortError(msg, content)  \nThis exception is raised when the urlretrieve() function detects that the amount of the downloaded data is less than the expected amount (given by the Content-Length header). The content attribute stores the downloaded (and supposedly truncated) data.","title":"python.library.urllib.error"}]}
{"task_id":22676,"prompt":"def f_22676(url):\n\treturn ","suffix":"","canonical_solution":"requests.get(url)","test_start":"\nimport requests \n\ndef check(candidate):","test":["\n    assert candidate(\"https:\/\/github.com\/\").url == \"https:\/\/github.com\/\"\n","\n    assert candidate(\"https:\/\/google.com\/\").url == \"https:\/\/www.google.com\/\"\n"],"entry_point":"f_22676","intent":"download a file `url` over HTTP","library":["requests"],"docs":[]}
{"task_id":22676,"prompt":"def f_22676(url):\n\t","suffix":"\n\treturn ","canonical_solution":"\n\tresponse = requests.get(url, stream=True)\n\twith open('10MB', 'wb') as handle:\n\t\tfor data in response.iter_content():\n\t\t\thandle.write(data)\n\t","test_start":"\nimport requests \n\ndef check(candidate):","test":["\n    candidate(\"https:\/\/github.com\/\")\n    with open(\"10MB\", 'rb') as fr: \n        all_data = [data for data in fr]\n    assert all_data[: 2] == [b'\\n', b'\\n']\n"],"entry_point":"f_22676","intent":"download a file `url` over HTTP and save to \"10MB\"","library":["requests"],"docs":[]}
{"task_id":15405636,"prompt":"def f_15405636(parser):\n\treturn ","suffix":"","canonical_solution":"parser.add_argument('--version', action='version', version='%(prog)s 2.0')","test_start":"\nimport argparse \n\ndef check(candidate):","test":["\n    parser = argparse.ArgumentParser()\n    output = candidate(parser)\n    assert output.option_strings == ['--version']\n    assert output.dest == 'version'\n    assert output.nargs == 0\n"],"entry_point":"f_15405636","intent":"argparse add argument with flag '--version' and version action of '%(prog)s 2.0' to parser `parser`","library":["argparse"],"docs":[{"text":"make_svn_version_py(delete=True)[source]\n \nAppends a data function to the data_files list that will generate __svn_version__.py file to the current package directory. Generate package __svn_version__.py file from SVN revision number, it will be removed after python exits but will be available when sdist, etc commands are executed. Notes If __svn_version__.py existed before, nothing is done. This is intended for working with source directories that are in an SVN repository.","title":"numpy.reference.distutils#numpy.distutils.misc_util.Configuration.make_svn_version_py"},{"text":"sys.version_info  \nA tuple containing the five components of the version number: major, minor, micro, releaselevel, and serial. All values except releaselevel are integers; the release level is 'alpha', 'beta', 'candidate', or 'final'. The version_info value corresponding to the Python version 2.0 is (2, 0, 0, 'final', 0). The components can also be accessed by name, so sys.version_info[0] is equivalent to sys.version_info.major and so on.  Changed in version 3.1: Added named component attributes.","title":"python.library.sys#sys.version_info"},{"text":"platform.python_version()  \nReturns the Python version as string 'major.minor.patchlevel'. Note that unlike the Python sys.version, the returned value will always include the patchlevel (it defaults to 0).","title":"python.library.platform#platform.python_version"},{"text":"ArgumentParser.exit(status=0, message=None)  \nThis method terminates the program, exiting with the specified status and, if given, it prints a message before that. The user can override this method to handle these steps differently: class ErrorCatchingArgumentParser(argparse.ArgumentParser):\n    def exit(self, status=0, message=None):\n        if status:\n            raise Exception(f'Exiting because of an error: {message}')\n        exit(status)","title":"python.library.argparse#argparse.ArgumentParser.exit"},{"text":"sys.version  \nA string containing the version number of the Python interpreter plus additional information on the build number and compiler used. This string is displayed when the interactive interpreter is started. Do not extract version information out of it, rather, use version_info and the functions provided by the platform module.","title":"python.library.sys#sys.version"},{"text":"numpy.distutils.ccompiler.simple_version_match   distutils.ccompiler.simple_version_match(pat='[-.\\\\d]+', ignore='', start='')[source]\n \nSimple matching of version numbers, for use in CCompiler and FCompiler.  Parameters \n \npatstr, optional\n\n\nA regular expression matching version numbers. Default is r'[-.\\d]+'.  \nignorestr, optional\n\n\nA regular expression matching patterns to skip. Default is '', in which case nothing is skipped.  \nstartstr, optional\n\n\nA regular expression matching the start of where to start looking for version numbers. Default is '', in which case searching is started at the beginning of the version string given to matcher.    Returns \n \nmatchercallable\n\n\nA function that is appropriate to use as the .version_match attribute of a CCompiler class. matcher takes a single parameter, a version string.","title":"numpy.reference.generated.numpy.distutils.ccompiler.simple_version_match"},{"text":"get_version(version_file=None, version_variable=None)[source]\n \nTry to get version string of a package. Return a version string of the current package or None if the version information could not be detected. Notes This method scans files named __version__.py, <packagename>_version.py, version.py, and __svn_version__.py for string variables version, __version__, and <packagename>_version, until a version number is found.","title":"numpy.reference.distutils#numpy.distutils.misc_util.Configuration.get_version"},{"text":"tf.keras.layers.Conv2D     View source on GitHub    2D convolution layer (e.g. spatial convolution over images). Inherits From: Layer, Module  View aliases  Main aliases \ntf.keras.layers.Convolution2D Compat aliases for migration See Migration guide for more details. tf.compat.v1.keras.layers.Conv2D, tf.compat.v1.keras.layers.Convolution2D  \ntf.keras.layers.Conv2D(\n    filters, kernel_size, strides=(1, 1), padding='valid',\n    data_format=None, dilation_rate=(1, 1), groups=1, activation=None,\n    use_bias=True, kernel_initializer='glorot_uniform',\n    bias_initializer='zeros', kernel_regularizer=None,\n    bias_regularizer=None, activity_regularizer=None, kernel_constraint=None,\n    bias_constraint=None, **kwargs\n)\n This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None, it is applied to the outputs as well. When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the sample axis), e.g. input_shape=(128, 128, 3) for 128x128 RGB pictures in data_format=\"channels_last\". Examples: \n# The inputs are 28x28 RGB images with `channels_last` and the batch\n# size is 4.\ninput_shape = (4, 28, 28, 3)\nx = tf.random.normal(input_shape)\ny = tf.keras.layers.Conv2D(\n2, 3, activation='relu', input_shape=input_shape[1:])(x)\nprint(y.shape)\n(4, 26, 26, 2)\n \n# With `dilation_rate` as 2.\ninput_shape = (4, 28, 28, 3)\nx = tf.random.normal(input_shape)\ny = tf.keras.layers.Conv2D(\n2, 3, activation='relu', dilation_rate=2, input_shape=input_shape[1:])(x)\nprint(y.shape)\n(4, 24, 24, 2)\n \n# With `padding` as \"same\".\ninput_shape = (4, 28, 28, 3)\nx = tf.random.normal(input_shape)\ny = tf.keras.layers.Conv2D(\n2, 3, activation='relu', padding=\"same\", input_shape=input_shape[1:])(x)\nprint(y.shape)\n(4, 28, 28, 2)\n \n# With extended batch shape [4, 7]:\ninput_shape = (4, 7, 28, 28, 3)\nx = tf.random.normal(input_shape)\ny = tf.keras.layers.Conv2D(\n2, 3, activation='relu', input_shape=input_shape[2:])(x)\nprint(y.shape)\n(4, 7, 26, 26, 2)\n\n \n\n\n Arguments\n  filters   Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution).  \n  kernel_size   An integer or tuple\/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions.  \n  strides   An integer or tuple\/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1.  \n  padding   one of \"valid\" or \"same\" (case-insensitive). \"valid\" means no padding. \"same\" results in padding evenly to the left\/right or up\/down of the input such that output has the same height\/width dimension as the input.  \n  data_format   A string, one of channels_last (default) or channels_first. The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch_size, height, width, channels) while channels_first corresponds to inputs with shape (batch_size, channels, height, width). It defaults to the image_data_format value found in your Keras config file at ~\/.keras\/keras.json. If you never set it, then it will be channels_last.  \n  dilation_rate   an integer or tuple\/list of 2 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1.  \n  groups   A positive integer specifying the number of groups in which the input is split along the channel axis. Each group is convolved separately with filters \/ groups filters. The output is the concatenation of all the groups results along the channel axis. Input channels and filters must both be divisible by groups.  \n  activation   Activation function to use. If you don't specify anything, no activation is applied (see keras.activations).  \n  use_bias   Boolean, whether the layer uses a bias vector.  \n  kernel_initializer   Initializer for the kernel weights matrix (see keras.initializers).  \n  bias_initializer   Initializer for the bias vector (see keras.initializers).  \n  kernel_regularizer   Regularizer function applied to the kernel weights matrix (see keras.regularizers).  \n  bias_regularizer   Regularizer function applied to the bias vector (see keras.regularizers).  \n  activity_regularizer   Regularizer function applied to the output of the layer (its \"activation\") (see keras.regularizers).  \n  kernel_constraint   Constraint function applied to the kernel matrix (see keras.constraints).  \n  bias_constraint   Constraint function applied to the bias vector (see keras.constraints).    Input shape: 4+D tensor with shape: batch_shape + (channels, rows, cols) if data_format='channels_first' or 4+D tensor with shape: batch_shape + (rows, cols, channels) if data_format='channels_last'. Output shape: 4+D tensor with shape: batch_shape + (filters, new_rows, new_cols) if data_format='channels_first' or 4+D tensor with shape: batch_shape + (new_rows, new_cols, filters) if data_format='channels_last'. rows and cols values might have changed due to padding.\n \n\n\n Returns   A tensor of rank 4+ representing activation(conv2d(inputs, kernel) + bias).  \n\n \n\n\n Raises\n  ValueError   if padding is \"causal\".  \n  ValueError   when both strides > 1 and dilation_rate > 1.","title":"tensorflow.keras.layers.conv2d"},{"text":"sysconfig.get_python_version()  \nReturn the MAJOR.MINOR Python version number as a string. Similar to '%d.%d' % sys.version_info[:2].","title":"python.library.sysconfig#sysconfig.get_python_version"},{"text":"platform.python_version_tuple()  \nReturns the Python version as tuple (major, minor, patchlevel) of strings. Note that unlike the Python sys.version, the returned value will always include the patchlevel (it defaults to '0').","title":"python.library.platform#platform.python_version_tuple"}]}
{"task_id":17665809,"prompt":"def f_17665809(d):\n\treturn ","suffix":"","canonical_solution":"{i: d[i] for i in d if i != 'c'}","test_start":"\ndef check(candidate):","test":["\n    assert candidate({'a': 1 , 'b': 2, 'c': 3}) == {'a': 1 , 'b': 2}\n","\n    assert candidate({'c': None}) == {}\n","\n    assert candidate({'a': 1 , 'b': 2, 'c': 3}) != {'a': 1 , 'b': 2, 'c': 3}\n","\n    assert candidate({'c': 1, 'cc': 2, 'ccc':3}) == {'cc': 2, 'ccc':3}\n","\n    assert 'c' not in candidate({'c':i for i in range(10)})\n"],"entry_point":"f_17665809","intent":"remove key 'c' from dictionary `d`","library":[],"docs":[]}
{"task_id":41861705,"prompt":"def f_41861705(split_df, csv_df):\n\treturn ","suffix":"","canonical_solution":"pd.merge(split_df, csv_df, on=['key'], suffixes=('_left', '_right'))","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    split_df = pd.DataFrame({'key': ['foo', 'bar'], 'value': [1, 2]})\n    csv_df = pd.DataFrame({'key': ['foo', 'baz'], 'value': [3, 4]})\n    result = pd.DataFrame({'key': ['foo'], 'value_left': [1],'value_right': [3]})\n    assert all(candidate(csv_df, split_df) == result)\n"],"entry_point":"f_41861705","intent":"Create new DataFrame object by merging columns \"key\" of  dataframes `split_df` and `csv_df` and rename the columns from dataframes `split_df` and `csv_df` with suffix `_left` and `_right` respectively","library":["pandas"],"docs":[{"text":"tf.raw_ops.BoostedTreesSparseCalculateBestFeatureSplit Calculates gains for each feature and returns the best possible split information for the feature.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.raw_ops.BoostedTreesSparseCalculateBestFeatureSplit  \ntf.raw_ops.BoostedTreesSparseCalculateBestFeatureSplit(\n    node_id_range, stats_summary_indices, stats_summary_values, stats_summary_shape,\n    l1, l2, tree_complexity, min_node_weight, logits_dimension,\n    split_type='inequality', name=None\n)\n The split information is the best threshold (bucket id), gains and left\/right node contributions per node for each feature. It is possible that not all nodes can be split on each feature. Hence, the list of possible nodes can differ between the features. Therefore, we return node_ids_list for each feature, containing the list of nodes that this feature can be used to split. In this manner, the output is the best split per features and per node, so that it needs to be combined later to produce the best split for each node (among all possible features). The output shapes are compatible in a way that the first dimension of all tensors are the same and equal to the number of possible split nodes for each feature.\n \n\n\n Args\n  node_id_range   A Tensor of type int32. A Rank 1 tensor (shape=[2]) to specify the range [first, last) of node ids to process within stats_summary_list. The nodes are iterated between the two nodes specified by the tensor, as like for node_id in range(node_id_range[0], node_id_range[1]) (Note that the last index node_id_range[1] is exclusive).  \n  stats_summary_indices   A Tensor of type int32. A Rank 2 int64 tensor of dense shape N, 4 for accumulated stats summary (gradient\/hessian) per node per bucket for each feature. The second dimension contains node id, feature dimension, bucket id, and stats dim. stats dim is the sum of logits dimension and hessian dimension, hessian dimension can either be logits dimension if diagonal hessian is used, or logits dimension^2 if full hessian is used.  \n  stats_summary_values   A Tensor of type float32. A Rank 1 float tensor of dense shape N, which supplies the values for each element in summary_indices.  \n  stats_summary_shape   A Tensor of type int32. A Rank 1 float tensor of dense shape [4], which specifies the dense shape of the sparse tensor, which is [num tree nodes, feature dimensions, num buckets, stats dim].  \n  l1   A Tensor of type float32. l1 regularization factor on leaf weights, per instance based.  \n  l2   A Tensor of type float32. l2 regularization factor on leaf weights, per instance based.  \n  tree_complexity   A Tensor of type float32. adjustment to the gain, per leaf based.  \n  min_node_weight   A Tensor of type float32. minimum avg of hessians in a node before required for the node to be considered for splitting.  \n  logits_dimension   An int that is >= 1. The dimension of logit, i.e., number of classes.  \n  split_type   An optional string from: \"inequality\". Defaults to \"inequality\". A string indicating if this Op should perform inequality split or equality split.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A tuple of Tensor objects (node_ids, gains, feature_dimensions, thresholds, left_node_contribs, right_node_contribs, split_with_default_directions).     node_ids   A Tensor of type int32.  \n  gains   A Tensor of type float32.  \n  feature_dimensions   A Tensor of type int32.  \n  thresholds   A Tensor of type int32.  \n  left_node_contribs   A Tensor of type float32.  \n  right_node_contribs   A Tensor of type float32.  \n  split_with_default_directions   A Tensor of type string.","title":"tensorflow.raw_ops.boostedtreessparsecalculatebestfeaturesplit"},{"text":"tf.raw_ops.BoostedTreesCalculateBestFeatureSplit Calculates gains for each feature and returns the best possible split information for the feature.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.raw_ops.BoostedTreesCalculateBestFeatureSplit  \ntf.raw_ops.BoostedTreesCalculateBestFeatureSplit(\n    node_id_range, stats_summary, l1, l2, tree_complexity, min_node_weight,\n    logits_dimension, split_type='inequality', name=None\n)\n The split information is the best threshold (bucket id), gains and left\/right node contributions per node for each feature. It is possible that not all nodes can be split on each feature. Hence, the list of possible nodes can differ between the features. Therefore, we return node_ids_list for each feature, containing the list of nodes that this feature can be used to split. In this manner, the output is the best split per features and per node, so that it needs to be combined later to produce the best split for each node (among all possible features). The output shapes are compatible in a way that the first dimension of all tensors are the same and equal to the number of possible split nodes for each feature.\n \n\n\n Args\n  node_id_range   A Tensor of type int32. A Rank 1 tensor (shape=[2]) to specify the range [first, last) of node ids to process within stats_summary_list. The nodes are iterated between the two nodes specified by the tensor, as like for node_id in range(node_id_range[0], node_id_range[1]) (Note that the last index node_id_range[1] is exclusive).  \n  stats_summary   A Tensor of type float32. A Rank 4 tensor (#shape=[max_splits, feature_dims, bucket, stats_dims]) for accumulated stats summary (gradient\/hessian) per node, per dimension, per buckets for each feature. The first dimension of the tensor is the maximum number of splits, and thus not all elements of it will be used, but only the indexes specified by node_ids will be used.  \n  l1   A Tensor of type float32. l1 regularization factor on leaf weights, per instance based.  \n  l2   A Tensor of type float32. l2 regularization factor on leaf weights, per instance based.  \n  tree_complexity   A Tensor of type float32. adjustment to the gain, per leaf based.  \n  min_node_weight   A Tensor of type float32. minimum avg of hessians in a node before required for the node to be considered for splitting.  \n  logits_dimension   An int that is >= 1. The dimension of logit, i.e., number of classes.  \n  split_type   An optional string from: \"inequality\", \"equality\". Defaults to \"inequality\". A string indicating if this Op should perform inequality split or equality split.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A tuple of Tensor objects (node_ids, gains, feature_dimensions, thresholds, left_node_contribs, right_node_contribs, split_with_default_directions).     node_ids   A Tensor of type int32.  \n  gains   A Tensor of type float32.  \n  feature_dimensions   A Tensor of type int32.  \n  thresholds   A Tensor of type int32.  \n  left_node_contribs   A Tensor of type float32.  \n  right_node_contribs   A Tensor of type float32.  \n  split_with_default_directions   A Tensor of type string.","title":"tensorflow.raw_ops.boostedtreescalculatebestfeaturesplit"},{"text":"tf.raw_ops.BoostedTreesCalculateBestFeatureSplitV2 Calculates gains for each feature and returns the best possible split information for each node. However, if no split is found, then no split information is returned for that node.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.raw_ops.BoostedTreesCalculateBestFeatureSplitV2  \ntf.raw_ops.BoostedTreesCalculateBestFeatureSplitV2(\n    node_id_range, stats_summaries_list, split_types, candidate_feature_ids, l1, l2,\n    tree_complexity, min_node_weight, logits_dimension, name=None\n)\n The split information is the best threshold (bucket id), gains and left\/right node contributions per node for each feature. It is possible that not all nodes can be split on each feature. Hence, the list of possible nodes can differ between the features. Therefore, we return node_ids_list for each feature, containing the list of nodes that this feature can be used to split. In this manner, the output is the best split per features and per node, so that it needs to be combined later to produce the best split for each node (among all possible features). The output shapes are compatible in a way that the first dimension of all tensors are the same and equal to the number of possible split nodes for each feature.\n \n\n\n Args\n  node_id_range   A Tensor of type int32. A Rank 1 tensor (shape=[2]) to specify the range [first, last) of node ids to process within stats_summary_list. The nodes are iterated between the two nodes specified by the tensor, as like for node_id in range(node_id_range[0], node_id_range[1]) (Note that the last index node_id_range[1] is exclusive).  \n  stats_summaries_list   A list of at least 1 Tensor objects with type float32. A list of Rank 4 tensor (#shape=[max_splits, feature_dims, bucket, stats_dims]) for accumulated stats summary (gradient\/hessian) per node, per dimension, per buckets for each feature. The first dimension of the tensor is the maximum number of splits, and thus not all elements of it will be used, but only the indexes specified by node_ids will be used.  \n  split_types   A Tensor of type string. A Rank 1 tensor indicating if this Op should perform inequality split or equality split per feature.  \n  candidate_feature_ids   A Tensor of type int32. Rank 1 tensor with ids for each feature. This is the real id of the feature.  \n  l1   A Tensor of type float32. l1 regularization factor on leaf weights, per instance based.  \n  l2   A Tensor of type float32. l2 regularization factor on leaf weights, per instance based.  \n  tree_complexity   A Tensor of type float32. adjustment to the gain, per leaf based.  \n  min_node_weight   A Tensor of type float32. minimum avg of hessians in a node before required for the node to be considered for splitting.  \n  logits_dimension   An int that is >= 1. The dimension of logit, i.e., number of classes.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A tuple of Tensor objects (node_ids, gains, feature_ids, feature_dimensions, thresholds, left_node_contribs, right_node_contribs, split_with_default_directions).     node_ids   A Tensor of type int32.  \n  gains   A Tensor of type float32.  \n  feature_ids   A Tensor of type int32.  \n  feature_dimensions   A Tensor of type int32.  \n  thresholds   A Tensor of type int32.  \n  left_node_contribs   A Tensor of type float32.  \n  right_node_contribs   A Tensor of type float32.  \n  split_with_default_directions   A Tensor of type string.","title":"tensorflow.raw_ops.boostedtreescalculatebestfeaturesplitv2"},{"text":"pandas.read_csv   pandas.read_csv(filepath_or_buffer, sep=NoDefault.no_default, delimiter=None, header='infer', names=NoDefault.no_default, index_col=None, usecols=None, squeeze=None, prefix=NoDefault.no_default, mangle_dupe_cols=True, dtype=None, engine=None, converters=None, true_values=None, false_values=None, skipinitialspace=False, skiprows=None, skipfooter=0, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=False, skip_blank_lines=True, parse_dates=None, infer_datetime_format=False, keep_date_col=False, date_parser=None, dayfirst=False, cache_dates=True, iterator=False, chunksize=None, compression='infer', thousands=None, decimal='.', lineterminator=None, quotechar='\"', quoting=0, doublequote=True, escapechar=None, comment=None, encoding=None, encoding_errors='strict', dialect=None, error_bad_lines=None, warn_bad_lines=None, on_bad_lines=None, delim_whitespace=False, low_memory=True, memory_map=False, float_precision=None, storage_options=None)[source]\n \nRead a comma-separated values (csv) file into DataFrame. Also supports optionally iterating or breaking of the file into chunks. Additional help can be found in the online docs for IO Tools.  Parameters \n \nfilepath_or_buffer:str, path object or file-like object\n\n\nAny valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is expected. A local file could be: file:\/\/localhost\/path\/to\/table.csv. If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handle (e.g. via builtin open function) or StringIO.  \nsep:str, default \u2018,\u2019\n\n\nDelimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python\u2019s builtin sniffer tool, csv.Sniffer. In addition, separators longer than 1 character and different from '\\s+' will be interpreted as regular expressions and will also force the use of the Python parsing engine. Note that regex delimiters are prone to ignoring quoted data. Regex example: '\\r\\t'.  \ndelimiter:str, default None\n\n\nAlias for sep.  \nheader:int, list of int, None, default \u2018infer\u2019\n\n\nRow number(s) to use as the column names, and the start of the data. Default behavior is to infer the column names: if no names are passed the behavior is identical to header=0 and column names are inferred from the first line of the file, if column names are passed explicitly then the behavior is identical to header=None. Explicitly pass header=0 to be able to replace existing names. The header can be a list of integers that specify row locations for a multi-index on the columns e.g. [0,1,3]. Intervening rows that are not specified will be skipped (e.g. 2 in this example is skipped). Note that this parameter ignores commented lines and empty lines if skip_blank_lines=True, so header=0 denotes the first line of data rather than the first line of the file.  \nnames:array-like, optional\n\n\nList of column names to use. If the file contains a header row, then you should explicitly pass header=0 to override the column names. Duplicates in this list are not allowed.  \nindex_col:int, str, sequence of int \/ str, or False, optional, default None\n\n\nColumn(s) to use as the row labels of the DataFrame, either given as string name or column index. If a sequence of int \/ str is given, a MultiIndex is used. Note: index_col=False can be used to force pandas to not use the first column as the index, e.g. when you have a malformed file with delimiters at the end of each line.  \nusecols:list-like or callable, optional\n\n\nReturn a subset of the columns. If list-like, all elements must either be positional (i.e. integer indices into the document columns) or strings that correspond to column names provided either by the user in names or inferred from the document header row(s). If names are given, the document header row(s) are not taken into account. For example, a valid list-like usecols parameter would be [0, 1, 2] or ['foo', 'bar', 'baz']. Element order is ignored, so usecols=[0, 1] is the same as [1, 0]. To instantiate a DataFrame from data with element order preserved use pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']] for columns in ['foo', 'bar'] order or pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']] for ['bar', 'foo'] order. If callable, the callable function will be evaluated against the column names, returning names where the callable function evaluates to True. An example of a valid callable argument would be lambda x: x.upper() in\n['AAA', 'BBB', 'DDD']. Using this parameter results in much faster parsing time and lower memory usage.  \nsqueeze:bool, default False\n\n\nIf the parsed data only contains one column then return a Series.  Deprecated since version 1.4.0: Append .squeeze(\"columns\") to the call to read_csv to squeeze the data.   \nprefix:str, optional\n\n\nPrefix to add to column numbers when no header, e.g. \u2018X\u2019 for X0, X1, \u2026  Deprecated since version 1.4.0: Use a list comprehension on the DataFrame\u2019s columns after calling read_csv.   \nmangle_dupe_cols:bool, default True\n\n\nDuplicate columns will be specified as \u2018X\u2019, \u2018X.1\u2019, \u2026\u2019X.N\u2019, rather than \u2018X\u2019\u2026\u2019X\u2019. Passing in False will cause data to be overwritten if there are duplicate names in the columns.  \ndtype:Type name or dict of column -> type, optional\n\n\nData type for data or columns. E.g. {\u2018a\u2019: np.float64, \u2018b\u2019: np.int32, \u2018c\u2019: \u2018Int64\u2019} Use str or object together with suitable na_values settings to preserve and not interpret dtype. If converters are specified, they will be applied INSTEAD of dtype conversion.  \nengine:{\u2018c\u2019, \u2018python\u2019, \u2018pyarrow\u2019}, optional\n\n\nParser engine to use. The C and pyarrow engines are faster, while the python engine is currently more feature-complete. Multithreading is currently only supported by the pyarrow engine.  New in version 1.4.0: The \u201cpyarrow\u201d engine was added as an experimental engine, and some features are unsupported, or may not work correctly, with this engine.   \nconverters:dict, optional\n\n\nDict of functions for converting values in certain columns. Keys can either be integers or column labels.  \ntrue_values:list, optional\n\n\nValues to consider as True.  \nfalse_values:list, optional\n\n\nValues to consider as False.  \nskipinitialspace:bool, default False\n\n\nSkip spaces after delimiter.  \nskiprows:list-like, int or callable, optional\n\n\nLine numbers to skip (0-indexed) or number of lines to skip (int) at the start of the file. If callable, the callable function will be evaluated against the row indices, returning True if the row should be skipped and False otherwise. An example of a valid callable argument would be lambda x: x in [0, 2].  \nskipfooter:int, default 0\n\n\nNumber of lines at bottom of file to skip (Unsupported with engine=\u2019c\u2019).  \nnrows:int, optional\n\n\nNumber of rows of file to read. Useful for reading pieces of large files.  \nna_values:scalar, str, list-like, or dict, optional\n\n\nAdditional strings to recognize as NA\/NaN. If dict passed, specific per-column NA values. By default the following values are interpreted as NaN: \u2018\u2019, \u2018#N\/A\u2019, \u2018#N\/A N\/A\u2019, \u2018#NA\u2019, \u2018-1.#IND\u2019, \u2018-1.#QNAN\u2019, \u2018-NaN\u2019, \u2018-nan\u2019, \u20181.#IND\u2019, \u20181.#QNAN\u2019, \u2018<NA>\u2019, \u2018N\/A\u2019, \u2018NA\u2019, \u2018NULL\u2019, \u2018NaN\u2019, \u2018n\/a\u2019, \u2018nan\u2019, \u2018null\u2019.  \nkeep_default_na:bool, default True\n\n\nWhether or not to include the default NaN values when parsing the data. Depending on whether na_values is passed in, the behavior is as follows:  If keep_default_na is True, and na_values are specified, na_values is appended to the default NaN values used for parsing. If keep_default_na is True, and na_values are not specified, only the default NaN values are used for parsing. If keep_default_na is False, and na_values are specified, only the NaN values specified na_values are used for parsing. If keep_default_na is False, and na_values are not specified, no strings will be parsed as NaN.  Note that if na_filter is passed in as False, the keep_default_na and na_values parameters will be ignored.  \nna_filter:bool, default True\n\n\nDetect missing value markers (empty strings and the value of na_values). In data without any NAs, passing na_filter=False can improve the performance of reading a large file.  \nverbose:bool, default False\n\n\nIndicate number of NA values placed in non-numeric columns.  \nskip_blank_lines:bool, default True\n\n\nIf True, skip over blank lines rather than interpreting as NaN values.  \nparse_dates:bool or list of int or names or list of lists or dict, default False\n\n\nThe behavior is as follows:  boolean. If True -> try parsing the index. list of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3 each as a separate date column. list of lists. e.g. If [[1, 3]] -> combine columns 1 and 3 and parse as a single date column. dict, e.g. {\u2018foo\u2019 : [1, 3]} -> parse columns 1, 3 as date and call result \u2018foo\u2019  If a column or index cannot be represented as an array of datetimes, say because of an unparsable value or a mixture of timezones, the column or index will be returned unaltered as an object data type. For non-standard datetime parsing, use pd.to_datetime after pd.read_csv. To parse an index or column with a mixture of timezones, specify date_parser to be a partially-applied pandas.to_datetime() with utc=True. See Parsing a CSV with mixed timezones for more. Note: A fast-path exists for iso8601-formatted dates.  \ninfer_datetime_format:bool, default False\n\n\nIf True and parse_dates is enabled, pandas will attempt to infer the format of the datetime strings in the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the parsing speed by 5-10x.  \nkeep_date_col:bool, default False\n\n\nIf True and parse_dates specifies combining multiple columns then keep the original columns.  \ndate_parser:function, optional\n\n\nFunction to use for converting a sequence of string columns to an array of datetime instances. The default uses dateutil.parser.parser to do the conversion. Pandas will try to call date_parser in three different ways, advancing to the next if an exception occurs: 1) Pass one or more arrays (as defined by parse_dates) as arguments; 2) concatenate (row-wise) the string values from the columns defined by parse_dates into a single array and pass that; and 3) call date_parser once for each row using one or more strings (corresponding to the columns defined by parse_dates) as arguments.  \ndayfirst:bool, default False\n\n\nDD\/MM format dates, international and European format.  \ncache_dates:bool, default True\n\n\nIf True, use a cache of unique, converted dates to apply the datetime conversion. May produce significant speed-up when parsing duplicate date strings, especially ones with timezone offsets.  New in version 0.25.0.   \niterator:bool, default False\n\n\nReturn TextFileReader object for iteration or getting chunks with get_chunk().  Changed in version 1.2: TextFileReader is a context manager.   \nchunksize:int, optional\n\n\nReturn TextFileReader object for iteration. See the IO Tools docs for more information on iterator and chunksize.  Changed in version 1.2: TextFileReader is a context manager.   \ncompression:str or dict, default \u2018infer\u2019\n\n\nFor on-the-fly decompression of on-disk data. If \u2018infer\u2019 and \u2018%s\u2019 is path-like, then detect compression from the following extensions: \u2018.gz\u2019, \u2018.bz2\u2019, \u2018.zip\u2019, \u2018.xz\u2019, or \u2018.zst\u2019 (otherwise no compression). If using \u2018zip\u2019, the ZIP file must contain only one data file to be read in. Set to None for no decompression. Can also be a dict with key 'method' set to one of {'zip', 'gzip', 'bz2', 'zstd'} and other key-value pairs are forwarded to zipfile.ZipFile, gzip.GzipFile, bz2.BZ2File, or zstandard.ZstdDecompressor, respectively. As an example, the following could be passed for Zstandard decompression using a custom compression dictionary: compression={'method': 'zstd', 'dict_data': my_compression_dict}.  Changed in version 1.4.0: Zstandard support.   \nthousands:str, optional\n\n\nThousands separator.  \ndecimal:str, default \u2018.\u2019\n\n\nCharacter to recognize as decimal point (e.g. use \u2018,\u2019 for European data).  \nlineterminator:str (length 1), optional\n\n\nCharacter to break file into lines. Only valid with C parser.  \nquotechar:str (length 1), optional\n\n\nThe character used to denote the start and end of a quoted item. Quoted items can include the delimiter and it will be ignored.  \nquoting:int or csv.QUOTE_* instance, default 0\n\n\nControl field quoting behavior per csv.QUOTE_* constants. Use one of QUOTE_MINIMAL (0), QUOTE_ALL (1), QUOTE_NONNUMERIC (2) or QUOTE_NONE (3).  \ndoublequote:bool, default True\n\n\nWhen quotechar is specified and quoting is not QUOTE_NONE, indicate whether or not to interpret two consecutive quotechar elements INSIDE a field as a single quotechar element.  \nescapechar:str (length 1), optional\n\n\nOne-character string used to escape other characters.  \ncomment:str, optional\n\n\nIndicates remainder of line should not be parsed. If found at the beginning of a line, the line will be ignored altogether. This parameter must be a single character. Like empty lines (as long as skip_blank_lines=True), fully commented lines are ignored by the parameter header but not by skiprows. For example, if comment='#', parsing #empty\\na,b,c\\n1,2,3 with header=0 will result in \u2018a,b,c\u2019 being treated as the header.  \nencoding:str, optional\n\n\nEncoding to use for UTF when reading\/writing (ex. \u2018utf-8\u2019). List of Python standard encodings .  Changed in version 1.2: When encoding is None, errors=\"replace\" is passed to open(). Otherwise, errors=\"strict\" is passed to open(). This behavior was previously only the case for engine=\"python\".   Changed in version 1.3.0: encoding_errors is a new argument. encoding has no longer an influence on how encoding errors are handled.   \nencoding_errors:str, optional, default \u201cstrict\u201d\n\n\nHow encoding errors are treated. List of possible values .  New in version 1.3.0.   \ndialect:str or csv.Dialect, optional\n\n\nIf provided, this parameter will override values (default or not) for the following parameters: delimiter, doublequote, escapechar, skipinitialspace, quotechar, and quoting. If it is necessary to override values, a ParserWarning will be issued. See csv.Dialect documentation for more details.  \nerror_bad_lines:bool, optional, default None\n\n\nLines with too many fields (e.g. a csv line with too many commas) will by default cause an exception to be raised, and no DataFrame will be returned. If False, then these \u201cbad lines\u201d will be dropped from the DataFrame that is returned.  Deprecated since version 1.3.0: The on_bad_lines parameter should be used instead to specify behavior upon encountering a bad line instead.   \nwarn_bad_lines:bool, optional, default None\n\n\nIf error_bad_lines is False, and warn_bad_lines is True, a warning for each \u201cbad line\u201d will be output.  Deprecated since version 1.3.0: The on_bad_lines parameter should be used instead to specify behavior upon encountering a bad line instead.   \non_bad_lines:{\u2018error\u2019, \u2018warn\u2019, \u2018skip\u2019} or callable, default \u2018error\u2019\n\n\nSpecifies what to do upon encountering a bad line (a line with too many fields). Allowed values are :  \n \u2018error\u2019, raise an Exception when a bad line is encountered. \u2018warn\u2019, raise a warning when a bad line is encountered and skip that line. \u2018skip\u2019, skip bad lines without raising or warning when they are encountered.  \n  New in version 1.3.0:   callable, function with signature (bad_line: list[str]) -> list[str] | None that will process a single bad line. bad_line is a list of strings split by the sep. If the function returns None`, the bad line will be ignored.\nIf the function returns a new list of strings with more elements than\nexpected, a ``ParserWarning will be emitted while dropping extra elements. Only supported when engine=\"python\"    New in version 1.4.0.   \ndelim_whitespace:bool, default False\n\n\nSpecifies whether or not whitespace (e.g. ' ' or '\u00a0\u00a0\u00a0 ') will be used as the sep. Equivalent to setting sep='\\s+'. If this option is set to True, nothing should be passed in for the delimiter parameter.  \nlow_memory:bool, default True\n\n\nInternally process the file in chunks, resulting in lower memory use while parsing, but possibly mixed type inference. To ensure no mixed types either set False, or specify the type with the dtype parameter. Note that the entire file is read into a single DataFrame regardless, use the chunksize or iterator parameter to return the data in chunks. (Only valid with C parser).  \nmemory_map:bool, default False\n\n\nIf a filepath is provided for filepath_or_buffer, map the file object directly onto memory and access the data directly from there. Using this option can improve performance because there is no longer any I\/O overhead.  \nfloat_precision:str, optional\n\n\nSpecifies which converter the C engine should use for floating-point values. The options are None or \u2018high\u2019 for the ordinary converter, \u2018legacy\u2019 for the original lower precision pandas converter, and \u2018round_trip\u2019 for the round-trip converter.  Changed in version 1.2.   \nstorage_options:dict, optional\n\n\nExtra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib as header options. For other URLs (e.g. starting with \u201cs3:\/\/\u201d, and \u201cgcs:\/\/\u201d) the key-value pairs are forwarded to fsspec. Please see fsspec and urllib for more details.  New in version 1.2.     Returns \n DataFrame or TextParser\n\nA comma-separated values (csv) file is returned as two-dimensional data structure with labeled axes.      See also  DataFrame.to_csv\n\nWrite DataFrame to a comma-separated values (csv) file.  read_csv\n\nRead a comma-separated values (csv) file into DataFrame.  read_fwf\n\nRead a table of fixed-width formatted lines into DataFrame.    Examples \n>>> pd.read_csv('data.csv')","title":"pandas.reference.api.pandas.read_csv"},{"text":"pandas.merge_asof   pandas.merge_asof(left, right, on=None, left_on=None, right_on=None, left_index=False, right_index=False, by=None, left_by=None, right_by=None, suffixes=('_x', '_y'), tolerance=None, allow_exact_matches=True, direction='backward')[source]\n \nPerform a merge by key distance. This is similar to a left-join except that we match on nearest key rather than equal keys. Both DataFrames must be sorted by the key. For each row in the left DataFrame:  \n A \u201cbackward\u201d search selects the last row in the right DataFrame whose \u2018on\u2019 key is less than or equal to the left\u2019s key. A \u201cforward\u201d search selects the first row in the right DataFrame whose \u2018on\u2019 key is greater than or equal to the left\u2019s key. A \u201cnearest\u201d search selects the row in the right DataFrame whose \u2018on\u2019 key is closest in absolute distance to the left\u2019s key.  \n The default is \u201cbackward\u201d and is compatible in versions below 0.20.0. The direction parameter was added in version 0.20.0 and introduces \u201cforward\u201d and \u201cnearest\u201d. Optionally match on equivalent keys with \u2018by\u2019 before searching with \u2018on\u2019.  Parameters \n \nleft:DataFrame or named Series\n\n\nright:DataFrame or named Series\n\n\non:label\n\n\nField name to join on. Must be found in both DataFrames. The data MUST be ordered. Furthermore this must be a numeric column, such as datetimelike, integer, or float. On or left_on\/right_on must be given.  \nleft_on:label\n\n\nField name to join on in left DataFrame.  \nright_on:label\n\n\nField name to join on in right DataFrame.  \nleft_index:bool\n\n\nUse the index of the left DataFrame as the join key.  \nright_index:bool\n\n\nUse the index of the right DataFrame as the join key.  \nby:column name or list of column names\n\n\nMatch on these columns before performing merge operation.  \nleft_by:column name\n\n\nField names to match on in the left DataFrame.  \nright_by:column name\n\n\nField names to match on in the right DataFrame.  \nsuffixes:2-length sequence (tuple, list, \u2026)\n\n\nSuffix to apply to overlapping column names in the left and right side, respectively.  \ntolerance:int or Timedelta, optional, default None\n\n\nSelect asof tolerance within this range; must be compatible with the merge index.  \nallow_exact_matches:bool, default True\n\n\n If True, allow matching with the same \u2018on\u2019 value (i.e. less-than-or-equal-to \/ greater-than-or-equal-to) If False, don\u2019t match the same \u2018on\u2019 value (i.e., strictly less-than \/ strictly greater-than).   \ndirection:\u2018backward\u2019 (default), \u2018forward\u2019, or \u2018nearest\u2019\n\n\nWhether to search for prior, subsequent, or closest matches.    Returns \n \nmerged:DataFrame\n\n    See also  merge\n\nMerge with a database-style join.  merge_ordered\n\nMerge with optional filling\/interpolation.    Examples \n>>> left = pd.DataFrame({\"a\": [1, 5, 10], \"left_val\": [\"a\", \"b\", \"c\"]})\n>>> left\n    a left_val\n0   1        a\n1   5        b\n2  10        c\n  \n>>> right = pd.DataFrame({\"a\": [1, 2, 3, 6, 7], \"right_val\": [1, 2, 3, 6, 7]})\n>>> right\n   a  right_val\n0  1          1\n1  2          2\n2  3          3\n3  6          6\n4  7          7\n  \n>>> pd.merge_asof(left, right, on=\"a\")\n    a left_val  right_val\n0   1        a          1\n1   5        b          3\n2  10        c          7\n  \n>>> pd.merge_asof(left, right, on=\"a\", allow_exact_matches=False)\n    a left_val  right_val\n0   1        a        NaN\n1   5        b        3.0\n2  10        c        7.0\n  \n>>> pd.merge_asof(left, right, on=\"a\", direction=\"forward\")\n    a left_val  right_val\n0   1        a        1.0\n1   5        b        6.0\n2  10        c        NaN\n  \n>>> pd.merge_asof(left, right, on=\"a\", direction=\"nearest\")\n    a left_val  right_val\n0   1        a          1\n1   5        b          6\n2  10        c          7\n  We can use indexed DataFrames as well. \n>>> left = pd.DataFrame({\"left_val\": [\"a\", \"b\", \"c\"]}, index=[1, 5, 10])\n>>> left\n   left_val\n1         a\n5         b\n10        c\n  \n>>> right = pd.DataFrame({\"right_val\": [1, 2, 3, 6, 7]}, index=[1, 2, 3, 6, 7])\n>>> right\n   right_val\n1          1\n2          2\n3          3\n6          6\n7          7\n  \n>>> pd.merge_asof(left, right, left_index=True, right_index=True)\n   left_val  right_val\n1         a          1\n5         b          3\n10        c          7\n  Here is a real-world times-series example \n>>> quotes = pd.DataFrame(\n...     {\n...         \"time\": [\n...             pd.Timestamp(\"2016-05-25 13:30:00.023\"),\n...             pd.Timestamp(\"2016-05-25 13:30:00.023\"),\n...             pd.Timestamp(\"2016-05-25 13:30:00.030\"),\n...             pd.Timestamp(\"2016-05-25 13:30:00.041\"),\n...             pd.Timestamp(\"2016-05-25 13:30:00.048\"),\n...             pd.Timestamp(\"2016-05-25 13:30:00.049\"),\n...             pd.Timestamp(\"2016-05-25 13:30:00.072\"),\n...             pd.Timestamp(\"2016-05-25 13:30:00.075\")\n...         ],\n...         \"ticker\": [\n...                \"GOOG\",\n...                \"MSFT\",\n...                \"MSFT\",\n...                \"MSFT\",\n...                \"GOOG\",\n...                \"AAPL\",\n...                \"GOOG\",\n...                \"MSFT\"\n...            ],\n...            \"bid\": [720.50, 51.95, 51.97, 51.99, 720.50, 97.99, 720.50, 52.01],\n...            \"ask\": [720.93, 51.96, 51.98, 52.00, 720.93, 98.01, 720.88, 52.03]\n...     }\n... )\n>>> quotes\n                     time ticker     bid     ask\n0 2016-05-25 13:30:00.023   GOOG  720.50  720.93\n1 2016-05-25 13:30:00.023   MSFT   51.95   51.96\n2 2016-05-25 13:30:00.030   MSFT   51.97   51.98\n3 2016-05-25 13:30:00.041   MSFT   51.99   52.00\n4 2016-05-25 13:30:00.048   GOOG  720.50  720.93\n5 2016-05-25 13:30:00.049   AAPL   97.99   98.01\n6 2016-05-25 13:30:00.072   GOOG  720.50  720.88\n7 2016-05-25 13:30:00.075   MSFT   52.01   52.03\n  \n>>> trades = pd.DataFrame(\n...        {\n...            \"time\": [\n...                pd.Timestamp(\"2016-05-25 13:30:00.023\"),\n...                pd.Timestamp(\"2016-05-25 13:30:00.038\"),\n...                pd.Timestamp(\"2016-05-25 13:30:00.048\"),\n...                pd.Timestamp(\"2016-05-25 13:30:00.048\"),\n...                pd.Timestamp(\"2016-05-25 13:30:00.048\")\n...            ],\n...            \"ticker\": [\"MSFT\", \"MSFT\", \"GOOG\", \"GOOG\", \"AAPL\"],\n...            \"price\": [51.95, 51.95, 720.77, 720.92, 98.0],\n...            \"quantity\": [75, 155, 100, 100, 100]\n...        }\n...    )\n>>> trades\n                     time ticker   price  quantity\n0 2016-05-25 13:30:00.023   MSFT   51.95        75\n1 2016-05-25 13:30:00.038   MSFT   51.95       155\n2 2016-05-25 13:30:00.048   GOOG  720.77       100\n3 2016-05-25 13:30:00.048   GOOG  720.92       100\n4 2016-05-25 13:30:00.048   AAPL   98.00       100\n  By default we are taking the asof of the quotes \n>>> pd.merge_asof(trades, quotes, on=\"time\", by=\"ticker\")\n                     time ticker   price  quantity     bid     ask\n0 2016-05-25 13:30:00.023   MSFT   51.95        75   51.95   51.96\n1 2016-05-25 13:30:00.038   MSFT   51.95       155   51.97   51.98\n2 2016-05-25 13:30:00.048   GOOG  720.77       100  720.50  720.93\n3 2016-05-25 13:30:00.048   GOOG  720.92       100  720.50  720.93\n4 2016-05-25 13:30:00.048   AAPL   98.00       100     NaN     NaN\n  We only asof within 2ms between the quote time and the trade time \n>>> pd.merge_asof(\n...     trades, quotes, on=\"time\", by=\"ticker\", tolerance=pd.Timedelta(\"2ms\")\n... )\n                     time ticker   price  quantity     bid     ask\n0 2016-05-25 13:30:00.023   MSFT   51.95        75   51.95   51.96\n1 2016-05-25 13:30:00.038   MSFT   51.95       155     NaN     NaN\n2 2016-05-25 13:30:00.048   GOOG  720.77       100  720.50  720.93\n3 2016-05-25 13:30:00.048   GOOG  720.92       100  720.50  720.93\n4 2016-05-25 13:30:00.048   AAPL   98.00       100     NaN     NaN\n  We only asof within 10ms between the quote time and the trade time and we exclude exact matches on time. However prior data will propagate forward \n>>> pd.merge_asof(\n...     trades,\n...     quotes,\n...     on=\"time\",\n...     by=\"ticker\",\n...     tolerance=pd.Timedelta(\"10ms\"),\n...     allow_exact_matches=False\n... )\n                     time ticker   price  quantity     bid     ask\n0 2016-05-25 13:30:00.023   MSFT   51.95        75     NaN     NaN\n1 2016-05-25 13:30:00.038   MSFT   51.95       155   51.97   51.98\n2 2016-05-25 13:30:00.048   GOOG  720.77       100     NaN     NaN\n3 2016-05-25 13:30:00.048   GOOG  720.92       100     NaN     NaN\n4 2016-05-25 13:30:00.048   AAPL   98.00       100     NaN     NaN","title":"pandas.reference.api.pandas.merge_asof"},{"text":"tf.feature_column.crossed_column     View source on GitHub    Returns a column for performing crosses of categorical features.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.feature_column.crossed_column  \ntf.feature_column.crossed_column(\n    keys, hash_bucket_size, hash_key=None\n)\n Crossed features will be hashed according to hash_bucket_size. Conceptually, the transformation can be thought of as: Hash(cartesian product of features) % hash_bucket_size For example, if the input features are:  \nSparseTensor referred by first key: shape = [2, 2]\n{\n    [0, 0]: \"a\"\n    [1, 0]: \"b\"\n    [1, 1]: \"c\"\n}\n\n \nSparseTensor referred by second key: shape = [2, 1]\n{\n    [0, 0]: \"d\"\n    [1, 0]: \"e\"\n}\n\n  then crossed feature will look like:  shape = [2, 2]\n{\n    [0, 0]: Hash64(\"d\", Hash64(\"a\")) % hash_bucket_size\n    [1, 0]: Hash64(\"e\", Hash64(\"b\")) % hash_bucket_size\n    [1, 1]: Hash64(\"e\", Hash64(\"c\")) % hash_bucket_size\n}\n Here is an example to create a linear model with crosses of string features: keywords_x_doc_terms = crossed_column(['keywords', 'doc_terms'], 50K)\ncolumns = [keywords_x_doc_terms, ...]\nfeatures = tf.io.parse_example(..., features=make_parse_example_spec(columns))\nlinear_prediction = linear_model(features, columns)\n You could also use vocabulary lookup before crossing: keywords = categorical_column_with_vocabulary_file(\n    'keywords', '\/path\/to\/vocabulary\/file', vocabulary_size=1K)\nkeywords_x_doc_terms = crossed_column([keywords, 'doc_terms'], 50K)\ncolumns = [keywords_x_doc_terms, ...]\nfeatures = tf.io.parse_example(..., features=make_parse_example_spec(columns))\nlinear_prediction = linear_model(features, columns)\n If an input feature is of numeric type, you can use categorical_column_with_identity, or bucketized_column, as in the example: # vertical_id is an integer categorical feature.\nvertical_id = categorical_column_with_identity('vertical_id', 10K)\nprice = numeric_column('price')\n# bucketized_column converts numerical feature to a categorical one.\nbucketized_price = bucketized_column(price, boundaries=[...])\nvertical_id_x_price = crossed_column([vertical_id, bucketized_price], 50K)\ncolumns = [vertical_id_x_price, ...]\nfeatures = tf.io.parse_example(..., features=make_parse_example_spec(columns))\nlinear_prediction = linear_model(features, columns)\n To use crossed column in DNN model, you need to add it in an embedding column as in this example: vertical_id_x_price = crossed_column([vertical_id, bucketized_price], 50K)\nvertical_id_x_price_embedded = embedding_column(vertical_id_x_price, 10)\ndense_tensor = input_layer(features, [vertical_id_x_price_embedded, ...])\n\n \n\n\n Args\n  keys   An iterable identifying the features to be crossed. Each element can be either:  string: Will use the corresponding feature which must be of string type. \nCategoricalColumn: Will use the transformed tensor produced by this column. Does not support hashed categorical column. \n\n \n  hash_bucket_size   An int > 1. The number of buckets.  \n  hash_key   Specify the hash_key that will be used by the FingerprintCat64 function to combine the crosses fingerprints on SparseCrossOp (optional).   \n \n\n\n Returns   A CrossedColumn.  \n\n \n\n\n Raises\n  ValueError   If len(keys) < 2.  \n  ValueError   If any of the keys is neither a string nor CategoricalColumn.  \n  ValueError   If any of the keys is HashedCategoricalColumn.  \n  ValueError   If hash_bucket_size < 1.","title":"tensorflow.feature_column.crossed_column"},{"text":"pandas.DataFrame.merge   DataFrame.merge(right, how='inner', on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=False, suffixes=('_x', '_y'), copy=True, indicator=False, validate=None)[source]\n \nMerge DataFrame or named Series objects with a database-style join. A named Series object is treated as a DataFrame with a single named column. The join is done on columns or indexes. If joining columns on columns, the DataFrame indexes will be ignored. Otherwise if joining indexes on indexes or indexes on a column or columns, the index will be passed on. When performing a cross merge, no column specifications to merge on are allowed.  Warning If both key columns contain rows where the key is a null value, those rows will be matched against each other. This is different from usual SQL join behaviour and can lead to unexpected results.   Parameters \n \nright:DataFrame or named Series\n\n\nObject to merge with.  \nhow:{\u2018left\u2019, \u2018right\u2019, \u2018outer\u2019, \u2018inner\u2019, \u2018cross\u2019}, default \u2018inner\u2019\n\n\nType of merge to be performed.  left: use only keys from left frame, similar to a SQL left outer join; preserve key order. right: use only keys from right frame, similar to a SQL right outer join; preserve key order. outer: use union of keys from both frames, similar to a SQL full outer join; sort keys lexicographically. inner: use intersection of keys from both frames, similar to a SQL inner join; preserve the order of the left keys. \ncross: creates the cartesian product from both frames, preserves the order of the left keys.  New in version 1.2.0.     \non:label or list\n\n\nColumn or index level names to join on. These must be found in both DataFrames. If on is None and not merging on indexes then this defaults to the intersection of the columns in both DataFrames.  \nleft_on:label or list, or array-like\n\n\nColumn or index level names to join on in the left DataFrame. Can also be an array or list of arrays of the length of the left DataFrame. These arrays are treated as if they are columns.  \nright_on:label or list, or array-like\n\n\nColumn or index level names to join on in the right DataFrame. Can also be an array or list of arrays of the length of the right DataFrame. These arrays are treated as if they are columns.  \nleft_index:bool, default False\n\n\nUse the index from the left DataFrame as the join key(s). If it is a MultiIndex, the number of keys in the other DataFrame (either the index or a number of columns) must match the number of levels.  \nright_index:bool, default False\n\n\nUse the index from the right DataFrame as the join key. Same caveats as left_index.  \nsort:bool, default False\n\n\nSort the join keys lexicographically in the result DataFrame. If False, the order of the join keys depends on the join type (how keyword).  \nsuffixes:list-like, default is (\u201c_x\u201d, \u201c_y\u201d)\n\n\nA length-2 sequence where each element is optionally a string indicating the suffix to add to overlapping column names in left and right respectively. Pass a value of None instead of a string to indicate that the column name from left or right should be left as-is, with no suffix. At least one of the values must not be None.  \ncopy:bool, default True\n\n\nIf False, avoid copy if possible.  \nindicator:bool or str, default False\n\n\nIf True, adds a column to the output DataFrame called \u201c_merge\u201d with information on the source of each row. The column can be given a different name by providing a string argument. The column will have a Categorical type with the value of \u201cleft_only\u201d for observations whose merge key only appears in the left DataFrame, \u201cright_only\u201d for observations whose merge key only appears in the right DataFrame, and \u201cboth\u201d if the observation\u2019s merge key is found in both DataFrames.  \nvalidate:str, optional\n\n\nIf specified, checks if merge is of specified type.  \u201cone_to_one\u201d or \u201c1:1\u201d: check if merge keys are unique in both left and right datasets. \u201cone_to_many\u201d or \u201c1:m\u201d: check if merge keys are unique in left dataset. \u201cmany_to_one\u201d or \u201cm:1\u201d: check if merge keys are unique in right dataset. \u201cmany_to_many\u201d or \u201cm:m\u201d: allowed, but does not result in checks.     Returns \n DataFrame\n\nA DataFrame of the two merged objects.      See also  merge_ordered\n\nMerge with optional filling\/interpolation.  merge_asof\n\nMerge on nearest keys.  DataFrame.join\n\nSimilar method using indices.    Notes Support for specifying index levels as the on, left_on, and right_on parameters was added in version 0.23.0 Support for merging named Series objects was added in version 0.24.0 Examples \n>>> df1 = pd.DataFrame({'lkey': ['foo', 'bar', 'baz', 'foo'],\n...                     'value': [1, 2, 3, 5]})\n>>> df2 = pd.DataFrame({'rkey': ['foo', 'bar', 'baz', 'foo'],\n...                     'value': [5, 6, 7, 8]})\n>>> df1\n    lkey value\n0   foo      1\n1   bar      2\n2   baz      3\n3   foo      5\n>>> df2\n    rkey value\n0   foo      5\n1   bar      6\n2   baz      7\n3   foo      8\n  Merge df1 and df2 on the lkey and rkey columns. The value columns have the default suffixes, _x and _y, appended. \n>>> df1.merge(df2, left_on='lkey', right_on='rkey')\n  lkey  value_x rkey  value_y\n0  foo        1  foo        5\n1  foo        1  foo        8\n2  foo        5  foo        5\n3  foo        5  foo        8\n4  bar        2  bar        6\n5  baz        3  baz        7\n  Merge DataFrames df1 and df2 with specified left and right suffixes appended to any overlapping columns. \n>>> df1.merge(df2, left_on='lkey', right_on='rkey',\n...           suffixes=('_left', '_right'))\n  lkey  value_left rkey  value_right\n0  foo           1  foo            5\n1  foo           1  foo            8\n2  foo           5  foo            5\n3  foo           5  foo            8\n4  bar           2  bar            6\n5  baz           3  baz            7\n  Merge DataFrames df1 and df2, but raise an exception if the DataFrames have any overlapping columns. \n>>> df1.merge(df2, left_on='lkey', right_on='rkey', suffixes=(False, False))\nTraceback (most recent call last):\n...\nValueError: columns overlap but no suffix specified:\n    Index(['value'], dtype='object')\n  \n>>> df1 = pd.DataFrame({'a': ['foo', 'bar'], 'b': [1, 2]})\n>>> df2 = pd.DataFrame({'a': ['foo', 'baz'], 'c': [3, 4]})\n>>> df1\n      a  b\n0   foo  1\n1   bar  2\n>>> df2\n      a  c\n0   foo  3\n1   baz  4\n  \n>>> df1.merge(df2, how='inner', on='a')\n      a  b  c\n0   foo  1  3\n  \n>>> df1.merge(df2, how='left', on='a')\n      a  b  c\n0   foo  1  3.0\n1   bar  2  NaN\n  \n>>> df1 = pd.DataFrame({'left': ['foo', 'bar']})\n>>> df2 = pd.DataFrame({'right': [7, 8]})\n>>> df1\n    left\n0   foo\n1   bar\n>>> df2\n    right\n0   7\n1   8\n  \n>>> df1.merge(df2, how='cross')\n   left  right\n0   foo      7\n1   foo      8\n2   bar      7\n3   bar      8","title":"pandas.reference.api.pandas.dataframe.merge"},{"text":"pandas.DataFrame.join   DataFrame.join(other, on=None, how='left', lsuffix='', rsuffix='', sort=False)[source]\n \nJoin columns of another DataFrame. Join columns with other DataFrame either on index or on a key column. Efficiently join multiple DataFrame objects by index at once by passing a list.  Parameters \n \nother:DataFrame, Series, or list of DataFrame\n\n\nIndex should be similar to one of the columns in this one. If a Series is passed, its name attribute must be set, and that will be used as the column name in the resulting joined DataFrame.  \non:str, list of str, or array-like, optional\n\n\nColumn or index level name(s) in the caller to join on the index in other, otherwise joins index-on-index. If multiple values given, the other DataFrame must have a MultiIndex. Can pass an array as the join key if it is not already contained in the calling DataFrame. Like an Excel VLOOKUP operation.  \nhow:{\u2018left\u2019, \u2018right\u2019, \u2018outer\u2019, \u2018inner\u2019}, default \u2018left\u2019\n\n\nHow to handle the operation of the two objects.  left: use calling frame\u2019s index (or column if on is specified) right: use other\u2019s index. outer: form union of calling frame\u2019s index (or column if on is specified) with other\u2019s index, and sort it. lexicographically. inner: form intersection of calling frame\u2019s index (or column if on is specified) with other\u2019s index, preserving the order of the calling\u2019s one. \ncross: creates the cartesian product from both frames, preserves the order of the left keys.  New in version 1.2.0.     \nlsuffix:str, default \u2018\u2019\n\n\nSuffix to use from left frame\u2019s overlapping columns.  \nrsuffix:str, default \u2018\u2019\n\n\nSuffix to use from right frame\u2019s overlapping columns.  \nsort:bool, default False\n\n\nOrder result DataFrame lexicographically by the join key. If False, the order of the join key depends on the join type (how keyword).    Returns \n DataFrame\n\nA dataframe containing columns from both the caller and other.      See also  DataFrame.merge\n\nFor column(s)-on-column(s) operations.    Notes Parameters on, lsuffix, and rsuffix are not supported when passing a list of DataFrame objects. Support for specifying index levels as the on parameter was added in version 0.23.0. Examples \n>>> df = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3', 'K4', 'K5'],\n...                    'A': ['A0', 'A1', 'A2', 'A3', 'A4', 'A5']})\n  \n>>> df\n  key   A\n0  K0  A0\n1  K1  A1\n2  K2  A2\n3  K3  A3\n4  K4  A4\n5  K5  A5\n  \n>>> other = pd.DataFrame({'key': ['K0', 'K1', 'K2'],\n...                       'B': ['B0', 'B1', 'B2']})\n  \n>>> other\n  key   B\n0  K0  B0\n1  K1  B1\n2  K2  B2\n  Join DataFrames using their indexes. \n>>> df.join(other, lsuffix='_caller', rsuffix='_other')\n  key_caller   A key_other    B\n0         K0  A0        K0   B0\n1         K1  A1        K1   B1\n2         K2  A2        K2   B2\n3         K3  A3       NaN  NaN\n4         K4  A4       NaN  NaN\n5         K5  A5       NaN  NaN\n  If we want to join using the key columns, we need to set key to be the index in both df and other. The joined DataFrame will have key as its index. \n>>> df.set_index('key').join(other.set_index('key'))\n      A    B\nkey\nK0   A0   B0\nK1   A1   B1\nK2   A2   B2\nK3   A3  NaN\nK4   A4  NaN\nK5   A5  NaN\n  Another option to join using the key columns is to use the on parameter. DataFrame.join always uses other\u2019s index but we can use any column in df. This method preserves the original DataFrame\u2019s index in the result. \n>>> df.join(other.set_index('key'), on='key')\n  key   A    B\n0  K0  A0   B0\n1  K1  A1   B1\n2  K2  A2   B2\n3  K3  A3  NaN\n4  K4  A4  NaN\n5  K5  A5  NaN\n  Using non-unique key values shows how they are matched. \n>>> df = pd.DataFrame({'key': ['K0', 'K1', 'K1', 'K3', 'K0', 'K1'],\n...                    'A': ['A0', 'A1', 'A2', 'A3', 'A4', 'A5']})\n  \n>>> df\n  key   A\n0  K0  A0\n1  K1  A1\n2  K1  A2\n3  K3  A3\n4  K0  A4\n5  K1  A5\n  \n>>> df.join(other.set_index('key'), on='key')\n  key   A    B\n0  K0  A0   B0\n1  K1  A1   B1\n2  K1  A2   B1\n3  K3  A3  NaN\n4  K0  A4   B0\n5  K1  A5   B1","title":"pandas.reference.api.pandas.dataframe.join"},{"text":"pandas.read_clipboard   pandas.read_clipboard(sep='\\\\s+', **kwargs)[source]\n \nRead text from clipboard and pass to read_csv.  Parameters \n \nsep:str, default \u2018s+\u2019\n\n\nA string or regex delimiter. The default of \u2018s+\u2019 denotes one or more whitespace characters.  **kwargs\n\nSee read_csv for the full argument list.    Returns \n DataFrame\n\nA parsed DataFrame object.","title":"pandas.reference.api.pandas.read_clipboard"},{"text":"pandas.errors.ParserError   exceptionpandas.errors.ParserError[source]\n \nException that is raised by an error encountered in parsing file contents. This is a generic error raised for errors encountered when functions like read_csv or read_html are parsing contents of a file.  See also  read_csv\n\nRead CSV (comma-separated) file into a DataFrame.  read_html\n\nRead HTML table into a DataFrame.","title":"pandas.reference.api.pandas.errors.parsererror"}]}
{"task_id":10697757,"prompt":"def f_10697757(s):\n\treturn ","suffix":"","canonical_solution":"s.split(' ', 4)","test_start":"\ndef check(candidate):","test":["\n    assert candidate('1 0 A10B 100 Description: This is a description with spaces') ==         ['1', '0', 'A10B', '100', 'Description: This is a description with spaces']\n","\n    assert candidate('this-is-a-continuous-sequence') == ['this-is-a-continuous-sequence']\n","\n    assert candidate('') == ['']\n","\n    assert candidate('\t') == ['\t']\n"],"entry_point":"f_10697757","intent":"Split a string `s` by space with `4` splits","library":[],"docs":[]}
{"task_id":16344756,"prompt":"def f_16344756(app):\n\treturn ","suffix":"","canonical_solution":"app.run(debug=True)","test_start":"\nfrom flask import Flask\nfrom unittest.mock import Mock\n\ndef check(candidate):","test":["\n    Flask = Mock()\n    app = Flask('mai')\n    try:\n        candidate(app)\n    except:\n        return False\n"],"entry_point":"f_16344756","intent":"enable debug mode on Flask application `app`","library":["flask"],"docs":[{"text":"DEBUG  \nWhether debug mode is enabled. When using flask run to start the development server, an interactive debugger will be shown for unhandled exceptions, and the server will be reloaded when code changes. The debug attribute maps to this config key. This is enabled when ENV is 'development' and is overridden by the FLASK_DEBUG environment variable. It may not behave as expected if set in code. Do not enable debug mode when deploying in production. Default: True if ENV is 'development', or False otherwise.","title":"flask.config.index#DEBUG"},{"text":"class werkzeug.debug.DebuggedApplication(app, evalex=False, request_key='werkzeug.request', console_path='\/console', console_init_func=None, show_hidden_frames=False, pin_security=True, pin_logging=True)  \nEnables debugging support for a given application: from werkzeug.debug import DebuggedApplication\nfrom myapp import app\napp = DebuggedApplication(app, evalex=True)\n The evalex keyword argument allows evaluating expressions in a traceback\u2019s frame context.  Parameters \n \napp (WSGIApplication) \u2013 the WSGI application to run debugged. \nevalex (bool) \u2013 enable exception evaluation feature (interactive debugging). This requires a non-forking server. \nrequest_key (str) \u2013 The key that points to the request object in ths environment. This parameter is ignored in current versions. \nconsole_path (str) \u2013 the URL for a general purpose console. \nconsole_init_func (Optional[Callable[[], Dict[str, Any]]]) \u2013 the function that is executed before starting the general purpose console. The return value is used as initial namespace. \nshow_hidden_frames (bool) \u2013 by default hidden traceback frames are skipped. You can show them by setting this parameter to True. \npin_security (bool) \u2013 can be used to disable the pin based security system. \npin_logging (bool) \u2013 enables the logging of the pin system.   Return type \nNone","title":"werkzeug.debug.index#werkzeug.debug.DebuggedApplication"},{"text":"debug()","title":"django.ref.templates.api#django.template.context_processors.debug"},{"text":"loop.set_debug(enabled: bool)  \nSet the debug mode of the event loop.  Changed in version 3.7: The new Python Development Mode can now also be used to enable the debug mode.","title":"python.library.asyncio-eventloop#asyncio.loop.set_debug"},{"text":"class werkzeug.middleware.lint.LintMiddleware(app)  \nWarns about common errors in the WSGI and HTTP behavior of the server and wrapped application. Some of the issues it checks are:  invalid status codes non-bytes sent to the WSGI server strings returned from the WSGI application non-empty conditional responses unquoted etags relative URLs in the Location header unsafe calls to wsgi.input unclosed iterators  Error information is emitted using the warnings module.  Parameters \napp (WSGIApplication) \u2013 The WSGI application to wrap.  Return type \nNone   from werkzeug.middleware.lint import LintMiddleware\napp = LintMiddleware(app)","title":"werkzeug.middleware.lint.index#werkzeug.middleware.lint.LintMiddleware"},{"text":"Logging Flask uses standard Python logging. Messages about your Flask application are logged with app.logger, which takes the same name as app.name. This logger can also be used to log your own messages. @app.route('\/login', methods=['POST'])\ndef login():\n    user = get_user(request.form['username'])\n\n    if user.check_password(request.form['password']):\n        login_user(user)\n        app.logger.info('%s logged in successfully', user.username)\n        return redirect(url_for('index'))\n    else:\n        app.logger.info('%s failed to log in', user.username)\n        abort(401)\n If you don\u2019t configure logging, Python\u2019s default log level is usually \u2018warning\u2019. Nothing below the configured level will be visible. Basic Configuration When you want to configure logging for your project, you should do it as soon as possible when the program starts. If app.logger is accessed before logging is configured, it will add a default handler. If possible, configure logging before creating the application object. This example uses dictConfig() to create a logging configuration similar to Flask\u2019s default, except for all logs: from logging.config import dictConfig\n\ndictConfig({\n    'version': 1,\n    'formatters': {'default': {\n        'format': '[%(asctime)s] %(levelname)s in %(module)s: %(message)s',\n    }},\n    'handlers': {'wsgi': {\n        'class': 'logging.StreamHandler',\n        'stream': 'ext:\/\/flask.logging.wsgi_errors_stream',\n        'formatter': 'default'\n    }},\n    'root': {\n        'level': 'INFO',\n        'handlers': ['wsgi']\n    }\n})\n\napp = Flask(__name__)\n Default Configuration If you do not configure logging yourself, Flask will add a StreamHandler to app.logger automatically. During requests, it will write to the stream specified by the WSGI server in environ['wsgi.errors'] (which is usually sys.stderr). Outside a request, it will log to sys.stderr. Removing the Default Handler If you configured logging after accessing app.logger, and need to remove the default handler, you can import and remove it: from flask.logging import default_handler\n\napp.logger.removeHandler(default_handler)\n Email Errors to Admins When running the application on a remote server for production, you probably won\u2019t be looking at the log messages very often. The WSGI server will probably send log messages to a file, and you\u2019ll only check that file if a user tells you something went wrong. To be proactive about discovering and fixing bugs, you can configure a logging.handlers.SMTPHandler to send an email when errors and higher are logged. import logging\nfrom logging.handlers import SMTPHandler\n\nmail_handler = SMTPHandler(\n    mailhost='127.0.0.1',\n    fromaddr='server-error@example.com',\n    toaddrs=['admin@example.com'],\n    subject='Application Error'\n)\nmail_handler.setLevel(logging.ERROR)\nmail_handler.setFormatter(logging.Formatter(\n    '[%(asctime)s] %(levelname)s in %(module)s: %(message)s'\n))\n\nif not app.debug:\n    app.logger.addHandler(mail_handler)\n This requires that you have an SMTP server set up on the same server. See the Python docs for more information about configuring the handler. Injecting Request Information Seeing more information about the request, such as the IP address, may help debugging some errors. You can subclass logging.Formatter to inject your own fields that can be used in messages. You can change the formatter for Flask\u2019s default handler, the mail handler defined above, or any other handler. from flask import has_request_context, request\nfrom flask.logging import default_handler\n\nclass RequestFormatter(logging.Formatter):\n    def format(self, record):\n        if has_request_context():\n            record.url = request.url\n            record.remote_addr = request.remote_addr\n        else:\n            record.url = None\n            record.remote_addr = None\n\n        return super().format(record)\n\nformatter = RequestFormatter(\n    '[%(asctime)s] %(remote_addr)s requested %(url)s\\n'\n    '%(levelname)s in %(module)s: %(message)s'\n)\ndefault_handler.setFormatter(formatter)\nmail_handler.setFormatter(formatter)\n Other Libraries Other libraries may use logging extensively, and you want to see relevant messages from those logs too. The simplest way to do this is to add handlers to the root logger instead of only the app logger. from flask.logging import default_handler\n\nroot = logging.getLogger()\nroot.addHandler(default_handler)\nroot.addHandler(mail_handler)\n Depending on your project, it may be more useful to configure each logger you care about separately, instead of configuring only the root logger. for logger in (\n    app.logger,\n    logging.getLogger('sqlalchemy'),\n    logging.getLogger('other_package'),\n):\n    logger.addHandler(default_handler)\n    logger.addHandler(mail_handler)\n Werkzeug Werkzeug logs basic request\/response information to the 'werkzeug' logger. If the root logger has no handlers configured, Werkzeug adds a StreamHandler to its logger. Flask Extensions Depending on the situation, an extension may choose to log to app.logger or its own named logger. Consult each extension\u2019s documentation for details.","title":"flask.logging.index"},{"text":"class RequireDebugFalse  \nThis filter will only pass on records when settings.DEBUG is False. This filter is used as follows in the default LOGGING configuration to ensure that the AdminEmailHandler only sends error emails to admins when DEBUG is False: 'filters': {\n    'require_debug_false': {\n        '()': 'django.utils.log.RequireDebugFalse',\n    },\n},\n'handlers': {\n    'mail_admins': {\n        'level': 'ERROR',\n        'filters': ['require_debug_false'],\n        'class': 'django.utils.log.AdminEmailHandler',\n    },\n},","title":"django.ref.logging#django.utils.log.RequireDebugFalse"},{"text":"Template.debug(flag)  \nIf flag is true, turn debugging on. Otherwise, turn debugging off. When debugging is on, commands to be executed are printed, and the shell is given set -x command to be more verbose.","title":"python.library.pipes#pipes.Template.debug"},{"text":"Extensions Extensions are extra packages that add functionality to a Flask application. For example, an extension might add support for sending email or connecting to a database. Some extensions add entire new frameworks to help build certain types of applications, like a REST API. Finding Extensions Flask extensions are usually named \u201cFlask-Foo\u201d or \u201cFoo-Flask\u201d. You can search PyPI for packages tagged with Framework :: Flask. Using Extensions Consult each extension\u2019s documentation for installation, configuration, and usage instructions. Generally, extensions pull their own configuration from app.config and are passed an application instance during initialization. For example, an extension called \u201cFlask-Foo\u201d might be used like this: from flask_foo import Foo\n\nfoo = Foo()\n\napp = Flask(__name__)\napp.config.update(\n    FOO_BAR='baz',\n    FOO_SPAM='eggs',\n)\n\nfoo.init_app(app)\n Building Extensions While the PyPI contains many Flask extensions, you may not find an extension that fits your need. If this is the case, you can create your own. Read Flask Extension Development to develop your own Flask extension.","title":"flask.extensions.index"},{"text":"make_middleware(app)  \nWrap a WSGI application so that cleaning up happens after request end.  Parameters \napp (WSGIApplication) \u2013   Return type \nWSGIApplication","title":"werkzeug.local.index#werkzeug.local.LocalManager.make_middleware"}]}
{"task_id":40133826,"prompt":"def f_40133826(mylist):\n\t","suffix":"\n\treturn ","canonical_solution":"pickle.dump(mylist, open('save.txt', 'wb'))","test_start":"\nimport pickle\n\ndef check(candidate):","test":["\n    candidate([i for i in range(10)])\n    data = pickle.load(open('save.txt', 'rb'))\n    assert data == [i for i in range(10)]\n","\n    candidate([\"hello\", \"world\", \"!\"])\n    data = pickle.load(open('save.txt', 'rb'))\n    assert data == [\"hello\", \"world\", \"!\"]\n"],"entry_point":"f_40133826","intent":"python save list `mylist` to file object 'save.txt'","library":["pickle"],"docs":[{"text":"numpy.savetxt   numpy.savetxt(fname, X, fmt='%.18e', delimiter=' ', newline='\\n', header='', footer='', comments='# ', encoding=None)[source]\n \nSave an array to a text file.  Parameters \n \nfnamefilename or file handle\n\n\nIf the filename ends in .gz, the file is automatically saved in compressed gzip format. loadtxt understands gzipped files transparently.  \nX1D or 2D array_like\n\n\nData to be saved to a text file.  \nfmtstr or sequence of strs, optional\n\n\nA single format (%10.5f), a sequence of formats, or a multi-format string, e.g. \u2018Iteration %d \u2013 %10.5f\u2019, in which case delimiter is ignored. For complex X, the legal options for fmt are:  a single specifier, fmt=\u2019%.4e\u2019, resulting in numbers formatted like \u2018 (%s+%sj)\u2019 % (fmt, fmt)\n a full string specifying every real and imaginary part, e.g. \u2018 %.4e %+.4ej %.4e %+.4ej %.4e %+.4ej\u2019 for 3 columns a list of specifiers, one per column - in this case, the real and imaginary part must have separate specifiers, e.g. [\u2018%.3e + %.3ej\u2019, \u2018(%.15e%+.15ej)\u2019] for 2 columns   \ndelimiterstr, optional\n\n\nString or character separating columns.  \nnewlinestr, optional\n\n\nString or character separating lines.  New in version 1.5.0.   \nheaderstr, optional\n\n\nString that will be written at the beginning of the file.  New in version 1.7.0.   \nfooterstr, optional\n\n\nString that will be written at the end of the file.  New in version 1.7.0.   \ncommentsstr, optional\n\n\nString that will be prepended to the header and footer strings, to mark them as comments. Default: \u2018# \u2018, as expected by e.g. numpy.loadtxt.  New in version 1.7.0.   \nencoding{None, str}, optional\n\n\nEncoding used to encode the outputfile. Does not apply to output streams. If the encoding is something other than \u2018bytes\u2019 or \u2018latin1\u2019 you will not be able to load the file in NumPy versions < 1.14. Default is \u2018latin1\u2019.  New in version 1.14.0.       See also  save\n\nSave an array to a binary file in NumPy .npy format  savez\n\nSave several arrays into an uncompressed .npz archive  savez_compressed\n\nSave several arrays into a compressed .npz archive    Notes Further explanation of the fmt parameter (%[flag]width[.precision]specifier):  flags:\n\n- : left justify + : Forces to precede result with + or -. 0 : Left pad the number with zeros instead of space (see width).  width:\n\nMinimum number of characters to be printed. The value is not truncated if it has more characters.  precision:\n\n For integer specifiers (eg. d,i,o,x), the minimum number of digits. For e, E and f specifiers, the number of digits to print after the decimal point. For g and G, the maximum number of significant digits. For s, the maximum number of characters.   specifiers:\n\nc : character d or i : signed decimal integer e or E : scientific notation with e or E. f : decimal floating point g,G : use the shorter of e,E or f o : signed octal s : string of characters u : unsigned decimal integer x,X : unsigned hexadecimal integer   This explanation of fmt is not complete, for an exhaustive specification see [1]. References  1 \nFormat Specification Mini-Language, Python Documentation.   Examples >>> x = y = z = np.arange(0.0,5.0,1.0)\n>>> np.savetxt('test.out', x, delimiter=',')   # X is an array\n>>> np.savetxt('test.out', (x,y,z))   # x,y,z equal sized 1D arrays\n>>> np.savetxt('test.out', x, fmt='%1.4e')   # use exponential notation","title":"numpy.reference.generated.numpy.savetxt"},{"text":"handleError(record)  \nThis method should be called from handlers when an exception is encountered during an emit() call. If the module-level attribute raiseExceptions is False, exceptions get silently ignored. This is what is mostly wanted for a logging system - most users will not care about errors in the logging system, they are more interested in application errors. You could, however, replace this with a custom handler if you wish. The specified record is the one which was being processed when the exception occurred. (The default value of raiseExceptions is True, as that is more useful during development).","title":"python.library.logging#logging.Handler.handleError"},{"text":"predict(X) [source]\n \nPredict the labels for the data samples in X using trained model.  Parameters \n \nXarray-like of shape (n_samples, n_features) \n\nList of n_features-dimensional data points. Each row corresponds to a single data point.    Returns \n \nlabelsarray, shape (n_samples,) \n\nComponent labels.","title":"sklearn.modules.generated.sklearn.mixture.bayesiangaussianmixture#sklearn.mixture.BayesianGaussianMixture.predict"},{"text":"curses.doupdate()  \nUpdate the physical screen. The curses library keeps two data structures, one representing the current physical screen contents and a virtual screen representing the desired next state. The doupdate() ground updates the physical screen to match the virtual screen. The virtual screen may be updated by a noutrefresh() call after write operations such as addstr() have been performed on a window. The normal refresh() call is simply noutrefresh() followed by doupdate(); if you have to update multiple windows, you can speed performance and perhaps reduce screen flicker by issuing noutrefresh() calls on all windows, followed by a single doupdate().","title":"python.library.curses#curses.doupdate"},{"text":"fit(X, y) [source]\n \nRun score function on (X, y) and get the appropriate features.  Parameters \n \nXarray-like of shape (n_samples, n_features) \n\nThe training input samples.  \nyarray-like of shape (n_samples,) \n\nThe target values (class labels in classification, real numbers in regression).    Returns \n \nselfobject","title":"sklearn.modules.generated.sklearn.feature_selection.selectkbest#sklearn.feature_selection.SelectKBest.fit"},{"text":"predict(X) [source]\n \nPredict the labels for the data samples in X using trained model.  Parameters \n \nXarray-like of shape (n_samples, n_features) \n\nList of n_features-dimensional data points. Each row corresponds to a single data point.    Returns \n \nlabelsarray, shape (n_samples,) \n\nComponent labels.","title":"sklearn.modules.generated.sklearn.mixture.gaussianmixture#sklearn.mixture.GaussianMixture.predict"},{"text":"transform(X) [source]\n \nReduce X to the selected features.  Parameters \n \nXarray of shape [n_samples, n_features] \n\nThe input samples.    Returns \n \nX_rarray of shape [n_samples, n_selected_features] \n\nThe input samples with only the selected features.","title":"sklearn.modules.generated.sklearn.feature_selection.selectfdr#sklearn.feature_selection.SelectFdr.transform"},{"text":"readline(size=-1)  \nRead until newline or EOF and return a single str. If the stream is already at EOF, an empty string is returned. If size is specified, at most size characters will be read.","title":"python.library.io#io.TextIOBase.readline"},{"text":"is_complex() \u2192 bool  \nReturns True if the data type of self is a complex data type.","title":"torch.tensors#torch.Tensor.is_complex"},{"text":"transform(X) [source]\n \nReduce X to the selected features.  Parameters \n \nXarray of shape [n_samples, n_features] \n\nThe input samples.    Returns \n \nX_rarray of shape [n_samples, n_selected_features] \n\nThe input samples with only the selected features.","title":"sklearn.modules.generated.sklearn.feature_selection.selectfpr#sklearn.feature_selection.SelectFpr.transform"}]}
{"task_id":4490961,"prompt":"def f_4490961(P, T):\n\treturn ","suffix":"","canonical_solution":"scipy.tensordot(P, T, axes=[1, 1]).swapaxes(0, 1)","test_start":"\nimport scipy\nimport numpy as np\n\ndef check(candidate):","test":["\n    P = np.array([[6, 2, 7], [1, 1, 8], [8, 7, 1], [9, 6, 4], [2, 1, 1]])\n    T = np.array([[[9, 7, 2, 3], [9, 6, 8, 2], [6, 6, 2, 8]],\n                  [[4, 5, 5, 3], [1, 8, 3, 5], [2, 8, 1, 6]]])\n    result = np.array([[[114,  96,  42,  78], [ 66,  61,  26,  69], [141, 104,  74,  46], [159, 123,  74,  71],  [ 33,  26,  14,  16]], \n                      [[ 40, 102,  43,  70], [ 21,  77,  16,  56], [ 41, 104,  62,  65], [ 50, 125,  67,  81], [ 11,  26,  14,  17]]])\n    assert np.array_equal(candidate(P, T), result)\n"],"entry_point":"f_4490961","intent":"Multiply a matrix `P` with a 3d tensor `T` in scipy","library":["numpy","scipy"],"docs":[]}
{"task_id":2173087,"prompt":"def f_2173087():\n\treturn ","suffix":"","canonical_solution":"numpy.zeros((3, 3, 3))","test_start":"\nimport numpy \nimport numpy as np\n\ndef check(candidate):","test":["\n    result = np.array([[[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]],\n                          [[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]],\n                          [[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]]])\n    assert np.array_equal(candidate(), result)\n"],"entry_point":"f_2173087","intent":"Create 3d array of zeroes of size `(3,3,3)`","library":["numpy"],"docs":[{"text":"tf.experimental.numpy.zeros TensorFlow variant of NumPy's zeros. \ntf.experimental.numpy.zeros(\n    shape, dtype=float\n)\n See the NumPy documentation for numpy.zeros.","title":"tensorflow.experimental.numpy.zeros"},{"text":"tf.zeros     View source on GitHub    Creates a tensor with all elements set to zero.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.zeros  \ntf.zeros(\n    shape, dtype=tf.dtypes.float32, name=None\n)\n See also tf.zeros_like, tf.ones, tf.fill, tf.eye. This operation returns a tensor of type dtype with shape shape and all elements set to zero. \ntf.zeros([3, 4], tf.int32)\n<tf.Tensor: shape=(3, 4), dtype=int32, numpy=\narray([[0, 0, 0, 0],\n       [0, 0, 0, 0],\n       [0, 0, 0, 0]], dtype=int32)>\n\n \n\n\n Args\n  shape   A list of integers, a tuple of integers, or a 1-D Tensor of type int32.  \n  dtype   The DType of an element in the resulting Tensor.  \n  name   Optional string. A name for the operation.   \n \n\n\n Returns   A Tensor with all elements set to zero.","title":"tensorflow.zeros"},{"text":"numpy.ma.zeros   ma.zeros(shape, dtype=float, order='C', *, like=None) = <numpy.ma.core._convert2ma object>\n \nReturn a new array of given shape and type, filled with zeros.  Parameters \n \nshapeint or tuple of ints\n\n\nShape of the new array, e.g., (2, 3) or 2.  \ndtypedata-type, optional\n\n\nThe desired data-type for the array, e.g., numpy.int8. Default is numpy.float64.  \norder{\u2018C\u2019, \u2018F\u2019}, optional, default: \u2018C\u2019\n\n\nWhether to store multi-dimensional data in row-major (C-style) or column-major (Fortran-style) order in memory.  \nlikearray_like\n\n\nReference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as like supports the __array_function__ protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.  New in version 1.20.0.     Returns \n \noutMaskedArray\n\n\nArray of zeros with the given shape, dtype, and order.      See also  zeros_like\n\nReturn an array of zeros with shape and type of input.  empty\n\nReturn a new uninitialized array.  ones\n\nReturn a new array setting values to one.  full\n\nReturn a new array of given shape filled with value.    Examples >>> np.zeros(5)\narray([ 0.,  0.,  0.,  0.,  0.])\n >>> np.zeros((5,), dtype=int)\narray([0, 0, 0, 0, 0])\n >>> np.zeros((2, 1))\narray([[ 0.],\n       [ 0.]])\n >>> s = (2,2)\n>>> np.zeros(s)\narray([[ 0.,  0.],\n       [ 0.,  0.]])\n >>> np.zeros((2,), dtype=[('x', 'i4'), ('y', 'i4')]) # custom dtype\narray([(0, 0), (0, 0)],\n      dtype=[('x', '<i4'), ('y', '<i4')])","title":"numpy.reference.generated.numpy.ma.zeros"},{"text":"numpy.zeros   numpy.zeros(shape, dtype=float, order='C', *, like=None)\n \nReturn a new array of given shape and type, filled with zeros.  Parameters \n \nshapeint or tuple of ints\n\n\nShape of the new array, e.g., (2, 3) or 2.  \ndtypedata-type, optional\n\n\nThe desired data-type for the array, e.g., numpy.int8. Default is numpy.float64.  \norder{\u2018C\u2019, \u2018F\u2019}, optional, default: \u2018C\u2019\n\n\nWhether to store multi-dimensional data in row-major (C-style) or column-major (Fortran-style) order in memory.  \nlikearray_like\n\n\nReference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as like supports the __array_function__ protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.  New in version 1.20.0.     Returns \n \noutndarray\n\n\nArray of zeros with the given shape, dtype, and order.      See also  zeros_like\n\nReturn an array of zeros with shape and type of input.  empty\n\nReturn a new uninitialized array.  ones\n\nReturn a new array setting values to one.  full\n\nReturn a new array of given shape filled with value.    Examples >>> np.zeros(5)\narray([ 0.,  0.,  0.,  0.,  0.])\n >>> np.zeros((5,), dtype=int)\narray([0, 0, 0, 0, 0])\n >>> np.zeros((2, 1))\narray([[ 0.],\n       [ 0.]])\n >>> s = (2,2)\n>>> np.zeros(s)\narray([[ 0.,  0.],\n       [ 0.,  0.]])\n >>> np.zeros((2,), dtype=[('x', 'i4'), ('y', 'i4')]) # custom dtype\narray([(0, 0), (0, 0)],\n      dtype=[('x', '<i4'), ('y', '<i4')])","title":"numpy.reference.generated.numpy.zeros"},{"text":"numpy.zeros_like   numpy.zeros_like(a, dtype=None, order='K', subok=True, shape=None)[source]\n \nReturn an array of zeros with the same shape and type as a given array.  Parameters \n \naarray_like\n\n\nThe shape and data-type of a define these same attributes of the returned array.  \ndtypedata-type, optional\n\n\nOverrides the data type of the result.  New in version 1.6.0.   \norder{\u2018C\u2019, \u2018F\u2019, \u2018A\u2019, or \u2018K\u2019}, optional\n\n\nOverrides the memory layout of the result. \u2018C\u2019 means C-order, \u2018F\u2019 means F-order, \u2018A\u2019 means \u2018F\u2019 if a is Fortran contiguous, \u2018C\u2019 otherwise. \u2018K\u2019 means match the layout of a as closely as possible.  New in version 1.6.0.   \nsubokbool, optional.\n\n\nIf True, then the newly created array will use the sub-class type of a, otherwise it will be a base-class array. Defaults to True.  \nshapeint or sequence of ints, optional.\n\n\nOverrides the shape of the result. If order=\u2019K\u2019 and the number of dimensions is unchanged, will try to keep order, otherwise, order=\u2019C\u2019 is implied.  New in version 1.17.0.     Returns \n \noutndarray\n\n\nArray of zeros with the same shape and type as a.      See also  empty_like\n\nReturn an empty array with shape and type of input.  ones_like\n\nReturn an array of ones with shape and type of input.  full_like\n\nReturn a new array with shape of input filled with value.  zeros\n\nReturn a new array setting values to zero.    Examples >>> x = np.arange(6)\n>>> x = x.reshape((2, 3))\n>>> x\narray([[0, 1, 2],\n       [3, 4, 5]])\n>>> np.zeros_like(x)\narray([[0, 0, 0],\n       [0, 0, 0]])\n >>> y = np.arange(3, dtype=float)\n>>> y\narray([0., 1., 2.])\n>>> np.zeros_like(y)\narray([0.,  0.,  0.])","title":"numpy.reference.generated.numpy.zeros_like"},{"text":"tf.raw_ops.ZerosLike Returns a tensor of zeros with the same shape and type as x.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.raw_ops.ZerosLike  \ntf.raw_ops.ZerosLike(\n    x, name=None\n)\n\n \n\n\n Args\n  x   A Tensor. a tensor of type T.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor. Has the same type as x.","title":"tensorflow.raw_ops.zeroslike"},{"text":"tf.experimental.numpy.zeros_like TensorFlow variant of NumPy's zeros_like. \ntf.experimental.numpy.zeros_like(\n    a, dtype=None\n)\n Unsupported arguments: order, subok, shape. See the NumPy documentation for numpy.zeros_like.","title":"tensorflow.experimental.numpy.zeros_like"},{"text":"numpy.ma.zeros_like   ma.zeros_like(*args, **kwargs) = <numpy.ma.core._convert2ma object>\n \nReturn an array of zeros with the same shape and type as a given array.  Parameters \n \naarray_like\n\n\nThe shape and data-type of a define these same attributes of the returned array.  \ndtypedata-type, optional\n\n\nOverrides the data type of the result.  New in version 1.6.0.   \norder{\u2018C\u2019, \u2018F\u2019, \u2018A\u2019, or \u2018K\u2019}, optional\n\n\nOverrides the memory layout of the result. \u2018C\u2019 means C-order, \u2018F\u2019 means F-order, \u2018A\u2019 means \u2018F\u2019 if a is Fortran contiguous, \u2018C\u2019 otherwise. \u2018K\u2019 means match the layout of a as closely as possible.  New in version 1.6.0.   \nsubokbool, optional.\n\n\nIf True, then the newly created array will use the sub-class type of a, otherwise it will be a base-class array. Defaults to True.  \nshapeint or sequence of ints, optional.\n\n\nOverrides the shape of the result. If order=\u2019K\u2019 and the number of dimensions is unchanged, will try to keep order, otherwise, order=\u2019C\u2019 is implied.  New in version 1.17.0.     Returns \n \noutMaskedArray\n\n\nArray of zeros with the same shape and type as a.      See also  empty_like\n\nReturn an empty array with shape and type of input.  ones_like\n\nReturn an array of ones with shape and type of input.  full_like\n\nReturn a new array with shape of input filled with value.  zeros\n\nReturn a new array setting values to zero.    Examples >>> x = np.arange(6)\n>>> x = x.reshape((2, 3))\n>>> x\narray([[0, 1, 2],\n       [3, 4, 5]])\n>>> np.zeros_like(x)\narray([[0, 0, 0],\n       [0, 0, 0]])\n >>> y = np.arange(3, dtype=float)\n>>> y\narray([0., 1., 2.])\n>>> np.zeros_like(y)\narray([0.,  0.,  0.])","title":"numpy.reference.generated.numpy.ma.zeros_like"},{"text":"numpy.matlib.zeros   matlib.zeros(shape, dtype=None, order='C')[source]\n \nReturn a matrix of given shape and type, filled with zeros.  Parameters \n \nshapeint or sequence of ints\n\n\nShape of the matrix  \ndtypedata-type, optional\n\n\nThe desired data-type for the matrix, default is float.  \norder{\u2018C\u2019, \u2018F\u2019}, optional\n\n\nWhether to store the result in C- or Fortran-contiguous order, default is \u2018C\u2019.    Returns \n \noutmatrix\n\n\nZero matrix of given shape, dtype, and order.      See also  numpy.zeros\n\nEquivalent array function.  matlib.ones\n\nReturn a matrix of ones.    Notes If shape has length one i.e. (N,), or is a scalar N, out becomes a single row matrix of shape (1,N). Examples >>> import numpy.matlib\n>>> np.matlib.zeros((2, 3))\nmatrix([[0.,  0.,  0.],\n        [0.,  0.,  0.]])\n >>> np.matlib.zeros(2)\nmatrix([[0.,  0.]])","title":"numpy.reference.generated.numpy.matlib.zeros"},{"text":"tf.zeros_like     View source on GitHub    Creates a tensor with all elements set to zero. \ntf.zeros_like(\n    input, dtype=None, name=None\n)\n See also tf.zeros. Given a single tensor or array-like object (input), this operation returns a tensor of the same type and shape as input with all elements set to zero. Optionally, you can use dtype to specify a new type for the returned tensor. Examples: \ntensor = tf.constant([[1, 2, 3], [4, 5, 6]])\ntf.zeros_like(tensor)\n<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\narray([[0, 0, 0],\n       [0, 0, 0]], dtype=int32)>\n \ntf.zeros_like(tensor, dtype=tf.float32)\n<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[0., 0., 0.],\n       [0., 0., 0.]], dtype=float32)>\n \ntf.zeros_like([[1, 2, 3], [4, 5, 6]])\n<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\narray([[0, 0, 0],\n       [0, 0, 0]], dtype=int32)>\n\n \n\n\n Args\n  input   A Tensor or array-like object.  \n  dtype   A type for the returned Tensor. Must be float16, float32, float64, int8, uint8, int16, uint16, int32, int64, complex64, complex128, bool or string (optional).  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor with all elements set to zero.","title":"tensorflow.zeros_like"}]}
{"task_id":6266727,"prompt":"def f_6266727(content):\n\treturn ","suffix":"","canonical_solution":"\"\"\" \"\"\".join(content.split(' ')[:-1])","test_start":"\ndef check(candidate):","test":["\n    assert candidate('test') == ''\n","\n    assert candidate('this is an example content') == 'this is an example'\n","\n    assert candidate('  ') == ' '\n","\n    assert candidate('') == ''\n","\n    assert candidate('blank and tab\t') == 'blank and'\n"],"entry_point":"f_6266727","intent":"cut off the last word of a sentence `content`","library":[],"docs":[]}
{"task_id":30385151,"prompt":"def f_30385151(x):\n\t","suffix":"\n\treturn x","canonical_solution":"x = np.asarray(x).reshape(1, -1)[(0), :]","test_start":"\nimport numpy as np\n\ndef check(candidate):","test":["\n    assert all(candidate(1.) == np.asarray(1.))\n","\n    assert all(candidate(123) == np.asarray(123))\n","\n    assert all(candidate('a') == np.asarray('a'))\n","\n    assert all(candidate(False) == np.asarray(False))\n"],"entry_point":"f_30385151","intent":"convert scalar `x` to array","library":["numpy"],"docs":[{"text":"numpy.float32[source]\n \nalias of numpy.single","title":"numpy.reference.arrays.scalars#numpy.float32"},{"text":"tf.experimental.numpy.asarray TensorFlow variant of NumPy's asarray. \ntf.experimental.numpy.asarray(\n    a, dtype=None\n)\n Unsupported arguments: order. See the NumPy documentation for numpy.asarray.","title":"tensorflow.experimental.numpy.asarray"},{"text":"tf.experimental.numpy.array TensorFlow variant of NumPy's array. \ntf.experimental.numpy.array(\n    val, dtype=None, copy=True, ndmin=0\n)\n Since Tensors are immutable, a copy is made only if val is placed on a different device than the current one. Even if copy is False, a new Tensor may need to be built to satisfy dtype and ndim. This is used only if val is an ndarray or a Tensor. See the NumPy documentation for numpy.array.","title":"tensorflow.experimental.numpy.array"},{"text":"numpy.float64[source]\n \nalias of numpy.double","title":"numpy.reference.arrays.scalars#numpy.float64"},{"text":"sklearn.utils.as_float_array  \nsklearn.utils.as_float_array(X, *, copy=True, force_all_finite=True) [source]\n \nConverts an array-like to an array of floats. The new dtype will be np.float32 or np.float64, depending on the original type. The function can create a copy or modify the argument depending on the argument copy.  Parameters \n \nX{array-like, sparse matrix} \n\ncopybool, default=True \n\nIf True, a copy of X will be created. If False, a copy may still be returned if X\u2019s dtype is not a floating point type.  \nforce_all_finitebool or \u2018allow-nan\u2019, default=True \n\nWhether to raise an error on np.inf, np.nan, pd.NA in X. The possibilities are:  True: Force all values of X to be finite. False: accepts np.inf, np.nan, pd.NA in X. \u2018allow-nan\u2019: accepts only np.nan and pd.NA values in X. Values cannot be infinite.   New in version 0.20: force_all_finite accepts the string 'allow-nan'.   Changed in version 0.23: Accepts pd.NA and converts it into np.nan     Returns \n \nXT{ndarray, sparse matrix} \n\nAn array of type float.","title":"sklearn.modules.generated.sklearn.utils.as_float_array"},{"text":"numpy.generic.__array__ method   generic.__array__()\n \nsc.__array__(dtype) return 0-dim array from scalar with specified dtype","title":"numpy.reference.generated.numpy.generic.__array__"},{"text":"numpy.cfloat[source]\n \nalias of numpy.cdouble","title":"numpy.reference.arrays.scalars#numpy.cfloat"},{"text":"sklearn.utils.as_float_array(X, *, copy=True, force_all_finite=True) [source]\n \nConverts an array-like to an array of floats. The new dtype will be np.float32 or np.float64, depending on the original type. The function can create a copy or modify the argument depending on the argument copy.  Parameters \n \nX{array-like, sparse matrix} \n\ncopybool, default=True \n\nIf True, a copy of X will be created. If False, a copy may still be returned if X\u2019s dtype is not a floating point type.  \nforce_all_finitebool or \u2018allow-nan\u2019, default=True \n\nWhether to raise an error on np.inf, np.nan, pd.NA in X. The possibilities are:  True: Force all values of X to be finite. False: accepts np.inf, np.nan, pd.NA in X. \u2018allow-nan\u2019: accepts only np.nan and pd.NA values in X. Values cannot be infinite.   New in version 0.20: force_all_finite accepts the string 'allow-nan'.   Changed in version 0.23: Accepts pd.NA and converts it into np.nan     Returns \n \nXT{ndarray, sparse matrix} \n\nAn array of type float.","title":"sklearn.modules.generated.sklearn.utils.as_float_array#sklearn.utils.as_float_array"},{"text":"Scalars Python defines only one type of a particular data class (there is only one integer type, one floating-point type, etc.). This can be convenient in applications that don\u2019t need to be concerned with all the ways data can be represented in a computer. For scientific computing, however, more control is often needed. In NumPy, there are 24 new fundamental Python types to describe different types of scalars. These type descriptors are mostly based on the types available in the C language that CPython is written in, with several additional types compatible with Python\u2019s types. Array scalars have the same attributes and methods as ndarrays. 1 This allows one to treat items of an array partly on the same footing as arrays, smoothing out rough edges that result when mixing scalar and array operations. Array scalars live in a hierarchy (see the Figure below) of data types. They can be detected using the hierarchy: For example, isinstance(val, np.generic) will return True if val is an array scalar object. Alternatively, what kind of array scalar is present can be determined using other members of the data type hierarchy. Thus, for example isinstance(val, np.complexfloating) will return True if val is a complex valued type, while isinstance(val, np.flexible) will return true if val is one of the flexible itemsize array types (str_, bytes_, void).    Figure: Hierarchy of type objects representing the array data types. Not shown are the two integer types intp and uintp which just point to the integer type that holds a pointer for the platform. All the number types can be obtained using bit-width names as well.    1 \nHowever, array scalars are immutable, so none of the array scalar attributes are settable.    Built-in scalar types The built-in scalar types are shown below. The C-like names are associated with character codes, which are shown in their descriptions. Use of the character codes, however, is discouraged. Some of the scalar types are essentially equivalent to fundamental Python types and therefore inherit from them as well as from the generic array scalar type:   \nArray scalar type Related Python type Inherits?   \nint_ int Python 2 only  \nfloat_ float yes  \ncomplex_ complex yes  \nbytes_ bytes yes  \nstr_ str yes  \nbool_ bool no  \ndatetime64 datetime.datetime no  \ntimedelta64 datetime.timedelta no   The bool_ data type is very similar to the Python bool but does not inherit from it because Python\u2019s bool does not allow itself to be inherited from, and on the C-level the size of the actual bool data is not the same as a Python Boolean scalar.  Warning The int_ type does not inherit from the int built-in under Python 3, because type int is no longer a fixed-width integer type.   Tip The default data type in NumPy is float_.    class numpy.generic[source]\n \nBase class for numpy scalar types. Class from which most (all?) numpy scalar types are derived. For consistency, exposes the same API as ndarray, despite many consequent attributes being either \u201cget-only,\u201d or completely irrelevant. This is the class from which it is strongly suggested users should derive custom scalar types. \n   class numpy.number[source]\n \nAbstract base class of all numeric scalar types. \n  Integer types   class numpy.integer[source]\n \nAbstract base class of all integer scalar types. \n  Note The numpy integer types mirror the behavior of C integers, and can therefore be subject to Overflow Errors.   Signed integer types   class numpy.signedinteger[source]\n \nAbstract base class of all signed integer scalar types. \n   class numpy.byte[source]\n \nSigned integer type, compatible with C char.  Character code \n'b'  Alias on this platform (Linux x86_64) \nnumpy.int8: 8-bit signed integer (-128 to 127).   \n   class numpy.short[source]\n \nSigned integer type, compatible with C short.  Character code \n'h'  Alias on this platform (Linux x86_64) \nnumpy.int16: 16-bit signed integer (-32_768 to 32_767).   \n   class numpy.intc[source]\n \nSigned integer type, compatible with C int.  Character code \n'i'  Alias on this platform (Linux x86_64) \nnumpy.int32: 32-bit signed integer (-2_147_483_648 to 2_147_483_647).   \n   class numpy.int_[source]\n \nSigned integer type, compatible with Python int and C long.  Character code \n'l'  Alias on this platform (Linux x86_64) \nnumpy.int64: 64-bit signed integer (-9_223_372_036_854_775_808 to 9_223_372_036_854_775_807).  Alias on this platform (Linux x86_64) \nnumpy.intp: Signed integer large enough to fit pointer, compatible with C intptr_t.   \n   class numpy.longlong[source]\n \nSigned integer type, compatible with C long long.  Character code \n'q'   \n   Unsigned integer types   class numpy.unsignedinteger[source]\n \nAbstract base class of all unsigned integer scalar types. \n   class numpy.ubyte[source]\n \nUnsigned integer type, compatible with C unsigned char.  Character code \n'B'  Alias on this platform (Linux x86_64) \nnumpy.uint8: 8-bit unsigned integer (0 to 255).   \n   class numpy.ushort[source]\n \nUnsigned integer type, compatible with C unsigned short.  Character code \n'H'  Alias on this platform (Linux x86_64) \nnumpy.uint16: 16-bit unsigned integer (0 to 65_535).   \n   class numpy.uintc[source]\n \nUnsigned integer type, compatible with C unsigned int.  Character code \n'I'  Alias on this platform (Linux x86_64) \nnumpy.uint32: 32-bit unsigned integer (0 to 4_294_967_295).   \n   class numpy.uint[source]\n \nUnsigned integer type, compatible with C unsigned long.  Character code \n'L'  Alias on this platform (Linux x86_64) \nnumpy.uint64: 64-bit unsigned integer (0 to 18_446_744_073_709_551_615).  Alias on this platform (Linux x86_64) \nnumpy.uintp: Unsigned integer large enough to fit pointer, compatible with C uintptr_t.   \n   class numpy.ulonglong[source]\n \nSigned integer type, compatible with C unsigned long long.  Character code \n'Q'   \n    Inexact types   class numpy.inexact[source]\n \nAbstract base class of all numeric scalar types with a (potentially) inexact representation of the values in its range, such as floating-point numbers. \n  Note Inexact scalars are printed using the fewest decimal digits needed to distinguish their value from other values of the same datatype, by judicious rounding. See the unique parameter of format_float_positional and format_float_scientific. This means that variables with equal binary values but whose datatypes are of different precisions may display differently: >>> f16 = np.float16(\"0.1\")\n>>> f32 = np.float32(f16)\n>>> f64 = np.float64(f32)\n>>> f16 == f32 == f64\nTrue\n>>> f16, f32, f64\n(0.1, 0.099975586, 0.0999755859375)\n Note that none of these floats hold the exact value \\(\\frac{1}{10}\\); f16 prints as 0.1 because it is as close to that value as possible, whereas the other types do not as they have more precision and therefore have closer values. Conversely, floating-point scalars of different precisions which approximate the same decimal value may compare unequal despite printing identically: >>> f16 = np.float16(\"0.1\")\n>>> f32 = np.float32(\"0.1\")\n>>> f64 = np.float64(\"0.1\")\n>>> f16 == f32 == f64\nFalse\n>>> f16, f32, f64\n(0.1, 0.1, 0.1)\n   Floating-point types   class numpy.floating[source]\n \nAbstract base class of all floating-point scalar types. \n   class numpy.half[source]\n \nHalf-precision floating-point number type.  Character code \n'e'  Alias on this platform (Linux x86_64) \nnumpy.float16: 16-bit-precision floating-point number type: sign bit, 5 bits exponent, 10 bits mantissa.   \n   class numpy.single[source]\n \nSingle-precision floating-point number type, compatible with C float.  Character code \n'f'  Alias on this platform (Linux x86_64) \nnumpy.float32: 32-bit-precision floating-point number type: sign bit, 8 bits exponent, 23 bits mantissa.   \n   class numpy.double(x=0, \/)[source]\n \nDouble-precision floating-point number type, compatible with Python float and C double.  Character code \n'd'  Alias \nnumpy.float_  Alias on this platform (Linux x86_64) \nnumpy.float64: 64-bit precision floating-point number type: sign bit, 11 bits exponent, 52 bits mantissa.   \n   class numpy.longdouble[source]\n \nExtended-precision floating-point number type, compatible with C long double but not necessarily with IEEE 754 quadruple-precision.  Character code \n'g'  Alias \nnumpy.longfloat  Alias on this platform (Linux x86_64) \nnumpy.float128: 128-bit extended-precision floating-point number type.   \n   Complex floating-point types   class numpy.complexfloating[source]\n \nAbstract base class of all complex number scalar types that are made up of floating-point numbers. \n   class numpy.csingle[source]\n \nComplex number type composed of two single-precision floating-point numbers.  Character code \n'F'  Alias \nnumpy.singlecomplex  Alias on this platform (Linux x86_64) \nnumpy.complex64: Complex number type composed of 2 32-bit-precision floating-point numbers.   \n   class numpy.cdouble(real=0, imag=0)[source]\n \nComplex number type composed of two double-precision floating-point numbers, compatible with Python complex.  Character code \n'D'  Alias \nnumpy.cfloat  Alias \nnumpy.complex_  Alias on this platform (Linux x86_64) \nnumpy.complex128: Complex number type composed of 2 64-bit-precision floating-point numbers.   \n   class numpy.clongdouble[source]\n \nComplex number type composed of two extended-precision floating-point numbers.  Character code \n'G'  Alias \nnumpy.clongfloat  Alias \nnumpy.longcomplex  Alias on this platform (Linux x86_64) \nnumpy.complex256: Complex number type composed of 2 128-bit extended-precision floating-point numbers.   \n    Other types   class numpy.bool_[source]\n \nBoolean type (True or False), stored as a byte.  Warning The bool_ type is not a subclass of the int_ type (the bool_ is not even a number type). This is different than Python\u2019s default implementation of bool as a sub-class of int.   Character code \n'?'  Alias \nnumpy.bool8   \n   class numpy.datetime64[source]\n \nIf created from a 64-bit integer, it represents an offset from 1970-01-01T00:00:00. If created from string, the string can be in ISO 8601 date or datetime format. >>> np.datetime64(10, 'Y')\nnumpy.datetime64('1980')\n>>> np.datetime64('1980', 'Y')\nnumpy.datetime64('1980')\n>>> np.datetime64(10, 'D')\nnumpy.datetime64('1970-01-11')\n See Datetimes and Timedeltas for more information.  Character code \n'M'   \n   class numpy.timedelta64[source]\n \nA timedelta stored as a 64-bit integer. See Datetimes and Timedeltas for more information.  Character code \n'm'   \n   class numpy.object_[source]\n \nAny Python object.  Character code \n'O'   \n  Note The data actually stored in object arrays (i.e., arrays having dtype object_) are references to Python objects, not the objects themselves. Hence, object arrays behave more like usual Python lists, in the sense that their contents need not be of the same Python type. The object type is also special because an array containing object_ items does not return an object_ object on item access, but instead returns the actual object that the array item refers to.  The following data types are flexible: they have no predefined size and the data they describe can be of different length in different arrays. (In the character codes # is an integer denoting how many elements the data type consists of.)   class numpy.flexible[source]\n \nAbstract base class of all scalar types without predefined length. The actual size of these types depends on the specific np.dtype instantiation. \n   class numpy.bytes_[source]\n \nA byte string. When used in arrays, this type strips trailing null bytes.  Character code \n'S'  Alias \nnumpy.string_   \n   class numpy.str_[source]\n \nA unicode string. When used in arrays, this type strips trailing null codepoints. Unlike the builtin str, this supports the Buffer Protocol, exposing its contents as UCS4: >>> m = memoryview(np.str_(\"abc\"))\n>>> m.format\n'3w'\n>>> m.tobytes()\nb'a\\x00\\x00\\x00b\\x00\\x00\\x00c\\x00\\x00\\x00'\n  Character code \n'U'  Alias \nnumpy.unicode_   \n   class numpy.void[source]\n \nEither an opaque sequence of bytes, or a structure. >>> np.void(b'abcd')\nvoid(b'\\x61\\x62\\x63\\x64')\n Structured void scalars can only be constructed via extraction from Structured arrays: >>> arr = np.array((1, 2), dtype=[('x', np.int8), ('y', np.int8)])\n>>> arr[()]\n(1, 2)  # looks like a tuple, but is `np.void`\n  Character code \n'V'   \n  Warning See Note on string types. Numeric Compatibility: If you used old typecode characters in your Numeric code (which was never recommended), you will need to change some of them to the new characters. In particular, the needed changes are c -> S1, b -> B, 1 -> b, s -> h, w ->\nH, and u -> I. These changes make the type character convention more consistent with other Python modules such as the struct module.    Sized aliases Along with their (mostly) C-derived names, the integer, float, and complex data-types are also available using a bit-width convention so that an array of the right size can always be ensured. Two aliases (numpy.intp and numpy.uintp) pointing to the integer type that is sufficiently large to hold a C pointer are also provided.   numpy.bool8[source]\n \nalias of numpy.bool_ \n   numpy.int8[source]\n  numpy.int16\n  numpy.int32\n  numpy.int64\n \nAliases for the signed integer types (one of numpy.byte, numpy.short, numpy.intc, numpy.int_ and numpy.longlong) with the specified number of bits. Compatible with the C99 int8_t, int16_t, int32_t, and int64_t, respectively. \n   numpy.uint8[source]\n  numpy.uint16\n  numpy.uint32\n  numpy.uint64\n \nAlias for the unsigned integer types (one of numpy.ubyte, numpy.ushort, numpy.uintc, numpy.uint and numpy.ulonglong) with the specified number of bits. Compatible with the C99 uint8_t, uint16_t, uint32_t, and uint64_t, respectively. \n   numpy.intp[source]\n \nAlias for the signed integer type (one of numpy.byte, numpy.short, numpy.intc, numpy.int_ and np.longlong) that is the same size as a pointer. Compatible with the C intptr_t.  Character code \n'p'   \n   numpy.uintp[source]\n \nAlias for the unsigned integer type (one of numpy.ubyte, numpy.ushort, numpy.uintc, numpy.uint and np.ulonglong) that is the same size as a pointer. Compatible with the C uintptr_t.  Character code \n'P'   \n   numpy.float16[source]\n \nalias of numpy.half \n   numpy.float32[source]\n \nalias of numpy.single \n   numpy.float64[source]\n \nalias of numpy.double \n   numpy.float96\n  numpy.float128[source]\n \nAlias for numpy.longdouble, named after its size in bits. The existence of these aliases depends on the platform. \n   numpy.complex64[source]\n \nalias of numpy.csingle \n   numpy.complex128[source]\n \nalias of numpy.cdouble \n   numpy.complex192\n  numpy.complex256[source]\n \nAlias for numpy.clongdouble, named after its size in bits. The existence of these aliases depends on the platform. \n   Other aliases The first two of these are conveniences which resemble the names of the builtin types, in the same style as bool_, int_, str_, bytes_, and object_:   numpy.float_[source]\n \nalias of numpy.double \n   numpy.complex_[source]\n \nalias of numpy.cdouble \n Some more use alternate naming conventions for extended-precision floats and complex numbers:   numpy.longfloat[source]\n \nalias of numpy.longdouble \n   numpy.singlecomplex[source]\n \nalias of numpy.csingle \n   numpy.cfloat[source]\n \nalias of numpy.cdouble \n   numpy.longcomplex[source]\n \nalias of numpy.clongdouble \n   numpy.clongfloat[source]\n \nalias of numpy.clongdouble \n The following aliases originate from Python 2, and it is recommended that they not be used in new code.   numpy.string_[source]\n \nalias of numpy.bytes_ \n   numpy.unicode_[source]\n \nalias of numpy.str_ \n    Attributes The array scalar objects have an array priority of NPY_SCALAR_PRIORITY (-1,000,000.0). They also do not (yet) have a ctypes attribute. Otherwise, they share the same attributes as arrays:  \ngeneric.flags The integer value of flags.  \ngeneric.shape Tuple of array dimensions.  \ngeneric.strides Tuple of bytes steps in each dimension.  \ngeneric.ndim The number of array dimensions.  \ngeneric.data Pointer to start of data.  \ngeneric.size The number of elements in the gentype.  \ngeneric.itemsize The length of one element in bytes.  \ngeneric.base Scalar attribute identical to the corresponding array attribute.  \ngeneric.dtype Get array data-descriptor.  \ngeneric.real The real part of the scalar.  \ngeneric.imag The imaginary part of the scalar.  \ngeneric.flat A 1-D view of the scalar.  \ngeneric.T Scalar attribute identical to the corresponding array attribute.  \ngeneric.__array_interface__ Array protocol: Python side  \ngeneric.__array_struct__ Array protocol: struct  \ngeneric.__array_priority__ Array priority.  \ngeneric.__array_wrap__ sc.__array_wrap__(obj) return scalar from array     Indexing  See also Indexing routines, Data type objects (dtype)  Array scalars can be indexed like 0-dimensional arrays: if x is an array scalar,  \nx[()] returns a copy of array scalar \nx[...] returns a 0-dimensional ndarray\n \nx['field-name'] returns the array scalar in the field field-name. (x can have fields, for example, when it corresponds to a structured data type.)    Methods Array scalars have exactly the same methods as arrays. The default behavior of these methods is to internally convert the scalar to an equivalent 0-dimensional array and to call the corresponding array method. In addition, math operations on array scalars are defined so that the same hardware flags are set and used to interpret the results as for ufunc, so that the error state used for ufuncs also carries over to the math on array scalars. The exceptions to the above rules are given below:  \ngeneric.__array__ sc.__array__(dtype) return 0-dim array from scalar with specified dtype  \ngeneric.__array_wrap__ sc.__array_wrap__(obj) return scalar from array  \ngeneric.squeeze Scalar method identical to the corresponding array attribute.  \ngeneric.byteswap Scalar method identical to the corresponding array attribute.  \ngeneric.__reduce__ Helper for pickle.  \ngeneric.__setstate__   \ngeneric.setflags Scalar method identical to the corresponding array attribute.   Utility method for typing:  \nnumber.__class_getitem__(item, \/) Return a parametrized wrapper around the number type.     Defining new types There are two ways to effectively define a new array scalar type (apart from composing structured types dtypes from the built-in scalar types): One way is to simply subclass the ndarray and overwrite the methods of interest. This will work to a degree, but internally certain behaviors are fixed by the data type of the array. To fully customize the data type of an array you need to define a new data-type, and register it with NumPy. Such new types can only be defined in C, using the NumPy C-API.","title":"numpy.reference.arrays.scalars"},{"text":"numpy.singlecomplex[source]\n \nalias of numpy.csingle","title":"numpy.reference.arrays.scalars#numpy.singlecomplex"}]}
{"task_id":15856127,"prompt":"def f_15856127(L):\n\treturn ","suffix":"","canonical_solution":"sum(sum(i) if isinstance(i, list) else i for i in L)","test_start":"\ndef check(candidate):","test":["\n    assert candidate([1,2,3,4]) == 10\n","\n    assert candidate([[1],[2],[3],[4]]) == 10\n","\n    assert candidate([1,1,1,1]) == 4\n","\n    assert candidate([1,[2,3],[4]]) == 10\n","\n    assert candidate([]) == 0\n","\n    assert candidate([[], []]) == 0\n"],"entry_point":"f_15856127","intent":"sum all elements of nested list `L`","library":[],"docs":[]}
{"task_id":1592158,"prompt":"def f_1592158():\n\treturn ","suffix":"","canonical_solution":"struct.unpack('!f', bytes.fromhex('470FC614'))[0]","test_start":"\nimport struct \n\ndef check(candidate):","test":["\n    assert (candidate() - 36806.078125) < 1e-6\n","\n    assert (candidate() - 32806.079125) > 1e-6\n"],"entry_point":"f_1592158","intent":"convert hex string '470FC614' to a float number","library":["struct"],"docs":[{"text":"classmethod float.fromhex(s)  \nClass method to return the float represented by a hexadecimal string s. The string s may have leading and trailing whitespace.","title":"python.library.stdtypes#float.fromhex"},{"text":"class float([x])  \nReturn a floating point number constructed from a number or string x. If the argument is a string, it should contain a decimal number, optionally preceded by a sign, and optionally embedded in whitespace. The optional sign may be '+' or '-'; a '+' sign has no effect on the value produced. The argument may also be a string representing a NaN (not-a-number), or a positive or negative infinity. More precisely, the input must conform to the following grammar after leading and trailing whitespace characters are removed: \nsign           ::=  \"+\" | \"-\"\ninfinity       ::=  \"Infinity\" | \"inf\"\nnan            ::=  \"nan\"\nnumeric_value  ::=  floatnumber | infinity | nan\nnumeric_string ::=  [sign] numeric_value\n Here floatnumber is the form of a Python floating-point literal, described in Floating point literals. Case is not significant, so, for example, \u201cinf\u201d, \u201cInf\u201d, \u201cINFINITY\u201d and \u201ciNfINity\u201d are all acceptable spellings for positive infinity. Otherwise, if the argument is an integer or a floating point number, a floating point number with the same value (within Python\u2019s floating point precision) is returned. If the argument is outside the range of a Python float, an OverflowError will be raised. For a general Python object x, float(x) delegates to x.__float__(). If __float__() is not defined then it falls back to __index__(). If no argument is given, 0.0 is returned. Examples: >>> float('+1.23')\n1.23\n>>> float('   -12345\\n')\n-12345.0\n>>> float('1e-003')\n0.001\n>>> float('+1E6')\n1000000.0\n>>> float('-Infinity')\n-inf\n The float type is described in Numeric Types \u2014 int, float, complex.  Changed in version 3.6: Grouping digits with underscores as in code literals is allowed.   Changed in version 3.7: x is now a positional-only parameter.   Changed in version 3.8: Falls back to __index__() if __float__() is not defined.","title":"python.library.functions#float"},{"text":"float.hex()  \nReturn a representation of a floating-point number as a hexadecimal string. For finite floating-point numbers, this representation will always include a leading 0x and a trailing p and exponent.","title":"python.library.stdtypes#float.hex"},{"text":"locale.atof(string)  \nConverts a string to a floating point number, following the LC_NUMERIC settings.","title":"python.library.locale#locale.atof"},{"text":"classmethod fromhex(string)  \nThis bytes class method returns a bytes object, decoding the given string object. The string must contain two hexadecimal digits per byte, with ASCII whitespace being ignored. >>> bytes.fromhex('2Ef0 F1f2  ')\nb'.\\xf0\\xf1\\xf2'\n  Changed in version 3.7: bytes.fromhex() now skips all ASCII whitespace in the string, not just spaces.","title":"python.library.stdtypes#bytes.fromhex"},{"text":"tf.compat.v1.to_float Casts a tensor to type float32. (deprecated) \ntf.compat.v1.to_float(\n    x, name='ToFloat'\n)\n Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use tf.cast instead.\n \n\n\n Args\n  x   A Tensor or SparseTensor or IndexedSlices.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor or SparseTensor or IndexedSlices with same shape as x with type float32.  \n\n \n\n\n Raises\n  TypeError   If x cannot be cast to the float32.","title":"tensorflow.compat.v1.to_float"},{"text":"Unpacker.unpack_float()  \nUnpacks a single-precision floating point number.","title":"python.library.xdrlib#xdrlib.Unpacker.unpack_float"},{"text":"tf.compat.v1.flags.FloatParser Parser of floating point values. Inherits From: ArgumentParser  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.app.flags.FloatParser  \ntf.compat.v1.flags.FloatParser(\n    lower_bound=None, upper_bound=None\n)\n Parsed value may be bounded to a given upper and lower bound. Methods convert \nconvert(\n    argument\n)\n Returns the float value of argument. flag_type \nflag_type()\n See base class. is_outside_bounds \nis_outside_bounds(\n    val\n)\n Returns whether the value is outside the bounds or not. parse \nparse(\n    argument\n)\n See base class.\n \n\n\n Class Variables\n  number_article   'a'  \n  number_name   'number'  \n  syntactic_help   'a number'","title":"tensorflow.compat.v1.flags.floatparser"},{"text":"class typing.SupportsFloat  \nAn ABC with one abstract method __float__.","title":"python.library.typing#typing.SupportsFloat"},{"text":"binhex.binhex(input, output)  \nConvert a binary file with filename input to binhex file output. The output parameter can either be a filename or a file-like object (any object supporting a write() and close() method).","title":"python.library.binhex#binhex.binhex"}]}
{"task_id":5010536,"prompt":"def f_5010536(my_dict):\n\t","suffix":"\n\treturn my_dict","canonical_solution":"my_dict.update((x, y * 2) for x, y in list(my_dict.items()))","test_start":"\ndef check(candidate):","test":["\n    assert candidate({'a': [1], 'b': 4.9}) == {'a': [1, 1], 'b': 9.8}\n","\n    assert candidate({1:1}) == {1:2}\n","\n    assert candidate({(1,2):[1]}) == {(1,2):[1,1]}\n","\n    assert candidate({'asd':0}) == {'asd':0}\n","\n    assert candidate({}) == {}\n"],"entry_point":"f_5010536","intent":"Multiple each value by `2` for all keys in a dictionary `my_dict`","library":[],"docs":[]}
{"task_id":13745648,"prompt":"def f_13745648():\n\treturn ","suffix":"","canonical_solution":"subprocess.call('sleep.sh', shell=True)","test_start":"\nimport subprocess \nfrom unittest.mock import Mock\n\ndef check(candidate):","test":["\n    subprocess.call = Mock()\n    try:\n        candidate()\n    except:\n        assert False\n"],"entry_point":"f_13745648","intent":"running bash script 'sleep.sh'","library":["subprocess"],"docs":[{"text":"curses.napms(ms)  \nSleep for ms milliseconds.","title":"python.library.curses#curses.napms"},{"text":"curses.def_shell_mode()  \nSave the current terminal mode as the \u201cshell\u201d mode, the mode when the running program is not using curses. (Its counterpart is the \u201cprogram\u201d mode, when the program is using curses capabilities.) Subsequent calls to reset_shell_mode() will restore this mode.","title":"python.library.curses#curses.def_shell_mode"},{"text":"time.sleep(secs)  \nSuspend execution of the calling thread for the given number of seconds. The argument may be a floating point number to indicate a more precise sleep time. The actual suspension time may be less than that requested because any caught signal will terminate the sleep() following execution of that signal\u2019s catching routine. Also, the suspension time may be longer than requested by an arbitrary amount because of the scheduling of other activity in the system.  Changed in version 3.5: The function now sleeps at least secs even if the sleep is interrupted by a signal, except if the signal handler raises an exception (see PEP 475 for the rationale).","title":"python.library.time#time.sleep"},{"text":"math.sin(x)  \nReturn the sine of x radians.","title":"python.library.math#math.sin"},{"text":"IDLE Source code: Lib\/idlelib\/ IDLE is Python\u2019s Integrated Development and Learning Environment. IDLE has the following features:  coded in 100% pure Python, using the tkinter GUI toolkit cross-platform: works mostly the same on Windows, Unix, and macOS Python shell window (interactive interpreter) with colorizing of code input, output, and error messages multi-window text editor with multiple undo, Python colorizing, smart indent, call tips, auto completion, and other features search within any window, replace within editor windows, and search through multiple files (grep) debugger with persistent breakpoints, stepping, and viewing of global and local namespaces configuration, browsers, and other dialogs  Menus IDLE has two main window types, the Shell window and the Editor window. It is possible to have multiple editor windows simultaneously. On Windows and Linux, each has its own top menu. Each menu documented below indicates which window type it is associated with. Output windows, such as used for Edit => Find in Files, are a subtype of editor window. They currently have the same top menu but a different default title and context menu. On macOS, there is one application menu. It dynamically changes according to the window currently selected. It has an IDLE menu, and some entries described below are moved around to conform to Apple guidelines. File menu (Shell and Editor)  New File\n\nCreate a new file editing window.  Open\u2026\n\nOpen an existing file with an Open dialog.  Recent Files\n\nOpen a list of recent files. Click one to open it.  Open Module\u2026\n\nOpen an existing module (searches sys.path).    Class Browser\n\nShow functions, classes, and methods in the current Editor file in a tree structure. In the shell, open a module first.  Path Browser\n\nShow sys.path directories, modules, functions, classes and methods in a tree structure.  Save\n\nSave the current window to the associated file, if there is one. Windows that have been changed since being opened or last saved have a * before and after the window title. If there is no associated file, do Save As instead.  Save As\u2026\n\nSave the current window with a Save As dialog. The file saved becomes the new associated file for the window.  Save Copy As\u2026\n\nSave the current window to different file without changing the associated file.  Print Window\n\nPrint the current window to the default printer.  Close\n\nClose the current window (ask to save if unsaved).  Exit\n\nClose all windows and quit IDLE (ask to save unsaved windows).   Edit menu (Shell and Editor)  Undo\n\nUndo the last change to the current window. A maximum of 1000 changes may be undone.  Redo\n\nRedo the last undone change to the current window.  Cut\n\nCopy selection into the system-wide clipboard; then delete the selection.  Copy\n\nCopy selection into the system-wide clipboard.  Paste\n\nInsert contents of the system-wide clipboard into the current window.   The clipboard functions are also available in context menus.  Select All\n\nSelect the entire contents of the current window.  Find\u2026\n\nOpen a search dialog with many options  Find Again\n\nRepeat the last search, if there is one.  Find Selection\n\nSearch for the currently selected string, if there is one.  Find in Files\u2026\n\nOpen a file search dialog. Put results in a new output window.  Replace\u2026\n\nOpen a search-and-replace dialog.  Go to Line\n\nMove the cursor to the beginning of the line requested and make that line visible. A request past the end of the file goes to the end. Clear any selection and update the line and column status.  Show Completions\n\nOpen a scrollable list allowing selection of existing names. See Completions in the Editing and navigation section below.  Expand Word\n\nExpand a prefix you have typed to match a full word in the same window; repeat to get a different expansion.  Show call tip\n\nAfter an unclosed parenthesis for a function, open a small window with function parameter hints. See Calltips in the Editing and navigation section below.  Show surrounding parens\n\nHighlight the surrounding parenthesis.   Format menu (Editor window only)  Indent Region\n\nShift selected lines right by the indent width (default 4 spaces).  Dedent Region\n\nShift selected lines left by the indent width (default 4 spaces).  Comment Out Region\n\nInsert ## in front of selected lines.  Uncomment Region\n\nRemove leading # or ## from selected lines.  Tabify Region\n\nTurn leading stretches of spaces into tabs. (Note: We recommend using 4 space blocks to indent Python code.)  Untabify Region\n\nTurn all tabs into the correct number of spaces.  Toggle Tabs\n\nOpen a dialog to switch between indenting with spaces and tabs.  New Indent Width\n\nOpen a dialog to change indent width. The accepted default by the Python community is 4 spaces.  Format Paragraph\n\nReformat the current blank-line-delimited paragraph in comment block or multiline string or selected line in a string. All lines in the paragraph will be formatted to less than N columns, where N defaults to 72.  Strip trailing whitespace\n\nRemove trailing space and other whitespace characters after the last non-whitespace character of a line by applying str.rstrip to each line, including lines within multiline strings. Except for Shell windows, remove extra newlines at the end of the file.   Run menu (Editor window only)  Run Module\n\nDo Check Module. If no error, restart the shell to clean the environment, then execute the module. Output is displayed in the Shell window. Note that output requires use of print or write. When execution is complete, the Shell retains focus and displays a prompt. At this point, one may interactively explore the result of execution. This is similar to executing a file with python -i file at a command line.    Run\u2026 Customized\n\nSame as Run Module, but run the module with customized settings. Command Line Arguments extend sys.argv as if passed on a command line. The module can be run in the Shell without restarting.    Check Module\n\nCheck the syntax of the module currently open in the Editor window. If the module has not been saved IDLE will either prompt the user to save or autosave, as selected in the General tab of the Idle Settings dialog. If there is a syntax error, the approximate location is indicated in the Editor window.    Python Shell\n\nOpen or wake up the Python Shell window.   Shell menu (Shell window only)  View Last Restart\n\nScroll the shell window to the last Shell restart.  Restart Shell\n\nRestart the shell to clean the environment and reset display and exception handling.  Previous History\n\nCycle through earlier commands in history which match the current entry.  Next History\n\nCycle through later commands in history which match the current entry.  Interrupt Execution\n\nStop a running program.   Debug menu (Shell window only)  Go to File\/Line\n\nLook on the current line. with the cursor, and the line above for a filename and line number. If found, open the file if not already open, and show the line. Use this to view source lines referenced in an exception traceback and lines found by Find in Files. Also available in the context menu of the Shell window and Output windows.    Debugger (toggle)\n\nWhen activated, code entered in the Shell or run from an Editor will run under the debugger. In the Editor, breakpoints can be set with the context menu. This feature is still incomplete and somewhat experimental.  Stack Viewer\n\nShow the stack traceback of the last exception in a tree widget, with access to locals and globals.  Auto-open Stack Viewer\n\nToggle automatically opening the stack viewer on an unhandled exception.   Options menu (Shell and Editor)  Configure IDLE\n\nOpen a configuration dialog and change preferences for the following: fonts, indentation, keybindings, text color themes, startup windows and size, additional help sources, and extensions. On macOS, open the configuration dialog by selecting Preferences in the application menu. For more details, see Setting preferences under Help and preferences.   Most configuration options apply to all windows or all future windows. The option items below only apply to the active window.  Show\/Hide Code Context (Editor Window only)\n\nOpen a pane at the top of the edit window which shows the block context of the code which has scrolled above the top of the window. See Code Context in the Editing and Navigation section below.  Show\/Hide Line Numbers (Editor Window only)\n\nOpen a column to the left of the edit window which shows the number of each line of text. The default is off, which may be changed in the preferences (see Setting preferences).  Zoom\/Restore Height\n\nToggles the window between normal size and maximum height. The initial size defaults to 40 lines by 80 chars unless changed on the General tab of the Configure IDLE dialog. The maximum height for a screen is determined by momentarily maximizing a window the first time one is zoomed on the screen. Changing screen settings may invalidate the saved height. This toggle has no effect when a window is maximized.   Window menu (Shell and Editor) Lists the names of all open windows; select one to bring it to the foreground (deiconifying it if necessary). Help menu (Shell and Editor)  About IDLE\n\nDisplay version, copyright, license, credits, and more.  IDLE Help\n\nDisplay this IDLE document, detailing the menu options, basic editing and navigation, and other tips.  Python Docs\n\nAccess local Python documentation, if installed, or start a web browser and open docs.python.org showing the latest Python documentation.  Turtle Demo\n\nRun the turtledemo module with example Python code and turtle drawings.   Additional help sources may be added here with the Configure IDLE dialog under the General tab. See the Help sources subsection below for more on Help menu choices. Context Menus Open a context menu by right-clicking in a window (Control-click on macOS). Context menus have the standard clipboard functions also on the Edit menu.  Cut\n\nCopy selection into the system-wide clipboard; then delete the selection.  Copy\n\nCopy selection into the system-wide clipboard.  Paste\n\nInsert contents of the system-wide clipboard into the current window.   Editor windows also have breakpoint functions. Lines with a breakpoint set are specially marked. Breakpoints only have an effect when running under the debugger. Breakpoints for a file are saved in the user\u2019s .idlerc directory.  Set Breakpoint\n\nSet a breakpoint on the current line.  Clear Breakpoint\n\nClear the breakpoint on that line.   Shell and Output windows also have the following.  Go to file\/line\n\nSame as in Debug menu.   The Shell window also has an output squeezing facility explained in the Python Shell window subsection below.  Squeeze\n\nIf the cursor is over an output line, squeeze all the output between the code above and the prompt below down to a \u2018Squeezed text\u2019 label.   Editing and navigation Editor windows IDLE may open editor windows when it starts, depending on settings and how you start IDLE. Thereafter, use the File menu. There can be only one open editor window for a given file. The title bar contains the name of the file, the full path, and the version of Python and IDLE running the window. The status bar contains the line number (\u2018Ln\u2019) and column number (\u2018Col\u2019). Line numbers start with 1; column numbers with 0. IDLE assumes that files with a known .py* extension contain Python code and that other files do not. Run Python code with the Run menu. Key bindings In this section, \u2018C\u2019 refers to the Control key on Windows and Unix and the Command key on macOS.  \nBackspace deletes to the left; Del deletes to the right \nC-Backspace delete word left; C-Del delete word to the right Arrow keys and Page Up\/Page Down to move around \nC-LeftArrow and C-RightArrow moves by words \nHome\/End go to begin\/end of line \nC-Home\/C-End go to begin\/end of file \nSome useful Emacs bindings are inherited from Tcl\/Tk:  \nC-a beginning of line \nC-e end of line \nC-k kill line (but doesn\u2019t put it in clipboard) \nC-l center window around the insertion point \nC-b go backward one character without deleting (usually you can also use the cursor key for this) \nC-f go forward one character without deleting (usually you can also use the cursor key for this) \nC-p go up one line (usually you can also use the cursor key for this) \nC-d delete next character    Standard keybindings (like C-c to copy and C-v to paste) may work. Keybindings are selected in the Configure IDLE dialog. Automatic indentation After a block-opening statement, the next line is indented by 4 spaces (in the Python Shell window by one tab). After certain keywords (break, return etc.) the next line is dedented. In leading indentation, Backspace deletes up to 4 spaces if they are there. Tab inserts spaces (in the Python Shell window one tab), number depends on Indent width. Currently, tabs are restricted to four spaces due to Tcl\/Tk limitations. See also the indent\/dedent region commands on the Format menu. Completions Completions are supplied, when requested and available, for module names, attributes of classes or functions, or filenames. Each request method displays a completion box with existing names. (See tab completions below for an exception.) For any box, change the name being completed and the item highlighted in the box by typing and deleting characters; by hitting Up, Down, PageUp, PageDown, Home, and End keys; and by a single click within the box. Close the box with Escape, Enter, and double Tab keys or clicks outside the box. A double click within the box selects and closes. One way to open a box is to type a key character and wait for a predefined interval. This defaults to 2 seconds; customize it in the settings dialog. (To prevent auto popups, set the delay to a large number of milliseconds, such as 100000000.) For imported module names or class or function attributes, type \u2018.\u2019. For filenames in the root directory, type os.sep or os.altsep immediately after an opening quote. (On Windows, one can specify a drive first.) Move into subdirectories by typing a directory name and a separator. Instead of waiting, or after a box is closed, open a completion box immediately with Show Completions on the Edit menu. The default hot key is C-space. If one types a prefix for the desired name before opening the box, the first match or near miss is made visible. The result is the same as if one enters a prefix after the box is displayed. Show Completions after a quote completes filenames in the current directory instead of a root directory. Hitting Tab after a prefix usually has the same effect as Show Completions. (With no prefix, it indents.) However, if there is only one match to the prefix, that match is immediately added to the editor text without opening a box. Invoking \u2018Show Completions\u2019, or hitting Tab after a prefix, outside of a string and without a preceding \u2018.\u2019 opens a box with keywords, builtin names, and available module-level names. When editing code in an editor (as oppose to Shell), increase the available module-level names by running your code and not restarting the Shell thereafter. This is especially useful after adding imports at the top of a file. This also increases possible attribute completions. Completion boxes intially exclude names beginning with \u2018_\u2019 or, for modules, not included in \u2018__all__\u2019. The hidden names can be accessed by typing \u2018_\u2019 after \u2018.\u2019, either before or after the box is opened. Calltips A calltip is shown automatically when one types ( after the name of an accessible function. A function name expression may include dots and subscripts. A calltip remains until it is clicked, the cursor is moved out of the argument area, or ) is typed. Whenever the cursor is in the argument part of a definition, select Edit and \u201cShow Call Tip\u201d on the menu or enter its shortcut to display a calltip. The calltip consists of the function\u2019s signature and docstring up to the latter\u2019s first blank line or the fifth non-blank line. (Some builtin functions lack an accessible signature.) A \u2018\/\u2019 or \u2018*\u2019 in the signature indicates that the preceding or following arguments are passed by position or name (keyword) only. Details are subject to change. In Shell, the accessible functions depends on what modules have been imported into the user process, including those imported by Idle itself, and which definitions have been run, all since the last restart. For example, restart the Shell and enter itertools.count(. A calltip appears because Idle imports itertools into the user process for its own use. (This could change.) Enter turtle.write( and nothing appears. Idle does not itself import turtle. The menu entry and shortcut also do nothing. Enter import turtle. Thereafter, turtle.write( will display a calltip. In an editor, import statements have no effect until one runs the file. One might want to run a file after writing import statements, after adding function definitions, or after opening an existing file. Code Context Within an editor window containing Python code, code context can be toggled in order to show or hide a pane at the top of the window. When shown, this pane freezes the opening lines for block code, such as those beginning with class, def, or if keywords, that would have otherwise scrolled out of view. The size of the pane will be expanded and contracted as needed to show the all current levels of context, up to the maximum number of lines defined in the Configure IDLE dialog (which defaults to 15). If there are no current context lines and the feature is toggled on, a single blank line will display. Clicking on a line in the context pane will move that line to the top of the editor. The text and background colors for the context pane can be configured under the Highlights tab in the Configure IDLE dialog. Python Shell window With IDLE\u2019s Shell, one enters, edits, and recalls complete statements. Most consoles and terminals only work with a single physical line at a time. When one pastes code into Shell, it is not compiled and possibly executed until one hits Return. One may edit pasted code first. If one pastes more that one statement into Shell, the result will be a SyntaxError when multiple statements are compiled as if they were one. The editing features described in previous subsections work when entering code interactively. IDLE\u2019s Shell window also responds to the following keys.  \nC-c interrupts executing command \nC-d sends end-of-file; closes window if typed at a >>> prompt \nAlt-\/ (Expand word) is also useful to reduce typing Command history  \nAlt-p retrieves previous command matching what you have typed. On macOS use C-p. \nAlt-n retrieves next. On macOS use C-n. \nReturn while on any previous command retrieves that command    Text colors Idle defaults to black on white text, but colors text with special meanings. For the shell, these are shell output, shell error, user output, and user error. For Python code, at the shell prompt or in an editor, these are keywords, builtin class and function names, names following class and def, strings, and comments. For any text window, these are the cursor (when present), found text (when possible), and selected text. Text coloring is done in the background, so uncolorized text is occasionally visible. To change the color scheme, use the Configure IDLE dialog Highlighting tab. The marking of debugger breakpoint lines in the editor and text in popups and dialogs is not user-configurable. Startup and code execution Upon startup with the -s option, IDLE will execute the file referenced by the environment variables IDLESTARTUP or PYTHONSTARTUP. IDLE first checks for IDLESTARTUP; if IDLESTARTUP is present the file referenced is run. If IDLESTARTUP is not present, IDLE checks for PYTHONSTARTUP. Files referenced by these environment variables are convenient places to store functions that are used frequently from the IDLE shell, or for executing import statements to import common modules. In addition, Tk also loads a startup file if it is present. Note that the Tk file is loaded unconditionally. This additional file is .Idle.py and is looked for in the user\u2019s home directory. Statements in this file will be executed in the Tk namespace, so this file is not useful for importing functions to be used from IDLE\u2019s Python shell. Command line usage idle.py [-c command] [-d] [-e] [-h] [-i] [-r file] [-s] [-t title] [-] [arg] ...\n\n-c command  run command in the shell window\n-d          enable debugger and open shell window\n-e          open editor window\n-h          print help message with legal combinations and exit\n-i          open shell window\n-r file     run file in shell window\n-s          run $IDLESTARTUP or $PYTHONSTARTUP first, in shell window\n-t title    set title of shell window\n-           run stdin in shell (- must be last option before args)\n If there are arguments:  If -, -c, or r is used, all arguments are placed in sys.argv[1:...] and sys.argv[0] is set to '', '-c', or '-r'. No editor window is opened, even if that is the default set in the Options dialog. Otherwise, arguments are files opened for editing and sys.argv reflects the arguments passed to IDLE itself.  Startup failure IDLE uses a socket to communicate between the IDLE GUI process and the user code execution process. A connection must be established whenever the Shell starts or restarts. (The latter is indicated by a divider line that says \u2018RESTART\u2019). If the user process fails to connect to the GUI process, it usually displays a Tk error box with a \u2018cannot connect\u2019 message that directs the user here. It then exits. One specific connection failure on Unix systems results from misconfigured masquerading rules somewhere in a system\u2019s network setup. When IDLE is started from a terminal, one will see a message starting with ** Invalid host:. The valid value is 127.0.0.1 (idlelib.rpc.LOCALHOST). One can diagnose with tcpconnect -irv 127.0.0.1 6543 in one terminal window and tcplisten <same args> in another. A common cause of failure is a user-written file with the same name as a standard library module, such as random.py and tkinter.py. When such a file is located in the same directory as a file that is about to be run, IDLE cannot import the stdlib file. The current fix is to rename the user file. Though less common than in the past, an antivirus or firewall program may stop the connection. If the program cannot be taught to allow the connection, then it must be turned off for IDLE to work. It is safe to allow this internal connection because no data is visible on external ports. A similar problem is a network mis-configuration that blocks connections. Python installation issues occasionally stop IDLE: multiple versions can clash, or a single installation might need admin access. If one undo the clash, or cannot or does not want to run as admin, it might be easiest to completely remove Python and start over. A zombie pythonw.exe process could be a problem. On Windows, use Task Manager to check for one and stop it if there is. Sometimes a restart initiated by a program crash or Keyboard Interrupt (control-C) may fail to connect. Dismissing the error box or using Restart Shell on the Shell menu may fix a temporary problem. When IDLE first starts, it attempts to read user configuration files in ~\/.idlerc\/ (~ is one\u2019s home directory). If there is a problem, an error message should be displayed. Leaving aside random disk glitches, this can be prevented by never editing the files by hand. Instead, use the configuration dialog, under Options. Once there is an error in a user configuration file, the best solution may be to delete it and start over with the settings dialog. If IDLE quits with no message, and it was not started from a console, try starting it from a console or terminal (python -m idlelib) and see if this results in an error message. On Unix-based systems with tcl\/tk older than 8.6.11 (see About IDLE) certain characters of certain fonts can cause a tk failure with a message to the terminal. This can happen either if one starts IDLE to edit a file with such a character or later when entering such a character. If one cannot upgrade tcl\/tk, then re-configure IDLE to use a font that works better. Running user code With rare exceptions, the result of executing Python code with IDLE is intended to be the same as executing the same code by the default method, directly with Python in a text-mode system console or terminal window. However, the different interface and operation occasionally affect visible results. For instance, sys.modules starts with more entries, and threading.active_count() returns 2 instead of 1. By default, IDLE runs user code in a separate OS process rather than in the user interface process that runs the shell and editor. In the execution process, it replaces sys.stdin, sys.stdout, and sys.stderr with objects that get input from and send output to the Shell window. The original values stored in sys.__stdin__, sys.__stdout__, and sys.__stderr__ are not touched, but may be None. Sending print output from one process to a text widget in another is slower than printing to a system terminal in the same process. This has the most effect when printing multiple arguments, as the string for each argument, each separator, the newline are sent separately. For development, this is usually not a problem, but if one wants to print faster in IDLE, format and join together everything one wants displayed together and then print a single string. Both format strings and str.join() can help combine fields and lines. IDLE\u2019s standard stream replacements are not inherited by subprocesses created in the execution process, whether directly by user code or by modules such as multiprocessing. If such subprocess use input from sys.stdin or print or write to sys.stdout or sys.stderr, IDLE should be started in a command line window. The secondary subprocess will then be attached to that window for input and output. If sys is reset by user code, such as with importlib.reload(sys), IDLE\u2019s changes are lost and input from the keyboard and output to the screen will not work correctly. When Shell has the focus, it controls the keyboard and screen. This is normally transparent, but functions that directly access the keyboard and screen will not work. These include system-specific functions that determine whether a key has been pressed and if so, which. The IDLE code running in the execution process adds frames to the call stack that would not be there otherwise. IDLE wraps sys.getrecursionlimit and sys.setrecursionlimit to reduce the effect of the additional stack frames. When user code raises SystemExit either directly or by calling sys.exit, IDLE returns to a Shell prompt instead of exiting. User output in Shell When a program outputs text, the result is determined by the corresponding output device. When IDLE executes user code, sys.stdout and sys.stderr are connected to the display area of IDLE\u2019s Shell. Some of its features are inherited from the underlying Tk Text widget. Others are programmed additions. Where it matters, Shell is designed for development rather than production runs. For instance, Shell never throws away output. A program that sends unlimited output to Shell will eventually fill memory, resulting in a memory error. In contrast, some system text windows only keep the last n lines of output. A Windows console, for instance, keeps a user-settable 1 to 9999 lines, with 300 the default. A Tk Text widget, and hence IDLE\u2019s Shell, displays characters (codepoints) in the BMP (Basic Multilingual Plane) subset of Unicode. Which characters are displayed with a proper glyph and which with a replacement box depends on the operating system and installed fonts. Tab characters cause the following text to begin after the next tab stop. (They occur every 8 \u2018characters\u2019). Newline characters cause following text to appear on a new line. Other control characters are ignored or displayed as a space, box, or something else, depending on the operating system and font. (Moving the text cursor through such output with arrow keys may exhibit some surprising spacing behavior.) >>> s = 'a\\tb\\a<\\x02><\\r>\\bc\\nd'  # Enter 22 chars.\n>>> len(s)\n14\n>>> s  # Display repr(s)\n'a\\tb\\x07<\\x02><\\r>\\x08c\\nd'\n>>> print(s, end='')  # Display s as is.\n# Result varies by OS and font.  Try it.\n The repr function is used for interactive echo of expression values. It returns an altered version of the input string in which control codes, some BMP codepoints, and all non-BMP codepoints are replaced with escape codes. As demonstrated above, it allows one to identify the characters in a string, regardless of how they are displayed. Normal and error output are generally kept separate (on separate lines) from code input and each other. They each get different highlight colors. For SyntaxError tracebacks, the normal \u2018^\u2019 marking where the error was detected is replaced by coloring the text with an error highlight. When code run from a file causes other exceptions, one may right click on a traceback line to jump to the corresponding line in an IDLE editor. The file will be opened if necessary. Shell has a special facility for squeezing output lines down to a \u2018Squeezed text\u2019 label. This is done automatically for output over N lines (N = 50 by default). N can be changed in the PyShell section of the General page of the Settings dialog. Output with fewer lines can be squeezed by right clicking on the output. This can be useful lines long enough to slow down scrolling. Squeezed output is expanded in place by double-clicking the label. It can also be sent to the clipboard or a separate view window by right-clicking the label. Developing tkinter applications IDLE is intentionally different from standard Python in order to facilitate development of tkinter programs. Enter import tkinter as tk;\nroot = tk.Tk() in standard Python and nothing appears. Enter the same in IDLE and a tk window appears. In standard Python, one must also enter root.update() to see the window. IDLE does the equivalent in the background, about 20 times a second, which is about every 50 milliseconds. Next enter b = tk.Button(root, text='button'); b.pack(). Again, nothing visibly changes in standard Python until one enters root.update(). Most tkinter programs run root.mainloop(), which usually does not return until the tk app is destroyed. If the program is run with python -i or from an IDLE editor, a >>> shell prompt does not appear until mainloop() returns, at which time there is nothing left to interact with. When running a tkinter program from an IDLE editor, one can comment out the mainloop call. One then gets a shell prompt immediately and can interact with the live application. One just has to remember to re-enable the mainloop call when running in standard Python. Running without a subprocess By default, IDLE executes user code in a separate subprocess via a socket, which uses the internal loopback interface. This connection is not externally visible and no data is sent to or received from the Internet. If firewall software complains anyway, you can ignore it. If the attempt to make the socket connection fails, Idle will notify you. Such failures are sometimes transient, but if persistent, the problem may be either a firewall blocking the connection or misconfiguration of a particular system. Until the problem is fixed, one can run Idle with the -n command line switch. If IDLE is started with the -n command line switch it will run in a single process and will not create the subprocess which runs the RPC Python execution server. This can be useful if Python cannot create the subprocess or the RPC socket interface on your platform. However, in this mode user code is not isolated from IDLE itself. Also, the environment is not restarted when Run\/Run Module (F5) is selected. If your code has been modified, you must reload() the affected modules and re-import any specific items (e.g. from foo import baz) if the changes are to take effect. For these reasons, it is preferable to run IDLE with the default subprocess if at all possible.  Deprecated since version 3.4.  Help and preferences Help sources Help menu entry \u201cIDLE Help\u201d displays a formatted html version of the IDLE chapter of the Library Reference. The result, in a read-only tkinter text window, is close to what one sees in a web browser. Navigate through the text with a mousewheel, the scrollbar, or up and down arrow keys held down. Or click the TOC (Table of Contents) button and select a section header in the opened box. Help menu entry \u201cPython Docs\u201d opens the extensive sources of help, including tutorials, available at docs.python.org\/x.y, where \u2018x.y\u2019 is the currently running Python version. If your system has an off-line copy of the docs (this may be an installation option), that will be opened instead. Selected URLs can be added or removed from the help menu at any time using the General tab of the Configure IDLE dialog. Setting preferences The font preferences, highlighting, keys, and general preferences can be changed via Configure IDLE on the Option menu. Non-default user settings are saved in a .idlerc directory in the user\u2019s home directory. Problems caused by bad user configuration files are solved by editing or deleting one or more of the files in .idlerc. On the Font tab, see the text sample for the effect of font face and size on multiple characters in multiple languages. Edit the sample to add other characters of personal interest. Use the sample to select monospaced fonts. If particular characters have problems in Shell or an editor, add them to the top of the sample and try changing first size and then font. On the Highlights and Keys tab, select a built-in or custom color theme and key set. To use a newer built-in color theme or key set with older IDLEs, save it as a new custom theme or key set and it well be accessible to older IDLEs. IDLE on macOS Under System Preferences: Dock, one can set \u201cPrefer tabs when opening documents\u201d to \u201cAlways\u201d. This setting is not compatible with the tk\/tkinter GUI framework used by IDLE, and it breaks a few IDLE features. Extensions IDLE contains an extension facility. Preferences for extensions can be changed with the Extensions tab of the preferences dialog. See the beginning of config-extensions.def in the idlelib directory for further information. The only current default extension is zzdummy, an example also used for testing.","title":"python.library.idle"},{"text":"coroutine asyncio.sleep(delay, result=None, *, loop=None)  \nBlock for delay seconds. If result is provided, it is returned to the caller when the coroutine completes. sleep() always suspends the current task, allowing other tasks to run.  Deprecated since version 3.8, will be removed in version 3.10: The loop parameter.  Example of coroutine displaying the current date every second for 5 seconds: import asyncio\nimport datetime\n\nasync def display_date():\n    loop = asyncio.get_running_loop()\n    end_time = loop.time() + 5.0\n    while True:\n        print(datetime.datetime.now())\n        if (loop.time() + 1.0) >= end_time:\n            break\n        await asyncio.sleep(1)\n\nasyncio.run(display_date())","title":"python.library.asyncio-task#asyncio.sleep"},{"text":"iterator.__next__()  \nReturn the next item from the container. If there are no further items, raise the StopIteration exception. This method corresponds to the tp_iternext slot of the type structure for Python objects in the Python\/C API.","title":"python.library.stdtypes#iterator.__next__"},{"text":"pty \u2014 Pseudo-terminal utilities Source code: Lib\/pty.py The pty module defines operations for handling the pseudo-terminal concept: starting another process and being able to write to and read from its controlling terminal programmatically. Because pseudo-terminal handling is highly platform dependent, there is code to do it only for Linux. (The Linux code is supposed to work on other platforms, but hasn\u2019t been tested yet.) The pty module defines the following functions:  \npty.fork()  \nFork. Connect the child\u2019s controlling terminal to a pseudo-terminal. Return value is (pid, fd). Note that the child gets pid 0, and the fd is invalid. The parent\u2019s return value is the pid of the child, and fd is a file descriptor connected to the child\u2019s controlling terminal (and also to the child\u2019s standard input and output). \n  \npty.openpty()  \nOpen a new pseudo-terminal pair, using os.openpty() if possible, or emulation code for generic Unix systems. Return a pair of file descriptors (master, slave), for the master and the slave end, respectively. \n  \npty.spawn(argv[, master_read[, stdin_read]])  \nSpawn a process, and connect its controlling terminal with the current process\u2019s standard io. This is often used to baffle programs which insist on reading from the controlling terminal. It is expected that the process spawned behind the pty will eventually terminate, and when it does spawn will return. The functions master_read and stdin_read are passed a file descriptor which they should read from, and they should always return a byte string. In order to force spawn to return before the child process exits an OSError should be thrown. The default implementation for both functions will read and return up to 1024 bytes each time the function is called. The master_read callback is passed the pseudoterminal\u2019s master file descriptor to read output from the child process, and stdin_read is passed file descriptor 0, to read from the parent process\u2019s standard input. Returning an empty byte string from either callback is interpreted as an end-of-file (EOF) condition, and that callback will not be called after that. If stdin_read signals EOF the controlling terminal can no longer communicate with the parent process OR the child process. Unless the child process will quit without any input, spawn will then loop forever. If master_read signals EOF the same behavior results (on linux at least). If both callbacks signal EOF then spawn will probably never return, unless select throws an error on your platform when passed three empty lists. This is a bug, documented in issue 26228. Return the exit status value from os.waitpid() on the child process. waitstatus_to_exitcode() can be used to convert the exit status into an exit code. Raises an auditing event pty.spawn with argument argv.  Changed in version 3.4: spawn() now returns the status value from os.waitpid() on the child process.  \n Example The following program acts like the Unix command script(1), using a pseudo-terminal to record all input and output of a terminal session in a \u201ctypescript\u201d. import argparse\nimport os\nimport pty\nimport sys\nimport time\n\nparser = argparse.ArgumentParser()\nparser.add_argument('-a', dest='append', action='store_true')\nparser.add_argument('-p', dest='use_python', action='store_true')\nparser.add_argument('filename', nargs='?', default='typescript')\noptions = parser.parse_args()\n\nshell = sys.executable if options.use_python else os.environ.get('SHELL', 'sh')\nfilename = options.filename\nmode = 'ab' if options.append else 'wb'\n\nwith open(filename, mode) as script:\n    def read(fd):\n        data = os.read(fd, 1024)\n        script.write(data)\n        return data\n\n    print('Script started, file is', filename)\n    script.write(('Script started on %s\\n' % time.asctime()).encode())\n\n    pty.spawn(shell, read)\n\n    script.write(('Script done on %s\\n' % time.asctime()).encode())\n    print('Script done, file is', filename)","title":"python.library.pty"},{"text":"tf.raw_ops.ExperimentalSleepDataset  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.raw_ops.ExperimentalSleepDataset  \ntf.raw_ops.ExperimentalSleepDataset(\n    input_dataset, sleep_microseconds, output_types, output_shapes, name=None\n)\n\n \n\n\n Args\n  input_dataset   A Tensor of type variant.  \n  sleep_microseconds   A Tensor of type int64.  \n  output_types   A list of tf.DTypes that has length >= 1.  \n  output_shapes   A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor of type variant.","title":"tensorflow.raw_ops.experimentalsleepdataset"},{"text":"ZipFile.setpassword(pwd)  \nSet pwd as default password to extract encrypted files.","title":"python.library.zipfile#zipfile.ZipFile.setpassword"}]}
{"task_id":44778,"prompt":"def f_44778(l):\n\treturn ","suffix":"","canonical_solution":"\"\"\",\"\"\".join(l)","test_start":"\ndef check(candidate):","test":["\n    assert candidate(['a','b','c']) == 'a,b,c'\n","\n    assert candidate(['a','b']) == 'a,b'\n","\n    assert candidate([',',',',',']) == ',,,,,'\n","\n    assert candidate([' ','  ','c']) == ' ,  ,c'\n","\n    assert candidate([]) == ''\n"],"entry_point":"f_44778","intent":"Join elements of list `l` with a comma `,`","library":[],"docs":[]}
{"task_id":44778,"prompt":"def f_44778(myList):\n\t","suffix":"\n\treturn myList","canonical_solution":"myList = ','.join(map(str, myList))","test_start":"\ndef check(candidate):","test":["\n    assert candidate([1,2,3]) == '1,2,3'\n","\n    assert candidate([1,2,'a']) == '1,2,a'\n","\n    assert candidate([]) == ''\n","\n    assert candidate(['frg',3253]) == 'frg,3253'\n"],"entry_point":"f_44778","intent":"make a comma-separated string from a list `myList`","library":[],"docs":[]}
{"task_id":7286365,"prompt":"def f_7286365():\n\treturn ","suffix":"","canonical_solution":"list(reversed(list(range(10))))","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == [9,8,7,6,5,4,3,2,1,0]\n","\n    assert len(candidate()) == 10\n","\n    assert min(candidate()) == 0\n","\n    assert type(candidate()) == list\n","\n    assert type(candidate()[-2]) == int\n"],"entry_point":"f_7286365","intent":"reverse the list that contains 1 to 10","library":[],"docs":[]}
{"task_id":18454570,"prompt":"def f_18454570():\n\treturn ","suffix":"","canonical_solution":"'lamp, bag, mirror'.replace('bag,', '')","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == 'lamp,  mirror'\n    assert type(candidate()) == str\n    assert len(candidate()) == 13\n    assert candidate().startswith('lamp')\n"],"entry_point":"f_18454570","intent":"remove substring 'bag,' from a string 'lamp, bag, mirror'","library":[],"docs":[]}
{"task_id":4357787,"prompt":"def f_4357787(s):\n\treturn ","suffix":"","canonical_solution":"\"\"\".\"\"\".join(s.split('.')[::-1])","test_start":"\ndef check(candidate):","test":["\n    assert candidate('apple.orange.red.green.yellow') == 'yellow.green.red.orange.apple'\n","\n    assert candidate('apple') == 'apple'\n","\n    assert candidate('apple.orange') == 'orange.apple'\n","\n    assert candidate('123.456') == '456.123'\n","\n    assert candidate('.') == '.'\n"],"entry_point":"f_4357787","intent":"Reverse the order of words, delimited by `.`, in string `s`","library":[],"docs":[]}
{"task_id":21787496,"prompt":"def f_21787496(s):\n\treturn ","suffix":"","canonical_solution":"datetime.datetime.fromtimestamp(s).strftime('%Y-%m-%d %H:%M:%S.%f')","test_start":"\nimport time\nimport datetime\n\ndef check(candidate): ","test":["\n    assert candidate(1236472) == '1970-01-15 07:27:52.000000'\n","\n    assert candidate(0) == '1970-01-01 00:00:00.000000'\n","\n    assert candidate(5.3) == '1970-01-01 00:00:05.300000'\n"],"entry_point":"f_21787496","intent":"convert epoch time represented as milliseconds `s` to string using format '%Y-%m-%d %H:%M:%S.%f'","library":["datetime","time"],"docs":[{"text":"time.ctime([secs])  \nConvert a time expressed in seconds since the epoch to a string of a form: 'Sun Jun 20 23:21:05 1993' representing local time. The day field is two characters long and is space padded if the day is a single digit, e.g.: 'Wed Jun\u00a0 9 04:26:40 1993'. If secs is not provided or None, the current time as returned by time() is used. ctime(secs) is equivalent to asctime(localtime(secs)). Locale information is not used by ctime().","title":"python.library.time#time.ctime"},{"text":"http_date(epoch_seconds=None) [source]\n \nFormats the time to match the RFC 1123#section-5.2.14 date format as specified by HTTP RFC 7231#section-7.1.1.1. Accepts a floating point number expressed in seconds since the epoch in UTC\u2013such as that outputted by time.time(). If set to None, defaults to the current time. Outputs a string in the format Wdy, DD Mon YYYY HH:MM:SS GMT.","title":"django.ref.utils#django.utils.http.http_date"},{"text":"pandas.Timedelta.isoformat   Timedelta.isoformat()\n \nFormat Timedelta as ISO 8601 Duration like P[n]Y[n]M[n]DT[n]H[n]M[n]S, where the [n] s are replaced by the values. See https:\/\/en.wikipedia.org\/wiki\/ISO_8601#Durations.  Returns \n str\n    See also  Timestamp.isoformat\n\nFunction is used to convert the given Timestamp object into the ISO format.    Notes The longest component is days, whose value may be larger than 365. Every component is always included, even if its value is 0. Pandas uses nanosecond precision, so up to 9 decimal places may be included in the seconds component. Trailing 0\u2019s are removed from the seconds component after the decimal. We do not 0 pad components, so it\u2019s \u2026T5H\u2026, not \u2026T05H\u2026 Examples \n>>> td = pd.Timedelta(days=6, minutes=50, seconds=3,\n...                   milliseconds=10, microseconds=10, nanoseconds=12)\n  \n>>> td.isoformat()\n'P6DT0H50M3.010010012S'\n>>> pd.Timedelta(hours=1, seconds=10).isoformat()\n'P0DT1H0M10S'\n>>> pd.Timedelta(days=500.5).isoformat()\n'P500DT12H0M0S'","title":"pandas.reference.api.pandas.timedelta.isoformat"},{"text":"shlex.quote(s)  \nReturn a shell-escaped version of the string s. The returned value is a string that can safely be used as one token in a shell command line, for cases where you cannot use a list. This idiom would be unsafe: >>> filename = 'somefile; rm -rf ~'\n>>> command = 'ls -l {}'.format(filename)\n>>> print(command)  # executed by a shell: boom!\nls -l somefile; rm -rf ~\n quote() lets you plug the security hole: >>> from shlex import quote\n>>> command = 'ls -l {}'.format(quote(filename))\n>>> print(command)\nls -l 'somefile; rm -rf ~'\n>>> remote_command = 'ssh home {}'.format(quote(command))\n>>> print(remote_command)\nssh home 'ls -l '\"'\"'somefile; rm -rf ~'\"'\"''\n The quoting is compatible with UNIX shells and with split(): >>> from shlex import split\n>>> remote_command = split(remote_command)\n>>> remote_command\n['ssh', 'home', \"ls -l 'somefile; rm -rf ~'\"]\n>>> command = split(remote_command[-1])\n>>> command\n['ls', '-l', 'somefile; rm -rf ~']\n  New in version 3.3.","title":"python.library.shlex#shlex.quote"},{"text":"flask.escape()  \nReplace the characters &, <, >, ', and \" in the string with HTML-safe sequences. Use this if you need to display text that might contain such characters in HTML. If the object has an __html__ method, it is called and the return value is assumed to already be safe for HTML.  Parameters \ns \u2013 An object to be converted to a string and escaped.  Returns \nA Markup string with the escaped text.","title":"flask.api.index#flask.escape"},{"text":"time.isoformat(timespec='auto')  \nReturn a string representing the time in ISO 8601 format, one of:  \nHH:MM:SS.ffffff, if microsecond is not 0 \nHH:MM:SS, if microsecond is 0 \nHH:MM:SS.ffffff+HH:MM[:SS[.ffffff]], if utcoffset() does not return None\n \nHH:MM:SS+HH:MM[:SS[.ffffff]], if microsecond is 0 and utcoffset() does not return None\n  The optional argument timespec specifies the number of additional components of the time to include (the default is 'auto'). It can be one of the following:  \n'auto': Same as 'seconds' if microsecond is 0, same as 'microseconds' otherwise. \n'hours': Include the hour in the two-digit HH format. \n'minutes': Include hour and minute in HH:MM format. \n'seconds': Include hour, minute, and second in HH:MM:SS format. \n'milliseconds': Include full time, but truncate fractional second part to milliseconds. HH:MM:SS.sss format. \n'microseconds': Include full time in HH:MM:SS.ffffff format.   Note Excluded time components are truncated, not rounded.  ValueError will be raised on an invalid timespec argument. Example: >>> from datetime import time\n>>> time(hour=12, minute=34, second=56, microsecond=123456).isoformat(timespec='minutes')\n'12:34'\n>>> dt = time(hour=12, minute=34, second=56, microsecond=0)\n>>> dt.isoformat(timespec='microseconds')\n'12:34:56.000000'\n>>> dt.isoformat(timespec='auto')\n'12:34:56'\n  New in version 3.6: Added the timespec argument.","title":"python.library.datetime#datetime.time.isoformat"},{"text":"torch.fft.irfft2(input, s=None, dim=(-2, -1), norm=None) \u2192 Tensor  \nComputes the inverse of rfft2(). Equivalent to irfftn() but IFFTs only the last two dimensions by default. input is interpreted as a one-sided Hermitian signal in the Fourier domain, as produced by rfft2(). By the Hermitian property, the output will be real-valued.  Note Some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in the zero-frequency term cannot be represented in a real output and so will always be ignored.   Note The correct interpretation of the Hermitian input depends on the length of the original data, as given by s. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal shape s.   Parameters \n \ninput (Tensor) \u2013 the input tensor \ns (Tuple[int], optional) \u2013 Signal size in the transformed dimensions. If given, each dimension dim[i] will either be zero-padded or trimmed to the length s[i] before computing the real FFT. If a length -1 is specified, no padding is done in that dimension. Defaults to even output in the last dimension: s[-1] = 2*(input.size(dim[-1]) - 1). \ndim (Tuple[int], optional) \u2013 Dimensions to be transformed. The last dimension must be the half-Hermitian compressed dimension. Default: last two dimensions. \nnorm (str, optional) \u2013 \nNormalization mode. For the backward transform (irfft2()), these correspond to:  \n\"forward\" - no normalization \n\"backward\" - normalize by 1\/n\n \n\"ortho\" - normalize by 1\/sqrt(n) (making the real IFFT orthonormal)  Where n = prod(s) is the logical IFFT size. Calling the forward transform (rfft2()) with the same normalization mode will apply an overall normalization of 1\/n between the two transforms. This is required to make irfft2() the exact inverse. Default is \"backward\" (normalize by 1\/n).     Example >>> t = torch.rand(10, 9)\n>>> T = torch.fft.rfft2(t)\n Without specifying the output length to irfft2(), the output will not round-trip properly because the input is odd-length in the last dimension: >>> torch.fft.irfft2(T).size()\ntorch.Size([10, 10])\n So, it is recommended to always pass the signal shape s. >>> roundtrip = torch.fft.irfft2(T, t.size())\n>>> roundtrip.size()\ntorch.Size([10, 9])\n>>> torch.allclose(roundtrip, t)\nTrue","title":"torch.fft#torch.fft.irfft2"},{"text":"pandas.Timestamp.isoformat   Timestamp.isoformat()\n \nReturn the time formatted according to ISO 8610. The full format looks like \u2018YYYY-MM-DD HH:MM:SS.mmmmmmnnn\u2019. By default, the fractional part is omitted if self.microsecond == 0 and self.nanosecond == 0. If self.tzinfo is not None, the UTC offset is also attached, giving giving a full format of \u2018YYYY-MM-DD HH:MM:SS.mmmmmmnnn+HH:MM\u2019.  Parameters \n \nsep:str, default \u2018T\u2019\n\n\nString used as the separator between the date and time.  \ntimespec:str, default \u2018auto\u2019\n\n\nSpecifies the number of additional terms of the time to include. The valid values are \u2018auto\u2019, \u2018hours\u2019, \u2018minutes\u2019, \u2018seconds\u2019, \u2018milliseconds\u2019, \u2018microseconds\u2019, and \u2018nanoseconds\u2019.    Returns \n str\n   Examples \n>>> ts = pd.Timestamp('2020-03-14T15:32:52.192548651')\n>>> ts.isoformat()\n'2020-03-14T15:32:52.192548651'\n>>> ts.isoformat(timespec='microseconds')\n'2020-03-14T15:32:52.192548'","title":"pandas.reference.api.pandas.timestamp.isoformat"},{"text":"pandas.Timestamp.strftime   Timestamp.strftime(format)\n \nReturn a string representing the given POSIX timestamp controlled by an explicit format string.  Parameters \n \nformat:str\n\n\nFormat string to convert Timestamp to string. See strftime documentation for more information on the format string: https:\/\/docs.python.org\/3\/library\/datetime.html#strftime-and-strptime-behavior.     Examples \n>>> ts = pd.Timestamp('2020-03-14T15:32:52.192548651')\n>>> ts.strftime('%Y-%m-%d %X')\n'2020-03-14 15:32:52'","title":"pandas.reference.api.pandas.timestamp.strftime"},{"text":"torch.fft.rfft2(input, s=None, dim=(-2, -1), norm=None) \u2192 Tensor  \nComputes the 2-dimensional discrete Fourier transform of real input. Equivalent to rfftn() but FFTs only the last two dimensions by default. The FFT of a real signal is Hermitian-symmetric, X[i, j] = conj(X[-i, -j]), so the full fft2() output contains redundant information. rfft2() instead omits the negative frequencies in the last dimension.  Parameters \n \ninput (Tensor) \u2013 the input tensor \ns (Tuple[int], optional) \u2013 Signal size in the transformed dimensions. If given, each dimension dim[i] will either be zero-padded or trimmed to the length s[i] before computing the real FFT. If a length -1 is specified, no padding is done in that dimension. Default: s = [input.size(d) for d in dim]\n \ndim (Tuple[int], optional) \u2013 Dimensions to be transformed. Default: last two dimensions. \nnorm (str, optional) \u2013 \nNormalization mode. For the forward transform (rfft2()), these correspond to:  \n\"forward\" - normalize by 1\/n\n \n\"backward\" - no normalization \n\"ortho\" - normalize by 1\/sqrt(n) (making the real FFT orthonormal)  Where n = prod(s) is the logical FFT size. Calling the backward transform (irfft2()) with the same normalization mode will apply an overall normalization of 1\/n between the two transforms. This is required to make irfft2() the exact inverse. Default is \"backward\" (no normalization).     Example >>> t = torch.rand(10, 10)\n>>> rfft2 = torch.fft.rfft2(t)\n>>> rfft2.size()\ntorch.Size([10, 6])\n Compared against the full output from fft2(), we have all elements up to the Nyquist frequency. >>> fft2 = torch.fft.fft2(t)\n>>> torch.allclose(fft2[..., :6], rfft2)\nTrue\n The discrete Fourier transform is separable, so rfft2() here is equivalent to a combination of fft() and rfft(): >>> two_ffts = torch.fft.fft(torch.fft.rfft(x, dim=1), dim=0)\n>>> torch.allclose(rfft2, two_ffts)","title":"torch.fft#torch.fft.rfft2"}]}
{"task_id":21787496,"prompt":"def f_21787496():\n\treturn ","suffix":"","canonical_solution":"time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime(1236472051807 \/ 1000.0))","test_start":"\nimport time\n\ndef check(candidate): ","test":["\n    assert candidate() == '2009-03-08 00:27:31'\n"],"entry_point":"f_21787496","intent":"parse milliseconds epoch time '1236472051807' to format '%Y-%m-%d %H:%M:%S'","library":["time"],"docs":[{"text":"http_date(epoch_seconds=None) [source]\n \nFormats the time to match the RFC 1123#section-5.2.14 date format as specified by HTTP RFC 7231#section-7.1.1.1. Accepts a floating point number expressed in seconds since the epoch in UTC\u2013such as that outputted by time.time(). If set to None, defaults to the current time. Outputs a string in the format Wdy, DD Mon YYYY HH:MM:SS GMT.","title":"django.ref.utils#django.utils.http.http_date"},{"text":"time.asctime([t])  \nConvert a tuple or struct_time representing a time as returned by gmtime() or localtime() to a string of the following form: 'Sun Jun 20 23:21:05 1993'. The day field is two characters long and is space padded if the day is a single digit, e.g.: 'Wed Jun\u00a0 9 04:26:40 1993'. If t is not provided, the current time as returned by localtime() is used. Locale information is not used by asctime().  Note Unlike the C function of the same name, asctime() does not add a trailing newline.","title":"python.library.time#time.asctime"},{"text":"time.ctime([secs])  \nConvert a time expressed in seconds since the epoch to a string of a form: 'Sun Jun 20 23:21:05 1993' representing local time. The day field is two characters long and is space padded if the day is a single digit, e.g.: 'Wed Jun\u00a0 9 04:26:40 1993'. If secs is not provided or None, the current time as returned by time() is used. ctime(secs) is equivalent to asctime(localtime(secs)). Locale information is not used by ctime().","title":"python.library.time#time.ctime"},{"text":"skimage.color.separate_stains(rgb, conv_matrix) [source]\n \nRGB to stain color space conversion.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.  conv_matrix: ndarray\n\nThe stain separation matrix as described by G. Landini [1].    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in stain color space. Same dimensions as input.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     Notes Stain separation matrices available in the color module and their respective colorspace:  \nhed_from_rgb: Hematoxylin + Eosin + DAB \nhdx_from_rgb: Hematoxylin + DAB \nfgx_from_rgb: Feulgen + Light Green \nbex_from_rgb: Giemsa stain : Methyl Blue + Eosin \nrbd_from_rgb: FastRed + FastBlue + DAB \ngdx_from_rgb: Methyl Green + DAB \nhax_from_rgb: Hematoxylin + AEC \nbro_from_rgb: Blue matrix Anilline Blue + Red matrix Azocarmine + Orange matrix Orange-G \nbpx_from_rgb: Methyl Blue + Ponceau Fuchsin \nahx_from_rgb: Alcian Blue + Hematoxylin \nhpx_from_rgb: Hematoxylin + PAS  This implementation borrows some ideas from DIPlib [2], e.g. the compensation using a small value to avoid log artifacts when calculating the Beer-Lambert law. References  \n1  \nhttps:\/\/web.archive.org\/web\/20160624145052\/http:\/\/www.mecourse.com\/landinig\/software\/cdeconv\/cdeconv.html  \n2  \nhttps:\/\/github.com\/DIPlib\/diplib\/  \n3  \nA. C. Ruifrok and D. A. Johnston, \u201cQuantification of histochemical staining by color deconvolution,\u201d Anal. Quant. Cytol. Histol., vol. 23, no. 4, pp. 291\u2013299, Aug. 2001.   Examples >>> from skimage import data\n>>> from skimage.color import separate_stains, hdx_from_rgb\n>>> ihc = data.immunohistochemistry()\n>>> ihc_hdx = separate_stains(ihc, hdx_from_rgb)","title":"skimage.api.skimage.color#skimage.color.separate_stains"},{"text":"stat.IO_REPARSE_TAG_SYMLINK  \nstat.IO_REPARSE_TAG_MOUNT_POINT  \nstat.IO_REPARSE_TAG_APPEXECLINK  \n New in version 3.8.","title":"python.library.stat#stat.IO_REPARSE_TAG_MOUNT_POINT"},{"text":"entropy() [source]","title":"torch.distributions#torch.distributions.multivariate_normal.MultivariateNormal.entropy"},{"text":"stat.ST_MTIME  \nTime of last modification.","title":"python.library.stat#stat.ST_MTIME"},{"text":"entropy() [source]","title":"torch.distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.entropy"},{"text":"matplotlib.dates.set_epoch(epoch)[source]\n \nSet the epoch (origin for dates) for datetime calculations. The default epoch is rcParams[\"dates.epoch\"] (by default 1970-01-01T00:00). If microsecond accuracy is desired, the date being plotted needs to be within approximately 70 years of the epoch. Matplotlib internally represents dates as days since the epoch, so floating point dynamic range needs to be within a factor of 2^52. set_epoch must be called before any dates are converted (i.e. near the import section) or a RuntimeError will be raised. See also Date Precision and Epochs.  Parameters \n \nepochstr\n\n\nvalid UTC date parsable by numpy.datetime64 (do not include timezone).","title":"matplotlib.dates_api#matplotlib.dates.set_epoch"},{"text":"stat.ST_ATIME  \nTime of last access.","title":"python.library.stat#stat.ST_ATIME"}]}
{"task_id":20573459,"prompt":"def f_20573459():\n\treturn ","suffix":"","canonical_solution":"(datetime.datetime.now() - datetime.timedelta(days=7)).date()","test_start":"\nimport datetime\n\ndef check(candidate): ","test":["\n    assert datetime.datetime.now().date() - candidate() < datetime.timedelta(days = 7, seconds = 1)\n","\n    assert datetime.datetime.now().date() - candidate() >= datetime.timedelta(days = 7)\n"],"entry_point":"f_20573459","intent":"get the date 7 days before the current date","library":["datetime"],"docs":[{"text":"get_prev_week(date)  \nReturns a date object containing the first day of the week before the date provided. This function can also return None or raise an Http404 exception, depending on the values of allow_empty and allow_future.","title":"django.ref.class-based-views.mixins-date-based#django.views.generic.dates.WeekMixin.get_prev_week"},{"text":"get_next_week(date)  \nReturns a date object containing the first day of the week after the date provided. This function can also return None or raise an Http404 exception, depending on the values of allow_empty and allow_future.","title":"django.ref.class-based-views.mixins-date-based#django.views.generic.dates.WeekMixin.get_next_week"},{"text":"get_previous_day(date)  \nReturns a date object containing the previous valid day. This function can also return None or raise an Http404 exception, depending on the values of allow_empty and allow_future.","title":"django.ref.class-based-views.mixins-date-based#django.views.generic.dates.DayMixin.get_previous_day"},{"text":"str.removeprefix(prefix, \/)  \nIf the string starts with the prefix string, return string[len(prefix):]. Otherwise, return a copy of the original string: >>> 'TestHook'.removeprefix('Test')\n'Hook'\n>>> 'BaseTestCase'.removeprefix('Test')\n'BaseTestCase'\n  New in version 3.9.","title":"python.library.stdtypes#str.removeprefix"},{"text":"pandas.DataFrame.plot   DataFrame.plot(*args, **kwargs)[source]\n \nMake plots of Series or DataFrame. Uses the backend specified by the option plotting.backend. By default, matplotlib is used.  Parameters \n \ndata:Series or DataFrame\n\n\nThe object for which the method is called.  \nx:label or position, default None\n\n\nOnly used if data is a DataFrame.  \ny:label, position or list of label, positions, default None\n\n\nAllows plotting of one column versus another. Only used if data is a DataFrame.  \nkind:str\n\n\nThe kind of plot to produce:  \u2018line\u2019 : line plot (default) \u2018bar\u2019 : vertical bar plot \u2018barh\u2019 : horizontal bar plot \u2018hist\u2019 : histogram \u2018box\u2019 : boxplot \u2018kde\u2019 : Kernel Density Estimation plot \u2018density\u2019 : same as \u2018kde\u2019 \u2018area\u2019 : area plot \u2018pie\u2019 : pie plot \u2018scatter\u2019 : scatter plot (DataFrame only) \u2018hexbin\u2019 : hexbin plot (DataFrame only)   \nax:matplotlib axes object, default None\n\n\nAn axes of the current figure.  \nsubplots:bool, default False\n\n\nMake separate subplots for each column.  \nsharex:bool, default True if ax is None else False\n\n\nIn case subplots=True, share x axis and set some x axis labels to invisible; defaults to True if ax is None otherwise False if an ax is passed in; Be aware, that passing in both an ax and sharex=True will alter all x axis labels for all axis in a figure.  \nsharey:bool, default False\n\n\nIn case subplots=True, share y axis and set some y axis labels to invisible.  \nlayout:tuple, optional\n\n\n(rows, columns) for the layout of subplots.  \nfigsize:a tuple (width, height) in inches\n\n\nSize of a figure object.  \nuse_index:bool, default True\n\n\nUse index as ticks for x axis.  \ntitle:str or list\n\n\nTitle to use for the plot. If a string is passed, print the string at the top of the figure. If a list is passed and subplots is True, print each item in the list above the corresponding subplot.  \ngrid:bool, default None (matlab style default)\n\n\nAxis grid lines.  \nlegend:bool or {\u2018reverse\u2019}\n\n\nPlace legend on axis subplots.  \nstyle:list or dict\n\n\nThe matplotlib line style per column.  \nlogx:bool or \u2018sym\u2019, default False\n\n\nUse log scaling or symlog scaling on x axis. .. versionchanged:: 0.25.0  \nlogy:bool or \u2018sym\u2019 default False\n\n\nUse log scaling or symlog scaling on y axis. .. versionchanged:: 0.25.0  \nloglog:bool or \u2018sym\u2019, default False\n\n\nUse log scaling or symlog scaling on both x and y axes. .. versionchanged:: 0.25.0  \nxticks:sequence\n\n\nValues to use for the xticks.  \nyticks:sequence\n\n\nValues to use for the yticks.  \nxlim:2-tuple\/list\n\n\nSet the x limits of the current axes.  \nylim:2-tuple\/list\n\n\nSet the y limits of the current axes.  \nxlabel:label, optional\n\n\nName to use for the xlabel on x-axis. Default uses index name as xlabel, or the x-column name for planar plots.  New in version 1.1.0.   Changed in version 1.2.0: Now applicable to planar plots (scatter, hexbin).   \nylabel:label, optional\n\n\nName to use for the ylabel on y-axis. Default will show no ylabel, or the y-column name for planar plots.  New in version 1.1.0.   Changed in version 1.2.0: Now applicable to planar plots (scatter, hexbin).   \nrot:int, default None\n\n\nRotation for ticks (xticks for vertical, yticks for horizontal plots).  \nfontsize:int, default None\n\n\nFont size for xticks and yticks.  \ncolormap:str or matplotlib colormap object, default None\n\n\nColormap to select colors from. If string, load colormap with that name from matplotlib.  \ncolorbar:bool, optional\n\n\nIf True, plot colorbar (only relevant for \u2018scatter\u2019 and \u2018hexbin\u2019 plots).  \nposition:float\n\n\nSpecify relative alignments for bar plot layout. From 0 (left\/bottom-end) to 1 (right\/top-end). Default is 0.5 (center).  \ntable:bool, Series or DataFrame, default False\n\n\nIf True, draw a table using the data in the DataFrame and the data will be transposed to meet matplotlib\u2019s default layout. If a Series or DataFrame is passed, use passed data to draw a table.  \nyerr:DataFrame, Series, array-like, dict and str\n\n\nSee Plotting with Error Bars for detail.  \nxerr:DataFrame, Series, array-like, dict and str\n\n\nEquivalent to yerr.  \nstacked:bool, default False in line and bar plots, and True in area plot\n\n\nIf True, create stacked plot.  \nsort_columns:bool, default False\n\n\nSort column names to determine plot ordering.  \nsecondary_y:bool or sequence, default False\n\n\nWhether to plot on the secondary y-axis if a list\/tuple, which columns to plot on secondary y-axis.  \nmark_right:bool, default True\n\n\nWhen using a secondary_y axis, automatically mark the column labels with \u201c(right)\u201d in the legend.  \ninclude_bool:bool, default is False\n\n\nIf True, boolean values can be plotted.  \nbackend:str, default None\n\n\nBackend to use instead of the backend specified in the option plotting.backend. For instance, \u2018matplotlib\u2019. Alternatively, to specify the plotting.backend for the whole session, set pd.options.plotting.backend.  New in version 1.0.0.   **kwargs\n\nOptions to pass to matplotlib plotting method.    Returns \n \nmatplotlib.axes.Axes or numpy.ndarray of them\n\nIf the backend is not the default matplotlib one, the return value will be the object returned by the backend.     Notes  See matplotlib documentation online for more on this subject If kind = \u2018bar\u2019 or \u2018barh\u2019, you can specify relative alignments for bar plot layout by position keyword. From 0 (left\/bottom-end) to 1 (right\/top-end). Default is 0.5 (center)","title":"pandas.reference.api.pandas.dataframe.plot"},{"text":"Wave_read.rewind()  \nRewind the file pointer to the beginning of the audio stream.","title":"python.library.wave#wave.Wave_read.rewind"},{"text":"pandas.Series.plot   Series.plot(*args, **kwargs)[source]\n \nMake plots of Series or DataFrame. Uses the backend specified by the option plotting.backend. By default, matplotlib is used.  Parameters \n \ndata:Series or DataFrame\n\n\nThe object for which the method is called.  \nx:label or position, default None\n\n\nOnly used if data is a DataFrame.  \ny:label, position or list of label, positions, default None\n\n\nAllows plotting of one column versus another. Only used if data is a DataFrame.  \nkind:str\n\n\nThe kind of plot to produce:  \u2018line\u2019 : line plot (default) \u2018bar\u2019 : vertical bar plot \u2018barh\u2019 : horizontal bar plot \u2018hist\u2019 : histogram \u2018box\u2019 : boxplot \u2018kde\u2019 : Kernel Density Estimation plot \u2018density\u2019 : same as \u2018kde\u2019 \u2018area\u2019 : area plot \u2018pie\u2019 : pie plot \u2018scatter\u2019 : scatter plot (DataFrame only) \u2018hexbin\u2019 : hexbin plot (DataFrame only)   \nax:matplotlib axes object, default None\n\n\nAn axes of the current figure.  \nsubplots:bool, default False\n\n\nMake separate subplots for each column.  \nsharex:bool, default True if ax is None else False\n\n\nIn case subplots=True, share x axis and set some x axis labels to invisible; defaults to True if ax is None otherwise False if an ax is passed in; Be aware, that passing in both an ax and sharex=True will alter all x axis labels for all axis in a figure.  \nsharey:bool, default False\n\n\nIn case subplots=True, share y axis and set some y axis labels to invisible.  \nlayout:tuple, optional\n\n\n(rows, columns) for the layout of subplots.  \nfigsize:a tuple (width, height) in inches\n\n\nSize of a figure object.  \nuse_index:bool, default True\n\n\nUse index as ticks for x axis.  \ntitle:str or list\n\n\nTitle to use for the plot. If a string is passed, print the string at the top of the figure. If a list is passed and subplots is True, print each item in the list above the corresponding subplot.  \ngrid:bool, default None (matlab style default)\n\n\nAxis grid lines.  \nlegend:bool or {\u2018reverse\u2019}\n\n\nPlace legend on axis subplots.  \nstyle:list or dict\n\n\nThe matplotlib line style per column.  \nlogx:bool or \u2018sym\u2019, default False\n\n\nUse log scaling or symlog scaling on x axis. .. versionchanged:: 0.25.0  \nlogy:bool or \u2018sym\u2019 default False\n\n\nUse log scaling or symlog scaling on y axis. .. versionchanged:: 0.25.0  \nloglog:bool or \u2018sym\u2019, default False\n\n\nUse log scaling or symlog scaling on both x and y axes. .. versionchanged:: 0.25.0  \nxticks:sequence\n\n\nValues to use for the xticks.  \nyticks:sequence\n\n\nValues to use for the yticks.  \nxlim:2-tuple\/list\n\n\nSet the x limits of the current axes.  \nylim:2-tuple\/list\n\n\nSet the y limits of the current axes.  \nxlabel:label, optional\n\n\nName to use for the xlabel on x-axis. Default uses index name as xlabel, or the x-column name for planar plots.  New in version 1.1.0.   Changed in version 1.2.0: Now applicable to planar plots (scatter, hexbin).   \nylabel:label, optional\n\n\nName to use for the ylabel on y-axis. Default will show no ylabel, or the y-column name for planar plots.  New in version 1.1.0.   Changed in version 1.2.0: Now applicable to planar plots (scatter, hexbin).   \nrot:int, default None\n\n\nRotation for ticks (xticks for vertical, yticks for horizontal plots).  \nfontsize:int, default None\n\n\nFont size for xticks and yticks.  \ncolormap:str or matplotlib colormap object, default None\n\n\nColormap to select colors from. If string, load colormap with that name from matplotlib.  \ncolorbar:bool, optional\n\n\nIf True, plot colorbar (only relevant for \u2018scatter\u2019 and \u2018hexbin\u2019 plots).  \nposition:float\n\n\nSpecify relative alignments for bar plot layout. From 0 (left\/bottom-end) to 1 (right\/top-end). Default is 0.5 (center).  \ntable:bool, Series or DataFrame, default False\n\n\nIf True, draw a table using the data in the DataFrame and the data will be transposed to meet matplotlib\u2019s default layout. If a Series or DataFrame is passed, use passed data to draw a table.  \nyerr:DataFrame, Series, array-like, dict and str\n\n\nSee Plotting with Error Bars for detail.  \nxerr:DataFrame, Series, array-like, dict and str\n\n\nEquivalent to yerr.  \nstacked:bool, default False in line and bar plots, and True in area plot\n\n\nIf True, create stacked plot.  \nsort_columns:bool, default False\n\n\nSort column names to determine plot ordering.  \nsecondary_y:bool or sequence, default False\n\n\nWhether to plot on the secondary y-axis if a list\/tuple, which columns to plot on secondary y-axis.  \nmark_right:bool, default True\n\n\nWhen using a secondary_y axis, automatically mark the column labels with \u201c(right)\u201d in the legend.  \ninclude_bool:bool, default is False\n\n\nIf True, boolean values can be plotted.  \nbackend:str, default None\n\n\nBackend to use instead of the backend specified in the option plotting.backend. For instance, \u2018matplotlib\u2019. Alternatively, to specify the plotting.backend for the whole session, set pd.options.plotting.backend.  New in version 1.0.0.   **kwargs\n\nOptions to pass to matplotlib plotting method.    Returns \n \nmatplotlib.axes.Axes or numpy.ndarray of them\n\nIf the backend is not the default matplotlib one, the return value will be the object returned by the backend.     Notes  See matplotlib documentation online for more on this subject If kind = \u2018bar\u2019 or \u2018barh\u2019, you can specify relative alignments for bar plot layout by position keyword. From 0 (left\/bottom-end) to 1 (right\/top-end). Default is 0.5 (center)","title":"pandas.reference.api.pandas.series.plot"},{"text":"set_figure(figure)[source]","title":"matplotlib.backend_tools_api#matplotlib.backend_tools.ToolCursorPosition.set_figure"},{"text":"set_figure(figure)[source]","title":"matplotlib.backend_tools_api#matplotlib.backend_tools.SetCursorBase.set_figure"},{"text":"less_(other) \u2192 Tensor  \nIn-place version of less().","title":"torch.tensors#torch.Tensor.less_"}]}
{"task_id":15352457,"prompt":"def f_15352457(column, data):\n\treturn ","suffix":"","canonical_solution":"sum(row[column] for row in data)","test_start":"\ndef check(candidate): ","test":["\n    assert candidate(1, [[1,2,3], [4,5,6]]) == 7\n","\n    assert candidate(0, [[1,1,1], [0,1,1]]) == 1\n","\n    assert candidate(5, [[1,1,1,1,1,2], [0,1,1,1,1,1,1,1,1,1,1]]) == 3\n","\n    assert candidate(0, [[1],[2],[3],[4]]) == 10\n"],"entry_point":"f_15352457","intent":"sum elements at index `column` of each list in list `data`","library":[],"docs":[]}
{"task_id":15352457,"prompt":"def f_15352457(array):\n\treturn ","suffix":"","canonical_solution":"[sum(row[i] for row in array) for i in range(len(array[0]))]","test_start":"\ndef check(candidate): ","test":["\n    assert candidate([[1,2,3], [4,5,6]]) == [5, 7, 9]\n","\n    assert candidate([[1,1,1], [0,1,1]]) == [1, 2, 2]\n","\n    assert candidate([[1,1,1,1,1,2], [0,1,1,1,1,1,1,1,1,1,1]]) == [1, 2, 2, 2, 2, 3]\n","\n    assert candidate([[1],[2],[3],[4]]) == [10]\n"],"entry_point":"f_15352457","intent":"sum columns of a list `array`","library":[],"docs":[]}
{"task_id":23164058,"prompt":"def f_23164058():\n\treturn ","suffix":"","canonical_solution":"base64.b64encode(bytes('your string', 'utf-8'))","test_start":"\nimport base64\n\ndef check(candidate): ","test":["\n    assert candidate() == b'eW91ciBzdHJpbmc='\n"],"entry_point":"f_23164058","intent":"encode binary string 'your string' to base64 code","library":["base64"],"docs":[{"text":"base64.standard_b64encode(s)  \nEncode bytes-like object s using the standard Base64 alphabet and return the encoded bytes.","title":"python.library.base64#base64.standard_b64encode"},{"text":"binascii.a2b_base64(string)  \nConvert a block of base64 data back to binary and return the binary data. More than one line may be passed at a time.","title":"python.library.binascii#binascii.a2b_base64"},{"text":"base64.b16encode(s)  \nEncode the bytes-like object s using Base16 and return the encoded bytes.","title":"python.library.base64#base64.b16encode"},{"text":"tf.raw_ops.EncodeBase64 Encode strings into web-safe base64 format.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.raw_ops.EncodeBase64  \ntf.raw_ops.EncodeBase64(\n    input, pad=False, name=None\n)\n Refer to the following article for more information on base64 format: en.wikipedia.org\/wiki\/Base64. Base64 strings may have padding with '=' at the end so that the encoded has length multiple of 4. See Padding section of the link above. Web-safe means that the encoder uses - and _ instead of + and \/.\n \n\n\n Args\n  input   A Tensor of type string. Strings to be encoded.  \n  pad   An optional bool. Defaults to False. Bool whether padding is applied at the ends.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor of type string.","title":"tensorflow.raw_ops.encodebase64"},{"text":"tf.io.encode_base64 Encode strings into web-safe base64 format.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.encode_base64, tf.compat.v1.io.encode_base64  \ntf.io.encode_base64(\n    input, pad=False, name=None\n)\n Refer to the following article for more information on base64 format: en.wikipedia.org\/wiki\/Base64. Base64 strings may have padding with '=' at the end so that the encoded has length multiple of 4. See Padding section of the link above. Web-safe means that the encoder uses - and _ instead of + and \/.\n \n\n\n Args\n  input   A Tensor of type string. Strings to be encoded.  \n  pad   An optional bool. Defaults to False. Bool whether padding is applied at the ends.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor of type string.","title":"tensorflow.io.encode_base64"},{"text":"base64.encodebytes(s)  \nEncode the bytes-like object s, which can contain arbitrary binary data, and return bytes containing the base64-encoded data, with newlines (b'\\n') inserted after every 76 bytes of output, and ensuring that there is a trailing newline, as per RFC 2045 (MIME).  New in version 3.1.","title":"python.library.base64#base64.encodebytes"},{"text":"base64.encode(input, output)  \nEncode the contents of the binary input file and write the resulting base64 encoded data to the output file. input and output must be file objects. input will be read until input.read() returns an empty bytes object. encode() inserts a newline character (b'\\n') after every 76 bytes of the output, as well as ensuring that the output always ends with a newline, as per RFC 2045 (MIME).","title":"python.library.base64#base64.encode"},{"text":"binascii.b2a_base64(data, *, newline=True)  \nConvert binary data to a line of ASCII characters in base64 coding. The return value is the converted line, including a newline char if newline is true. The output of this function conforms to RFC 3548.  Changed in version 3.6: Added the newline parameter.","title":"python.library.binascii#binascii.b2a_base64"},{"text":"base64.b32encode(s)  \nEncode the bytes-like object s using Base32 and return the encoded bytes.","title":"python.library.base64#base64.b32encode"},{"text":"email.encoders.encode_base64(msg)  \nEncodes the payload into base64 form and sets the Content-Transfer-Encoding header to base64. This is a good encoding to use when most of your payload is unprintable data since it is a more compact form than quoted-printable. The drawback of base64 encoding is that it renders the text non-human readable.","title":"python.library.email.encoders#email.encoders.encode_base64"}]}
{"task_id":11533274,"prompt":"def f_11533274(dicts):\n\treturn ","suffix":"","canonical_solution":"dict((k, [d[k] for d in dicts]) for k in dicts[0])","test_start":"\ndef check(candidate): ","test":["\n    assert candidate([{'cat': 1, 'dog': 3}, {'cat' : 2, 'dog': ['happy']}]) ==         {'cat': [1, 2], 'dog': [3, ['happy']]}\n","\n    assert candidate([{'cat': 1}, {'cat' : 2}]) != {'cat': 3}\n"],"entry_point":"f_11533274","intent":"combine list of dictionaries `dicts` with the same keys in each list to a single dictionary","library":[],"docs":[]}
{"task_id":11533274,"prompt":"def f_11533274(dicts):\n\treturn ","suffix":"","canonical_solution":"{k: [d[k] for d in dicts] for k in dicts[0]}","test_start":"\ndef check(candidate): ","test":["\n    assert candidate([{'cat': 1, 'dog': 3}, {'cat' : 2, 'dog': ['happy']}]) ==         {'cat': [1, 2], 'dog': [3, ['happy']]}\n","\n    assert candidate([{'cat': 1}, {'cat' : 2}]) != {'cat': 3}\n"],"entry_point":"f_11533274","intent":"Merge a nested dictionary `dicts` into a flat dictionary by concatenating nested values with the same key `k`","library":[],"docs":[]}
{"task_id":14026704,"prompt":"def f_14026704(request):\n\treturn ","suffix":"","canonical_solution":"request.args['myParam']","test_start":"\nimport multidict\n\nclass Request:\n        def __init__(self, args):\n            self.args = args\n\ndef check(candidate): ","test":["\n    args = multidict.MultiDict([('myParam' , 'popeye')])\n    request = Request(args)\n    assert candidate(request) == 'popeye'\n"],"entry_point":"f_14026704","intent":"get the url parameter 'myParam' in a Flask view","library":["multidict"],"docs":[]}
{"task_id":11236006,"prompt":"def f_11236006(mylist):\n\treturn ","suffix":"","canonical_solution":"[k for k, v in list(Counter(mylist).items()) if v > 1]","test_start":"\nfrom collections import Counter\n\ndef check(candidate):","test":["\n    assert candidate([1,3,2,2,1,4]) == [1, 2]\n","\n    assert candidate([1,3,2,2,1,4]) != [3,4]\n","\n    assert candidate([]) == []\n","\n    assert candidate([1,1,1,1,1]) == [1]\n","\n    assert candidate([1.,1.,1.]) == [1.]\n"],"entry_point":"f_11236006","intent":"identify duplicate values in list `mylist`","library":["collections"],"docs":[{"text":"IMAP4.myrights(mailbox)  \nShow my ACLs for a mailbox (i.e. the rights that I have on mailbox).","title":"python.library.imaplib#imaplib.IMAP4.myrights"},{"text":"update_viewlim()[source]\n \n[Deprecated] Notes  Deprecated since version 3.4:","title":"matplotlib._as_gen.mpl_toolkits.axes_grid1.parasite_axes.parasiteaxesbase#mpl_toolkits.axes_grid1.parasite_axes.ParasiteAxesBase.update_viewlim"},{"text":"update_viewlim()[source]\n \n[Deprecated] Notes  Deprecated since version 3.4:","title":"matplotlib._as_gen.mpl_toolkits.axes_grid1.parasite_axes.parasiteaxesauxtransbase#mpl_toolkits.axes_grid1.parasite_axes.ParasiteAxesAuxTransBase.update_viewlim"},{"text":"array.fromlist(list)  \nAppend items from the list. This is equivalent to for x in list:\na.append(x) except that if there is a type error, the array is unchanged.","title":"python.library.array#array.array.fromlist"},{"text":"FieldStorage.getlist(name)  \nThis method always returns a list of values associated with form field name. The method returns an empty list if no such form field or value exists for name. It returns a list consisting of one item if only one such value exists.","title":"python.library.cgi#cgi.FieldStorage.getlist"},{"text":"tolist() \u2192 list or number  \nReturns the tensor as a (nested) list. For scalars, a standard Python number is returned, just like with item(). Tensors are automatically moved to the CPU first if necessary. This operation is not differentiable. Examples: >>> a = torch.randn(2, 2)\n>>> a.tolist()\n[[0.012766935862600803, 0.5415473580360413],\n [-0.08909505605697632, 0.7729271650314331]]\n>>> a[0,0].tolist()\n0.012766935862600803","title":"torch.tensors#torch.Tensor.tolist"},{"text":"lock()  \nunlock()  \nThree locking mechanisms are used\u2014dot locking and, if available, the flock() and lockf() system calls. For MH mailboxes, locking the mailbox means locking the .mh_sequences file and, only for the duration of any operations that affect them, locking individual message files.","title":"python.library.mailbox#mailbox.MH.lock"},{"text":"parameters(recurse=True) [source]\n \nReturns an iterator over module parameters. This is typically passed to an optimizer.  Parameters \nrecurse (bool) \u2013 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.  Yields \nParameter \u2013 module parameter   Example: >>> for param in model.parameters():\n>>>     print(type(param), param.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)","title":"torch.generated.torch.nn.module#torch.nn.Module.parameters"},{"text":"lock()  \nunlock()  \nThree locking mechanisms are used\u2014dot locking and, if available, the flock() and lockf() system calls. For MH mailboxes, locking the mailbox means locking the .mh_sequences file and, only for the duration of any operations that affect them, locking individual message files.","title":"python.library.mailbox#mailbox.MH.unlock"},{"text":"module_repr(module)  \nA legacy method which when implemented calculates and returns the given module\u2019s repr, as a string. The module type\u2019s default repr() will use the result of this method as appropriate.  New in version 3.3.   Changed in version 3.4: Made optional instead of an abstractmethod.   Deprecated since version 3.4: The import machinery now takes care of this automatically.","title":"python.library.importlib#importlib.abc.Loader.module_repr"}]}
{"task_id":20211942,"prompt":"def f_20211942(db):\n\treturn ","suffix":"","canonical_solution":"db.execute(\"INSERT INTO present VALUES('test2', ?, 10)\", (None,))","test_start":"\nimport sqlite3\n\ndef check(candidate):","test":["\n    sqliteConnection = sqlite3.connect('dev.db')\n    db = sqliteConnection.cursor()\n    print(\"Database created and Successfully Connected to SQLite\")\n    db.execute(\"CREATE TABLE present (name VARCHAR(5), age INTEGER, height INTEGER)\")\n    try:\n        candidate(db)\n    except:\n        assert False\n"],"entry_point":"f_20211942","intent":"Insert a 'None' value into a SQLite3 table.","library":["sqlite3"],"docs":[{"text":"SET_NULL  \nSet the ForeignKey null; this is only possible if null is True.","title":"django.ref.models.fields#django.db.models.SET_NULL"},{"text":"none()","title":"django.ref.models.querysets#django.db.models.query.QuerySet.none"},{"text":"empty_result_set_value  \n New in Django 4.0.  Override empty_result_set_value to None since most aggregate functions result in NULL when applied to an empty result set.","title":"django.ref.models.expressions#django.db.models.Aggregate.empty_result_set_value"},{"text":"empty_result_set_value  \n New in Django 4.0.  Tells Django which value should be returned when the expression is used to apply a function over an empty result set. Defaults to NotImplemented which forces the expression to be computed on the database.","title":"django.ref.models.expressions#django.db.models.Expression.empty_result_set_value"},{"text":"None  \nThe sole value of the type NoneType. None is frequently used to represent the absence of a value, as when default arguments are not passed to a function. Assignments to None are illegal and raise a SyntaxError.","title":"python.library.constants#None"},{"text":"Field.null","title":"django.ref.models.fields#django.db.models.Field.null"},{"text":"IMAP4.noop()  \nSend NOOP to server.","title":"python.library.imaplib#imaplib.IMAP4.noop"},{"text":"tf.raw_ops.OptionalNone Creates an Optional variant with no value.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.raw_ops.OptionalNone  \ntf.raw_ops.OptionalNone(\n    name=None\n)\n\n \n\n\n Args\n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor of type variant.","title":"tensorflow.raw_ops.optionalnone"},{"text":"RunSQL.noop  \nPass the RunSQL.noop attribute to sql or reverse_sql when you want the operation not to do anything in the given direction. This is especially useful in making the operation reversible.","title":"django.ref.migration-operations#django.db.migrations.operations.RunSQL.noop"},{"text":"autoscale_None()[source]\n \nAutoscale the scalar limits on the norm instance using the current array, changing only limits that are None","title":"matplotlib.cm_api#matplotlib.cm.ScalarMappable.autoscale_None"}]}
{"task_id":406121,"prompt":"def f_406121(list_of_menuitems):\n\treturn ","suffix":"","canonical_solution":"[image for menuitem in list_of_menuitems for image in menuitem]","test_start":"\nfrom collections import Counter\n\ndef check(candidate):","test":["\n    assert candidate([[1,2],[3,4,5]]) == [1,2,3,4,5]\n","\n    assert candidate([[],[]]) == []\n","\n    assert candidate([[1,1,1], []]) == [1,1,1]\n","\n    assert candidate([['1'],['2']]) == ['1','2']\n"],"entry_point":"f_406121","intent":"flatten list `list_of_menuitems`","library":["collections"],"docs":[]}
{"task_id":4741537,"prompt":"def f_4741537(a, b):\n\t","suffix":"\n\treturn a","canonical_solution":"a.extend(b)","test_start":"\ndef check(candidate):","test":["\n    assert candidate([1, 2, 2, 3], {4, 5, 2}) == [1, 2, 2, 3, 2, 4, 5]\n","\n    assert candidate([], {4,5,2}) == [2,4,5]\n","\n    assert candidate([1,2,3,4],{2}) == [1,2,3,4,2]\n","\n    assert candidate([1], {'a'}) == [1, 'a']\n"],"entry_point":"f_4741537","intent":"append elements of a set `b` to a list `a`","library":[],"docs":[]}
{"task_id":15851568,"prompt":"def f_15851568(x):\n\treturn ","suffix":"","canonical_solution":"x.rpartition('-')[0]","test_start":"\ndef check(candidate):","test":["\n    assert candidate('djhajhdjk-dadwqd-dahdjkahsk') == 'djhajhdjk-dadwqd'\n","\n    assert candidate('\/-\/') == '\/'\n","\n    assert candidate('---') == '--'\n","\n    assert candidate('') == ''\n"],"entry_point":"f_15851568","intent":"Split a string `x` by last occurrence of character `-`","library":[],"docs":[]}
{"task_id":15851568,"prompt":"def f_15851568(x):\n\treturn ","suffix":"","canonical_solution":"x.rsplit('-', 1)[0]","test_start":"\ndef check(candidate):","test":["\n    assert candidate('2022-03-01') == '2022-03'\n","\n    assert candidate('2020-2022') == '2020'\n"],"entry_point":"f_15851568","intent":"get the last part of a string before the character '-'","library":[],"docs":[]}
{"task_id":17438096,"prompt":"def f_17438096(filename, ftp):\n\t","suffix":"\n\treturn ","canonical_solution":"ftp.storlines('STOR ' + filename, open(filename, 'r'))","test_start":"\nimport ftplib\nfrom unittest.mock import Mock\n\ndef check(candidate):","test":["\n    ftplib.FTP = Mock()\n    ftp = ftplib.FTP(\"10.10.10.10\")\n    ftp.storlines = Mock()\n    file_name = 'readme.txt'\n    with open (file_name, 'a') as f:\n        f.write('apple')\n    candidate(file_name, ftp)\n"],"entry_point":"f_17438096","intent":"upload file using FTP","library":["ftplib"],"docs":[{"text":"class urllib.request.FTPHandler  \nOpen FTP URLs.","title":"python.library.urllib.request#urllib.request.FTPHandler"},{"text":"FTP.storbinary(cmd, fp, blocksize=8192, callback=None, rest=None)  \nStore a file in binary transfer mode. cmd should be an appropriate STOR command: \"STOR filename\". fp is a file object (opened in binary mode) which is read until EOF using its read() method in blocks of size blocksize to provide the data to be stored. The blocksize argument defaults to 8192. callback is an optional single parameter callable that is called on each block of data after it is sent. rest means the same thing as in the transfercmd() method.  Changed in version 3.2: rest parameter added.","title":"python.library.ftplib#ftplib.FTP.storbinary"},{"text":"FTP.storlines(cmd, fp, callback=None)  \nStore a file in line mode. cmd should be an appropriate STOR command (see storbinary()). Lines are read until EOF from the file object fp (opened in binary mode) using its readline() method to provide the data to be stored. callback is an optional single parameter callable that is called on each line after it is sent.","title":"python.library.ftplib#ftplib.FTP.storlines"},{"text":"FTP.login(user='anonymous', passwd='', acct='')  \nLog in as the given user. The passwd and acct parameters are optional and default to the empty string. If no user is specified, it defaults to 'anonymous'. If user is 'anonymous', the default passwd is 'anonymous@'. This function should be called only once for each instance, after a connection has been established; it should not be called at all if a host and user were given when the instance was created. Most FTP commands are only allowed after the client has logged in. The acct parameter supplies \u201caccounting information\u201d; few systems implement this.","title":"python.library.ftplib#ftplib.FTP.login"},{"text":"FTP.voidcmd(cmd)  \nSend a simple command string to the server and handle the response. Return nothing if a response code corresponding to success (codes in the range 200\u2013299) is received. Raise error_reply otherwise. Raises an auditing event ftplib.sendcmd with arguments self, cmd.","title":"python.library.ftplib#ftplib.FTP.voidcmd"},{"text":"FTP.rename(fromname, toname)  \nRename file fromname on the server to toname.","title":"python.library.ftplib#ftplib.FTP.rename"},{"text":"FTP.sendcmd(cmd)  \nSend a simple command string to the server and return the response string. Raises an auditing event ftplib.sendcmd with arguments self, cmd.","title":"python.library.ftplib#ftplib.FTP.sendcmd"},{"text":"FTP_TLS.prot_p()  \nSet up secure data connection.","title":"python.library.ftplib#ftplib.FTP_TLS.prot_p"},{"text":"FTPHandler.ftp_open(req)  \nOpen the FTP file indicated by req. The login is always done with empty username and password.","title":"python.library.urllib.request#urllib.request.FTPHandler.ftp_open"},{"text":"FTP.transfercmd(cmd, rest=None)  \nInitiate a transfer over the data connection. If the transfer is active, send an EPRT or PORT command and the transfer command specified by cmd, and accept the connection. If the server is passive, send an EPSV or PASV command, connect to it, and start the transfer command. Either way, return the socket for the connection. If optional rest is given, a REST command is sent to the server, passing rest as an argument. rest is usually a byte offset into the requested file, telling the server to restart sending the file\u2019s bytes at the requested offset, skipping over the initial bytes. Note however that the transfercmd() method converts rest to a string with the encoding parameter specified at initialization, but no check is performed on the string\u2019s contents. If the server does not recognize the REST command, an error_reply exception will be raised. If this happens, simply call transfercmd() without a rest argument.","title":"python.library.ftplib#ftplib.FTP.transfercmd"}]}
{"task_id":28742436,"prompt":"def f_28742436():\n\treturn ","suffix":"","canonical_solution":"np.maximum([2, 3, 4], [1, 5, 2])","test_start":"\nimport numpy as np \n\ndef check(candidate):","test":["\n    assert all(candidate() == np.array([2, 5, 4]))\n"],"entry_point":"f_28742436","intent":"create array containing the maximum value of respective elements of array `[2, 3, 4]` and array `[1, 5, 2]`","library":["numpy"],"docs":[{"text":"maximum(other) \u2192 Tensor  \nSee torch.maximum()","title":"torch.tensors#torch.Tensor.maximum"},{"text":"numpy.matrix.max method   matrix.max(axis=None, out=None)[source]\n \nReturn the maximum value along an axis.  Parameters \n See `amax` for complete descriptions\n    See also  \namax, ndarray.max\n\n  Notes This is the same as ndarray.max, but returns a matrix object where ndarray.max would return an ndarray. Examples >>> x = np.matrix(np.arange(12).reshape((3,4))); x\nmatrix([[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]])\n>>> x.max()\n11\n>>> x.max(0)\nmatrix([[ 8,  9, 10, 11]])\n>>> x.max(1)\nmatrix([[ 3],\n        [ 7],\n        [11]])","title":"numpy.reference.generated.numpy.matrix.max"},{"text":"torch.maximum(input, other, *, out=None) \u2192 Tensor  \nComputes the element-wise maximum of input and other.  Note If one of the elements being compared is a NaN, then that element is returned. maximum() is not supported for tensors with complex dtypes.   Parameters \n \ninput (Tensor) \u2013 the input tensor. \nother (Tensor) \u2013 the second input tensor   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.tensor((1, 2, -1))\n>>> b = torch.tensor((3, 0, 4))\n>>> torch.maximum(a, b)\ntensor([3, 2, 4])","title":"torch.generated.torch.maximum#torch.maximum"},{"text":"max(dim=None, keepdim=False) -> Tensor or (Tensor, Tensor)  \nSee torch.max()","title":"torch.tensors#torch.Tensor.max"},{"text":"torch.fmax(input, other, *, out=None) \u2192 Tensor  \nComputes the element-wise maximum of input and other. This is like torch.maximum() except it handles NaNs differently: if exactly one of the two elements being compared is a NaN then the non-NaN element is taken as the maximum. Only if both elements are NaN is NaN propagated. This function is a wrapper around C++\u2019s std::fmax and is similar to NumPy\u2019s fmax function. Supports broadcasting to a common shape, type promotion, and integer and floating-point inputs.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \nother (Tensor) \u2013 the second input tensor   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.tensor([9.7, float('nan'), 3.1, float('nan')])\n>>> b = torch.tensor([-2.2, 0.5, float('nan'), float('nan')])\n>>> torch.fmax(a, b)\ntensor([9.7000, 0.5000, 3.1000,    nan])","title":"torch.generated.torch.fmax#torch.fmax"},{"text":"numpy.matrix.argmax method   matrix.argmax(axis=None, out=None)[source]\n \nIndexes of the maximum values along an axis. Return the indexes of the first occurrences of the maximum values along the specified axis. If axis is None, the index is for the flattened matrix.  Parameters \n See `numpy.argmax` for complete descriptions\n    See also  numpy.argmax\n  Notes This is the same as ndarray.argmax, but returns a matrix object where ndarray.argmax would return an ndarray. Examples >>> x = np.matrix(np.arange(12).reshape((3,4))); x\nmatrix([[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]])\n>>> x.argmax()\n11\n>>> x.argmax(0)\nmatrix([[2, 2, 2, 2]])\n>>> x.argmax(1)\nmatrix([[3],\n        [3],\n        [3]])","title":"numpy.reference.generated.numpy.matrix.argmax"},{"text":"amax(dim=None, keepdim=False) \u2192 Tensor  \nSee torch.amax()","title":"torch.tensors#torch.Tensor.amax"},{"text":"tf.experimental.numpy.maximum TensorFlow variant of NumPy's maximum. \ntf.experimental.numpy.maximum(\n    x1, x2\n)\n Unsupported arguments: out, where, casting, order, dtype, subok, signature, extobj. See the NumPy documentation for numpy.maximum.","title":"tensorflow.experimental.numpy.maximum"},{"text":"fmax(other) \u2192 Tensor  \nSee torch.fmax()","title":"torch.tensors#torch.Tensor.fmax"},{"text":"torch.max(input) \u2192 Tensor  \nReturns the maximum value of all elements in the input tensor.  Warning This function produces deterministic (sub)gradients unlike max(dim=0)   Parameters \ninput (Tensor) \u2013 the input tensor.   Example: >>> a = torch.randn(1, 3)\n>>> a\ntensor([[ 0.6763,  0.7445, -2.2369]])\n>>> torch.max(a)\ntensor(0.7445)\n  \ntorch.max(input, dim, keepdim=False, *, out=None) -> (Tensor, LongTensor) \n Returns a namedtuple (values, indices) where values is the maximum value of each row of the input tensor in the given dimension dim. And indices is the index location of each maximum value found (argmax). If keepdim is True, the output tensors are of the same size as input except in the dimension dim where they are of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensors having 1 fewer dimension than input.  Note If there are multiple maximal values in a reduced row then the indices of the first maximal value are returned.   Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int) \u2013 the dimension to reduce. \nkeepdim (bool) \u2013 whether the output tensor has dim retained or not. Default: False.   Keyword Arguments \nout (tuple, optional) \u2013 the result tuple of two output tensors (max, max_indices)   Example: >>> a = torch.randn(4, 4)\n>>> a\ntensor([[-1.2360, -0.2942, -0.1222,  0.8475],\n        [ 1.1949, -1.1127, -2.2379, -0.6702],\n        [ 1.5717, -0.9207,  0.1297, -1.8768],\n        [-0.6172,  1.0036, -0.6060, -0.2432]])\n>>> torch.max(a, 1)\ntorch.return_types.max(values=tensor([0.8475, 1.1949, 1.5717, 1.0036]), indices=tensor([3, 0, 0, 1]))\n  \ntorch.max(input, other, *, out=None) \u2192 Tensor \n See torch.maximum().","title":"torch.generated.torch.max#torch.max"}]}
{"task_id":34280147,"prompt":"def f_34280147(l):\n\treturn ","suffix":"","canonical_solution":"l[3:] + l[:3]","test_start":"\ndef check(candidate):","test":["\n    assert candidate(\"my-string\") == \"stringmy-\"\n","\n    assert candidate(\"my \") == \"my \"\n","\n    assert candidate(\"n;ho0-4w606[q\") == \"o0-4w606[qn;h\"\n"],"entry_point":"f_34280147","intent":"print a list `l` and move first 3 elements to the end of the list","library":[],"docs":[]}
{"task_id":4172131,"prompt":"def f_4172131():\n\treturn ","suffix":"","canonical_solution":"[int(1000 * random.random()) for i in range(10000)]","test_start":"\nimport random\n\ndef check(candidate):","test":["\n    result = candidate()\n    assert isinstance(result, list)\n    assert all([isinstance(item, int) for item in result])\n"],"entry_point":"f_4172131","intent":"create a random list of integers","library":["random"],"docs":[{"text":"tf.config.LogicalDevice Abstraction for a logical device initialized by the runtime.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.config.LogicalDevice  \ntf.config.LogicalDevice(\n    name, device_type\n)\n A tf.config.LogicalDevice corresponds to an initialized logical device on a tf.config.PhysicalDevice or a remote device visible to the cluster. Tensors and operations can be placed on a specific logical device by calling tf.device with a specified tf.config.LogicalDevice. Fields:  \nname: The fully qualified name of the device. Can be used for Op or function placement. \ndevice_type: String declaring the type of device such as \"CPU\" or \"GPU\". \n \n\n\n Attributes\n  name  \n \n  device_type","title":"tensorflow.config.logicaldevice"},{"text":"matplotlib.axes.Axes.get_aspect   Axes.get_aspect()[source]\n \nReturn the aspect ratio of the axes scaling. This is either \"auto\" or a float giving the ratio of y\/x-scale.","title":"matplotlib._as_gen.matplotlib.axes.axes.get_aspect"},{"text":"buffer_rgba()[source]\n \nGet the image as a memoryview to the renderer's buffer. draw must be called at least once before this function will work and to update the renderer for any subsequent changes to the Figure.","title":"matplotlib.backend_agg_api#matplotlib.backends.backend_agg.FigureCanvasAgg.buffer_rgba"},{"text":"remove_callback(oid)[source]\n \nRemove a callback based on its observer id.  See also  add_callback","title":"matplotlib.collections_api#matplotlib.collections.PathCollection.remove_callback"},{"text":"numpy.record.tolist method   record.tolist()\n \nScalar method identical to the corresponding array attribute. Please see ndarray.tolist.","title":"numpy.reference.generated.numpy.record.tolist"},{"text":"remove_callback(oid)[source]\n \nRemove a callback based on its observer id.  See also  add_callback","title":"matplotlib.collections_api#matplotlib.collections.CircleCollection.remove_callback"},{"text":"set_data(arr)[source]","title":"matplotlib.offsetbox_api#matplotlib.offsetbox.OffsetImage.set_data"},{"text":"class list([iterable])  \nRather than being a function, list is actually a mutable sequence type, as documented in Lists and Sequence Types \u2014 list, tuple, range.","title":"python.library.functions#list"},{"text":"remove_callback(oid)[source]\n \nRemove a callback based on its observer id.  See also  add_callback","title":"matplotlib.collections_api#matplotlib.collections.TriMesh.remove_callback"},{"text":"matplotlib.axes.Axes.get_box_aspect   Axes.get_box_aspect()[source]\n \nReturn the Axes box aspect, i.e. the ratio of height to width. The box aspect is None (i.e. chosen depending on the available figure space) unless explicitly specified.  See also  matplotlib.axes.Axes.set_box_aspect\n\nfor a description of box aspect.  matplotlib.axes.Axes.set_aspect\n\nfor a description of aspect handling.","title":"matplotlib._as_gen.matplotlib.axes.axes.get_box_aspect"}]}
{"task_id":6677332,"prompt":"def f_6677332():\n\treturn ","suffix":"","canonical_solution":"datetime.datetime.now().strftime('%H:%M:%S.%f')","test_start":"\nimport datetime\n\ndef check(candidate):","test":["\n    time_now = datetime.datetime.now().strftime('%H:%M:%S.%f')\n    assert candidate().split('.')[0] == time_now.split('.')[0]\n"],"entry_point":"f_6677332","intent":"Using %f with strftime() in Python to get microseconds","library":["datetime"],"docs":[{"text":"time.microsecond  \nIn range(1000000).","title":"python.library.datetime#datetime.time.microsecond"},{"text":"datetime.microsecond  \nIn range(1000000).","title":"python.library.datetime#datetime.datetime.microsecond"},{"text":"pandas.Timestamp.microsecond   Timestamp.microsecond","title":"pandas.reference.api.pandas.timestamp.microsecond"},{"text":"pandas.Series.dt.microseconds   Series.dt.microseconds\n \nNumber of microseconds (>= 0 and less than 1 second) for each element.","title":"pandas.reference.api.pandas.series.dt.microseconds"},{"text":"pandas.DatetimeIndex.microsecond   propertyDatetimeIndex.microsecond\n \nThe microseconds of the datetime. Examples \n>>> datetime_series = pd.Series(\n...     pd.date_range(\"2000-01-01\", periods=3, freq=\"us\")\n... )\n>>> datetime_series\n0   2000-01-01 00:00:00.000000\n1   2000-01-01 00:00:00.000001\n2   2000-01-01 00:00:00.000002\ndtype: datetime64[ns]\n>>> datetime_series.dt.microsecond\n0       0\n1       1\n2       2\ndtype: int64","title":"pandas.reference.api.pandas.datetimeindex.microsecond"},{"text":"pandas.Series.dt.microsecond   Series.dt.microsecond\n \nThe microseconds of the datetime. Examples \n>>> datetime_series = pd.Series(\n...     pd.date_range(\"2000-01-01\", periods=3, freq=\"us\")\n... )\n>>> datetime_series\n0   2000-01-01 00:00:00.000000\n1   2000-01-01 00:00:00.000001\n2   2000-01-01 00:00:00.000002\ndtype: datetime64[ns]\n>>> datetime_series.dt.microsecond\n0       0\n1       1\n2       2\ndtype: int64","title":"pandas.reference.api.pandas.series.dt.microsecond"},{"text":"pandas.TimedeltaIndex.microseconds   propertyTimedeltaIndex.microseconds\n \nNumber of microseconds (>= 0 and less than 1 second) for each element.","title":"pandas.reference.api.pandas.timedeltaindex.microseconds"},{"text":"pandas.Timedelta.microseconds   Timedelta.microseconds\n \nNumber of microseconds (>= 0 and less than 1 second).","title":"pandas.reference.api.pandas.timedelta.microseconds"},{"text":"time.__format__(format)  \nSame as time.strftime(). This makes it possible to specify a format string for a time object in formatted string literals and when using str.format(). For a complete list of formatting directives, see strftime() and strptime() Behavior.","title":"python.library.datetime#datetime.time.__format__"},{"text":"time.strftime(format)  \nReturn a string representing the time, controlled by an explicit format string. For a complete list of formatting directives, see strftime() and strptime() Behavior.","title":"python.library.datetime#datetime.time.strftime"}]}
{"task_id":15325182,"prompt":"def f_15325182(df):\n\treturn ","suffix":"","canonical_solution":"df.b.str.contains('^f')","test_start":"\nimport pandas as pd \n\ndef check(candidate):","test":["\n    df = pd.DataFrame([[1, 'fat'], [2, 'hip'], [3, 'foo']], columns = ['a', 'b'])\n    expected = [True, False, True]\n    actual = candidate(df)\n    for i in range (0, len(expected)):\n        assert expected[i] == actual[i]\n"],"entry_point":"f_15325182","intent":"filter rows in pandas starting with alphabet 'f' using regular expression.","library":["pandas"],"docs":[{"text":"pandas.DataFrame.filter   DataFrame.filter(items=None, like=None, regex=None, axis=None)[source]\n \nSubset the dataframe rows or columns according to the specified index labels. Note that this routine does not filter a dataframe on its contents. The filter is applied to the labels of the index.  Parameters \n \nitems:list-like\n\n\nKeep labels from axis which are in items.  \nlike:str\n\n\nKeep labels from axis for which \u201clike in label == True\u201d.  \nregex:str (regular expression)\n\n\nKeep labels from axis for which re.search(regex, label) == True.  \naxis:{0 or \u2018index\u2019, 1 or \u2018columns\u2019, None}, default None\n\n\nThe axis to filter on, expressed either as an index (int) or axis name (str). By default this is the info axis, \u2018index\u2019 for Series, \u2018columns\u2019 for DataFrame.    Returns \n same type as input object\n    See also  DataFrame.loc\n\nAccess a group of rows and columns by label(s) or a boolean array.    Notes The items, like, and regex parameters are enforced to be mutually exclusive. axis defaults to the info axis that is used when indexing with []. Examples \n>>> df = pd.DataFrame(np.array(([1, 2, 3], [4, 5, 6])),\n...                   index=['mouse', 'rabbit'],\n...                   columns=['one', 'two', 'three'])\n>>> df\n        one  two  three\nmouse     1    2      3\nrabbit    4    5      6\n  \n>>> # select columns by name\n>>> df.filter(items=['one', 'three'])\n         one  three\nmouse     1      3\nrabbit    4      6\n  \n>>> # select columns by regular expression\n>>> df.filter(regex='e$', axis=1)\n         one  three\nmouse     1      3\nrabbit    4      6\n  \n>>> # select rows containing 'bbi'\n>>> df.filter(like='bbi', axis=0)\n         one  two  three\nrabbit    4    5      6","title":"pandas.reference.api.pandas.dataframe.filter"},{"text":"pandas.Series.filter   Series.filter(items=None, like=None, regex=None, axis=None)[source]\n \nSubset the dataframe rows or columns according to the specified index labels. Note that this routine does not filter a dataframe on its contents. The filter is applied to the labels of the index.  Parameters \n \nitems:list-like\n\n\nKeep labels from axis which are in items.  \nlike:str\n\n\nKeep labels from axis for which \u201clike in label == True\u201d.  \nregex:str (regular expression)\n\n\nKeep labels from axis for which re.search(regex, label) == True.  \naxis:{0 or \u2018index\u2019, 1 or \u2018columns\u2019, None}, default None\n\n\nThe axis to filter on, expressed either as an index (int) or axis name (str). By default this is the info axis, \u2018index\u2019 for Series, \u2018columns\u2019 for DataFrame.    Returns \n same type as input object\n    See also  DataFrame.loc\n\nAccess a group of rows and columns by label(s) or a boolean array.    Notes The items, like, and regex parameters are enforced to be mutually exclusive. axis defaults to the info axis that is used when indexing with []. Examples \n>>> df = pd.DataFrame(np.array(([1, 2, 3], [4, 5, 6])),\n...                   index=['mouse', 'rabbit'],\n...                   columns=['one', 'two', 'three'])\n>>> df\n        one  two  three\nmouse     1    2      3\nrabbit    4    5      6\n  \n>>> # select columns by name\n>>> df.filter(items=['one', 'three'])\n         one  three\nmouse     1      3\nrabbit    4      6\n  \n>>> # select columns by regular expression\n>>> df.filter(regex='e$', axis=1)\n         one  three\nmouse     1      3\nrabbit    4      6\n  \n>>> # select rows containing 'bbi'\n>>> df.filter(like='bbi', axis=0)\n         one  two  three\nrabbit    4    5      6","title":"pandas.reference.api.pandas.series.filter"},{"text":"pandas.Series.str.extractall   Series.str.extractall(pat, flags=0)[source]\n \nExtract capture groups in the regex pat as columns in DataFrame. For each subject string in the Series, extract groups from all matches of regular expression pat. When each subject string in the Series has exactly one match, extractall(pat).xs(0, level=\u2019match\u2019) is the same as extract(pat).  Parameters \n \npat:str\n\n\nRegular expression pattern with capturing groups.  \nflags:int, default 0 (no flags)\n\n\nA re module flag, for example re.IGNORECASE. These allow to modify regular expression matching for things like case, spaces, etc. Multiple flags can be combined with the bitwise OR operator, for example re.IGNORECASE | re.MULTILINE.    Returns \n DataFrame\n\nA DataFrame with one row for each match, and one column for each group. Its rows have a MultiIndex with first levels that come from the subject Series. The last level is named \u2018match\u2019 and indexes the matches in each item of the Series. Any capture group names in regular expression pat will be used for column names; otherwise capture group numbers will be used.      See also  extract\n\nReturns first match only (not all matches).    Examples A pattern with one group will return a DataFrame with one column. Indices with no matches will not appear in the result. \n>>> s = pd.Series([\"a1a2\", \"b1\", \"c1\"], index=[\"A\", \"B\", \"C\"])\n>>> s.str.extractall(r\"[ab](\\d)\")\n        0\nmatch\nA 0      1\n  1      2\nB 0      1\n  Capture group names are used for column names of the result. \n>>> s.str.extractall(r\"[ab](?P<digit>\\d)\")\n        digit\nmatch\nA 0         1\n  1         2\nB 0         1\n  A pattern with two groups will return a DataFrame with two columns. \n>>> s.str.extractall(r\"(?P<letter>[ab])(?P<digit>\\d)\")\n        letter digit\nmatch\nA 0          a     1\n  1          a     2\nB 0          b     1\n  Optional groups that do not match are NaN in the result. \n>>> s.str.extractall(r\"(?P<letter>[ab])?(?P<digit>\\d)\")\n        letter digit\nmatch\nA 0          a     1\n  1          a     2\nB 0          b     1\nC 0        NaN     1","title":"pandas.reference.api.pandas.series.str.extractall"},{"text":"frame_type  \nThis attribute is set to 'ROWS'.","title":"django.ref.models.expressions#django.db.models.expressions.RowRange.frame_type"},{"text":"class RTrim(expression, **extra)","title":"django.ref.models.database-functions#django.db.models.functions.RTrim"},{"text":"pandas.DataFrame.T   propertyDataFrame.T","title":"pandas.reference.api.pandas.dataframe.t"},{"text":"class F","title":"django.ref.models.expressions#django.db.models.F"},{"text":"class RowRange(start=None, end=None)  \n \nframe_type  \nThis attribute is set to 'ROWS'.","title":"django.ref.models.expressions#django.db.models.expressions.RowRange"},{"text":"fnmatch.filter(names, pattern)  \nConstruct a list from those elements of the iterable names that match pattern. It is the same as [n for n in names if fnmatch(n, pattern)], but implemented more efficiently.","title":"python.library.fnmatch#fnmatch.filter"},{"text":"pandas.core.groupby.DataFrameGroupBy.filter   DataFrameGroupBy.filter(func, dropna=True, *args, **kwargs)[source]\n \nReturn a copy of a DataFrame excluding filtered elements. Elements from groups are filtered if they do not satisfy the boolean criterion specified by func.  Parameters \n \nfunc:function\n\n\nFunction to apply to each subframe. Should return True or False.  \ndropna:Drop groups that do not pass the filter. True by default;\n\n\nIf False, groups that evaluate False are filled with NaNs.    Returns \n \nfiltered:DataFrame\n\n   Notes Each subframe is endowed the attribute \u2018name\u2019 in case you need to know which group you are working on. Functions that mutate the passed object can produce unexpected behavior or errors and are not supported. See Mutating with User Defined Function (UDF) methods for more details. Examples \n>>> df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',\n...                           'foo', 'bar'],\n...                    'B' : [1, 2, 3, 4, 5, 6],\n...                    'C' : [2.0, 5., 8., 1., 2., 9.]})\n>>> grouped = df.groupby('A')\n>>> grouped.filter(lambda x: x['B'].mean() > 3.)\n     A  B    C\n1  bar  2  5.0\n3  bar  4  1.0\n5  bar  6  9.0","title":"pandas.reference.api.pandas.core.groupby.dataframegroupby.filter"}]}
{"task_id":583557,"prompt":"def f_583557(tab):\n\treturn ","suffix":"","canonical_solution":"'\\n'.join('\\t'.join(str(col) for col in row) for row in tab)","test_start":"\ndef check(candidate):","test":["\n    assert candidate([[1,2,3],[4,5,6]]) == \"1\\t2\\t3\\n4\\t5\\t6\"\n","\n    assert candidate([[1, 'x' ,3],[4.4,5,\"six\"]]) == \"1\\tx\\t3\\n4.4\\t5\\tsix\"\n","\n    assert candidate([]) == \"\"\n","\n    assert candidate([[],[],[]]) == \"\\n\\n\"\n"],"entry_point":"f_583557","intent":"print a 2 dimensional list `tab` as a table with delimiters","library":[],"docs":[]}
{"task_id":38535931,"prompt":"def f_38535931(df, tuples):\n\treturn ","suffix":"","canonical_solution":"df.set_index(list('BC')).drop(tuples, errors='ignore').reset_index()","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    df = pd.DataFrame([[3, 4], [4, 5], [-1, -2]], columns = ['B', 'C'])\n    tuples = [(3, 4), (-1, -2)]\n    expected = pd.DataFrame([[4, 5]], columns = ['B', 'C'])\n    actual = candidate(df, tuples)\n    assert pd.DataFrame.equals(actual, expected)\n"],"entry_point":"f_38535931","intent":"pandas: delete rows in dataframe `df` based on multiple columns values","library":["pandas"],"docs":[{"text":"pandas.DataFrame.drop   DataFrame.drop(labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise')[source]\n \nDrop specified labels from rows or columns. Remove rows or columns by specifying label names and corresponding axis, or by specifying directly index or column names. When using a multi-index, labels on different levels can be removed by specifying the level. See the user guide <advanced.shown_levels> for more information about the now unused levels.  Parameters \n \nlabels:single label or list-like\n\n\nIndex or column labels to drop. A tuple will be used as a single label and not treated as a list-like.  \naxis:{0 or \u2018index\u2019, 1 or \u2018columns\u2019}, default 0\n\n\nWhether to drop labels from the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).  \nindex:single label or list-like\n\n\nAlternative to specifying axis (labels, axis=0 is equivalent to index=labels).  \ncolumns:single label or list-like\n\n\nAlternative to specifying axis (labels, axis=1 is equivalent to columns=labels).  \nlevel:int or level name, optional\n\n\nFor MultiIndex, level from which the labels will be removed.  \ninplace:bool, default False\n\n\nIf False, return a copy. Otherwise, do operation inplace and return None.  \nerrors:{\u2018ignore\u2019, \u2018raise\u2019}, default \u2018raise\u2019\n\n\nIf \u2018ignore\u2019, suppress error and only existing labels are dropped.    Returns \n DataFrame or None\n\nDataFrame without the removed index or column labels or None if inplace=True.    Raises \n KeyError\n\nIf any of the labels is not found in the selected axis.      See also  DataFrame.loc\n\nLabel-location based indexer for selection by label.  DataFrame.dropna\n\nReturn DataFrame with labels on given axis omitted where (all or any) data are missing.  DataFrame.drop_duplicates\n\nReturn DataFrame with duplicate rows removed, optionally only considering certain columns.  Series.drop\n\nReturn Series with specified index labels removed.    Examples \n>>> df = pd.DataFrame(np.arange(12).reshape(3, 4),\n...                   columns=['A', 'B', 'C', 'D'])\n>>> df\n   A  B   C   D\n0  0  1   2   3\n1  4  5   6   7\n2  8  9  10  11\n  Drop columns \n>>> df.drop(['B', 'C'], axis=1)\n   A   D\n0  0   3\n1  4   7\n2  8  11\n  \n>>> df.drop(columns=['B', 'C'])\n   A   D\n0  0   3\n1  4   7\n2  8  11\n  Drop a row by index \n>>> df.drop([0, 1])\n   A  B   C   D\n2  8  9  10  11\n  Drop columns and\/or rows of MultiIndex DataFrame \n>>> midx = pd.MultiIndex(levels=[['lama', 'cow', 'falcon'],\n...                              ['speed', 'weight', 'length']],\n...                      codes=[[0, 0, 0, 1, 1, 1, 2, 2, 2],\n...                             [0, 1, 2, 0, 1, 2, 0, 1, 2]])\n>>> df = pd.DataFrame(index=midx, columns=['big', 'small'],\n...                   data=[[45, 30], [200, 100], [1.5, 1], [30, 20],\n...                         [250, 150], [1.5, 0.8], [320, 250],\n...                         [1, 0.8], [0.3, 0.2]])\n>>> df\n                big     small\nlama    speed   45.0    30.0\n        weight  200.0   100.0\n        length  1.5     1.0\ncow     speed   30.0    20.0\n        weight  250.0   150.0\n        length  1.5     0.8\nfalcon  speed   320.0   250.0\n        weight  1.0     0.8\n        length  0.3     0.2\n  Drop a specific index combination from the MultiIndex DataFrame, i.e., drop the combination 'falcon' and 'weight', which deletes only the corresponding row \n>>> df.drop(index=('falcon', 'weight'))\n                big     small\nlama    speed   45.0    30.0\n        weight  200.0   100.0\n        length  1.5     1.0\ncow     speed   30.0    20.0\n        weight  250.0   150.0\n        length  1.5     0.8\nfalcon  speed   320.0   250.0\n        length  0.3     0.2\n  \n>>> df.drop(index='cow', columns='small')\n                big\nlama    speed   45.0\n        weight  200.0\n        length  1.5\nfalcon  speed   320.0\n        weight  1.0\n        length  0.3\n  \n>>> df.drop(index='length', level=1)\n                big     small\nlama    speed   45.0    30.0\n        weight  200.0   100.0\ncow     speed   30.0    20.0\n        weight  250.0   150.0\nfalcon  speed   320.0   250.0\n        weight  1.0     0.8","title":"pandas.reference.api.pandas.dataframe.drop"},{"text":"pandas.DataFrame.dropna   DataFrame.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)[source]\n \nRemove missing values. See the User Guide for more on which values are considered missing, and how to work with missing data.  Parameters \n \naxis:{0 or \u2018index\u2019, 1 or \u2018columns\u2019}, default 0\n\n\nDetermine if rows or columns which contain missing values are removed.  0, or \u2018index\u2019 : Drop rows which contain missing values. 1, or \u2018columns\u2019 : Drop columns which contain missing value.   Changed in version 1.0.0: Pass tuple or list to drop on multiple axes. Only a single axis is allowed.   \nhow:{\u2018any\u2019, \u2018all\u2019}, default \u2018any\u2019\n\n\nDetermine if row or column is removed from DataFrame, when we have at least one NA or all NA.  \u2018any\u2019 : If any NA values are present, drop that row or column. \u2018all\u2019 : If all values are NA, drop that row or column.   \nthresh:int, optional\n\n\nRequire that many non-NA values.  \nsubset:column label or sequence of labels, optional\n\n\nLabels along other axis to consider, e.g. if you are dropping rows these would be a list of columns to include.  \ninplace:bool, default False\n\n\nIf True, do operation inplace and return None.    Returns \n DataFrame or None\n\nDataFrame with NA entries dropped from it or None if inplace=True.      See also  DataFrame.isna\n\nIndicate missing values.  DataFrame.notna\n\nIndicate existing (non-missing) values.  DataFrame.fillna\n\nReplace missing values.  Series.dropna\n\nDrop missing values.  Index.dropna\n\nDrop missing indices.    Examples \n>>> df = pd.DataFrame({\"name\": ['Alfred', 'Batman', 'Catwoman'],\n...                    \"toy\": [np.nan, 'Batmobile', 'Bullwhip'],\n...                    \"born\": [pd.NaT, pd.Timestamp(\"1940-04-25\"),\n...                             pd.NaT]})\n>>> df\n       name        toy       born\n0    Alfred        NaN        NaT\n1    Batman  Batmobile 1940-04-25\n2  Catwoman   Bullwhip        NaT\n  Drop the rows where at least one element is missing. \n>>> df.dropna()\n     name        toy       born\n1  Batman  Batmobile 1940-04-25\n  Drop the columns where at least one element is missing. \n>>> df.dropna(axis='columns')\n       name\n0    Alfred\n1    Batman\n2  Catwoman\n  Drop the rows where all elements are missing. \n>>> df.dropna(how='all')\n       name        toy       born\n0    Alfred        NaN        NaT\n1    Batman  Batmobile 1940-04-25\n2  Catwoman   Bullwhip        NaT\n  Keep only the rows with at least 2 non-NA values. \n>>> df.dropna(thresh=2)\n       name        toy       born\n1    Batman  Batmobile 1940-04-25\n2  Catwoman   Bullwhip        NaT\n  Define in which columns to look for missing values. \n>>> df.dropna(subset=['name', 'toy'])\n       name        toy       born\n1    Batman  Batmobile 1940-04-25\n2  Catwoman   Bullwhip        NaT\n  Keep the DataFrame with valid entries in the same variable. \n>>> df.dropna(inplace=True)\n>>> df\n     name        toy       born\n1  Batman  Batmobile 1940-04-25","title":"pandas.reference.api.pandas.dataframe.dropna"},{"text":"pandas.DataFrame.drop_duplicates   DataFrame.drop_duplicates(subset=None, keep='first', inplace=False, ignore_index=False)[source]\n \nReturn DataFrame with duplicate rows removed. Considering certain columns is optional. Indexes, including time indexes are ignored.  Parameters \n \nsubset:column label or sequence of labels, optional\n\n\nOnly consider certain columns for identifying duplicates, by default use all of the columns.  \nkeep:{\u2018first\u2019, \u2018last\u2019, False}, default \u2018first\u2019\n\n\nDetermines which duplicates (if any) to keep. - first : Drop duplicates except for the first occurrence. - last : Drop duplicates except for the last occurrence. - False : Drop all duplicates.  \ninplace:bool, default False\n\n\nWhether to drop duplicates in place or to return a copy.  \nignore_index:bool, default False\n\n\nIf True, the resulting axis will be labeled 0, 1, \u2026, n - 1.  New in version 1.0.0.     Returns \n DataFrame or None\n\nDataFrame with duplicates removed or None if inplace=True.      See also  DataFrame.value_counts\n\nCount unique combinations of columns.    Examples Consider dataset containing ramen rating. \n>>> df = pd.DataFrame({\n...     'brand': ['Yum Yum', 'Yum Yum', 'Indomie', 'Indomie', 'Indomie'],\n...     'style': ['cup', 'cup', 'cup', 'pack', 'pack'],\n...     'rating': [4, 4, 3.5, 15, 5]\n... })\n>>> df\n    brand style  rating\n0  Yum Yum   cup     4.0\n1  Yum Yum   cup     4.0\n2  Indomie   cup     3.5\n3  Indomie  pack    15.0\n4  Indomie  pack     5.0\n  By default, it removes duplicate rows based on all columns. \n>>> df.drop_duplicates()\n    brand style  rating\n0  Yum Yum   cup     4.0\n2  Indomie   cup     3.5\n3  Indomie  pack    15.0\n4  Indomie  pack     5.0\n  To remove duplicates on specific column(s), use subset. \n>>> df.drop_duplicates(subset=['brand'])\n    brand style  rating\n0  Yum Yum   cup     4.0\n2  Indomie   cup     3.5\n  To remove duplicates and keep last occurrences, use keep. \n>>> df.drop_duplicates(subset=['brand', 'style'], keep='last')\n    brand style  rating\n1  Yum Yum   cup     4.0\n2  Indomie   cup     3.5\n4  Indomie  pack     5.0","title":"pandas.reference.api.pandas.dataframe.drop_duplicates"},{"text":"Cookbook This is a repository for short and sweet examples and links for useful pandas recipes. We encourage users to add to this documentation. Adding interesting links and\/or inline examples to this section is a great First Pull Request. Simplified, condensed, new-user friendly, in-line examples have been inserted where possible to augment the Stack-Overflow and GitHub links. Many of the links contain expanded information, above what the in-line examples offer. pandas (pd) and NumPy (np) are the only two abbreviated imported modules. The rest are kept explicitly imported for newer users.  Idioms These are some neat pandas idioms if-then\/if-then-else on one column, and assignment to another one or more columns: \nIn [1]: df = pd.DataFrame(\n   ...:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ...: )\n   ...: \n\nIn [2]: df\nOut[2]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n   if-then\u2026 An if-then on one column \nIn [3]: df.loc[df.AAA >= 5, \"BBB\"] = -1\n\nIn [4]: df\nOut[4]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   -1   50\n2    6   -1  -30\n3    7   -1  -50\n  An if-then with assignment to 2 columns: \nIn [5]: df.loc[df.AAA >= 5, [\"BBB\", \"CCC\"]] = 555\n\nIn [6]: df\nOut[6]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5  555  555\n2    6  555  555\n3    7  555  555\n  Add another line with different logic, to do the -else \nIn [7]: df.loc[df.AAA < 5, [\"BBB\", \"CCC\"]] = 2000\n\nIn [8]: df\nOut[8]: \n   AAA   BBB   CCC\n0    4  2000  2000\n1    5   555   555\n2    6   555   555\n3    7   555   555\n  Or use pandas where after you\u2019ve set up a mask \nIn [9]: df_mask = pd.DataFrame(\n   ...:     {\"AAA\": [True] * 4, \"BBB\": [False] * 4, \"CCC\": [True, False] * 2}\n   ...: )\n   ...: \n\nIn [10]: df.where(df_mask, -1000)\nOut[10]: \n   AAA   BBB   CCC\n0    4 -1000  2000\n1    5 -1000 -1000\n2    6 -1000   555\n3    7 -1000 -1000\n  if-then-else using NumPy\u2019s where() \nIn [11]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [12]: df\nOut[12]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [13]: df[\"logic\"] = np.where(df[\"AAA\"] > 5, \"high\", \"low\")\n\nIn [14]: df\nOut[14]: \n   AAA  BBB  CCC logic\n0    4   10  100   low\n1    5   20   50   low\n2    6   30  -30  high\n3    7   40  -50  high\n    Splitting Split a frame with a boolean criterion \nIn [15]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [16]: df\nOut[16]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [17]: df[df.AAA <= 5]\nOut[17]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n\nIn [18]: df[df.AAA > 5]\nOut[18]: \n   AAA  BBB  CCC\n2    6   30  -30\n3    7   40  -50\n    Building criteria Select with multi-column criteria \nIn [19]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [20]: df\nOut[20]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n  \u2026and (without assignment returns a Series) \nIn [21]: df.loc[(df[\"BBB\"] < 25) & (df[\"CCC\"] >= -40), \"AAA\"]\nOut[21]: \n0    4\n1    5\nName: AAA, dtype: int64\n  \u2026or (without assignment returns a Series) \nIn [22]: df.loc[(df[\"BBB\"] > 25) | (df[\"CCC\"] >= -40), \"AAA\"]\nOut[22]: \n0    4\n1    5\n2    6\n3    7\nName: AAA, dtype: int64\n  \u2026or (with assignment modifies the DataFrame.) \nIn [23]: df.loc[(df[\"BBB\"] > 25) | (df[\"CCC\"] >= 75), \"AAA\"] = 0.1\n\nIn [24]: df\nOut[24]: \n   AAA  BBB  CCC\n0  0.1   10  100\n1  5.0   20   50\n2  0.1   30  -30\n3  0.1   40  -50\n  Select rows with data closest to certain value using argsort \nIn [25]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [26]: df\nOut[26]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [27]: aValue = 43.0\n\nIn [28]: df.loc[(df.CCC - aValue).abs().argsort()]\nOut[28]: \n   AAA  BBB  CCC\n1    5   20   50\n0    4   10  100\n2    6   30  -30\n3    7   40  -50\n  Dynamically reduce a list of criteria using a binary operators \nIn [29]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [30]: df\nOut[30]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [31]: Crit1 = df.AAA <= 5.5\n\nIn [32]: Crit2 = df.BBB == 10.0\n\nIn [33]: Crit3 = df.CCC > -40.0\n  One could hard code: \nIn [34]: AllCrit = Crit1 & Crit2 & Crit3\n  \u2026Or it can be done with a list of dynamically built criteria \nIn [35]: import functools\n\nIn [36]: CritList = [Crit1, Crit2, Crit3]\n\nIn [37]: AllCrit = functools.reduce(lambda x, y: x & y, CritList)\n\nIn [38]: df[AllCrit]\nOut[38]: \n   AAA  BBB  CCC\n0    4   10  100\n     Selection  Dataframes The indexing docs. Using both row labels and value conditionals \nIn [39]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [40]: df\nOut[40]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [41]: df[(df.AAA <= 6) & (df.index.isin([0, 2, 4]))]\nOut[41]: \n   AAA  BBB  CCC\n0    4   10  100\n2    6   30  -30\n  Use loc for label-oriented slicing and iloc positional slicing GH2904 \nIn [42]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]},\n   ....:     index=[\"foo\", \"bar\", \"boo\", \"kar\"],\n   ....: )\n   ....: \n  There are 2 explicit slicing methods, with a third general case  Positional-oriented (Python slicing style : exclusive of end) Label-oriented (Non-Python slicing style : inclusive of end) General (Either slicing style : depends on if the slice contains labels or positions)  \nIn [43]: df.loc[\"bar\":\"kar\"]  # Label\nOut[43]: \n     AAA  BBB  CCC\nbar    5   20   50\nboo    6   30  -30\nkar    7   40  -50\n\n# Generic\nIn [44]: df[0:3]\nOut[44]: \n     AAA  BBB  CCC\nfoo    4   10  100\nbar    5   20   50\nboo    6   30  -30\n\nIn [45]: df[\"bar\":\"kar\"]\nOut[45]: \n     AAA  BBB  CCC\nbar    5   20   50\nboo    6   30  -30\nkar    7   40  -50\n  Ambiguity arises when an index consists of integers with a non-zero start or non-unit increment. \nIn [46]: data = {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n\nIn [47]: df2 = pd.DataFrame(data=data, index=[1, 2, 3, 4])  # Note index starts at 1.\n\nIn [48]: df2.iloc[1:3]  # Position-oriented\nOut[48]: \n   AAA  BBB  CCC\n2    5   20   50\n3    6   30  -30\n\nIn [49]: df2.loc[1:3]  # Label-oriented\nOut[49]: \n   AAA  BBB  CCC\n1    4   10  100\n2    5   20   50\n3    6   30  -30\n  Using inverse operator (~) to take the complement of a mask \nIn [50]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [51]: df\nOut[51]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [52]: df[~((df.AAA <= 6) & (df.index.isin([0, 2, 4])))]\nOut[52]: \n   AAA  BBB  CCC\n1    5   20   50\n3    7   40  -50\n    New columns Efficiently and dynamically creating new columns using applymap \nIn [53]: df = pd.DataFrame({\"AAA\": [1, 2, 1, 3], \"BBB\": [1, 1, 2, 2], \"CCC\": [2, 1, 3, 1]})\n\nIn [54]: df\nOut[54]: \n   AAA  BBB  CCC\n0    1    1    2\n1    2    1    1\n2    1    2    3\n3    3    2    1\n\nIn [55]: source_cols = df.columns  # Or some subset would work too\n\nIn [56]: new_cols = [str(x) + \"_cat\" for x in source_cols]\n\nIn [57]: categories = {1: \"Alpha\", 2: \"Beta\", 3: \"Charlie\"}\n\nIn [58]: df[new_cols] = df[source_cols].applymap(categories.get)\n\nIn [59]: df\nOut[59]: \n   AAA  BBB  CCC  AAA_cat BBB_cat  CCC_cat\n0    1    1    2    Alpha   Alpha     Beta\n1    2    1    1     Beta   Alpha    Alpha\n2    1    2    3    Alpha    Beta  Charlie\n3    3    2    1  Charlie    Beta    Alpha\n  Keep other columns when using min() with groupby \nIn [60]: df = pd.DataFrame(\n   ....:     {\"AAA\": [1, 1, 1, 2, 2, 2, 3, 3], \"BBB\": [2, 1, 3, 4, 5, 1, 2, 3]}\n   ....: )\n   ....: \n\nIn [61]: df\nOut[61]: \n   AAA  BBB\n0    1    2\n1    1    1\n2    1    3\n3    2    4\n4    2    5\n5    2    1\n6    3    2\n7    3    3\n  Method 1 : idxmin() to get the index of the minimums \nIn [62]: df.loc[df.groupby(\"AAA\")[\"BBB\"].idxmin()]\nOut[62]: \n   AAA  BBB\n1    1    1\n5    2    1\n6    3    2\n  Method 2 : sort then take first of each \nIn [63]: df.sort_values(by=\"BBB\").groupby(\"AAA\", as_index=False).first()\nOut[63]: \n   AAA  BBB\n0    1    1\n1    2    1\n2    3    2\n  Notice the same results, with the exception of the index.    Multiindexing The multindexing docs. Creating a MultiIndex from a labeled frame \nIn [64]: df = pd.DataFrame(\n   ....:     {\n   ....:         \"row\": [0, 1, 2],\n   ....:         \"One_X\": [1.1, 1.1, 1.1],\n   ....:         \"One_Y\": [1.2, 1.2, 1.2],\n   ....:         \"Two_X\": [1.11, 1.11, 1.11],\n   ....:         \"Two_Y\": [1.22, 1.22, 1.22],\n   ....:     }\n   ....: )\n   ....: \n\nIn [65]: df\nOut[65]: \n   row  One_X  One_Y  Two_X  Two_Y\n0    0    1.1    1.2   1.11   1.22\n1    1    1.1    1.2   1.11   1.22\n2    2    1.1    1.2   1.11   1.22\n\n# As Labelled Index\nIn [66]: df = df.set_index(\"row\")\n\nIn [67]: df\nOut[67]: \n     One_X  One_Y  Two_X  Two_Y\nrow                            \n0      1.1    1.2   1.11   1.22\n1      1.1    1.2   1.11   1.22\n2      1.1    1.2   1.11   1.22\n\n# With Hierarchical Columns\nIn [68]: df.columns = pd.MultiIndex.from_tuples([tuple(c.split(\"_\")) for c in df.columns])\n\nIn [69]: df\nOut[69]: \n     One        Two      \n       X    Y     X     Y\nrow                      \n0    1.1  1.2  1.11  1.22\n1    1.1  1.2  1.11  1.22\n2    1.1  1.2  1.11  1.22\n\n# Now stack & Reset\nIn [70]: df = df.stack(0).reset_index(1)\n\nIn [71]: df\nOut[71]: \n    level_1     X     Y\nrow                    \n0       One  1.10  1.20\n0       Two  1.11  1.22\n1       One  1.10  1.20\n1       Two  1.11  1.22\n2       One  1.10  1.20\n2       Two  1.11  1.22\n\n# And fix the labels (Notice the label 'level_1' got added automatically)\nIn [72]: df.columns = [\"Sample\", \"All_X\", \"All_Y\"]\n\nIn [73]: df\nOut[73]: \n    Sample  All_X  All_Y\nrow                     \n0      One   1.10   1.20\n0      Two   1.11   1.22\n1      One   1.10   1.20\n1      Two   1.11   1.22\n2      One   1.10   1.20\n2      Two   1.11   1.22\n   Arithmetic Performing arithmetic with a MultiIndex that needs broadcasting \nIn [74]: cols = pd.MultiIndex.from_tuples(\n   ....:     [(x, y) for x in [\"A\", \"B\", \"C\"] for y in [\"O\", \"I\"]]\n   ....: )\n   ....: \n\nIn [75]: df = pd.DataFrame(np.random.randn(2, 6), index=[\"n\", \"m\"], columns=cols)\n\nIn [76]: df\nOut[76]: \n          A                   B                   C          \n          O         I         O         I         O         I\nn  0.469112 -0.282863 -1.509059 -1.135632  1.212112 -0.173215\nm  0.119209 -1.044236 -0.861849 -2.104569 -0.494929  1.071804\n\nIn [77]: df = df.div(df[\"C\"], level=1)\n\nIn [78]: df\nOut[78]: \n          A                   B              C     \n          O         I         O         I    O    I\nn  0.387021  1.633022 -1.244983  6.556214  1.0  1.0\nm -0.240860 -0.974279  1.741358 -1.963577  1.0  1.0\n    Slicing Slicing a MultiIndex with xs \nIn [79]: coords = [(\"AA\", \"one\"), (\"AA\", \"six\"), (\"BB\", \"one\"), (\"BB\", \"two\"), (\"BB\", \"six\")]\n\nIn [80]: index = pd.MultiIndex.from_tuples(coords)\n\nIn [81]: df = pd.DataFrame([11, 22, 33, 44, 55], index, [\"MyData\"])\n\nIn [82]: df\nOut[82]: \n        MyData\nAA one      11\n   six      22\nBB one      33\n   two      44\n   six      55\n  To take the cross section of the 1st level and 1st axis the index: \n# Note : level and axis are optional, and default to zero\nIn [83]: df.xs(\"BB\", level=0, axis=0)\nOut[83]: \n     MyData\none      33\ntwo      44\nsix      55\n  \u2026and now the 2nd level of the 1st axis. \nIn [84]: df.xs(\"six\", level=1, axis=0)\nOut[84]: \n    MyData\nAA      22\nBB      55\n  Slicing a MultiIndex with xs, method #2 \nIn [85]: import itertools\n\nIn [86]: index = list(itertools.product([\"Ada\", \"Quinn\", \"Violet\"], [\"Comp\", \"Math\", \"Sci\"]))\n\nIn [87]: headr = list(itertools.product([\"Exams\", \"Labs\"], [\"I\", \"II\"]))\n\nIn [88]: indx = pd.MultiIndex.from_tuples(index, names=[\"Student\", \"Course\"])\n\nIn [89]: cols = pd.MultiIndex.from_tuples(headr)  # Notice these are un-named\n\nIn [90]: data = [[70 + x + y + (x * y) % 3 for x in range(4)] for y in range(9)]\n\nIn [91]: df = pd.DataFrame(data, indx, cols)\n\nIn [92]: df\nOut[92]: \n               Exams     Labs    \n                   I  II    I  II\nStudent Course                   \nAda     Comp      70  71   72  73\n        Math      71  73   75  74\n        Sci       72  75   75  75\nQuinn   Comp      73  74   75  76\n        Math      74  76   78  77\n        Sci       75  78   78  78\nViolet  Comp      76  77   78  79\n        Math      77  79   81  80\n        Sci       78  81   81  81\n\nIn [93]: All = slice(None)\n\nIn [94]: df.loc[\"Violet\"]\nOut[94]: \n       Exams     Labs    \n           I  II    I  II\nCourse                   \nComp      76  77   78  79\nMath      77  79   81  80\nSci       78  81   81  81\n\nIn [95]: df.loc[(All, \"Math\"), All]\nOut[95]: \n               Exams     Labs    \n                   I  II    I  II\nStudent Course                   \nAda     Math      71  73   75  74\nQuinn   Math      74  76   78  77\nViolet  Math      77  79   81  80\n\nIn [96]: df.loc[(slice(\"Ada\", \"Quinn\"), \"Math\"), All]\nOut[96]: \n               Exams     Labs    \n                   I  II    I  II\nStudent Course                   \nAda     Math      71  73   75  74\nQuinn   Math      74  76   78  77\n\nIn [97]: df.loc[(All, \"Math\"), (\"Exams\")]\nOut[97]: \n                 I  II\nStudent Course        \nAda     Math    71  73\nQuinn   Math    74  76\nViolet  Math    77  79\n\nIn [98]: df.loc[(All, \"Math\"), (All, \"II\")]\nOut[98]: \n               Exams Labs\n                  II   II\nStudent Course           \nAda     Math      73   74\nQuinn   Math      76   77\nViolet  Math      79   80\n  Setting portions of a MultiIndex with xs   Sorting Sort by specific column or an ordered list of columns, with a MultiIndex \nIn [99]: df.sort_values(by=(\"Labs\", \"II\"), ascending=False)\nOut[99]: \n               Exams     Labs    \n                   I  II    I  II\nStudent Course                   \nViolet  Sci       78  81   81  81\n        Math      77  79   81  80\n        Comp      76  77   78  79\nQuinn   Sci       75  78   78  78\n        Math      74  76   78  77\n        Comp      73  74   75  76\nAda     Sci       72  75   75  75\n        Math      71  73   75  74\n        Comp      70  71   72  73\n  Partial selection, the need for sortedness GH2995   Levels Prepending a level to a multiindex Flatten Hierarchical columns    Missing data The missing data docs. Fill forward a reversed timeseries \nIn [100]: df = pd.DataFrame(\n   .....:     np.random.randn(6, 1),\n   .....:     index=pd.date_range(\"2013-08-01\", periods=6, freq=\"B\"),\n   .....:     columns=list(\"A\"),\n   .....: )\n   .....: \n\nIn [101]: df.loc[df.index[3], \"A\"] = np.nan\n\nIn [102]: df\nOut[102]: \n                   A\n2013-08-01  0.721555\n2013-08-02 -0.706771\n2013-08-05 -1.039575\n2013-08-06       NaN\n2013-08-07 -0.424972\n2013-08-08  0.567020\n\nIn [103]: df.reindex(df.index[::-1]).ffill()\nOut[103]: \n                   A\n2013-08-08  0.567020\n2013-08-07 -0.424972\n2013-08-06 -0.424972\n2013-08-05 -1.039575\n2013-08-02 -0.706771\n2013-08-01  0.721555\n  cumsum reset at NaN values  Replace Using replace with backrefs    Grouping The grouping docs. Basic grouping with apply Unlike agg, apply\u2019s callable is passed a sub-DataFrame which gives you access to all the columns \nIn [104]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"animal\": \"cat dog cat fish dog cat cat\".split(),\n   .....:         \"size\": list(\"SSMMMLL\"),\n   .....:         \"weight\": [8, 10, 11, 1, 20, 12, 12],\n   .....:         \"adult\": [False] * 5 + [True] * 2,\n   .....:     }\n   .....: )\n   .....: \n\nIn [105]: df\nOut[105]: \n  animal size  weight  adult\n0    cat    S       8  False\n1    dog    S      10  False\n2    cat    M      11  False\n3   fish    M       1  False\n4    dog    M      20  False\n5    cat    L      12   True\n6    cat    L      12   True\n\n# List the size of the animals with the highest weight.\nIn [106]: df.groupby(\"animal\").apply(lambda subf: subf[\"size\"][subf[\"weight\"].idxmax()])\nOut[106]: \nanimal\ncat     L\ndog     M\nfish    M\ndtype: object\n  Using get_group \nIn [107]: gb = df.groupby([\"animal\"])\n\nIn [108]: gb.get_group(\"cat\")\nOut[108]: \n  animal size  weight  adult\n0    cat    S       8  False\n2    cat    M      11  False\n5    cat    L      12   True\n6    cat    L      12   True\n  Apply to different items in a group \nIn [109]: def GrowUp(x):\n   .....:     avg_weight = sum(x[x[\"size\"] == \"S\"].weight * 1.5)\n   .....:     avg_weight += sum(x[x[\"size\"] == \"M\"].weight * 1.25)\n   .....:     avg_weight += sum(x[x[\"size\"] == \"L\"].weight)\n   .....:     avg_weight \/= len(x)\n   .....:     return pd.Series([\"L\", avg_weight, True], index=[\"size\", \"weight\", \"adult\"])\n   .....: \n\nIn [110]: expected_df = gb.apply(GrowUp)\n\nIn [111]: expected_df\nOut[111]: \n       size   weight  adult\nanimal                     \ncat       L  12.4375   True\ndog       L  20.0000   True\nfish      L   1.2500   True\n  Expanding apply \nIn [112]: S = pd.Series([i \/ 100.0 for i in range(1, 11)])\n\nIn [113]: def cum_ret(x, y):\n   .....:     return x * (1 + y)\n   .....: \n\nIn [114]: def red(x):\n   .....:     return functools.reduce(cum_ret, x, 1.0)\n   .....: \n\nIn [115]: S.expanding().apply(red, raw=True)\nOut[115]: \n0    1.010000\n1    1.030200\n2    1.061106\n3    1.103550\n4    1.158728\n5    1.228251\n6    1.314229\n7    1.419367\n8    1.547110\n9    1.701821\ndtype: float64\n  Replacing some values with mean of the rest of a group \nIn [116]: df = pd.DataFrame({\"A\": [1, 1, 2, 2], \"B\": [1, -1, 1, 2]})\n\nIn [117]: gb = df.groupby(\"A\")\n\nIn [118]: def replace(g):\n   .....:     mask = g < 0\n   .....:     return g.where(mask, g[~mask].mean())\n   .....: \n\nIn [119]: gb.transform(replace)\nOut[119]: \n     B\n0  1.0\n1 -1.0\n2  1.5\n3  1.5\n  Sort groups by aggregated data \nIn [120]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"code\": [\"foo\", \"bar\", \"baz\"] * 2,\n   .....:         \"data\": [0.16, -0.21, 0.33, 0.45, -0.59, 0.62],\n   .....:         \"flag\": [False, True] * 3,\n   .....:     }\n   .....: )\n   .....: \n\nIn [121]: code_groups = df.groupby(\"code\")\n\nIn [122]: agg_n_sort_order = code_groups[[\"data\"]].transform(sum).sort_values(by=\"data\")\n\nIn [123]: sorted_df = df.loc[agg_n_sort_order.index]\n\nIn [124]: sorted_df\nOut[124]: \n  code  data   flag\n1  bar -0.21   True\n4  bar -0.59  False\n0  foo  0.16  False\n3  foo  0.45   True\n2  baz  0.33  False\n5  baz  0.62   True\n  Create multiple aggregated columns \nIn [125]: rng = pd.date_range(start=\"2014-10-07\", periods=10, freq=\"2min\")\n\nIn [126]: ts = pd.Series(data=list(range(10)), index=rng)\n\nIn [127]: def MyCust(x):\n   .....:     if len(x) > 2:\n   .....:         return x[1] * 1.234\n   .....:     return pd.NaT\n   .....: \n\nIn [128]: mhc = {\"Mean\": np.mean, \"Max\": np.max, \"Custom\": MyCust}\n\nIn [129]: ts.resample(\"5min\").apply(mhc)\nOut[129]: \n                     Mean  Max Custom\n2014-10-07 00:00:00   1.0    2  1.234\n2014-10-07 00:05:00   3.5    4    NaT\n2014-10-07 00:10:00   6.0    7  7.404\n2014-10-07 00:15:00   8.5    9    NaT\n\nIn [130]: ts\nOut[130]: \n2014-10-07 00:00:00    0\n2014-10-07 00:02:00    1\n2014-10-07 00:04:00    2\n2014-10-07 00:06:00    3\n2014-10-07 00:08:00    4\n2014-10-07 00:10:00    5\n2014-10-07 00:12:00    6\n2014-10-07 00:14:00    7\n2014-10-07 00:16:00    8\n2014-10-07 00:18:00    9\nFreq: 2T, dtype: int64\n  Create a value counts column and reassign back to the DataFrame \nIn [131]: df = pd.DataFrame(\n   .....:     {\"Color\": \"Red Red Red Blue\".split(), \"Value\": [100, 150, 50, 50]}\n   .....: )\n   .....: \n\nIn [132]: df\nOut[132]: \n  Color  Value\n0   Red    100\n1   Red    150\n2   Red     50\n3  Blue     50\n\nIn [133]: df[\"Counts\"] = df.groupby([\"Color\"]).transform(len)\n\nIn [134]: df\nOut[134]: \n  Color  Value  Counts\n0   Red    100       3\n1   Red    150       3\n2   Red     50       3\n3  Blue     50       1\n  Shift groups of the values in a column based on the index \nIn [135]: df = pd.DataFrame(\n   .....:     {\"line_race\": [10, 10, 8, 10, 10, 8], \"beyer\": [99, 102, 103, 103, 88, 100]},\n   .....:     index=[\n   .....:         \"Last Gunfighter\",\n   .....:         \"Last Gunfighter\",\n   .....:         \"Last Gunfighter\",\n   .....:         \"Paynter\",\n   .....:         \"Paynter\",\n   .....:         \"Paynter\",\n   .....:     ],\n   .....: )\n   .....: \n\nIn [136]: df\nOut[136]: \n                 line_race  beyer\nLast Gunfighter         10     99\nLast Gunfighter         10    102\nLast Gunfighter          8    103\nPaynter                 10    103\nPaynter                 10     88\nPaynter                  8    100\n\nIn [137]: df[\"beyer_shifted\"] = df.groupby(level=0)[\"beyer\"].shift(1)\n\nIn [138]: df\nOut[138]: \n                 line_race  beyer  beyer_shifted\nLast Gunfighter         10     99            NaN\nLast Gunfighter         10    102           99.0\nLast Gunfighter          8    103          102.0\nPaynter                 10    103            NaN\nPaynter                 10     88          103.0\nPaynter                  8    100           88.0\n  Select row with maximum value from each group \nIn [139]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"host\": [\"other\", \"other\", \"that\", \"this\", \"this\"],\n   .....:         \"service\": [\"mail\", \"web\", \"mail\", \"mail\", \"web\"],\n   .....:         \"no\": [1, 2, 1, 2, 1],\n   .....:     }\n   .....: ).set_index([\"host\", \"service\"])\n   .....: \n\nIn [140]: mask = df.groupby(level=0).agg(\"idxmax\")\n\nIn [141]: df_count = df.loc[mask[\"no\"]].reset_index()\n\nIn [142]: df_count\nOut[142]: \n    host service  no\n0  other     web   2\n1   that    mail   1\n2   this    mail   2\n  Grouping like Python\u2019s itertools.groupby \nIn [143]: df = pd.DataFrame([0, 1, 0, 1, 1, 1, 0, 1, 1], columns=[\"A\"])\n\nIn [144]: df[\"A\"].groupby((df[\"A\"] != df[\"A\"].shift()).cumsum()).groups\nOut[144]: {1: [0], 2: [1], 3: [2], 4: [3, 4, 5], 5: [6], 6: [7, 8]}\n\nIn [145]: df[\"A\"].groupby((df[\"A\"] != df[\"A\"].shift()).cumsum()).cumsum()\nOut[145]: \n0    0\n1    1\n2    0\n3    1\n4    2\n5    3\n6    0\n7    1\n8    2\nName: A, dtype: int64\n   Expanding data Alignment and to-date Rolling Computation window based on values instead of counts Rolling Mean by Time Interval   Splitting Splitting a frame Create a list of dataframes, split using a delineation based on logic included in rows. \nIn [146]: df = pd.DataFrame(\n   .....:     data={\n   .....:         \"Case\": [\"A\", \"A\", \"A\", \"B\", \"A\", \"A\", \"B\", \"A\", \"A\"],\n   .....:         \"Data\": np.random.randn(9),\n   .....:     }\n   .....: )\n   .....: \n\nIn [147]: dfs = list(\n   .....:     zip(\n   .....:         *df.groupby(\n   .....:             (1 * (df[\"Case\"] == \"B\"))\n   .....:             .cumsum()\n   .....:             .rolling(window=3, min_periods=1)\n   .....:             .median()\n   .....:         )\n   .....:     )\n   .....: )[-1]\n   .....: \n\nIn [148]: dfs[0]\nOut[148]: \n  Case      Data\n0    A  0.276232\n1    A -1.087401\n2    A -0.673690\n3    B  0.113648\n\nIn [149]: dfs[1]\nOut[149]: \n  Case      Data\n4    A -1.478427\n5    A  0.524988\n6    B  0.404705\n\nIn [150]: dfs[2]\nOut[150]: \n  Case      Data\n7    A  0.577046\n8    A -1.715002\n    Pivot The Pivot docs. Partial sums and subtotals \nIn [151]: df = pd.DataFrame(\n   .....:     data={\n   .....:         \"Province\": [\"ON\", \"QC\", \"BC\", \"AL\", \"AL\", \"MN\", \"ON\"],\n   .....:         \"City\": [\n   .....:             \"Toronto\",\n   .....:             \"Montreal\",\n   .....:             \"Vancouver\",\n   .....:             \"Calgary\",\n   .....:             \"Edmonton\",\n   .....:             \"Winnipeg\",\n   .....:             \"Windsor\",\n   .....:         ],\n   .....:         \"Sales\": [13, 6, 16, 8, 4, 3, 1],\n   .....:     }\n   .....: )\n   .....: \n\nIn [152]: table = pd.pivot_table(\n   .....:     df,\n   .....:     values=[\"Sales\"],\n   .....:     index=[\"Province\"],\n   .....:     columns=[\"City\"],\n   .....:     aggfunc=np.sum,\n   .....:     margins=True,\n   .....: )\n   .....: \n\nIn [153]: table.stack(\"City\")\nOut[153]: \n                    Sales\nProvince City            \nAL       All         12.0\n         Calgary      8.0\n         Edmonton     4.0\nBC       All         16.0\n         Vancouver   16.0\n...                   ...\nAll      Montreal     6.0\n         Toronto     13.0\n         Vancouver   16.0\n         Windsor      1.0\n         Winnipeg     3.0\n\n[20 rows x 1 columns]\n  Frequency table like plyr in R \nIn [154]: grades = [48, 99, 75, 80, 42, 80, 72, 68, 36, 78]\n\nIn [155]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"ID\": [\"x%d\" % r for r in range(10)],\n   .....:         \"Gender\": [\"F\", \"M\", \"F\", \"M\", \"F\", \"M\", \"F\", \"M\", \"M\", \"M\"],\n   .....:         \"ExamYear\": [\n   .....:             \"2007\",\n   .....:             \"2007\",\n   .....:             \"2007\",\n   .....:             \"2008\",\n   .....:             \"2008\",\n   .....:             \"2008\",\n   .....:             \"2008\",\n   .....:             \"2009\",\n   .....:             \"2009\",\n   .....:             \"2009\",\n   .....:         ],\n   .....:         \"Class\": [\n   .....:             \"algebra\",\n   .....:             \"stats\",\n   .....:             \"bio\",\n   .....:             \"algebra\",\n   .....:             \"algebra\",\n   .....:             \"stats\",\n   .....:             \"stats\",\n   .....:             \"algebra\",\n   .....:             \"bio\",\n   .....:             \"bio\",\n   .....:         ],\n   .....:         \"Participated\": [\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"no\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:         ],\n   .....:         \"Passed\": [\"yes\" if x > 50 else \"no\" for x in grades],\n   .....:         \"Employed\": [\n   .....:             True,\n   .....:             True,\n   .....:             True,\n   .....:             False,\n   .....:             False,\n   .....:             False,\n   .....:             False,\n   .....:             True,\n   .....:             True,\n   .....:             False,\n   .....:         ],\n   .....:         \"Grade\": grades,\n   .....:     }\n   .....: )\n   .....: \n\nIn [156]: df.groupby(\"ExamYear\").agg(\n   .....:     {\n   .....:         \"Participated\": lambda x: x.value_counts()[\"yes\"],\n   .....:         \"Passed\": lambda x: sum(x == \"yes\"),\n   .....:         \"Employed\": lambda x: sum(x),\n   .....:         \"Grade\": lambda x: sum(x) \/ len(x),\n   .....:     }\n   .....: )\n   .....: \nOut[156]: \n          Participated  Passed  Employed      Grade\nExamYear                                           \n2007                 3       2         3  74.000000\n2008                 3       3         0  68.500000\n2009                 3       2         2  60.666667\n  Plot pandas DataFrame with year over year data To create year and month cross tabulation: \nIn [157]: df = pd.DataFrame(\n   .....:     {\"value\": np.random.randn(36)},\n   .....:     index=pd.date_range(\"2011-01-01\", freq=\"M\", periods=36),\n   .....: )\n   .....: \n\nIn [158]: pd.pivot_table(\n   .....:     df, index=df.index.month, columns=df.index.year, values=\"value\", aggfunc=\"sum\"\n   .....: )\n   .....: \nOut[158]: \n        2011      2012      2013\n1  -1.039268 -0.968914  2.565646\n2  -0.370647 -1.294524  1.431256\n3  -1.157892  0.413738  1.340309\n4  -1.344312  0.276662 -1.170299\n5   0.844885 -0.472035 -0.226169\n6   1.075770 -0.013960  0.410835\n7  -0.109050 -0.362543  0.813850\n8   1.643563 -0.006154  0.132003\n9  -1.469388 -0.923061 -0.827317\n10  0.357021  0.895717 -0.076467\n11 -0.674600  0.805244 -1.187678\n12 -1.776904 -1.206412  1.130127\n    Apply Rolling apply to organize - Turning embedded lists into a MultiIndex frame \nIn [159]: df = pd.DataFrame(\n   .....:     data={\n   .....:         \"A\": [[2, 4, 8, 16], [100, 200], [10, 20, 30]],\n   .....:         \"B\": [[\"a\", \"b\", \"c\"], [\"jj\", \"kk\"], [\"ccc\"]],\n   .....:     },\n   .....:     index=[\"I\", \"II\", \"III\"],\n   .....: )\n   .....: \n\nIn [160]: def SeriesFromSubList(aList):\n   .....:     return pd.Series(aList)\n   .....: \n\nIn [161]: df_orgz = pd.concat(\n   .....:     {ind: row.apply(SeriesFromSubList) for ind, row in df.iterrows()}\n   .....: )\n   .....: \n\nIn [162]: df_orgz\nOut[162]: \n         0     1     2     3\nI   A    2     4     8  16.0\n    B    a     b     c   NaN\nII  A  100   200   NaN   NaN\n    B   jj    kk   NaN   NaN\nIII A   10  20.0  30.0   NaN\n    B  ccc   NaN   NaN   NaN\n  Rolling apply with a DataFrame returning a Series Rolling Apply to multiple columns where function calculates a Series before a Scalar from the Series is returned \nIn [163]: df = pd.DataFrame(\n   .....:     data=np.random.randn(2000, 2) \/ 10000,\n   .....:     index=pd.date_range(\"2001-01-01\", periods=2000),\n   .....:     columns=[\"A\", \"B\"],\n   .....: )\n   .....: \n\nIn [164]: df\nOut[164]: \n                   A         B\n2001-01-01 -0.000144 -0.000141\n2001-01-02  0.000161  0.000102\n2001-01-03  0.000057  0.000088\n2001-01-04 -0.000221  0.000097\n2001-01-05 -0.000201 -0.000041\n...              ...       ...\n2006-06-19  0.000040 -0.000235\n2006-06-20 -0.000123 -0.000021\n2006-06-21 -0.000113  0.000114\n2006-06-22  0.000136  0.000109\n2006-06-23  0.000027  0.000030\n\n[2000 rows x 2 columns]\n\nIn [165]: def gm(df, const):\n   .....:     v = ((((df[\"A\"] + df[\"B\"]) + 1).cumprod()) - 1) * const\n   .....:     return v.iloc[-1]\n   .....: \n\nIn [166]: s = pd.Series(\n   .....:     {\n   .....:         df.index[i]: gm(df.iloc[i: min(i + 51, len(df) - 1)], 5)\n   .....:         for i in range(len(df) - 50)\n   .....:     }\n   .....: )\n   .....: \n\nIn [167]: s\nOut[167]: \n2001-01-01    0.000930\n2001-01-02    0.002615\n2001-01-03    0.001281\n2001-01-04    0.001117\n2001-01-05    0.002772\n                ...   \n2006-04-30    0.003296\n2006-05-01    0.002629\n2006-05-02    0.002081\n2006-05-03    0.004247\n2006-05-04    0.003928\nLength: 1950, dtype: float64\n  Rolling apply with a DataFrame returning a Scalar Rolling Apply to multiple columns where function returns a Scalar (Volume Weighted Average Price) \nIn [168]: rng = pd.date_range(start=\"2014-01-01\", periods=100)\n\nIn [169]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"Open\": np.random.randn(len(rng)),\n   .....:         \"Close\": np.random.randn(len(rng)),\n   .....:         \"Volume\": np.random.randint(100, 2000, len(rng)),\n   .....:     },\n   .....:     index=rng,\n   .....: )\n   .....: \n\nIn [170]: df\nOut[170]: \n                Open     Close  Volume\n2014-01-01 -1.611353 -0.492885    1219\n2014-01-02 -3.000951  0.445794    1054\n2014-01-03 -0.138359 -0.076081    1381\n2014-01-04  0.301568  1.198259    1253\n2014-01-05  0.276381 -0.669831    1728\n...              ...       ...     ...\n2014-04-06 -0.040338  0.937843    1188\n2014-04-07  0.359661 -0.285908    1864\n2014-04-08  0.060978  1.714814     941\n2014-04-09  1.759055 -0.455942    1065\n2014-04-10  0.138185 -1.147008    1453\n\n[100 rows x 3 columns]\n\nIn [171]: def vwap(bars):\n   .....:     return (bars.Close * bars.Volume).sum() \/ bars.Volume.sum()\n   .....: \n\nIn [172]: window = 5\n\nIn [173]: s = pd.concat(\n   .....:     [\n   .....:         (pd.Series(vwap(df.iloc[i: i + window]), index=[df.index[i + window]]))\n   .....:         for i in range(len(df) - window)\n   .....:     ]\n   .....: )\n   .....: \n\nIn [174]: s.round(2)\nOut[174]: \n2014-01-06    0.02\n2014-01-07    0.11\n2014-01-08    0.10\n2014-01-09    0.07\n2014-01-10   -0.29\n              ... \n2014-04-06   -0.63\n2014-04-07   -0.02\n2014-04-08   -0.03\n2014-04-09    0.34\n2014-04-10    0.29\nLength: 95, dtype: float64\n     Timeseries Between times Using indexer between time Constructing a datetime range that excludes weekends and includes only certain times Vectorized Lookup Aggregation and plotting time series Turn a matrix with hours in columns and days in rows into a continuous row sequence in the form of a time series. How to rearrange a Python pandas DataFrame? Dealing with duplicates when reindexing a timeseries to a specified frequency Calculate the first day of the month for each entry in a DatetimeIndex \nIn [175]: dates = pd.date_range(\"2000-01-01\", periods=5)\n\nIn [176]: dates.to_period(freq=\"M\").to_timestamp()\nOut[176]: \nDatetimeIndex(['2000-01-01', '2000-01-01', '2000-01-01', '2000-01-01',\n               '2000-01-01'],\n              dtype='datetime64[ns]', freq=None)\n   Resampling The Resample docs. Using Grouper instead of TimeGrouper for time grouping of values Time grouping with some missing values Valid frequency arguments to Grouper Timeseries Grouping using a MultiIndex Using TimeGrouper and another grouping to create subgroups, then apply a custom function GH3791 Resampling with custom periods Resample intraday frame without adding new days Resample minute data Resample with groupby    Merge The Join docs. Concatenate two dataframes with overlapping index (emulate R rbind) \nIn [177]: rng = pd.date_range(\"2000-01-01\", periods=6)\n\nIn [178]: df1 = pd.DataFrame(np.random.randn(6, 3), index=rng, columns=[\"A\", \"B\", \"C\"])\n\nIn [179]: df2 = df1.copy()\n  Depending on df construction, ignore_index may be needed \nIn [180]: df = pd.concat([df1, df2], ignore_index=True)\n\nIn [181]: df\nOut[181]: \n           A         B         C\n0  -0.870117 -0.479265 -0.790855\n1   0.144817  1.726395 -0.464535\n2  -0.821906  1.597605  0.187307\n3  -0.128342 -1.511638 -0.289858\n4   0.399194 -1.430030 -0.639760\n5   1.115116 -2.012600  1.810662\n6  -0.870117 -0.479265 -0.790855\n7   0.144817  1.726395 -0.464535\n8  -0.821906  1.597605  0.187307\n9  -0.128342 -1.511638 -0.289858\n10  0.399194 -1.430030 -0.639760\n11  1.115116 -2.012600  1.810662\n  Self Join of a DataFrame GH2996 \nIn [182]: df = pd.DataFrame(\n   .....:     data={\n   .....:         \"Area\": [\"A\"] * 5 + [\"C\"] * 2,\n   .....:         \"Bins\": [110] * 2 + [160] * 3 + [40] * 2,\n   .....:         \"Test_0\": [0, 1, 0, 1, 2, 0, 1],\n   .....:         \"Data\": np.random.randn(7),\n   .....:     }\n   .....: )\n   .....: \n\nIn [183]: df\nOut[183]: \n  Area  Bins  Test_0      Data\n0    A   110       0 -0.433937\n1    A   110       1 -0.160552\n2    A   160       0  0.744434\n3    A   160       1  1.754213\n4    A   160       2  0.000850\n5    C    40       0  0.342243\n6    C    40       1  1.070599\n\nIn [184]: df[\"Test_1\"] = df[\"Test_0\"] - 1\n\nIn [185]: pd.merge(\n   .....:     df,\n   .....:     df,\n   .....:     left_on=[\"Bins\", \"Area\", \"Test_0\"],\n   .....:     right_on=[\"Bins\", \"Area\", \"Test_1\"],\n   .....:     suffixes=(\"_L\", \"_R\"),\n   .....: )\n   .....: \nOut[185]: \n  Area  Bins  Test_0_L    Data_L  Test_1_L  Test_0_R    Data_R  Test_1_R\n0    A   110         0 -0.433937        -1         1 -0.160552         0\n1    A   160         0  0.744434        -1         1  1.754213         0\n2    A   160         1  1.754213         0         2  0.000850         1\n3    C    40         0  0.342243        -1         1  1.070599         0\n  How to set the index and join KDB like asof join Join with a criteria based on the values Using searchsorted to merge based on values inside a range   Plotting The Plotting docs. Make Matplotlib look like R Setting x-axis major and minor labels Plotting multiple charts in an IPython Jupyter notebook Creating a multi-line plot Plotting a heatmap Annotate a time-series plot Annotate a time-series plot #2 Generate Embedded plots in excel files using Pandas, Vincent and xlsxwriter Boxplot for each quartile of a stratifying variable \nIn [186]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"stratifying_var\": np.random.uniform(0, 100, 20),\n   .....:         \"price\": np.random.normal(100, 5, 20),\n   .....:     }\n   .....: )\n   .....: \n\nIn [187]: df[\"quartiles\"] = pd.qcut(\n   .....:     df[\"stratifying_var\"], 4, labels=[\"0-25%\", \"25-50%\", \"50-75%\", \"75-100%\"]\n   .....: )\n   .....: \n\nIn [188]: df.boxplot(column=\"price\", by=\"quartiles\")\nOut[188]: <AxesSubplot:title={'center':'price'}, xlabel='quartiles'>\n     Data in\/out Performance comparison of SQL vs HDF5  CSV The CSV docs read_csv in action appending to a csv Reading a csv chunk-by-chunk Reading only certain rows of a csv chunk-by-chunk Reading the first few lines of a frame Reading a file that is compressed but not by gzip\/bz2 (the native compressed formats which read_csv understands). This example shows a WinZipped file, but is a general application of opening the file within a context manager and using that handle to read. See here Inferring dtypes from a file Dealing with bad lines GH2886 Write a multi-row index CSV without writing duplicates  Reading multiple files to create a single DataFrame The best way to combine multiple files into a single DataFrame is to read the individual frames one by one, put all of the individual frames into a list, and then combine the frames in the list using pd.concat(): \nIn [189]: for i in range(3):\n   .....:     data = pd.DataFrame(np.random.randn(10, 4))\n   .....:     data.to_csv(\"file_{}.csv\".format(i))\n   .....: \n\nIn [190]: files = [\"file_0.csv\", \"file_1.csv\", \"file_2.csv\"]\n\nIn [191]: result = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n  You can use the same approach to read all files matching a pattern. Here is an example using glob: \nIn [192]: import glob\n\nIn [193]: import os\n\nIn [194]: files = glob.glob(\"file_*.csv\")\n\nIn [195]: result = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n  Finally, this strategy will work with the other pd.read_*(...) functions described in the io docs.   Parsing date components in multi-columns Parsing date components in multi-columns is faster with a format \nIn [196]: i = pd.date_range(\"20000101\", periods=10000)\n\nIn [197]: df = pd.DataFrame({\"year\": i.year, \"month\": i.month, \"day\": i.day})\n\nIn [198]: df.head()\nOut[198]: \n   year  month  day\n0  2000      1    1\n1  2000      1    2\n2  2000      1    3\n3  2000      1    4\n4  2000      1    5\n\nIn [199]: %timeit pd.to_datetime(df.year * 10000 + df.month * 100 + df.day, format='%Y%m%d')\n   .....: ds = df.apply(lambda x: \"%04d%02d%02d\" % (x[\"year\"], x[\"month\"], x[\"day\"]), axis=1)\n   .....: ds.head()\n   .....: %timeit pd.to_datetime(ds)\n   .....: \n8.7 ms +- 765 us per loop (mean +- std. dev. of 7 runs, 100 loops each)\n2.1 ms +- 419 us per loop (mean +- std. dev. of 7 runs, 100 loops each)\n    Skip row between header and data \nIn [200]: data = \"\"\";;;;\n   .....:  ;;;;\n   .....:  ;;;;\n   .....:  ;;;;\n   .....:  ;;;;\n   .....:  ;;;;\n   .....: ;;;;\n   .....:  ;;;;\n   .....:  ;;;;\n   .....: ;;;;\n   .....: date;Param1;Param2;Param4;Param5\n   .....:     ;m\u00b2;\u00b0C;m\u00b2;m\n   .....: ;;;;\n   .....: 01.01.1990 00:00;1;1;2;3\n   .....: 01.01.1990 01:00;5;3;4;5\n   .....: 01.01.1990 02:00;9;5;6;7\n   .....: 01.01.1990 03:00;13;7;8;9\n   .....: 01.01.1990 04:00;17;9;10;11\n   .....: 01.01.1990 05:00;21;11;12;13\n   .....: \"\"\"\n   .....: \n   Option 1: pass rows explicitly to skip rows \nIn [201]: from io import StringIO\n\nIn [202]: pd.read_csv(\n   .....:     StringIO(data),\n   .....:     sep=\";\",\n   .....:     skiprows=[11, 12],\n   .....:     index_col=0,\n   .....:     parse_dates=True,\n   .....:     header=10,\n   .....: )\n   .....: \nOut[202]: \n                     Param1  Param2  Param4  Param5\ndate                                               \n1990-01-01 00:00:00       1       1       2       3\n1990-01-01 01:00:00       5       3       4       5\n1990-01-01 02:00:00       9       5       6       7\n1990-01-01 03:00:00      13       7       8       9\n1990-01-01 04:00:00      17       9      10      11\n1990-01-01 05:00:00      21      11      12      13\n    Option 2: read column names and then data \nIn [203]: pd.read_csv(StringIO(data), sep=\";\", header=10, nrows=10).columns\nOut[203]: Index(['date', 'Param1', 'Param2', 'Param4', 'Param5'], dtype='object')\n\nIn [204]: columns = pd.read_csv(StringIO(data), sep=\";\", header=10, nrows=10).columns\n\nIn [205]: pd.read_csv(\n   .....:     StringIO(data), sep=\";\", index_col=0, header=12, parse_dates=True, names=columns\n   .....: )\n   .....: \nOut[205]: \n                     Param1  Param2  Param4  Param5\ndate                                               \n1990-01-01 00:00:00       1       1       2       3\n1990-01-01 01:00:00       5       3       4       5\n1990-01-01 02:00:00       9       5       6       7\n1990-01-01 03:00:00      13       7       8       9\n1990-01-01 04:00:00      17       9      10      11\n1990-01-01 05:00:00      21      11      12      13\n      SQL The SQL docs Reading from databases with SQL   Excel The Excel docs Reading from a filelike handle Modifying formatting in XlsxWriter output Loading only visible sheets GH19842#issuecomment-892150745   HTML Reading HTML tables from a server that cannot handle the default request header   HDFStore The HDFStores docs Simple queries with a Timestamp Index Managing heterogeneous data using a linked multiple table hierarchy GH3032 Merging on-disk tables with millions of rows Avoiding inconsistencies when writing to a store from multiple processes\/threads De-duplicating a large store by chunks, essentially a recursive reduction operation. Shows a function for taking in data from csv file and creating a store by chunks, with date parsing as well. See here Creating a store chunk-by-chunk from a csv file Appending to a store, while creating a unique index Large Data work flows Reading in a sequence of files, then providing a global unique index to a store while appending Groupby on a HDFStore with low group density Groupby on a HDFStore with high group density Hierarchical queries on a HDFStore Counting with a HDFStore Troubleshoot HDFStore exceptions Setting min_itemsize with strings Using ptrepack to create a completely-sorted-index on a store Storing Attributes to a group node \nIn [206]: df = pd.DataFrame(np.random.randn(8, 3))\n\nIn [207]: store = pd.HDFStore(\"test.h5\")\n\nIn [208]: store.put(\"df\", df)\n\n# you can store an arbitrary Python object via pickle\nIn [209]: store.get_storer(\"df\").attrs.my_attribute = {\"A\": 10}\n\nIn [210]: store.get_storer(\"df\").attrs.my_attribute\nOut[210]: {'A': 10}\n  You can create or load a HDFStore in-memory by passing the driver parameter to PyTables. Changes are only written to disk when the HDFStore is closed. \nIn [211]: store = pd.HDFStore(\"test.h5\", \"w\", driver=\"H5FD_CORE\")\n\nIn [212]: df = pd.DataFrame(np.random.randn(8, 3))\n\nIn [213]: store[\"test\"] = df\n\n# only after closing the store, data is written to disk:\nIn [214]: store.close()\n    Binary files pandas readily accepts NumPy record arrays, if you need to read in a binary file consisting of an array of C structs. For example, given this C program in a file called main.c compiled with gcc main.c -std=gnu99 on a 64-bit machine, \n#include <stdio.h>\n#include <stdint.h>\n\ntypedef struct _Data\n{\n    int32_t count;\n    double avg;\n    float scale;\n} Data;\n\nint main(int argc, const char *argv[])\n{\n    size_t n = 10;\n    Data d[n];\n\n    for (int i = 0; i < n; ++i)\n    {\n        d[i].count = i;\n        d[i].avg = i + 1.0;\n        d[i].scale = (float) i + 2.0f;\n    }\n\n    FILE *file = fopen(\"binary.dat\", \"wb\");\n    fwrite(&d, sizeof(Data), n, file);\n    fclose(file);\n\n    return 0;\n}\n  the following Python code will read the binary file 'binary.dat' into a pandas DataFrame, where each element of the struct corresponds to a column in the frame: \nnames = \"count\", \"avg\", \"scale\"\n\n# note that the offsets are larger than the size of the type because of\n# struct padding\noffsets = 0, 8, 16\nformats = \"i4\", \"f8\", \"f4\"\ndt = np.dtype({\"names\": names, \"offsets\": offsets, \"formats\": formats}, align=True)\ndf = pd.DataFrame(np.fromfile(\"binary.dat\", dt))\n   Note The offsets of the structure elements may be different depending on the architecture of the machine on which the file was created. Using a raw binary file format like this for general data storage is not recommended, as it is not cross platform. We recommended either HDF5 or parquet, both of which are supported by pandas\u2019 IO facilities.     Computation Numerical integration (sample-based) of a time series  Correlation Often it\u2019s useful to obtain the lower (or upper) triangular form of a correlation matrix calculated from DataFrame.corr(). This can be achieved by passing a boolean mask to where as follows: \nIn [215]: df = pd.DataFrame(np.random.random(size=(100, 5)))\n\nIn [216]: corr_mat = df.corr()\n\nIn [217]: mask = np.tril(np.ones_like(corr_mat, dtype=np.bool_), k=-1)\n\nIn [218]: corr_mat.where(mask)\nOut[218]: \n          0         1         2        3   4\n0       NaN       NaN       NaN      NaN NaN\n1 -0.079861       NaN       NaN      NaN NaN\n2 -0.236573  0.183801       NaN      NaN NaN\n3 -0.013795 -0.051975  0.037235      NaN NaN\n4 -0.031974  0.118342 -0.073499 -0.02063 NaN\n  The method argument within DataFrame.corr can accept a callable in addition to the named correlation types. Here we compute the distance correlation matrix for a DataFrame object. \nIn [219]: def distcorr(x, y):\n   .....:     n = len(x)\n   .....:     a = np.zeros(shape=(n, n))\n   .....:     b = np.zeros(shape=(n, n))\n   .....:     for i in range(n):\n   .....:         for j in range(i + 1, n):\n   .....:             a[i, j] = abs(x[i] - x[j])\n   .....:             b[i, j] = abs(y[i] - y[j])\n   .....:     a += a.T\n   .....:     b += b.T\n   .....:     a_bar = np.vstack([np.nanmean(a, axis=0)] * n)\n   .....:     b_bar = np.vstack([np.nanmean(b, axis=0)] * n)\n   .....:     A = a - a_bar - a_bar.T + np.full(shape=(n, n), fill_value=a_bar.mean())\n   .....:     B = b - b_bar - b_bar.T + np.full(shape=(n, n), fill_value=b_bar.mean())\n   .....:     cov_ab = np.sqrt(np.nansum(A * B)) \/ n\n   .....:     std_a = np.sqrt(np.sqrt(np.nansum(A ** 2)) \/ n)\n   .....:     std_b = np.sqrt(np.sqrt(np.nansum(B ** 2)) \/ n)\n   .....:     return cov_ab \/ std_a \/ std_b\n   .....: \n\nIn [220]: df = pd.DataFrame(np.random.normal(size=(100, 3)))\n\nIn [221]: df.corr(method=distcorr)\nOut[221]: \n          0         1         2\n0  1.000000  0.197613  0.216328\n1  0.197613  1.000000  0.208749\n2  0.216328  0.208749  1.000000\n     Timedeltas The Timedeltas docs. Using timedeltas \nIn [222]: import datetime\n\nIn [223]: s = pd.Series(pd.date_range(\"2012-1-1\", periods=3, freq=\"D\"))\n\nIn [224]: s - s.max()\nOut[224]: \n0   -2 days\n1   -1 days\n2    0 days\ndtype: timedelta64[ns]\n\nIn [225]: s.max() - s\nOut[225]: \n0   2 days\n1   1 days\n2   0 days\ndtype: timedelta64[ns]\n\nIn [226]: s - datetime.datetime(2011, 1, 1, 3, 5)\nOut[226]: \n0   364 days 20:55:00\n1   365 days 20:55:00\n2   366 days 20:55:00\ndtype: timedelta64[ns]\n\nIn [227]: s + datetime.timedelta(minutes=5)\nOut[227]: \n0   2012-01-01 00:05:00\n1   2012-01-02 00:05:00\n2   2012-01-03 00:05:00\ndtype: datetime64[ns]\n\nIn [228]: datetime.datetime(2011, 1, 1, 3, 5) - s\nOut[228]: \n0   -365 days +03:05:00\n1   -366 days +03:05:00\n2   -367 days +03:05:00\ndtype: timedelta64[ns]\n\nIn [229]: datetime.timedelta(minutes=5) + s\nOut[229]: \n0   2012-01-01 00:05:00\n1   2012-01-02 00:05:00\n2   2012-01-03 00:05:00\ndtype: datetime64[ns]\n  Adding and subtracting deltas and dates \nIn [230]: deltas = pd.Series([datetime.timedelta(days=i) for i in range(3)])\n\nIn [231]: df = pd.DataFrame({\"A\": s, \"B\": deltas})\n\nIn [232]: df\nOut[232]: \n           A      B\n0 2012-01-01 0 days\n1 2012-01-02 1 days\n2 2012-01-03 2 days\n\nIn [233]: df[\"New Dates\"] = df[\"A\"] + df[\"B\"]\n\nIn [234]: df[\"Delta\"] = df[\"A\"] - df[\"New Dates\"]\n\nIn [235]: df\nOut[235]: \n           A      B  New Dates   Delta\n0 2012-01-01 0 days 2012-01-01  0 days\n1 2012-01-02 1 days 2012-01-03 -1 days\n2 2012-01-03 2 days 2012-01-05 -2 days\n\nIn [236]: df.dtypes\nOut[236]: \nA             datetime64[ns]\nB            timedelta64[ns]\nNew Dates     datetime64[ns]\nDelta        timedelta64[ns]\ndtype: object\n  Another example Values can be set to NaT using np.nan, similar to datetime \nIn [237]: y = s - s.shift()\n\nIn [238]: y\nOut[238]: \n0      NaT\n1   1 days\n2   1 days\ndtype: timedelta64[ns]\n\nIn [239]: y[1] = np.nan\n\nIn [240]: y\nOut[240]: \n0      NaT\n1      NaT\n2   1 days\ndtype: timedelta64[ns]\n    Creating example data To create a dataframe from every combination of some given values, like R\u2019s expand.grid() function, we can create a dict where the keys are column names and the values are lists of the data values: \nIn [241]: def expand_grid(data_dict):\n   .....:     rows = itertools.product(*data_dict.values())\n   .....:     return pd.DataFrame.from_records(rows, columns=data_dict.keys())\n   .....: \n\nIn [242]: df = expand_grid(\n   .....:     {\"height\": [60, 70], \"weight\": [100, 140, 180], \"sex\": [\"Male\", \"Female\"]}\n   .....: )\n   .....: \n\nIn [243]: df\nOut[243]: \n    height  weight     sex\n0       60     100    Male\n1       60     100  Female\n2       60     140    Male\n3       60     140  Female\n4       60     180    Male\n5       60     180  Female\n6       70     100    Male\n7       70     100  Female\n8       70     140    Male\n9       70     140  Female\n10      70     180    Male\n11      70     180  Female","title":"pandas.user_guide.cookbook"},{"text":"pygame.event.Event() \n create a new event object Event(type, dict) -> EventType instance Event(type, **attributes) -> EventType instance  Creates a new event with the given type and attributes. The attributes can come from a dictionary argument with string keys or from keyword arguments.","title":"pygame.ref.event#pygame.event.Event"},{"text":"tf.summary.experimental.get_step Returns the default summary step for the current thread. \ntf.summary.experimental.get_step()\n\n \n\n\n Returns   The step set by tf.summary.experimental.set_step() if one has been set, otherwise None.","title":"tensorflow.summary.experimental.get_step"},{"text":"is_namespace()  \nReturn True if name binding introduces new namespace. If the name is used as the target of a function or class statement, this will be true. For example: >>> table = symtable.symtable(\"def some_func(): pass\", \"string\", \"exec\")\n>>> table.lookup(\"some_func\").is_namespace()\nTrue\n Note that a single name can be bound to multiple objects. If the result is True, the name may also be bound to other objects, like an int or list, that does not introduce a new namespace.","title":"python.library.symtable#symtable.Symbol.is_namespace"},{"text":"pandas.DataFrame.eq   DataFrame.eq(other, axis='columns', level=None)[source]\n \nGet Equal to of dataframe and other, element-wise (binary operator eq). Among flexible wrappers (eq, ne, le, lt, ge, gt) to comparison operators. Equivalent to ==, !=, <=, <, >=, > with support to choose axis (rows or columns) and level for comparison.  Parameters \n \nother:scalar, sequence, Series, or DataFrame\n\n\nAny single or multiple element data structure, or list-like object.  \naxis:{0 or \u2018index\u2019, 1 or \u2018columns\u2019}, default \u2018columns\u2019\n\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).  \nlevel:int or label\n\n\nBroadcast across a level, matching Index values on the passed MultiIndex level.    Returns \n DataFrame of bool\n\nResult of the comparison.      See also  DataFrame.eq\n\nCompare DataFrames for equality elementwise.  DataFrame.ne\n\nCompare DataFrames for inequality elementwise.  DataFrame.le\n\nCompare DataFrames for less than inequality or equality elementwise.  DataFrame.lt\n\nCompare DataFrames for strictly less than inequality elementwise.  DataFrame.ge\n\nCompare DataFrames for greater than inequality or equality elementwise.  DataFrame.gt\n\nCompare DataFrames for strictly greater than inequality elementwise.    Notes Mismatched indices will be unioned together. NaN values are considered different (i.e. NaN != NaN). Examples \n>>> df = pd.DataFrame({'cost': [250, 150, 100],\n...                    'revenue': [100, 250, 300]},\n...                   index=['A', 'B', 'C'])\n>>> df\n   cost  revenue\nA   250      100\nB   150      250\nC   100      300\n  Comparison with a scalar, using either the operator or method: \n>>> df == 100\n    cost  revenue\nA  False     True\nB  False    False\nC   True    False\n  \n>>> df.eq(100)\n    cost  revenue\nA  False     True\nB  False    False\nC   True    False\n  When other is a Series, the columns of a DataFrame are aligned with the index of other and broadcast: \n>>> df != pd.Series([100, 250], index=[\"cost\", \"revenue\"])\n    cost  revenue\nA   True     True\nB   True    False\nC  False     True\n  Use the method to control the broadcast axis: \n>>> df.ne(pd.Series([100, 300], index=[\"A\", \"D\"]), axis='index')\n   cost  revenue\nA  True    False\nB  True     True\nC  True     True\nD  True     True\n  When comparing to an arbitrary sequence, the number of columns must match the number elements in other: \n>>> df == [250, 100]\n    cost  revenue\nA   True     True\nB  False    False\nC  False    False\n  Use the method to control the axis: \n>>> df.eq([250, 250, 100], axis='index')\n    cost  revenue\nA   True    False\nB  False     True\nC   True    False\n  Compare to a DataFrame of different shape. \n>>> other = pd.DataFrame({'revenue': [300, 250, 100, 150]},\n...                      index=['A', 'B', 'C', 'D'])\n>>> other\n   revenue\nA      300\nB      250\nC      100\nD      150\n  \n>>> df.gt(other)\n    cost  revenue\nA  False    False\nB  False    False\nC  False     True\nD  False    False\n  Compare to a MultiIndex by level. \n>>> df_multindex = pd.DataFrame({'cost': [250, 150, 100, 150, 300, 220],\n...                              'revenue': [100, 250, 300, 200, 175, 225]},\n...                             index=[['Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2'],\n...                                    ['A', 'B', 'C', 'A', 'B', 'C']])\n>>> df_multindex\n      cost  revenue\nQ1 A   250      100\n   B   150      250\n   C   100      300\nQ2 A   150      200\n   B   300      175\n   C   220      225\n  \n>>> df.le(df_multindex, level=1)\n       cost  revenue\nQ1 A   True     True\n   B   True     True\n   C   True     True\nQ2 A  False     True\n   B   True    False\n   C   True    False","title":"pandas.reference.api.pandas.dataframe.eq"},{"text":"clip() \n crops a rectangle inside another clip(Rect) -> Rect  Returns a new rectangle that is cropped to be completely inside the argument Rect. If the two rectangles do not overlap to begin with, a Rect with 0 size is returned.","title":"pygame.ref.rect#pygame.Rect.clip"},{"text":"tf.keras.applications.EfficientNetB3 Instantiates the EfficientNetB3 architecture.  View aliases  Main aliases \ntf.keras.applications.efficientnet.EfficientNetB3 Compat aliases for migration See Migration guide for more details. tf.compat.v1.keras.applications.EfficientNetB3, tf.compat.v1.keras.applications.efficientnet.EfficientNetB3  \ntf.keras.applications.EfficientNetB3(\n    include_top=True, weights='imagenet', input_tensor=None,\n    input_shape=None, pooling=None, classes=1000,\n    classifier_activation='softmax', **kwargs\n)\n Reference:  \nEfficientNet: Rethinking Model Scaling for Convolutional Neural Networks (ICML 2019)  Optionally loads weights pre-trained on ImageNet. Note that the data format convention used by the model is the one specified in your Keras config at ~\/.keras\/keras.json. If you have never configured it, it defaults to \"channels_last\".\n \n\n\n Arguments\n  include_top   Whether to include the fully-connected layer at the top of the network. Defaults to True.  \n  weights   One of None (random initialization), 'imagenet' (pre-training on ImageNet), or the path to the weights file to be loaded. Defaults to 'imagenet'.  \n  input_tensor   Optional Keras tensor (i.e. output of layers.Input()) to use as image input for the model.  \n  input_shape   Optional shape tuple, only to be specified if include_top is False. It should have exactly 3 inputs channels.  \n  pooling   Optional pooling mode for feature extraction when include_top is False. Defaults to None.  \nNone means that the output of the model will be the 4D tensor output of the last convolutional layer. \navg means that global average pooling will be applied to the output of the last convolutional layer, and thus the output of the model will be a 2D tensor. \nmax means that global max pooling will be applied. \n\n \n  classes   Optional number of classes to classify images into, only to be specified if include_top is True, and if no weights argument is specified. Defaults to 1000 (number of ImageNet classes).  \n  classifier_activation   A str or callable. The activation function to use on the \"top\" layer. Ignored unless include_top=True. Set classifier_activation=None to return the logits of the \"top\" layer. Defaults to 'softmax'.   \n \n\n\n Returns   A keras.Model instance.","title":"tensorflow.keras.applications.efficientnetb3"}]}
{"task_id":13945749,"prompt":"def f_13945749(goals, penalties):\n\treturn ","suffix":"","canonical_solution":"\"\"\"({:d} goals, ${:d})\"\"\".format(goals, penalties)","test_start":"\ndef check(candidate):","test":["\n    assert candidate(0, 0) == \"(0 goals, $0)\"\n","\n    assert candidate(123, 2) == \"(123 goals, $2)\"\n"],"entry_point":"f_13945749","intent":"format the variables `goals` and `penalties` using string formatting","library":[],"docs":[]}
{"task_id":13945749,"prompt":"def f_13945749(goals, penalties):\n\treturn ","suffix":"","canonical_solution":"\"\"\"({} goals, ${})\"\"\".format(goals, penalties)","test_start":"\ndef check(candidate):","test":["\n    assert candidate(0, 0) == \"(0 goals, $0)\"\n","\n    assert candidate(123, \"???\") == \"(123 goals, $???)\"\n","\n    assert candidate(\"x\", 0.0) == \"(x goals, $0.0)\"\n"],"entry_point":"f_13945749","intent":"format string \"({} goals, ${})\" with variables `goals` and `penalties`","library":[],"docs":[]}
{"task_id":18524642,"prompt":"def f_18524642(L):\n\treturn ","suffix":"","canonical_solution":"[int(''.join(str(d) for d in x)) for x in L]","test_start":"\ndef check(candidate):","test":["\n    assert candidate([[1,2], [2,3,4], [1,0,0]]) == [12,234,100]\n","\n    assert candidate([[1], [2], [3]]) == [1,2,3]\n"],"entry_point":"f_18524642","intent":"convert list of lists `L` to list of integers","library":[],"docs":[]}
{"task_id":18524642,"prompt":"def f_18524642(L):\n\t","suffix":"\n\treturn L","canonical_solution":"L = [int(''.join([str(y) for y in x])) for x in L]","test_start":"\ndef check(candidate):","test":["\n    assert candidate([[1,2], [2,3,4], [1,0,0]]) == [12,234,100]\n","\n    assert candidate([[1], [2], [3]]) == [1,2,3]\n","\n    assert candidate([[1, 0], [0, 2], [3], [0, 0, 0, 0]]) == [10,2,3, 0]\n"],"entry_point":"f_18524642","intent":"convert a list of lists `L` to list of integers","library":[],"docs":[]}
{"task_id":7138686,"prompt":"def f_7138686(lines, myfile):\n\t","suffix":"\n\treturn ","canonical_solution":"myfile.write('\\n'.join(lines))","test_start":"\ndef check(candidate):","test":["\n    with open('tmp.txt', 'w') as myfile:\n        candidate([\"first\", \"second\", \"third\"], myfile)\n    with open('tmp.txt', 'r') as fr: \n        lines = fr.readlines()\n    assert lines == [\"first\\n\", \"second\\n\", \"third\"]\n"],"entry_point":"f_7138686","intent":"write the elements of list `lines` concatenated by special character '\\n' to file `myfile`","library":[],"docs":[]}
{"task_id":17238587,"prompt":"def f_17238587(text):\n\t","suffix":"\n\treturn text","canonical_solution":"text = re.sub('\\\\b(\\\\w+)( \\\\1\\\\b)+', '\\\\1', text)","test_start":"\nimport re \n\ndef check(candidate):","test":["\n    assert candidate(\"text\") == \"text\"\n","\n    assert candidate(\"text text\") == \"text\"\n","\n    assert candidate(\"texttext\") == \"texttext\"\n","\n    assert candidate(\"text and text\") == \"text and text\"\n"],"entry_point":"f_17238587","intent":"Remove duplicate words from a string `text` using regex","library":["re"],"docs":[{"text":"comment(text)  \nCreates a comment with the given text. If insert_comments is true, this will also add it to the tree.  New in version 3.8.","title":"python.library.xml.etree.elementtree#xml.etree.ElementTree.TreeBuilder.comment"},{"text":"text  \nThe source code text involved in the error.","title":"python.library.exceptions#SyntaxError.text"},{"text":"token_generator  \nInstance of the class to check the password. This will default to default_token_generator, it\u2019s an instance of django.contrib.auth.tokens.PasswordResetTokenGenerator.","title":"django.topics.auth.default#django.contrib.auth.views.PasswordResetConfirmView.token_generator"},{"text":"token_generator  \nInstance of the class to check the one time link. This will default to default_token_generator, it\u2019s an instance of django.contrib.auth.tokens.PasswordResetTokenGenerator.","title":"django.topics.auth.default#django.contrib.auth.views.PasswordResetView.token_generator"},{"text":"textwrap.dedent(text)  \nRemove any common leading whitespace from every line in text. This can be used to make triple-quoted strings line up with the left edge of the display, while still presenting them in the source code in indented form. Note that tabs and spaces are both treated as whitespace, but they are not equal: the lines \"\u00a0 hello\" and \"\\thello\" are considered to have no common leading whitespace. Lines containing only whitespace are ignored in the input and normalized to a single newline character in the output. For example: def test():\n    # end first line with \\ to avoid the empty line!\n    s = '''\\\n    hello\n      world\n    '''\n    print(repr(s))          # prints '    hello\\n      world\\n    '\n    print(repr(dedent(s)))  # prints 'hello\\n  world\\n'","title":"python.library.textwrap#textwrap.dedent"},{"text":"transform(X) [source]\n \nReturn the predictions for X for each estimator.  Parameters \n \nX{array-like, sparse matrix} of shape (n_samples, n_features) \n\nTraining vectors, where n_samples is the number of samples and n_features is the number of features.    Returns \n \ny_predsndarray of shape (n_samples, n_estimators) \n\nPrediction outputs for each estimator.","title":"sklearn.modules.generated.sklearn.ensemble.stackingregressor#sklearn.ensemble.StackingRegressor.transform"},{"text":"locale.NOEXPR  \nGet a regular expression that can be used with the regex(3) function to recognize a negative response to a yes\/no question.","title":"python.library.locale#locale.NOEXPR"},{"text":"locale.YESEXPR  \nGet a regular expression that can be used with the regex function to recognize a positive response to a yes\/no question.  Note The expression is in the syntax suitable for the regex() function from the C library, which might differ from the syntax used in re.","title":"python.library.locale#locale.YESEXPR"},{"text":"sklearn.utils.multiclass.unique_labels(*ys) [source]\n \nExtract an ordered array of unique labels.  We don\u2019t allow:\n\n mix of multilabel and multiclass (single label) targets mix of label indicator matrix and anything else, because there are no explicit labels) mix of label indicator matrices of different sizes mix of string and integer labels    At the moment, we also don\u2019t allow \u201cmulticlass-multioutput\u201d input type.  Parameters \n \n*ysarray-likes \n  Returns \n \noutndarray of shape (n_unique_labels,) \n\nAn ordered array of unique labels.     Examples >>> from sklearn.utils.multiclass import unique_labels\n>>> unique_labels([3, 5, 5, 5, 7, 7])\narray([3, 5, 7])\n>>> unique_labels([1, 2, 3, 4], [2, 2, 3, 4])\narray([1, 2, 3, 4])\n>>> unique_labels([1, 2, 10], [5, 11])\narray([ 1,  2,  5, 10, 11])","title":"sklearn.modules.generated.sklearn.utils.multiclass.unique_labels#sklearn.utils.multiclass.unique_labels"},{"text":"pandas.Flags.allows_duplicate_labels   propertyFlags.allows_duplicate_labels\n \nWhether this object allows duplicate labels. Setting allows_duplicate_labels=False ensures that the index (and columns of a DataFrame) are unique. Most methods that accept and return a Series or DataFrame will propagate the value of allows_duplicate_labels. See Duplicate Labels for more.  See also  DataFrame.attrs\n\nSet global metadata on this object.  DataFrame.set_flags\n\nSet global flags on this object.    Examples \n>>> df = pd.DataFrame({\"A\": [1, 2]}, index=['a', 'a'])\n>>> df.flags.allows_duplicate_labels\nTrue\n>>> df.flags.allows_duplicate_labels = False\nTraceback (most recent call last):\n    ...\npandas.errors.DuplicateLabelError: Index has duplicates.\n      positions\nlabel\na        [0, 1]","title":"pandas.reference.api.pandas.flags.allows_duplicate_labels"}]}
{"task_id":26053849,"prompt":"def f_26053849(df):\n\treturn ","suffix":"","canonical_solution":"df.astype(bool).sum(axis=1)","test_start":"\nimport pandas as pd \n\ndef check(candidate):","test":["\n    df1 = pd.DataFrame([[0,0,0], [0,1,0], [1,1,1]])\n    assert candidate(df1).to_list() == [0, 1, 3]\n","\n    df2 = pd.DataFrame([[0,0,0], [0,2,0], [1,10,8.9]])\n    assert candidate(df1).to_list() == [0, 1, 3]\n","\n    df2 = pd.DataFrame([[0,0.0,0], [0,2.0,0], [1,10,8.9]])\n    assert candidate(df1).to_list() == [0, 1, 3]\n","\n    df = df = pd.DataFrame([[4, 0, 0], [1, 0, 1]])\n    expected = [1, 2]\n    actual = candidate(df)\n    for i in range(0, len(expected)):\n        assert expected[i] == actual[i]\n"],"entry_point":"f_26053849","intent":"count non zero values in each column in pandas data frame `df`","library":["pandas"],"docs":[{"text":"pandas.DataFrame.count   DataFrame.count(axis=0, level=None, numeric_only=False)[source]\n \nCount non-NA cells for each column or row. The values None, NaN, NaT, and optionally numpy.inf (depending on pandas.options.mode.use_inf_as_na) are considered NA.  Parameters \n \naxis:{0 or \u2018index\u2019, 1 or \u2018columns\u2019}, default 0\n\n\nIf 0 or \u2018index\u2019 counts are generated for each column. If 1 or \u2018columns\u2019 counts are generated for each row.  \nlevel:int or str, optional\n\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a DataFrame. A str specifies the level name.  \nnumeric_only:bool, default False\n\n\nInclude only float, int or boolean data.    Returns \n Series or DataFrame\n\nFor each column\/row the number of non-NA\/null entries. If level is specified returns a DataFrame.      See also  Series.count\n\nNumber of non-NA elements in a Series.  DataFrame.value_counts\n\nCount unique combinations of columns.  DataFrame.shape\n\nNumber of DataFrame rows and columns (including NA elements).  DataFrame.isna\n\nBoolean same-sized DataFrame showing places of NA elements.    Examples Constructing DataFrame from a dictionary: \n>>> df = pd.DataFrame({\"Person\":\n...                    [\"John\", \"Myla\", \"Lewis\", \"John\", \"Myla\"],\n...                    \"Age\": [24., np.nan, 21., 33, 26],\n...                    \"Single\": [False, True, True, True, False]})\n>>> df\n   Person   Age  Single\n0    John  24.0   False\n1    Myla   NaN    True\n2   Lewis  21.0    True\n3    John  33.0    True\n4    Myla  26.0   False\n  Notice the uncounted NA values: \n>>> df.count()\nPerson    5\nAge       4\nSingle    5\ndtype: int64\n  Counts for each row: \n>>> df.count(axis='columns')\n0    3\n1    2\n2    3\n3    3\n4    3\ndtype: int64","title":"pandas.reference.api.pandas.dataframe.count"},{"text":"pandas.DataFrame.value_counts   DataFrame.value_counts(subset=None, normalize=False, sort=True, ascending=False, dropna=True)[source]\n \nReturn a Series containing counts of unique rows in the DataFrame.  New in version 1.1.0.   Parameters \n \nsubset:list-like, optional\n\n\nColumns to use when counting unique combinations.  \nnormalize:bool, default False\n\n\nReturn proportions rather than frequencies.  \nsort:bool, default True\n\n\nSort by frequencies.  \nascending:bool, default False\n\n\nSort in ascending order.  \ndropna:bool, default True\n\n\nDon\u2019t include counts of rows that contain NA values.  New in version 1.3.0.     Returns \n Series\n    See also  Series.value_counts\n\nEquivalent method on Series.    Notes The returned Series will have a MultiIndex with one level per input column. By default, rows that contain any NA values are omitted from the result. By default, the resulting Series will be in descending order so that the first element is the most frequently-occurring row. Examples \n>>> df = pd.DataFrame({'num_legs': [2, 4, 4, 6],\n...                    'num_wings': [2, 0, 0, 0]},\n...                   index=['falcon', 'dog', 'cat', 'ant'])\n>>> df\n        num_legs  num_wings\nfalcon         2          2\ndog            4          0\ncat            4          0\nant            6          0\n  \n>>> df.value_counts()\nnum_legs  num_wings\n4         0            2\n2         2            1\n6         0            1\ndtype: int64\n  \n>>> df.value_counts(sort=False)\nnum_legs  num_wings\n2         2            1\n4         0            2\n6         0            1\ndtype: int64\n  \n>>> df.value_counts(ascending=True)\nnum_legs  num_wings\n2         2            1\n6         0            1\n4         0            2\ndtype: int64\n  \n>>> df.value_counts(normalize=True)\nnum_legs  num_wings\n4         0            0.50\n2         2            0.25\n6         0            0.25\ndtype: float64\n  With dropna set to False we can also count rows with NA values. \n>>> df = pd.DataFrame({'first_name': ['John', 'Anne', 'John', 'Beth'],\n...                    'middle_name': ['Smith', pd.NA, pd.NA, 'Louise']})\n>>> df\n  first_name middle_name\n0       John       Smith\n1       Anne        <NA>\n2       John        <NA>\n3       Beth      Louise\n  \n>>> df.value_counts()\nfirst_name  middle_name\nBeth        Louise         1\nJohn        Smith          1\ndtype: int64\n  \n>>> df.value_counts(dropna=False)\nfirst_name  middle_name\nAnne        NaN            1\nBeth        Louise         1\nJohn        Smith          1\n            NaN            1\ndtype: int64","title":"pandas.reference.api.pandas.dataframe.value_counts"},{"text":"pandas.Series.count   Series.count(level=None)[source]\n \nReturn number of non-NA\/null observations in the Series.  Parameters \n \nlevel:int or level name, default None\n\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a smaller Series.    Returns \n int or Series (if level specified)\n\nNumber of non-null values in the Series.      See also  DataFrame.count\n\nCount non-NA cells for each column or row.    Examples \n>>> s = pd.Series([0.0, 1.0, np.nan])\n>>> s.count()\n2","title":"pandas.reference.api.pandas.series.count"},{"text":"pandas.Series.value_counts   Series.value_counts(normalize=False, sort=True, ascending=False, bins=None, dropna=True)[source]\n \nReturn a Series containing counts of unique values. The resulting object will be in descending order so that the first element is the most frequently-occurring element. Excludes NA values by default.  Parameters \n \nnormalize:bool, default False\n\n\nIf True then the object returned will contain the relative frequencies of the unique values.  \nsort:bool, default True\n\n\nSort by frequencies.  \nascending:bool, default False\n\n\nSort in ascending order.  \nbins:int, optional\n\n\nRather than count values, group them into half-open bins, a convenience for pd.cut, only works with numeric data.  \ndropna:bool, default True\n\n\nDon\u2019t include counts of NaN.    Returns \n Series\n    See also  Series.count\n\nNumber of non-NA elements in a Series.  DataFrame.count\n\nNumber of non-NA elements in a DataFrame.  DataFrame.value_counts\n\nEquivalent method on DataFrames.    Examples \n>>> index = pd.Index([3, 1, 2, 3, 4, np.nan])\n>>> index.value_counts()\n3.0    2\n1.0    1\n2.0    1\n4.0    1\ndtype: int64\n  With normalize set to True, returns the relative frequency by dividing all values by the sum of values. \n>>> s = pd.Series([3, 1, 2, 3, 4, np.nan])\n>>> s.value_counts(normalize=True)\n3.0    0.4\n1.0    0.2\n2.0    0.2\n4.0    0.2\ndtype: float64\n  bins Bins can be useful for going from a continuous variable to a categorical variable; instead of counting unique apparitions of values, divide the index in the specified number of half-open bins. \n>>> s.value_counts(bins=3)\n(0.996, 2.0]    2\n(2.0, 3.0]      2\n(3.0, 4.0]      1\ndtype: int64\n  dropna With dropna set to False we can also see NaN index values. \n>>> s.value_counts(dropna=False)\n3.0    2\n1.0    1\n2.0    1\n4.0    1\nNaN    1\ndtype: int64","title":"pandas.reference.api.pandas.series.value_counts"},{"text":"pandas.core.groupby.DataFrameGroupBy.nunique   DataFrameGroupBy.nunique(dropna=True)[source]\n \nReturn DataFrame with counts of unique elements in each position.  Parameters \n \ndropna:bool, default True\n\n\nDon\u2019t include NaN in the counts.    Returns \n nunique: DataFrame\n   Examples \n>>> df = pd.DataFrame({'id': ['spam', 'egg', 'egg', 'spam',\n...                           'ham', 'ham'],\n...                    'value1': [1, 5, 5, 2, 5, 5],\n...                    'value2': list('abbaxy')})\n>>> df\n     id  value1 value2\n0  spam       1      a\n1   egg       5      b\n2   egg       5      b\n3  spam       2      a\n4   ham       5      x\n5   ham       5      y\n  \n>>> df.groupby('id').nunique()\n      value1  value2\nid\negg        1       1\nham        1       2\nspam       2       1\n  Check for rows with the same id but conflicting values: \n>>> df.groupby('id').filter(lambda g: (g.nunique() > 1).any())\n     id  value1 value2\n0  spam       1      a\n3  spam       2      a\n4   ham       5      x\n5   ham       5      y","title":"pandas.reference.api.pandas.core.groupby.dataframegroupby.nunique"},{"text":"pandas.Index.value_counts   Index.value_counts(normalize=False, sort=True, ascending=False, bins=None, dropna=True)[source]\n \nReturn a Series containing counts of unique values. The resulting object will be in descending order so that the first element is the most frequently-occurring element. Excludes NA values by default.  Parameters \n \nnormalize:bool, default False\n\n\nIf True then the object returned will contain the relative frequencies of the unique values.  \nsort:bool, default True\n\n\nSort by frequencies.  \nascending:bool, default False\n\n\nSort in ascending order.  \nbins:int, optional\n\n\nRather than count values, group them into half-open bins, a convenience for pd.cut, only works with numeric data.  \ndropna:bool, default True\n\n\nDon\u2019t include counts of NaN.    Returns \n Series\n    See also  Series.count\n\nNumber of non-NA elements in a Series.  DataFrame.count\n\nNumber of non-NA elements in a DataFrame.  DataFrame.value_counts\n\nEquivalent method on DataFrames.    Examples \n>>> index = pd.Index([3, 1, 2, 3, 4, np.nan])\n>>> index.value_counts()\n3.0    2\n1.0    1\n2.0    1\n4.0    1\ndtype: int64\n  With normalize set to True, returns the relative frequency by dividing all values by the sum of values. \n>>> s = pd.Series([3, 1, 2, 3, 4, np.nan])\n>>> s.value_counts(normalize=True)\n3.0    0.4\n1.0    0.2\n2.0    0.2\n4.0    0.2\ndtype: float64\n  bins Bins can be useful for going from a continuous variable to a categorical variable; instead of counting unique apparitions of values, divide the index in the specified number of half-open bins. \n>>> s.value_counts(bins=3)\n(0.996, 2.0]    2\n(2.0, 3.0]      2\n(3.0, 4.0]      1\ndtype: int64\n  dropna With dropna set to False we can also see NaN index values. \n>>> s.value_counts(dropna=False)\n3.0    2\n1.0    1\n2.0    1\n4.0    1\nNaN    1\ndtype: int64","title":"pandas.reference.api.pandas.index.value_counts"},{"text":"pandas.DataFrame.nunique   DataFrame.nunique(axis=0, dropna=True)[source]\n \nCount number of distinct elements in specified axis. Return Series with number of distinct elements. Can ignore NaN values.  Parameters \n \naxis:{0 or \u2018index\u2019, 1 or \u2018columns\u2019}, default 0\n\n\nThe axis to use. 0 or \u2018index\u2019 for row-wise, 1 or \u2018columns\u2019 for column-wise.  \ndropna:bool, default True\n\n\nDon\u2019t include NaN in the counts.    Returns \n Series\n    See also  Series.nunique\n\nMethod nunique for Series.  DataFrame.count\n\nCount non-NA cells for each column or row.    Examples \n>>> df = pd.DataFrame({'A': [4, 5, 6], 'B': [4, 1, 1]})\n>>> df.nunique()\nA    3\nB    2\ndtype: int64\n  \n>>> df.nunique(axis=1)\n0    1\n1    2\n2    2\ndtype: int64","title":"pandas.reference.api.pandas.dataframe.nunique"},{"text":"count_nonzero(dim=None) \u2192 Tensor  \nSee torch.count_nonzero()","title":"torch.tensors#torch.Tensor.count_nonzero"},{"text":"pandas.core.groupby.DataFrameGroupBy.value_counts   DataFrameGroupBy.value_counts(subset=None, normalize=False, sort=True, ascending=False, dropna=True)[source]\n \nReturn a Series or DataFrame containing counts of unique rows.  New in version 1.4.0.   Parameters \n \nsubset:list-like, optional\n\n\nColumns to use when counting unique combinations.  \nnormalize:bool, default False\n\n\nReturn proportions rather than frequencies.  \nsort:bool, default True\n\n\nSort by frequencies.  \nascending:bool, default False\n\n\nSort in ascending order.  \ndropna:bool, default True\n\n\nDon\u2019t include counts of rows that contain NA values.    Returns \n Series or DataFrame\n\nSeries if the groupby as_index is True, otherwise DataFrame.      See also  Series.value_counts\n\nEquivalent method on Series.  DataFrame.value_counts\n\nEquivalent method on DataFrame.  SeriesGroupBy.value_counts\n\nEquivalent method on SeriesGroupBy.    Notes  If the groupby as_index is True then the returned Series will have a MultiIndex with one level per input column. If the groupby as_index is False then the returned DataFrame will have an additional column with the value_counts. The column is labelled \u2018count\u2019 or \u2018proportion\u2019, depending on the normalize parameter.  By default, rows that contain any NA values are omitted from the result. By default, the result will be in descending order so that the first element of each group is the most frequently-occurring row. Examples \n>>> df = pd.DataFrame({\n...    'gender': ['male', 'male', 'female', 'male', 'female', 'male'],\n...    'education': ['low', 'medium', 'high', 'low', 'high', 'low'],\n...    'country': ['US', 'FR', 'US', 'FR', 'FR', 'FR']\n... })\n  \n>>> df\n    gender      education       country\n0       male    low         US\n1       male    medium      FR\n2       female  high        US\n3       male    low         FR\n4       female  high        FR\n5       male    low         FR\n  \n>>> df.groupby('gender').value_counts()\ngender  education  country\nfemale  high       FR         1\n                   US         1\nmale    low        FR         2\n                   US         1\n        medium     FR         1\ndtype: int64\n  \n>>> df.groupby('gender').value_counts(ascending=True)\ngender  education  country\nfemale  high       FR         1\n                   US         1\nmale    low        US         1\n        medium     FR         1\n        low        FR         2\ndtype: int64\n  \n>>> df.groupby('gender').value_counts(normalize=True)\ngender  education  country\nfemale  high       FR         0.50\n                   US         0.50\nmale    low        FR         0.50\n                   US         0.25\n        medium     FR         0.25\ndtype: float64\n  \n>>> df.groupby('gender', as_index=False).value_counts()\n   gender education country  count\n0  female      high      FR      1\n1  female      high      US      1\n2    male       low      FR      2\n3    male       low      US      1\n4    male    medium      FR      1\n  \n>>> df.groupby('gender', as_index=False).value_counts(normalize=True)\n   gender education country  proportion\n0  female      high      FR        0.50\n1  female      high      US        0.50\n2    male       low      FR        0.50\n3    male       low      US        0.25\n4    male    medium      FR        0.25","title":"pandas.reference.api.pandas.core.groupby.dataframegroupby.value_counts"},{"text":"pandas.Series.nunique   Series.nunique(dropna=True)[source]\n \nReturn number of unique elements in the object. Excludes NA values by default.  Parameters \n \ndropna:bool, default True\n\n\nDon\u2019t include NaN in the count.    Returns \n int\n    See also  DataFrame.nunique\n\nMethod nunique for DataFrame.  Series.count\n\nCount non-NA\/null observations in the Series.    Examples \n>>> s = pd.Series([1, 3, 5, 7, 7])\n>>> s\n0    1\n1    3\n2    5\n3    7\n4    7\ndtype: int64\n  \n>>> s.nunique()\n4","title":"pandas.reference.api.pandas.series.nunique"}]}
{"task_id":15534223,"prompt":"def f_15534223():\n\treturn ","suffix":"","canonical_solution":"re.search('(?<!Distillr)\\\\\\\\AcroTray\\\\.exe', 'C:\\\\SomeDir\\\\AcroTray.exe')","test_start":"\nimport re \n\ndef check(candidate):","test":["\n    result = candidate()\n    assert result.span() == (10, 23)\n    assert result.string == \"C:\\SomeDir\\AcroTray.exe\"\n"],"entry_point":"f_15534223","intent":"search for string that matches regular expression pattern '(?<!Distillr)\\\\\\\\AcroTray\\\\.exe' in string 'C:\\\\SomeDir\\\\AcroTray.exe'","library":["re"],"docs":[{"text":"winreg.ExpandEnvironmentStrings(str)  \nExpands environment variable placeholders %NAME% in strings like REG_EXPAND_SZ: >>> ExpandEnvironmentStrings('%windir%')\n'C:\\\\Windows'\n Raises an auditing event winreg.ExpandEnvironmentStrings with argument str.","title":"python.library.winreg#winreg.ExpandEnvironmentStrings"},{"text":"glob.escape(pathname)  \nEscape all special characters ('?', '*' and '['). This is useful if you want to match an arbitrary literal string that may have special characters in it. Special characters in drive\/UNC sharepoints are not escaped, e.g. on Windows escape('\/\/?\/c:\/Quo vadis?.txt') returns '\/\/?\/c:\/Quo vadis[?].txt'.  New in version 3.4.","title":"python.library.glob#glob.escape"},{"text":"email.utils.quote(str)  \nReturn a new string with backslashes in str replaced by two backslashes, and double quotes replaced by backslash-double quote.","title":"python.library.email.utils#email.utils.quote"},{"text":"numpy.distutils.misc_util.cyg2win32(path: str) \u2192 str[source]\n \nConvert a path from Cygwin-native to Windows-native. Uses the cygpath utility (part of the Base install) to do the actual conversion. Falls back to returning the original path if this fails. Handles the default \/cygdrive mount prefix as well as the \/proc\/cygdrive portable prefix, custom cygdrive prefixes such as \/ or \/mnt, and absolute paths such as \/usr\/src\/ or \/home\/username  Parameters \n \npathstr\n\n\nThe path to convert    Returns \n \nconverted_pathstr\n\n\nThe converted path     Notes Documentation for cygpath utility: https:\/\/cygwin.com\/cygwin-ug-net\/cygpath.html Documentation for the C function it wraps: https:\/\/cygwin.com\/cygwin-api\/func-cygwin-conv-path.html","title":"numpy.reference.distutils.misc_util#numpy.distutils.misc_util.cyg2win32"},{"text":"winreg.HKEY_DYN_DATA  \nThis key is not used in versions of Windows after 98.","title":"python.library.winreg#winreg.HKEY_DYN_DATA"},{"text":"fnmatch.fnmatch(filename, pattern)  \nTest whether the filename string matches the pattern string, returning True or False. Both parameters are case-normalized using os.path.normcase(). fnmatchcase() can be used to perform a case-sensitive comparison, regardless of whether that\u2019s standard for the operating system. This example will print all file names in the current directory with the extension .txt: import fnmatch\nimport os\n\nfor file in os.listdir('.'):\n    if fnmatch.fnmatch(file, '*.txt'):\n        print(file)","title":"python.library.fnmatch#fnmatch.fnmatch"},{"text":"re.S  \nre.DOTALL  \nMake the '.' special character match any character at all, including a newline; without this flag, '.' will match anything except a newline. Corresponds to the inline flag (?s).","title":"python.library.re#re.S"},{"text":"SimpleTemplateResponse.is_rendered  \nA boolean indicating whether the response content has been rendered.","title":"django.ref.template-response#django.template.response.SimpleTemplateResponse.is_rendered"},{"text":"winreg \u2014 Windows registry access These functions expose the Windows registry API to Python. Instead of using an integer as the registry handle, a handle object is used to ensure that the handles are closed correctly, even if the programmer neglects to explicitly close them.  Changed in version 3.3: Several functions in this module used to raise a WindowsError, which is now an alias of OSError.  Functions This module offers the following functions:  \nwinreg.CloseKey(hkey)  \nCloses a previously opened registry key. The hkey argument specifies a previously opened key.  Note If hkey is not closed using this method (or via hkey.Close()), it is closed when the hkey object is destroyed by Python.  \n  \nwinreg.ConnectRegistry(computer_name, key)  \nEstablishes a connection to a predefined registry handle on another computer, and returns a handle object. computer_name is the name of the remote computer, of the form r\"\\\\computername\". If None, the local computer is used. key is the predefined handle to connect to. The return value is the handle of the opened key. If the function fails, an OSError exception is raised. Raises an auditing event winreg.ConnectRegistry with arguments computer_name, key.  Changed in version 3.3: See above.  \n  \nwinreg.CreateKey(key, sub_key)  \nCreates or opens the specified key, returning a handle object. key is an already open key, or one of the predefined HKEY_* constants. sub_key is a string that names the key this method opens or creates. If key is one of the predefined keys, sub_key may be None. In that case, the handle returned is the same key handle passed in to the function. If the key already exists, this function opens the existing key. The return value is the handle of the opened key. If the function fails, an OSError exception is raised. Raises an auditing event winreg.CreateKey with arguments key, sub_key, access. Raises an auditing event winreg.OpenKey\/result with argument key.  Changed in version 3.3: See above.  \n  \nwinreg.CreateKeyEx(key, sub_key, reserved=0, access=KEY_WRITE)  \nCreates or opens the specified key, returning a handle object. key is an already open key, or one of the predefined HKEY_* constants. sub_key is a string that names the key this method opens or creates. reserved is a reserved integer, and must be zero. The default is zero. access is an integer that specifies an access mask that describes the desired security access for the key. Default is KEY_WRITE. See Access Rights for other allowed values. If key is one of the predefined keys, sub_key may be None. In that case, the handle returned is the same key handle passed in to the function. If the key already exists, this function opens the existing key. The return value is the handle of the opened key. If the function fails, an OSError exception is raised. Raises an auditing event winreg.CreateKey with arguments key, sub_key, access. Raises an auditing event winreg.OpenKey\/result with argument key.  New in version 3.2.   Changed in version 3.3: See above.  \n  \nwinreg.DeleteKey(key, sub_key)  \nDeletes the specified key. key is an already open key, or one of the predefined HKEY_* constants. sub_key is a string that must be a subkey of the key identified by the key parameter. This value must not be None, and the key may not have subkeys. This method can not delete keys with subkeys. If the method succeeds, the entire key, including all of its values, is removed. If the method fails, an OSError exception is raised. Raises an auditing event winreg.DeleteKey with arguments key, sub_key, access.  Changed in version 3.3: See above.  \n  \nwinreg.DeleteKeyEx(key, sub_key, access=KEY_WOW64_64KEY, reserved=0)  \nDeletes the specified key.  Note The DeleteKeyEx() function is implemented with the RegDeleteKeyEx Windows API function, which is specific to 64-bit versions of Windows. See the RegDeleteKeyEx documentation.  key is an already open key, or one of the predefined HKEY_* constants. sub_key is a string that must be a subkey of the key identified by the key parameter. This value must not be None, and the key may not have subkeys. reserved is a reserved integer, and must be zero. The default is zero. access is an integer that specifies an access mask that describes the desired security access for the key. Default is KEY_WOW64_64KEY. See Access Rights for other allowed values. This method can not delete keys with subkeys. If the method succeeds, the entire key, including all of its values, is removed. If the method fails, an OSError exception is raised. On unsupported Windows versions, NotImplementedError is raised. Raises an auditing event winreg.DeleteKey with arguments key, sub_key, access.  New in version 3.2.   Changed in version 3.3: See above.  \n  \nwinreg.DeleteValue(key, value)  \nRemoves a named value from a registry key. key is an already open key, or one of the predefined HKEY_* constants. value is a string that identifies the value to remove. Raises an auditing event winreg.DeleteValue with arguments key, value. \n  \nwinreg.EnumKey(key, index)  \nEnumerates subkeys of an open registry key, returning a string. key is an already open key, or one of the predefined HKEY_* constants. index is an integer that identifies the index of the key to retrieve. The function retrieves the name of one subkey each time it is called. It is typically called repeatedly until an OSError exception is raised, indicating, no more values are available. Raises an auditing event winreg.EnumKey with arguments key, index.  Changed in version 3.3: See above.  \n  \nwinreg.EnumValue(key, index)  \nEnumerates values of an open registry key, returning a tuple. key is an already open key, or one of the predefined HKEY_* constants. index is an integer that identifies the index of the value to retrieve. The function retrieves the name of one subkey each time it is called. It is typically called repeatedly, until an OSError exception is raised, indicating no more values. The result is a tuple of 3 items:   \nIndex Meaning   \n0 A string that identifies the value name  \n1 An object that holds the value data, and whose type depends on the underlying registry type  \n2 An integer that identifies the type of the value data (see table in docs for SetValueEx())   Raises an auditing event winreg.EnumValue with arguments key, index.  Changed in version 3.3: See above.  \n  \nwinreg.ExpandEnvironmentStrings(str)  \nExpands environment variable placeholders %NAME% in strings like REG_EXPAND_SZ: >>> ExpandEnvironmentStrings('%windir%')\n'C:\\\\Windows'\n Raises an auditing event winreg.ExpandEnvironmentStrings with argument str. \n  \nwinreg.FlushKey(key)  \nWrites all the attributes of a key to the registry. key is an already open key, or one of the predefined HKEY_* constants. It is not necessary to call FlushKey() to change a key. Registry changes are flushed to disk by the registry using its lazy flusher. Registry changes are also flushed to disk at system shutdown. Unlike CloseKey(), the FlushKey() method returns only when all the data has been written to the registry. An application should only call FlushKey() if it requires absolute certainty that registry changes are on disk.  Note If you don\u2019t know whether a FlushKey() call is required, it probably isn\u2019t.  \n  \nwinreg.LoadKey(key, sub_key, file_name)  \nCreates a subkey under the specified key and stores registration information from a specified file into that subkey. key is a handle returned by ConnectRegistry() or one of the constants HKEY_USERS or HKEY_LOCAL_MACHINE. sub_key is a string that identifies the subkey to load. file_name is the name of the file to load registry data from. This file must have been created with the SaveKey() function. Under the file allocation table (FAT) file system, the filename may not have an extension. A call to LoadKey() fails if the calling process does not have the SE_RESTORE_PRIVILEGE privilege. Note that privileges are different from permissions \u2013 see the RegLoadKey documentation for more details. If key is a handle returned by ConnectRegistry(), then the path specified in file_name is relative to the remote computer. Raises an auditing event winreg.LoadKey with arguments key, sub_key, file_name. \n  \nwinreg.OpenKey(key, sub_key, reserved=0, access=KEY_READ)  \nwinreg.OpenKeyEx(key, sub_key, reserved=0, access=KEY_READ)  \nOpens the specified key, returning a handle object. key is an already open key, or one of the predefined HKEY_* constants. sub_key is a string that identifies the sub_key to open. reserved is a reserved integer, and must be zero. The default is zero. access is an integer that specifies an access mask that describes the desired security access for the key. Default is KEY_READ. See Access Rights for other allowed values. The result is a new handle to the specified key. If the function fails, OSError is raised. Raises an auditing event winreg.OpenKey with arguments key, sub_key, access. Raises an auditing event winreg.OpenKey\/result with argument key.  Changed in version 3.2: Allow the use of named arguments.   Changed in version 3.3: See above.  \n  \nwinreg.QueryInfoKey(key)  \nReturns information about a key, as a tuple. key is an already open key, or one of the predefined HKEY_* constants. The result is a tuple of 3 items:   \nIndex Meaning   \n0 An integer giving the number of sub keys this key has.  \n1 An integer giving the number of values this key has.  \n2 An integer giving when the key was last modified (if available) as 100\u2019s of nanoseconds since Jan 1, 1601.   Raises an auditing event winreg.QueryInfoKey with argument key. \n  \nwinreg.QueryValue(key, sub_key)  \nRetrieves the unnamed value for a key, as a string. key is an already open key, or one of the predefined HKEY_* constants. sub_key is a string that holds the name of the subkey with which the value is associated. If this parameter is None or empty, the function retrieves the value set by the SetValue() method for the key identified by key. Values in the registry have name, type, and data components. This method retrieves the data for a key\u2019s first value that has a NULL name. But the underlying API call doesn\u2019t return the type, so always use QueryValueEx() if possible. Raises an auditing event winreg.QueryValue with arguments key, sub_key, value_name. \n  \nwinreg.QueryValueEx(key, value_name)  \nRetrieves the type and data for a specified value name associated with an open registry key. key is an already open key, or one of the predefined HKEY_* constants. value_name is a string indicating the value to query. The result is a tuple of 2 items:   \nIndex Meaning   \n0 The value of the registry item.  \n1 An integer giving the registry type for this value (see table in docs for SetValueEx())   Raises an auditing event winreg.QueryValue with arguments key, sub_key, value_name. \n  \nwinreg.SaveKey(key, file_name)  \nSaves the specified key, and all its subkeys to the specified file. key is an already open key, or one of the predefined HKEY_* constants. file_name is the name of the file to save registry data to. This file cannot already exist. If this filename includes an extension, it cannot be used on file allocation table (FAT) file systems by the LoadKey() method. If key represents a key on a remote computer, the path described by file_name is relative to the remote computer. The caller of this method must possess the SeBackupPrivilege security privilege. Note that privileges are different than permissions \u2013 see the Conflicts Between User Rights and Permissions documentation for more details. This function passes NULL for security_attributes to the API. Raises an auditing event winreg.SaveKey with arguments key, file_name. \n  \nwinreg.SetValue(key, sub_key, type, value)  \nAssociates a value with a specified key. key is an already open key, or one of the predefined HKEY_* constants. sub_key is a string that names the subkey with which the value is associated. type is an integer that specifies the type of the data. Currently this must be REG_SZ, meaning only strings are supported. Use the SetValueEx() function for support for other data types. value is a string that specifies the new value. If the key specified by the sub_key parameter does not exist, the SetValue function creates it. Value lengths are limited by available memory. Long values (more than 2048 bytes) should be stored as files with the filenames stored in the configuration registry. This helps the registry perform efficiently. The key identified by the key parameter must have been opened with KEY_SET_VALUE access. Raises an auditing event winreg.SetValue with arguments key, sub_key, type, value. \n  \nwinreg.SetValueEx(key, value_name, reserved, type, value)  \nStores data in the value field of an open registry key. key is an already open key, or one of the predefined HKEY_* constants. value_name is a string that names the subkey with which the value is associated. reserved can be anything \u2013 zero is always passed to the API. type is an integer that specifies the type of the data. See Value Types for the available types. value is a string that specifies the new value. This method can also set additional value and type information for the specified key. The key identified by the key parameter must have been opened with KEY_SET_VALUE access. To open the key, use the CreateKey() or OpenKey() methods. Value lengths are limited by available memory. Long values (more than 2048 bytes) should be stored as files with the filenames stored in the configuration registry. This helps the registry perform efficiently. Raises an auditing event winreg.SetValue with arguments key, sub_key, type, value. \n  \nwinreg.DisableReflectionKey(key)  \nDisables registry reflection for 32-bit processes running on a 64-bit operating system. key is an already open key, or one of the predefined HKEY_* constants. Will generally raise NotImplementedError if executed on a 32-bit operating system. If the key is not on the reflection list, the function succeeds but has no effect. Disabling reflection for a key does not affect reflection of any subkeys. Raises an auditing event winreg.DisableReflectionKey with argument key. \n  \nwinreg.EnableReflectionKey(key)  \nRestores registry reflection for the specified disabled key. key is an already open key, or one of the predefined HKEY_* constants. Will generally raise NotImplementedError if executed on a 32-bit operating system. Restoring reflection for a key does not affect reflection of any subkeys. Raises an auditing event winreg.EnableReflectionKey with argument key. \n  \nwinreg.QueryReflectionKey(key)  \nDetermines the reflection state for the specified key. key is an already open key, or one of the predefined HKEY_* constants. Returns True if reflection is disabled. Will generally raise NotImplementedError if executed on a 32-bit operating system. Raises an auditing event winreg.QueryReflectionKey with argument key. \n Constants The following constants are defined for use in many _winreg functions. HKEY_* Constants  \nwinreg.HKEY_CLASSES_ROOT  \nRegistry entries subordinate to this key define types (or classes) of documents and the properties associated with those types. Shell and COM applications use the information stored under this key. \n  \nwinreg.HKEY_CURRENT_USER  \nRegistry entries subordinate to this key define the preferences of the current user. These preferences include the settings of environment variables, data about program groups, colors, printers, network connections, and application preferences. \n  \nwinreg.HKEY_LOCAL_MACHINE  \nRegistry entries subordinate to this key define the physical state of the computer, including data about the bus type, system memory, and installed hardware and software. \n  \nwinreg.HKEY_USERS  \nRegistry entries subordinate to this key define the default user configuration for new users on the local computer and the user configuration for the current user. \n  \nwinreg.HKEY_PERFORMANCE_DATA  \nRegistry entries subordinate to this key allow you to access performance data. The data is not actually stored in the registry; the registry functions cause the system to collect the data from its source. \n  \nwinreg.HKEY_CURRENT_CONFIG  \nContains information about the current hardware profile of the local computer system. \n  \nwinreg.HKEY_DYN_DATA  \nThis key is not used in versions of Windows after 98. \n Access Rights For more information, see Registry Key Security and Access.  \nwinreg.KEY_ALL_ACCESS  \nCombines the STANDARD_RIGHTS_REQUIRED, KEY_QUERY_VALUE, KEY_SET_VALUE, KEY_CREATE_SUB_KEY, KEY_ENUMERATE_SUB_KEYS, KEY_NOTIFY, and KEY_CREATE_LINK access rights. \n  \nwinreg.KEY_WRITE  \nCombines the STANDARD_RIGHTS_WRITE, KEY_SET_VALUE, and KEY_CREATE_SUB_KEY access rights. \n  \nwinreg.KEY_READ  \nCombines the STANDARD_RIGHTS_READ, KEY_QUERY_VALUE, KEY_ENUMERATE_SUB_KEYS, and KEY_NOTIFY values. \n  \nwinreg.KEY_EXECUTE  \nEquivalent to KEY_READ. \n  \nwinreg.KEY_QUERY_VALUE  \nRequired to query the values of a registry key. \n  \nwinreg.KEY_SET_VALUE  \nRequired to create, delete, or set a registry value. \n  \nwinreg.KEY_CREATE_SUB_KEY  \nRequired to create a subkey of a registry key. \n  \nwinreg.KEY_ENUMERATE_SUB_KEYS  \nRequired to enumerate the subkeys of a registry key. \n  \nwinreg.KEY_NOTIFY  \nRequired to request change notifications for a registry key or for subkeys of a registry key. \n  \nwinreg.KEY_CREATE_LINK  \nReserved for system use. \n 64-bit Specific For more information, see Accessing an Alternate Registry View.  \nwinreg.KEY_WOW64_64KEY  \nIndicates that an application on 64-bit Windows should operate on the 64-bit registry view. \n  \nwinreg.KEY_WOW64_32KEY  \nIndicates that an application on 64-bit Windows should operate on the 32-bit registry view. \n Value Types For more information, see Registry Value Types.  \nwinreg.REG_BINARY  \nBinary data in any form. \n  \nwinreg.REG_DWORD  \n32-bit number. \n  \nwinreg.REG_DWORD_LITTLE_ENDIAN  \nA 32-bit number in little-endian format. Equivalent to REG_DWORD. \n  \nwinreg.REG_DWORD_BIG_ENDIAN  \nA 32-bit number in big-endian format. \n  \nwinreg.REG_EXPAND_SZ  \nNull-terminated string containing references to environment variables (%PATH%). \n  \nwinreg.REG_LINK  \nA Unicode symbolic link. \n  \nwinreg.REG_MULTI_SZ  \nA sequence of null-terminated strings, terminated by two null characters. (Python handles this termination automatically.) \n  \nwinreg.REG_NONE  \nNo defined value type. \n  \nwinreg.REG_QWORD  \nA 64-bit number.  New in version 3.6.  \n  \nwinreg.REG_QWORD_LITTLE_ENDIAN  \nA 64-bit number in little-endian format. Equivalent to REG_QWORD.  New in version 3.6.  \n  \nwinreg.REG_RESOURCE_LIST  \nA device-driver resource list. \n  \nwinreg.REG_FULL_RESOURCE_DESCRIPTOR  \nA hardware setting. \n  \nwinreg.REG_RESOURCE_REQUIREMENTS_LIST  \nA hardware resource list. \n  \nwinreg.REG_SZ  \nA null-terminated string. \n Registry Handle Objects This object wraps a Windows HKEY object, automatically closing it when the object is destroyed. To guarantee cleanup, you can call either the Close() method on the object, or the CloseKey() function. All registry functions in this module return one of these objects. All registry functions in this module which accept a handle object also accept an integer, however, use of the handle object is encouraged. Handle objects provide semantics for __bool__() \u2013 thus if handle:\n    print(\"Yes\")\n will print Yes if the handle is currently valid (has not been closed or detached). The object also support comparison semantics, so handle objects will compare true if they both reference the same underlying Windows handle value. Handle objects can be converted to an integer (e.g., using the built-in int() function), in which case the underlying Windows handle value is returned. You can also use the Detach() method to return the integer handle, and also disconnect the Windows handle from the handle object.  \nPyHKEY.Close()  \nCloses the underlying Windows handle. If the handle is already closed, no error is raised. \n  \nPyHKEY.Detach()  \nDetaches the Windows handle from the handle object. The result is an integer that holds the value of the handle before it is detached. If the handle is already detached or closed, this will return zero. After calling this function, the handle is effectively invalidated, but the handle is not closed. You would call this function when you need the underlying Win32 handle to exist beyond the lifetime of the handle object. Raises an auditing event winreg.PyHKEY.Detach with argument key. \n  \nPyHKEY.__enter__()  \nPyHKEY.__exit__(*exc_info)  \nThe HKEY object implements __enter__() and __exit__() and thus supports the context protocol for the with statement: with OpenKey(HKEY_LOCAL_MACHINE, \"foo\") as key:\n    ...  # work with key\n will automatically close key when control leaves the with block.","title":"python.library.winreg"},{"text":"fnmatch \u2014 Unix filename pattern matching Source code: Lib\/fnmatch.py This module provides support for Unix shell-style wildcards, which are not the same as regular expressions (which are documented in the re module). The special characters used in shell-style wildcards are:   \nPattern Meaning   \n* matches everything  \n? matches any single character  \n[seq] matches any character in seq  \n[!seq] matches any character not in seq   For a literal match, wrap the meta-characters in brackets. For example, '[?]' matches the character '?'. Note that the filename separator ('\/' on Unix) is not special to this module. See module glob for pathname expansion (glob uses filter() to match pathname segments). Similarly, filenames starting with a period are not special for this module, and are matched by the * and ? patterns.  \nfnmatch.fnmatch(filename, pattern)  \nTest whether the filename string matches the pattern string, returning True or False. Both parameters are case-normalized using os.path.normcase(). fnmatchcase() can be used to perform a case-sensitive comparison, regardless of whether that\u2019s standard for the operating system. This example will print all file names in the current directory with the extension .txt: import fnmatch\nimport os\n\nfor file in os.listdir('.'):\n    if fnmatch.fnmatch(file, '*.txt'):\n        print(file)\n \n  \nfnmatch.fnmatchcase(filename, pattern)  \nTest whether filename matches pattern, returning True or False; the comparison is case-sensitive and does not apply os.path.normcase(). \n  \nfnmatch.filter(names, pattern)  \nConstruct a list from those elements of the iterable names that match pattern. It is the same as [n for n in names if fnmatch(n, pattern)], but implemented more efficiently. \n  \nfnmatch.translate(pattern)  \nReturn the shell-style pattern converted to a regular expression for using with re.match(). Example: >>> import fnmatch, re\n>>>\n>>> regex = fnmatch.translate('*.txt')\n>>> regex\n'(?s:.*\\\\.txt)\\\\Z'\n>>> reobj = re.compile(regex)\n>>> reobj.match('foobar.txt')\n<re.Match object; span=(0, 10), match='foobar.txt'>\n \n  See also  \nModule glob\n\n\nUnix shell-style path expansion.","title":"python.library.fnmatch"}]}
{"task_id":5453026,"prompt":"def f_5453026():\n\treturn ","suffix":"","canonical_solution":"\"\"\"QH QD JC KD JS\"\"\".split()","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == [\"QH\", \"QD\", \"JC\", \"KD\", \"JS\"]\n"],"entry_point":"f_5453026","intent":"split string 'QH QD JC KD JS' into a list on white spaces","library":[],"docs":[]}
{"task_id":18168684,"prompt":"def f_18168684(line):\n\treturn ","suffix":"","canonical_solution":"re.search('>.*<', line).group(0)","test_start":"\nimport re \n\ndef check(candidate):","test":["\n    assert candidate(\"hahhdsf>0.0<;sgnd\") == \">0.0<\"\n","\n    assert candidate(\"hahhdsf>2.34<;xbnfm\") == \">2.34<\"\n"],"entry_point":"f_18168684","intent":"search for occurrences of regex pattern '>.*<' in xml string `line`","library":["re"],"docs":[{"text":"xml.parsers.expat.errors.XML_ERROR_TAG_MISMATCH  \nAn end tag did not match the innermost open start tag.","title":"python.library.pyexpat#xml.parsers.expat.errors.XML_ERROR_TAG_MISMATCH"},{"text":"HTMLParser.get_starttag_text()  \nReturn the text of the most recently opened start tag. This should not normally be needed for structured processing, but may be useful in dealing with HTML \u201cas deployed\u201d or for re-generating input with minimal changes (whitespace between attributes can be preserved, etc.).","title":"python.library.html.parser#html.parser.HTMLParser.get_starttag_text"},{"text":"SimpleTestCase.assertInHTML(needle, haystack, count=None, msg_prefix='')  \nAsserts that the HTML fragment needle is contained in the haystack one. If the count integer argument is specified, then additionally the number of needle occurrences will be strictly verified. Whitespace in most cases is ignored, and attribute ordering is not significant. See assertHTMLEqual() for more details.","title":"django.topics.testing.tools#django.test.SimpleTestCase.assertInHTML"},{"text":"findtext(match, default=None, namespaces=None)  \nSame as Element.findtext(), starting at the root of the tree.","title":"python.library.xml.etree.elementtree#xml.etree.ElementTree.ElementTree.findtext"},{"text":"iterfind(match, namespaces=None)  \nSame as Element.iterfind(), starting at the root of the tree.  New in version 3.2.","title":"python.library.xml.etree.elementtree#xml.etree.ElementTree.ElementTree.iterfind"},{"text":"tag_has(tagname, item=None)  \nIf item is specified, returns 1 or 0 depending on whether the specified item has the given tagname. Otherwise, returns a list of all items that have the specified tag. Availability: Tk 8.6","title":"python.library.tkinter.ttk#tkinter.ttk.Treeview.tag_has"},{"text":"xml.parsers.expat.errors.XML_ERROR_MISPLACED_XML_PI  \nAn XML declaration was found somewhere other than the start of the input data.","title":"python.library.pyexpat#xml.parsers.expat.errors.XML_ERROR_MISPLACED_XML_PI"},{"text":"tf.raw_ops.ShuffleDataset Creates a dataset that shuffles elements from input_dataset pseudorandomly.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.raw_ops.ShuffleDataset  \ntf.raw_ops.ShuffleDataset(\n    input_dataset, buffer_size, seed, seed2, output_types, output_shapes,\n    reshuffle_each_iteration=True, name=None\n)\n\n \n\n\n Args\n  input_dataset   A Tensor of type variant.  \n  buffer_size   A Tensor of type int64. The number of output elements to buffer in an iterator over this dataset. Compare with the min_after_dequeue attr when creating a RandomShuffleQueue.  \n  seed   A Tensor of type int64. A scalar seed for the random number generator. If either seed or seed2 is set to be non-zero, the random number generator is seeded by the given seed. Otherwise, a random seed is used.  \n  seed2   A Tensor of type int64. A second scalar seed to avoid seed collision.  \n  output_types   A list of tf.DTypes that has length >= 1.  \n  output_shapes   A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.  \n  reshuffle_each_iteration   An optional bool. Defaults to True. If true, each iterator over this dataset will be given a different pseudorandomly generated seed, based on a sequence seeded by the seed and seed2 inputs. If false, each iterator will be given the same seed, and repeated iteration over this dataset will yield the exact same sequence of results.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor of type variant.","title":"tensorflow.raw_ops.shuffledataset"},{"text":"difflib.IS_LINE_JUNK(line)  \nReturn True for ignorable lines. The line line is ignorable if line is blank or contains a single '#', otherwise it is not ignorable. Used as a default for parameter linejunk in ndiff() in older versions.","title":"python.library.difflib#difflib.IS_LINE_JUNK"},{"text":"gettext_lazy(message)","title":"django.ref.utils#django.utils.translation.gettext_lazy"}]}
{"task_id":4914277,"prompt":"def f_4914277(filename):\n\treturn ","suffix":"","canonical_solution":"open(filename, 'w').close()","test_start":"\ndef check(candidate):","test":["\n    filename = 'tmp.txt'\n    with open(filename, 'w') as fw: fw.write(\"hello world!\")\n    with open(filename, 'r') as fr: \n        lines = fr.readlines()\n        assert len(lines) == 1 and lines[0] == \"hello world!\"\n    candidate(filename)\n    with open(filename, 'r') as fr: \n        lines = fr.readlines()\n        assert len(lines) == 0\n"],"entry_point":"f_4914277","intent":"erase all the contents of a file `filename`","library":[],"docs":[]}
{"task_id":19068269,"prompt":"def f_19068269(string_date):\n\treturn ","suffix":"","canonical_solution":"datetime.datetime.strptime(string_date, '%Y-%m-%d %H:%M:%S.%f')","test_start":"\nimport datetime \n\ndef check(candidate):","test":["\n    assert candidate('2022-10-22 11:59:59.20') == datetime.datetime(2022, 10, 22, 11, 59, 59, 200000)\n","\n    assert candidate('2000-01-01 11:59:59.20') == datetime.datetime(2000, 1, 1, 11, 59, 59, 200000)\n","\n    assert candidate('1990-09-09 09:59:59.24') == datetime.datetime(1990, 9, 9, 9, 59, 59, 240000)\n","\n    d = candidate('2022-12-14 07:06:00.25')\n    assert d == datetime.datetime(2022, 12, 14, 7, 6, 0, 250000)\n"],"entry_point":"f_19068269","intent":"convert a string `string_date` into datetime using the format '%Y-%m-%d %H:%M:%S.%f'","library":["datetime"],"docs":[{"text":"classmethod datetime.strptime(date_string, format)  \nReturn a datetime corresponding to date_string, parsed according to format. This is equivalent to: datetime(*(time.strptime(date_string, format)[0:6]))\n ValueError is raised if the date_string and format can\u2019t be parsed by time.strptime() or if it returns a value which isn\u2019t a time tuple. For a complete list of formatting directives, see strftime() and strptime() Behavior.","title":"python.library.datetime#datetime.datetime.strptime"},{"text":"classmethod date.fromisoformat(date_string)  \nReturn a date corresponding to a date_string given in the format YYYY-MM-DD: >>> from datetime import date\n>>> date.fromisoformat('2019-12-04')\ndatetime.date(2019, 12, 4)\n This is the inverse of date.isoformat(). It only supports the format YYYY-MM-DD.  New in version 3.7.","title":"python.library.datetime#datetime.date.fromisoformat"},{"text":"pandas.StringDtype   classpandas.StringDtype(storage=None)[source]\n \nExtension dtype for string data.  New in version 1.0.0.   Warning StringDtype is considered experimental. The implementation and parts of the API may change without warning. In particular, StringDtype.na_value may change to no longer be numpy.nan.   Parameters \n \nstorage:{\u201cpython\u201d, \u201cpyarrow\u201d}, optional\n\n\nIf not given, the value of pd.options.mode.string_storage.     Examples \n>>> pd.StringDtype()\nstring[python]\n  \n>>> pd.StringDtype(storage=\"pyarrow\")\nstring[pyarrow]\n  Attributes       \nNone     Methods       \nNone","title":"pandas.reference.api.pandas.stringdtype"},{"text":"classmethod datetime.fromisoformat(date_string)  \nReturn a datetime corresponding to a date_string in one of the formats emitted by date.isoformat() and datetime.isoformat(). Specifically, this function supports strings in the format: YYYY-MM-DD[*HH[:MM[:SS[.fff[fff]]]][+HH:MM[:SS[.ffffff]]]]\n where * can match any single character.  Caution This does not support parsing arbitrary ISO 8601 strings - it is only intended as the inverse operation of datetime.isoformat(). A more full-featured ISO 8601 parser, dateutil.parser.isoparse is available in the third-party package dateutil.  Examples: >>> from datetime import datetime\n>>> datetime.fromisoformat('2011-11-04')\ndatetime.datetime(2011, 11, 4, 0, 0)\n>>> datetime.fromisoformat('2011-11-04T00:05:23')\ndatetime.datetime(2011, 11, 4, 0, 5, 23)\n>>> datetime.fromisoformat('2011-11-04 00:05:23.283')\ndatetime.datetime(2011, 11, 4, 0, 5, 23, 283000)\n>>> datetime.fromisoformat('2011-11-04 00:05:23.283+00:00')\ndatetime.datetime(2011, 11, 4, 0, 5, 23, 283000, tzinfo=datetime.timezone.utc)\n>>> datetime.fromisoformat('2011-11-04T00:05:23+04:00')   \ndatetime.datetime(2011, 11, 4, 0, 5, 23,\n    tzinfo=datetime.timezone(datetime.timedelta(seconds=14400)))\n  New in version 3.7.","title":"python.library.datetime#datetime.datetime.fromisoformat"},{"text":"tf.keras.layers.Maximum     View source on GitHub    Layer that computes the maximum (element-wise) a list of inputs. Inherits From: Layer, Module  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.keras.layers.Maximum  \ntf.keras.layers.Maximum(\n    **kwargs\n)\n It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape). \ntf.keras.layers.Maximum()([np.arange(5).reshape(5, 1),\n                           np.arange(5, 10).reshape(5, 1)])\n<tf.Tensor: shape=(5, 1), dtype=int64, numpy=\narray([[5],\n     [6],\n     [7],\n     [8],\n     [9]])>\n \nx1 = tf.keras.layers.Dense(8)(np.arange(10).reshape(5, 2))\nx2 = tf.keras.layers.Dense(8)(np.arange(10, 20).reshape(5, 2))\nmaxed = tf.keras.layers.Maximum()([x1, x2])\nmaxed.shape\nTensorShape([5, 8])\n\n \n\n\n Arguments\n  **kwargs   standard layer keyword arguments.","title":"tensorflow.keras.layers.maximum"},{"text":"exception xml.dom.DomstringSizeErr  \nRaised when a specified range of text does not fit into a string. This is not known to be used in the Python DOM implementations, but may be received from DOM implementations not written in Python.","title":"python.library.xml.dom#xml.dom.DomstringSizeErr"},{"text":"Repr.maxstring  \nLimit on the number of characters in the representation of the string. Note that the \u201cnormal\u201d representation of the string is used as the character source: if escape sequences are needed in the representation, these may be mangled when the representation is shortened. The default is 30.","title":"python.library.reprlib#reprlib.Repr.maxstring"},{"text":"decode(string)  \nAccept a string as the instance\u2019s new time value.","title":"python.library.xmlrpc.client#xmlrpc.client.DateTime.decode"},{"text":"HttpResponse.has_header(header)  \nReturns True or False based on a case-insensitive check for a header with the given name.","title":"django.ref.request-response#django.http.HttpResponse.has_header"},{"text":"pandas.api.extensions.ExtensionDtype.construct_from_string   classmethodExtensionDtype.construct_from_string(string)[source]\n \nConstruct this type from a string. This is useful mainly for data types that accept parameters. For example, a period dtype accepts a frequency parameter that can be set as period[H] (where H means hourly frequency). By default, in the abstract class, just the name of the type is expected. But subclasses can overwrite this method to accept parameters.  Parameters \n \nstring:str\n\n\nThe name of the type, for example category.    Returns \n ExtensionDtype\n\nInstance of the dtype.    Raises \n TypeError\n\nIf a class cannot be constructed from this \u2018string\u2019.     Examples For extension dtypes with arguments the following may be an adequate implementation. \n>>> @classmethod\n... def construct_from_string(cls, string):\n...     pattern = re.compile(r\"^my_type\\[(?P<arg_name>.+)\\]$\")\n...     match = pattern.match(string)\n...     if match:\n...         return cls(**match.groupdict())\n...     else:\n...         raise TypeError(\n...             f\"Cannot construct a '{cls.__name__}' from '{string}'\"\n...         )","title":"pandas.reference.api.pandas.api.extensions.extensiondtype.construct_from_string"}]}
{"task_id":20683167,"prompt":"def f_20683167(thelist):\n\treturn ","suffix":"","canonical_solution":"[index for index, item in enumerate(thelist) if item[0] == '332']","test_start":"\ndef check(candidate):","test":["\n    assert candidate([[0,1,2], ['a','bb','ccc'], ['332',33,2], [33,22,332]]) == [2]\n","\n    assert candidate([[0,1,2], ['332'], ['332'], ['332']]) == [1,2,3]\n","\n    assert candidate([[0,1,2], [332], [332], [332]]) == []\n"],"entry_point":"f_20683167","intent":"find the index of a list with the first element equal to '332' within the list of lists `thelist`","library":[],"docs":[]}
{"task_id":30693804,"prompt":"def f_30693804(text):\n\treturn ","suffix":"","canonical_solution":"re.sub('[^\\\\sa-zA-Z0-9]', '', text).lower().strip()","test_start":"\nimport re\n\ndef check(candidate):","test":["\n    assert candidate('ABjfK329r0&&*#5t') == 'abjfk329r05t'\n","\n    assert candidate('jseguwphegoi339yup h') == 'jseguwphegoi339yup h'\n","\n    assert candidate('   ') == ''\n"],"entry_point":"f_30693804","intent":"lower a string `text` and remove non-alphanumeric characters aside from space","library":["re"],"docs":[{"text":"tf.strings.lower Converts all uppercase characters into their respective lowercase replacements.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.strings.lower  \ntf.strings.lower(\n    input, encoding='', name=None\n)\n Example: \ntf.strings.lower(\"CamelCase string and ALL CAPS\")\n<tf.Tensor: shape=(), dtype=string, numpy=b'camelcase string and all caps'>\n\n \n\n\n Args\n  input   A Tensor of type string.  \n  encoding   An optional string. Defaults to \"\".  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor of type string.","title":"tensorflow.strings.lower"},{"text":"numpy.char.lower   char.lower(a)[source]\n \nReturn an array with the elements converted to lowercase. Call str.lower element-wise. For 8-bit strings, this method is locale-dependent.  Parameters \n \naarray_like, {str, unicode}\n\n\nInput array.    Returns \n \noutndarray, {str, unicode}\n\n\nOutput array of str or unicode, depending on input type      See also  str.lower\n  Examples >>> c = np.array(['A1B C', '1BCA', 'BCA1']); c\narray(['A1B C', '1BCA', 'BCA1'], dtype='<U5')\n>>> np.char.lower(c)\narray(['a1b c', '1bca', 'bca1'], dtype='<U5')","title":"numpy.reference.generated.numpy.char.lower"},{"text":"operator.ilshift(a, b)  \noperator.__ilshift__(a, b)  \na = ilshift(a, b) is equivalent to a <<= b.","title":"python.library.operator#operator.__ilshift__"},{"text":"operator.ilshift(a, b)  \noperator.__ilshift__(a, b)  \na = ilshift(a, b) is equivalent to a <<= b.","title":"python.library.operator#operator.ilshift"},{"text":"str.isalnum()  \nReturn True if all characters in the string are alphanumeric and there is at least one character, False otherwise. A character c is alphanumeric if one of the following returns True: c.isalpha(), c.isdecimal(), c.isdigit(), or c.isnumeric().","title":"python.library.stdtypes#str.isalnum"},{"text":"numpy.chararray.lower method   chararray.lower()[source]\n \nReturn an array with the elements of self converted to lowercase.  See also  char.lower","title":"numpy.reference.generated.numpy.chararray.lower"},{"text":"tf.raw_ops.StringLower Converts all uppercase characters into their respective lowercase replacements.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.raw_ops.StringLower  \ntf.raw_ops.StringLower(\n    input, encoding='', name=None\n)\n Example: \ntf.strings.lower(\"CamelCase string and ALL CAPS\")\n<tf.Tensor: shape=(), dtype=string, numpy=b'camelcase string and all caps'>\n\n \n\n\n Args\n  input   A Tensor of type string.  \n  encoding   An optional string. Defaults to \"\".  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor of type string.","title":"tensorflow.raw_ops.stringlower"},{"text":"pandas.Series.str.lower   Series.str.lower()[source]\n \nConvert strings in the Series\/Index to lowercase. Equivalent to str.lower().  Returns \n Series or Index of object\n    See also  Series.str.lower\n\nConverts all characters to lowercase.  Series.str.upper\n\nConverts all characters to uppercase.  Series.str.title\n\nConverts first character of each word to uppercase and remaining to lowercase.  Series.str.capitalize\n\nConverts first character to uppercase and remaining to lowercase.  Series.str.swapcase\n\nConverts uppercase to lowercase and lowercase to uppercase.  Series.str.casefold\n\nRemoves all case distinctions in the string.    Examples \n>>> s = pd.Series(['lower', 'CAPITALS', 'this is a sentence', 'SwApCaSe'])\n>>> s\n0                 lower\n1              CAPITALS\n2    this is a sentence\n3              SwApCaSe\ndtype: object\n  \n>>> s.str.lower()\n0                 lower\n1              capitals\n2    this is a sentence\n3              swapcase\ndtype: object\n  \n>>> s.str.upper()\n0                 LOWER\n1              CAPITALS\n2    THIS IS A SENTENCE\n3              SWAPCASE\ndtype: object\n  \n>>> s.str.title()\n0                 Lower\n1              Capitals\n2    This Is A Sentence\n3              Swapcase\ndtype: object\n  \n>>> s.str.capitalize()\n0                 Lower\n1              Capitals\n2    This is a sentence\n3              Swapcase\ndtype: object\n  \n>>> s.str.swapcase()\n0                 LOWER\n1              capitals\n2    THIS IS A SENTENCE\n3              sWaPcAsE\ndtype: object","title":"pandas.reference.api.pandas.series.str.lower"},{"text":"numpy.char.chararray.lower method   char.chararray.lower()[source]\n \nReturn an array with the elements of self converted to lowercase.  See also  char.lower","title":"numpy.reference.generated.numpy.char.chararray.lower"},{"text":"str.lower()  \nReturn a copy of the string with all the cased characters 4 converted to lowercase. The lowercasing algorithm used is described in section 3.13 of the Unicode Standard.","title":"python.library.stdtypes#str.lower"}]}
{"task_id":30693804,"prompt":"def f_30693804(text):\n\treturn ","suffix":"","canonical_solution":"re.sub('(?!\\\\s)[\\\\W_]', '', text).lower().strip()","test_start":"\nimport re\n\ndef check(candidate):","test":["\n    assert candidate('ABjfK329r0&&*#5t') == 'abjfk329r05t'\n","\n    assert candidate('jseguwphegoi339yup h') == 'jseguwphegoi339yup h'\n","\n    assert candidate('   ') == ''\n"],"entry_point":"f_30693804","intent":"remove all non-alphanumeric characters except space from a string `text` and lower it","library":["re"],"docs":[{"text":"operator.ilshift(a, b)  \noperator.__ilshift__(a, b)  \na = ilshift(a, b) is equivalent to a <<= b.","title":"python.library.operator#operator.__ilshift__"},{"text":"operator.ilshift(a, b)  \noperator.__ilshift__(a, b)  \na = ilshift(a, b) is equivalent to a <<= b.","title":"python.library.operator#operator.ilshift"},{"text":"str.isalnum()  \nReturn True if all characters in the string are alphanumeric and there is at least one character, False otherwise. A character c is alphanumeric if one of the following returns True: c.isalpha(), c.isdecimal(), c.isdigit(), or c.isnumeric().","title":"python.library.stdtypes#str.isalnum"},{"text":"numpy.genfromtxt   numpy.genfromtxt(fname, dtype=<class 'float'>, comments='#', delimiter=None, skip_header=0, skip_footer=0, converters=None, missing_values=None, filling_values=None, usecols=None, names=None, excludelist=None, deletechars=\" !#$%&'()*+, -.\/:;<=>?@[\\\\]^{|}~\", replace_space='_', autostrip=False, case_sensitive=True, defaultfmt='f%i', unpack=None, usemask=False, loose=True, invalid_raise=True, max_rows=None, encoding='bytes', *, like=None)[source]\n \nLoad data from a text file, with missing values handled as specified. Each line past the first skip_header lines is split at the delimiter character, and characters following the comments character are discarded.  Parameters \n \nfnamefile, str, pathlib.Path, list of str, generator\n\n\nFile, filename, list, or generator to read. If the filename extension is .gz or .bz2, the file is first decompressed. Note that generators must return bytes or strings. The strings in a list or produced by a generator are treated as lines.  \ndtypedtype, optional\n\n\nData type of the resulting array. If None, the dtypes will be determined by the contents of each column, individually.  \ncommentsstr, optional\n\n\nThe character used to indicate the start of a comment. All the characters occurring on a line after a comment are discarded.  \ndelimiterstr, int, or sequence, optional\n\n\nThe string used to separate values. By default, any consecutive whitespaces act as delimiter. An integer or sequence of integers can also be provided as width(s) of each field.  \nskiprowsint, optional\n\n\nskiprows was removed in numpy 1.10. Please use skip_header instead.  \nskip_headerint, optional\n\n\nThe number of lines to skip at the beginning of the file.  \nskip_footerint, optional\n\n\nThe number of lines to skip at the end of the file.  \nconvertersvariable, optional\n\n\nThe set of functions that convert the data of a column to a value. The converters can also be used to provide a default value for missing data: converters = {3: lambda s: float(s or 0)}.  \nmissingvariable, optional\n\n\nmissing was removed in numpy 1.10. Please use missing_values instead.  \nmissing_valuesvariable, optional\n\n\nThe set of strings corresponding to missing data.  \nfilling_valuesvariable, optional\n\n\nThe set of values to be used as default when the data are missing.  \nusecolssequence, optional\n\n\nWhich columns to read, with 0 being the first. For example, usecols = (1, 4, 5) will extract the 2nd, 5th and 6th columns.  \nnames{None, True, str, sequence}, optional\n\n\nIf names is True, the field names are read from the first line after the first skip_header lines. This line can optionally be preceded by a comment delimiter. If names is a sequence or a single-string of comma-separated names, the names will be used to define the field names in a structured dtype. If names is None, the names of the dtype fields will be used, if any.  \nexcludelistsequence, optional\n\n\nA list of names to exclude. This list is appended to the default list [\u2018return\u2019,\u2019file\u2019,\u2019print\u2019]. Excluded names are appended with an underscore: for example, file would become file_.  \ndeletecharsstr, optional\n\n\nA string combining invalid characters that must be deleted from the names.  \ndefaultfmtstr, optional\n\n\nA format used to define default field names, such as \u201cf%i\u201d or \u201cf_%02i\u201d.  \nautostripbool, optional\n\n\nWhether to automatically strip white spaces from the variables.  \nreplace_spacechar, optional\n\n\nCharacter(s) used in replacement of white spaces in the variable names. By default, use a \u2018_\u2019.  \ncase_sensitive{True, False, \u2018upper\u2019, \u2018lower\u2019}, optional\n\n\nIf True, field names are case sensitive. If False or \u2018upper\u2019, field names are converted to upper case. If \u2018lower\u2019, field names are converted to lower case.  \nunpackbool, optional\n\n\nIf True, the returned array is transposed, so that arguments may be unpacked using x, y, z = genfromtxt(...). When used with a structured data-type, arrays are returned for each field. Default is False.  \nusemaskbool, optional\n\n\nIf True, return a masked array. If False, return a regular array.  \nloosebool, optional\n\n\nIf True, do not raise errors for invalid values.  \ninvalid_raisebool, optional\n\n\nIf True, an exception is raised if an inconsistency is detected in the number of columns. If False, a warning is emitted and the offending lines are skipped.  \nmax_rowsint, optional\n\n\nThe maximum number of rows to read. Must not be used with skip_footer at the same time. If given, the value must be at least 1. Default is to read the entire file.  New in version 1.10.0.   \nencodingstr, optional\n\n\nEncoding used to decode the inputfile. Does not apply when fname is a file object. The special value \u2018bytes\u2019 enables backward compatibility workarounds that ensure that you receive byte arrays when possible and passes latin1 encoded strings to converters. Override this value to receive unicode arrays and pass strings as input to converters. If set to None the system default is used. The default value is \u2018bytes\u2019.  New in version 1.14.0.   \nlikearray_like\n\n\nReference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as like supports the __array_function__ protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.  New in version 1.20.0.     Returns \n \noutndarray\n\n\nData read from the text file. If usemask is True, this is a masked array.      See also  numpy.loadtxt\n\nequivalent function when no data is missing.    Notes  When spaces are used as delimiters, or when no delimiter has been given as input, there should not be any missing data between two fields. When the variables are named (either by a flexible dtype or with names), there must not be any header in the file (else a ValueError exception is raised). Individual values are not stripped of spaces by default. When using a custom converter, make sure the function does remove spaces.  References  1 \nNumPy User Guide, section I\/O with NumPy.   Examples >>> from io import StringIO\n>>> import numpy as np\n Comma delimited file with mixed dtype >>> s = StringIO(u\"1,1.3,abcde\")\n>>> data = np.genfromtxt(s, dtype=[('myint','i8'),('myfloat','f8'),\n... ('mystring','S5')], delimiter=\",\")\n>>> data\narray((1, 1.3, b'abcde'),\n      dtype=[('myint', '<i8'), ('myfloat', '<f8'), ('mystring', 'S5')])\n Using dtype = None >>> _ = s.seek(0) # needed for StringIO example only\n>>> data = np.genfromtxt(s, dtype=None,\n... names = ['myint','myfloat','mystring'], delimiter=\",\")\n>>> data\narray((1, 1.3, b'abcde'),\n      dtype=[('myint', '<i8'), ('myfloat', '<f8'), ('mystring', 'S5')])\n Specifying dtype and names >>> _ = s.seek(0)\n>>> data = np.genfromtxt(s, dtype=\"i8,f8,S5\",\n... names=['myint','myfloat','mystring'], delimiter=\",\")\n>>> data\narray((1, 1.3, b'abcde'),\n      dtype=[('myint', '<i8'), ('myfloat', '<f8'), ('mystring', 'S5')])\n An example with fixed-width columns >>> s = StringIO(u\"11.3abcde\")\n>>> data = np.genfromtxt(s, dtype=None, names=['intvar','fltvar','strvar'],\n...     delimiter=[1,3,5])\n>>> data\narray((1, 1.3, b'abcde'),\n      dtype=[('intvar', '<i8'), ('fltvar', '<f8'), ('strvar', 'S5')])\n An example to show comments >>> f = StringIO('''\n... text,# of chars\n... hello world,11\n... numpy,5''')\n>>> np.genfromtxt(f, dtype='S12,S12', delimiter=',')\narray([(b'text', b''), (b'hello world', b'11'), (b'numpy', b'5')],\n  dtype=[('f0', 'S12'), ('f1', 'S12')])","title":"numpy.reference.generated.numpy.genfromtxt"},{"text":"numpy.char.lower   char.lower(a)[source]\n \nReturn an array with the elements converted to lowercase. Call str.lower element-wise. For 8-bit strings, this method is locale-dependent.  Parameters \n \naarray_like, {str, unicode}\n\n\nInput array.    Returns \n \noutndarray, {str, unicode}\n\n\nOutput array of str or unicode, depending on input type      See also  str.lower\n  Examples >>> c = np.array(['A1B C', '1BCA', 'BCA1']); c\narray(['A1B C', '1BCA', 'BCA1'], dtype='<U5')\n>>> np.char.lower(c)\narray(['a1b c', '1bca', 'bca1'], dtype='<U5')","title":"numpy.reference.generated.numpy.char.lower"},{"text":"curses.ascii.isalnum(c)  \nChecks for an ASCII alphanumeric character; it is equivalent to isalpha(c) or\nisdigit(c).","title":"python.library.curses.ascii#curses.ascii.isalnum"},{"text":"theme_settings(themename, settings)  \nTemporarily sets the current theme to themename, apply specified settings and then restore the previous theme. Each key in settings is a style and each value may contain the keys \u2018configure\u2019, \u2018map\u2019, \u2018layout\u2019 and \u2018element create\u2019 and they are expected to have the same format as specified by the methods Style.configure(), Style.map(), Style.layout() and Style.element_create() respectively. As an example, let\u2019s change the Combobox for the default theme a bit: from tkinter import ttk\nimport tkinter\n\nroot = tkinter.Tk()\n\nstyle = ttk.Style()\nstyle.theme_settings(\"default\", {\n   \"TCombobox\": {\n       \"configure\": {\"padding\": 5},\n       \"map\": {\n           \"background\": [(\"active\", \"green2\"),\n                          (\"!disabled\", \"green4\")],\n           \"fieldbackground\": [(\"!disabled\", \"green3\")],\n           \"foreground\": [(\"focus\", \"OliveDrab1\"),\n                          (\"!disabled\", \"OliveDrab2\")]\n       }\n   }\n})\n\ncombo = ttk.Combobox().pack()\n\nroot.mainloop()","title":"python.library.tkinter.ttk#tkinter.ttk.Style.theme_settings"},{"text":"torch.nn.functional.one_hot(tensor, num_classes=-1) \u2192 LongTensor  \nTakes LongTensor with index values of shape (*) and returns a tensor of shape (*, num_classes) that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1. See also One-hot on Wikipedia .  Parameters \n \ntensor (LongTensor) \u2013 class values of any shape. \nnum_classes (int) \u2013 Total number of classes. If set to -1, the number of classes will be inferred as one greater than the largest class value in the input tensor.   Returns \nLongTensor that has one more dimension with 1 values at the index of last dimension indicated by the input, and 0 everywhere else.   Examples >>> F.one_hot(torch.arange(0, 5) % 3)\ntensor([[1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1],\n        [1, 0, 0],\n        [0, 1, 0]])\n>>> F.one_hot(torch.arange(0, 5) % 3, num_classes=5)\ntensor([[1, 0, 0, 0, 0],\n        [0, 1, 0, 0, 0],\n        [0, 0, 1, 0, 0],\n        [1, 0, 0, 0, 0],\n        [0, 1, 0, 0, 0]])\n>>> F.one_hot(torch.arange(0, 6).view(3,2) % 3)\ntensor([[[1, 0, 0],\n         [0, 1, 0]],\n        [[0, 0, 1],\n         [1, 0, 0]],\n        [[0, 1, 0],\n         [0, 0, 1]]])","title":"torch.nn.functional#torch.nn.functional.one_hot"},{"text":"tf.strings.lower Converts all uppercase characters into their respective lowercase replacements.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.strings.lower  \ntf.strings.lower(\n    input, encoding='', name=None\n)\n Example: \ntf.strings.lower(\"CamelCase string and ALL CAPS\")\n<tf.Tensor: shape=(), dtype=string, numpy=b'camelcase string and all caps'>\n\n \n\n\n Args\n  input   A Tensor of type string.  \n  encoding   An optional string. Defaults to \"\".  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor of type string.","title":"tensorflow.strings.lower"},{"text":"test.support.SMALLEST  \nObject that is less than anything (except itself). Used to test mixed type comparison.","title":"python.library.test#test.support.SMALLEST"}]}
{"task_id":17138464,"prompt":"def f_17138464(x, y):\n\treturn ","suffix":"","canonical_solution":"plt.plot(x, y, label='H\\u2082O')","test_start":"\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef check(candidate):","test":["\n    pic = candidate(np.array([1,2,3]),np.array([4,5,6]))[0]\n    assert pic.get_label() == 'H\u2082O'\n    x, y = pic.get_data()\n    assert all(x == np.array([1,2,3]))\n    assert all(y == np.array([4,5,6]))\n","\n    pic = candidate(np.array([6, 7, 899]),np.array([0, 1, 245]))[0]\n    assert pic.get_label() == 'H\u2082O'\n    x, y = pic.get_data()\n    assert all(x == np.array([6, 7, 899]))\n    assert all(y == np.array([0, 1, 245]))\n"],"entry_point":"f_17138464","intent":"subscript text 'H20' with '2' as subscripted in matplotlib labels for arrays 'x' and 'y'.","library":["matplotlib","numpy"],"docs":[{"text":"fmt_ds='$%d.%s^\\\\mathrm{h}$'","title":"matplotlib._as_gen.mpl_toolkits.axisartist.angle_helper.formatterhms#mpl_toolkits.axisartist.angle_helper.FormatterHMS.fmt_ds"},{"text":"deg_mark='^\\\\mathrm{h}'","title":"matplotlib._as_gen.mpl_toolkits.axisartist.angle_helper.formatterhms#mpl_toolkits.axisartist.angle_helper.FormatterHMS.deg_mark"},{"text":"fmt_d='$%d^\\\\mathrm{h}$'","title":"matplotlib._as_gen.mpl_toolkits.axisartist.angle_helper.formatterhms#mpl_toolkits.axisartist.angle_helper.FormatterHMS.fmt_d"},{"text":"matplotlib.pyplot.semilogx   matplotlib.pyplot.semilogx(*args, **kwargs)[source]\n \nMake a plot with log scaling on the x axis. Call signatures: semilogx([x], y, [fmt], data=None, **kwargs)\nsemilogx([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n This is just a thin wrapper around plot which additionally changes the x-axis to log scaling. All of the concepts and parameters of plot can be used here as well. The additional parameters base, subs, and nonpositive control the x-axis properties. They are just forwarded to Axes.set_xscale.  Parameters \n \nbasefloat, default: 10\n\n\nBase of the x logarithm.  \nsubsarray-like, optional\n\n\nThe location of the minor xticks. If None, reasonable locations are automatically chosen depending on the number of decades in the plot. See Axes.set_xscale for details.  \nnonpositive{'mask', 'clip'}, default: 'mask'\n\n\nNon-positive values in x can be masked as invalid, or clipped to a very small positive number.  **kwargs\n\nAll parameters supported by plot.    Returns \n list of Line2D\n\n\nObjects representing the plotted data.","title":"matplotlib._as_gen.matplotlib.pyplot.semilogx"},{"text":"matplotlib.axes.Axes.semilogx   Axes.semilogx(*args, **kwargs)[source]\n \nMake a plot with log scaling on the x axis. Call signatures: semilogx([x], y, [fmt], data=None, **kwargs)\nsemilogx([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n This is just a thin wrapper around plot which additionally changes the x-axis to log scaling. All of the concepts and parameters of plot can be used here as well. The additional parameters base, subs, and nonpositive control the x-axis properties. They are just forwarded to Axes.set_xscale.  Parameters \n \nbasefloat, default: 10\n\n\nBase of the x logarithm.  \nsubsarray-like, optional\n\n\nThe location of the minor xticks. If None, reasonable locations are automatically chosen depending on the number of decades in the plot. See Axes.set_xscale for details.  \nnonpositive{'mask', 'clip'}, default: 'mask'\n\n\nNon-positive values in x can be masked as invalid, or clipped to a very small positive number.  **kwargs\n\nAll parameters supported by plot.    Returns \n list of Line2D\n\n\nObjects representing the plotted data.     \n  Examples using matplotlib.axes.Axes.semilogx\n \n   Log Demo   \n\n   Log Axis   \n\n   Transformations Tutorial","title":"matplotlib._as_gen.matplotlib.axes.axes.semilogx"},{"text":"sec_mark='^\\\\mathrm{s}'","title":"matplotlib._as_gen.mpl_toolkits.axisartist.angle_helper.formatterhms#mpl_toolkits.axisartist.angle_helper.FormatterHMS.sec_mark"},{"text":"sklearn.datasets.make_hastie_10_2(n_samples=12000, *, random_state=None) [source]\n \nGenerates data for binary classification used in Hastie et al. 2009, Example 10.2. The ten features are standard independent Gaussian and the target y is defined by: y[i] = 1 if np.sum(X[i] ** 2) > 9.34 else -1\n Read more in the User Guide.  Parameters \n \nn_samplesint, default=12000 \n\nThe number of samples.  \nrandom_stateint, RandomState instance or None, default=None \n\nDetermines random number generation for dataset creation. Pass an int for reproducible output across multiple function calls. See Glossary.    Returns \n \nXndarray of shape (n_samples, 10) \n\nThe input samples.  \nyndarray of shape (n_samples,) \n\nThe output values.      See also  \nmake_gaussian_quantiles\n\n\nA generalization of this dataset approach.    References  \n1  \nT. Hastie, R. Tibshirani and J. Friedman, \u201cElements of Statistical Learning Ed. 2\u201d, Springer, 2009.","title":"sklearn.modules.generated.sklearn.datasets.make_hastie_10_2#sklearn.datasets.make_hastie_10_2"},{"text":"sklearn.datasets.make_hastie_10_2  \nsklearn.datasets.make_hastie_10_2(n_samples=12000, *, random_state=None) [source]\n \nGenerates data for binary classification used in Hastie et al. 2009, Example 10.2. The ten features are standard independent Gaussian and the target y is defined by: y[i] = 1 if np.sum(X[i] ** 2) > 9.34 else -1\n Read more in the User Guide.  Parameters \n \nn_samplesint, default=12000 \n\nThe number of samples.  \nrandom_stateint, RandomState instance or None, default=None \n\nDetermines random number generation for dataset creation. Pass an int for reproducible output across multiple function calls. See Glossary.    Returns \n \nXndarray of shape (n_samples, 10) \n\nThe input samples.  \nyndarray of shape (n_samples,) \n\nThe output values.      See also  \nmake_gaussian_quantiles\n\n\nA generalization of this dataset approach.    References  \n1  \nT. Hastie, R. Tibshirani and J. Friedman, \u201cElements of Statistical Learning Ed. 2\u201d, Springer, 2009.   \n Examples using sklearn.datasets.make_hastie_10_2\n \n  Gradient Boosting regularization  \n\n  Discrete versus Real AdaBoost  \n\n  Early stopping of Gradient Boosting  \n\n  Demonstration of multi-metric evaluation on cross_val_score and GridSearchCV","title":"sklearn.modules.generated.sklearn.datasets.make_hastie_10_2"},{"text":"halign={'E': 'left', 'N': 'center', 'S': 'center', 'W': 'right'}","title":"matplotlib._as_gen.matplotlib.quiver.quiverkey#matplotlib.quiver.QuiverKey.halign"},{"text":"matplotlib.pyplot.semilogy   matplotlib.pyplot.semilogy(*args, **kwargs)[source]\n \nMake a plot with log scaling on the y axis. Call signatures: semilogy([x], y, [fmt], data=None, **kwargs)\nsemilogy([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n This is just a thin wrapper around plot which additionally changes the y-axis to log scaling. All of the concepts and parameters of plot can be used here as well. The additional parameters base, subs, and nonpositive control the y-axis properties. They are just forwarded to Axes.set_yscale.  Parameters \n \nbasefloat, default: 10\n\n\nBase of the y logarithm.  \nsubsarray-like, optional\n\n\nThe location of the minor yticks. If None, reasonable locations are automatically chosen depending on the number of decades in the plot. See Axes.set_yscale for details.  \nnonpositive{'mask', 'clip'}, default: 'mask'\n\n\nNon-positive values in y can be masked as invalid, or clipped to a very small positive number.  **kwargs\n\nAll parameters supported by plot.    Returns \n list of Line2D\n\n\nObjects representing the plotted data.","title":"matplotlib._as_gen.matplotlib.pyplot.semilogy"}]}
{"task_id":17138464,"prompt":"def f_17138464(x, y):\n\treturn ","suffix":"","canonical_solution":"plt.plot(x, y, label='$H_2O$')","test_start":"\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef check(candidate):","test":["\n    pic = candidate(np.array([1,2,3]),np.array([4,5,6]))[0]\n    assert pic.get_label() == '$H_2O$'\n    x, y = pic.get_data()\n    assert all(x == np.array([1,2,3]))\n    assert all(y == np.array([4,5,6]))\n","\n    pic = candidate(np.array([6, 7, 899]),np.array([0, 1, 245]))[0]\n    assert pic.get_label() == '$H_2O$'\n    x, y = pic.get_data()\n    assert all(x == np.array([6, 7, 899]))\n    assert all(y == np.array([0, 1, 245]))\n"],"entry_point":"f_17138464","intent":"subscript text 'H20' with '2' as subscripted in matplotlib labels for arrays 'x' and 'y'.","library":["matplotlib","numpy"],"docs":[{"text":"fmt_ds='$%d.%s^\\\\mathrm{h}$'","title":"matplotlib._as_gen.mpl_toolkits.axisartist.angle_helper.formatterhms#mpl_toolkits.axisartist.angle_helper.FormatterHMS.fmt_ds"},{"text":"deg_mark='^\\\\mathrm{h}'","title":"matplotlib._as_gen.mpl_toolkits.axisartist.angle_helper.formatterhms#mpl_toolkits.axisartist.angle_helper.FormatterHMS.deg_mark"},{"text":"fmt_d='$%d^\\\\mathrm{h}$'","title":"matplotlib._as_gen.mpl_toolkits.axisartist.angle_helper.formatterhms#mpl_toolkits.axisartist.angle_helper.FormatterHMS.fmt_d"},{"text":"matplotlib.pyplot.semilogx   matplotlib.pyplot.semilogx(*args, **kwargs)[source]\n \nMake a plot with log scaling on the x axis. Call signatures: semilogx([x], y, [fmt], data=None, **kwargs)\nsemilogx([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n This is just a thin wrapper around plot which additionally changes the x-axis to log scaling. All of the concepts and parameters of plot can be used here as well. The additional parameters base, subs, and nonpositive control the x-axis properties. They are just forwarded to Axes.set_xscale.  Parameters \n \nbasefloat, default: 10\n\n\nBase of the x logarithm.  \nsubsarray-like, optional\n\n\nThe location of the minor xticks. If None, reasonable locations are automatically chosen depending on the number of decades in the plot. See Axes.set_xscale for details.  \nnonpositive{'mask', 'clip'}, default: 'mask'\n\n\nNon-positive values in x can be masked as invalid, or clipped to a very small positive number.  **kwargs\n\nAll parameters supported by plot.    Returns \n list of Line2D\n\n\nObjects representing the plotted data.","title":"matplotlib._as_gen.matplotlib.pyplot.semilogx"},{"text":"matplotlib.axes.Axes.semilogx   Axes.semilogx(*args, **kwargs)[source]\n \nMake a plot with log scaling on the x axis. Call signatures: semilogx([x], y, [fmt], data=None, **kwargs)\nsemilogx([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n This is just a thin wrapper around plot which additionally changes the x-axis to log scaling. All of the concepts and parameters of plot can be used here as well. The additional parameters base, subs, and nonpositive control the x-axis properties. They are just forwarded to Axes.set_xscale.  Parameters \n \nbasefloat, default: 10\n\n\nBase of the x logarithm.  \nsubsarray-like, optional\n\n\nThe location of the minor xticks. If None, reasonable locations are automatically chosen depending on the number of decades in the plot. See Axes.set_xscale for details.  \nnonpositive{'mask', 'clip'}, default: 'mask'\n\n\nNon-positive values in x can be masked as invalid, or clipped to a very small positive number.  **kwargs\n\nAll parameters supported by plot.    Returns \n list of Line2D\n\n\nObjects representing the plotted data.     \n  Examples using matplotlib.axes.Axes.semilogx\n \n   Log Demo   \n\n   Log Axis   \n\n   Transformations Tutorial","title":"matplotlib._as_gen.matplotlib.axes.axes.semilogx"},{"text":"sec_mark='^\\\\mathrm{s}'","title":"matplotlib._as_gen.mpl_toolkits.axisartist.angle_helper.formatterhms#mpl_toolkits.axisartist.angle_helper.FormatterHMS.sec_mark"},{"text":"sklearn.datasets.make_hastie_10_2(n_samples=12000, *, random_state=None) [source]\n \nGenerates data for binary classification used in Hastie et al. 2009, Example 10.2. The ten features are standard independent Gaussian and the target y is defined by: y[i] = 1 if np.sum(X[i] ** 2) > 9.34 else -1\n Read more in the User Guide.  Parameters \n \nn_samplesint, default=12000 \n\nThe number of samples.  \nrandom_stateint, RandomState instance or None, default=None \n\nDetermines random number generation for dataset creation. Pass an int for reproducible output across multiple function calls. See Glossary.    Returns \n \nXndarray of shape (n_samples, 10) \n\nThe input samples.  \nyndarray of shape (n_samples,) \n\nThe output values.      See also  \nmake_gaussian_quantiles\n\n\nA generalization of this dataset approach.    References  \n1  \nT. Hastie, R. Tibshirani and J. Friedman, \u201cElements of Statistical Learning Ed. 2\u201d, Springer, 2009.","title":"sklearn.modules.generated.sklearn.datasets.make_hastie_10_2#sklearn.datasets.make_hastie_10_2"},{"text":"sklearn.datasets.make_hastie_10_2  \nsklearn.datasets.make_hastie_10_2(n_samples=12000, *, random_state=None) [source]\n \nGenerates data for binary classification used in Hastie et al. 2009, Example 10.2. The ten features are standard independent Gaussian and the target y is defined by: y[i] = 1 if np.sum(X[i] ** 2) > 9.34 else -1\n Read more in the User Guide.  Parameters \n \nn_samplesint, default=12000 \n\nThe number of samples.  \nrandom_stateint, RandomState instance or None, default=None \n\nDetermines random number generation for dataset creation. Pass an int for reproducible output across multiple function calls. See Glossary.    Returns \n \nXndarray of shape (n_samples, 10) \n\nThe input samples.  \nyndarray of shape (n_samples,) \n\nThe output values.      See also  \nmake_gaussian_quantiles\n\n\nA generalization of this dataset approach.    References  \n1  \nT. Hastie, R. Tibshirani and J. Friedman, \u201cElements of Statistical Learning Ed. 2\u201d, Springer, 2009.   \n Examples using sklearn.datasets.make_hastie_10_2\n \n  Gradient Boosting regularization  \n\n  Discrete versus Real AdaBoost  \n\n  Early stopping of Gradient Boosting  \n\n  Demonstration of multi-metric evaluation on cross_val_score and GridSearchCV","title":"sklearn.modules.generated.sklearn.datasets.make_hastie_10_2"},{"text":"halign={'E': 'left', 'N': 'center', 'S': 'center', 'W': 'right'}","title":"matplotlib._as_gen.matplotlib.quiver.quiverkey#matplotlib.quiver.QuiverKey.halign"},{"text":"matplotlib.pyplot.semilogy   matplotlib.pyplot.semilogy(*args, **kwargs)[source]\n \nMake a plot with log scaling on the y axis. Call signatures: semilogy([x], y, [fmt], data=None, **kwargs)\nsemilogy([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n This is just a thin wrapper around plot which additionally changes the y-axis to log scaling. All of the concepts and parameters of plot can be used here as well. The additional parameters base, subs, and nonpositive control the y-axis properties. They are just forwarded to Axes.set_yscale.  Parameters \n \nbasefloat, default: 10\n\n\nBase of the y logarithm.  \nsubsarray-like, optional\n\n\nThe location of the minor yticks. If None, reasonable locations are automatically chosen depending on the number of decades in the plot. See Axes.set_yscale for details.  \nnonpositive{'mask', 'clip'}, default: 'mask'\n\n\nNon-positive values in y can be masked as invalid, or clipped to a very small positive number.  **kwargs\n\nAll parameters supported by plot.    Returns \n list of Line2D\n\n\nObjects representing the plotted data.","title":"matplotlib._as_gen.matplotlib.pyplot.semilogy"}]}
{"task_id":9138112,"prompt":"def f_9138112(mylist):\n\treturn ","suffix":"","canonical_solution":"[x for x in mylist if len(x) == 3]","test_start":"\ndef check(candidate):","test":["\n    assert candidate([[1,2,3], 'abc', [345,53], 'avsvasf']) == [[1,2,3], 'abc']\n","\n    assert candidate([[435,654.4,45,2],[34,34,757,65,32423]]) == []\n"],"entry_point":"f_9138112","intent":"loop over a list `mylist` if sublists length equals 3","library":[],"docs":[]}
{"task_id":1807026,"prompt":"def f_1807026():\n\t","suffix":"\n\treturn lst","canonical_solution":"lst = [Object() for _ in range(100)]","test_start":"\nclass Object(): \n    def __init__(self): \n        self.name = \"object\"\n\ndef check(candidate):","test":["\n    lst = candidate()\n    assert all([x.name == \"object\" for x in lst])\n"],"entry_point":"f_1807026","intent":"initialize a list `lst` of 100 objects Object()","library":[],"docs":[]}
{"task_id":13793321,"prompt":"def f_13793321(df1, df2):\n\treturn ","suffix":"","canonical_solution":"df1.merge(df2, on='Date_Time')","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    df1 = pd.DataFrame([[1, 2, 3]], columns=[\"Date\", \"Time\", \"Date_Time\"])\n    df2 = pd.DataFrame([[1, 3],[4, 9]], columns=[\"Name\", \"Date_Time\"])\n    assert candidate(df1, df2).to_dict() == {'Date': {0: 1}, 'Time': {0: 2}, 'Date_Time': {0: 3}, 'Name': {0: 1}}\n"],"entry_point":"f_13793321","intent":"joining data from dataframe `df1` with data from dataframe `df2` based on matching values of column 'Date_Time' in both dataframes","library":["pandas"],"docs":[{"text":"tf.sparse.sparse_dense_matmul     View source on GitHub    Multiply SparseTensor (or dense Matrix) (of rank 2) \"A\" by dense matrix  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.sparse.matmul, tf.compat.v1.sparse.sparse_dense_matmul, tf.compat.v1.sparse_tensor_dense_matmul  \ntf.sparse.sparse_dense_matmul(\n    sp_a, b, adjoint_a=False, adjoint_b=False, name=None\n)\n (or SparseTensor) \"B\". Please note that one and only one of the inputs MUST be a SparseTensor and the other MUST be a dense matrix. No validity checking is performed on the indices of A. However, the following input format is recommended for optimal behavior:  If adjoint_a == false: A should be sorted in lexicographically increasing order. Use sparse.reorder if you're not sure. If adjoint_a == true: A should be sorted in order of increasing dimension 1 (i.e., \"column major\" order instead of \"row major\" order).  Using tf.nn.embedding_lookup_sparse for sparse multiplication: It's not obvious but you can consider embedding_lookup_sparse as another sparse and dense multiplication. In some situations, you may prefer to use embedding_lookup_sparse even though you're not dealing with embeddings. There are two questions to ask in the decision process: Do you need gradients computed as sparse too? Is your sparse data represented as two SparseTensors: ids and values? There is more explanation about data format below. If you answer any of these questions as yes, consider using tf.nn.embedding_lookup_sparse. Following explains differences between the expected SparseTensors: For example if dense form of your sparse data has shape [3, 5] and values: [[  a      ]\n [b       c]\n [    d    ]]\n SparseTensor format expected by sparse_tensor_dense_matmul: sp_a (indices, values): [0, 1]: a\n[1, 0]: b\n[1, 4]: c\n[2, 2]: d\n SparseTensor format expected by embedding_lookup_sparse: sp_ids sp_weights [0, 0]: 1                [0, 0]: a\n[1, 0]: 0                [1, 0]: b\n[1, 1]: 4                [1, 1]: c\n[2, 0]: 2                [2, 0]: d\n Deciding when to use sparse_tensor_dense_matmul vs. matmul(a_is_sparse=True): There are a number of questions to ask in the decision process, including:  Will the SparseTensor A fit in memory if densified? Is the column count of the product large (>> 1)? Is the density of A larger than approximately 15%?  If the answer to several of these questions is yes, consider converting the SparseTensor to a dense one and using tf.matmul with a_is_sparse=True. This operation tends to perform well when A is more sparse, if the column size of the product is small (e.g. matrix-vector multiplication), if sp_a.dense_shape takes on large values. Below is a rough speed comparison between sparse_tensor_dense_matmul, labeled 'sparse', and matmul(a_is_sparse=True), labeled 'dense'. For purposes of the comparison, the time spent converting from a SparseTensor to a dense Tensor is not included, so it is overly conservative with respect to the time ratio. Benchmark system: CPU: Intel Ivybridge with HyperThreading (6 cores) dL1:32KB dL2:256KB dL3:12MB GPU: NVidia Tesla k40c Compiled with: -c opt --config=cuda --copt=-mavx tensorflow\/python\/sparse_tensor_dense_matmul_op_test --benchmarks\nA sparse [m, k] with % nonzero values between 1% and 80%\nB dense [k, n]\n\n% nnz  n   gpu   m     k     dt(dense)     dt(sparse)   dt(sparse)\/dt(dense)\n0.01   1   True  100   100   0.000221166   0.00010154   0.459112\n0.01   1   True  100   1000  0.00033858    0.000109275  0.322745\n0.01   1   True  1000  100   0.000310557   9.85661e-05  0.317385\n0.01   1   True  1000  1000  0.0008721     0.000100875  0.115669\n0.01   1   False 100   100   0.000208085   0.000107603  0.51711\n0.01   1   False 100   1000  0.000327112   9.51118e-05  0.290762\n0.01   1   False 1000  100   0.000308222   0.00010345   0.335635\n0.01   1   False 1000  1000  0.000865721   0.000101397  0.117124\n0.01   10  True  100   100   0.000218522   0.000105537  0.482958\n0.01   10  True  100   1000  0.000340882   0.000111641  0.327506\n0.01   10  True  1000  100   0.000315472   0.000117376  0.372064\n0.01   10  True  1000  1000  0.000905493   0.000123263  0.136128\n0.01   10  False 100   100   0.000221529   9.82571e-05  0.44354\n0.01   10  False 100   1000  0.000330552   0.000112615  0.340687\n0.01   10  False 1000  100   0.000341277   0.000114097  0.334324\n0.01   10  False 1000  1000  0.000819944   0.000120982  0.147549\n0.01   25  True  100   100   0.000207806   0.000105977  0.509981\n0.01   25  True  100   1000  0.000322879   0.00012921   0.400181\n0.01   25  True  1000  100   0.00038262    0.00014158   0.370035\n0.01   25  True  1000  1000  0.000865438   0.000202083  0.233504\n0.01   25  False 100   100   0.000209401   0.000104696  0.499979\n0.01   25  False 100   1000  0.000321161   0.000130737  0.407076\n0.01   25  False 1000  100   0.000377012   0.000136801  0.362856\n0.01   25  False 1000  1000  0.000861125   0.00020272   0.235413\n0.2    1   True  100   100   0.000206952   9.69219e-05  0.46833\n0.2    1   True  100   1000  0.000348674   0.000147475  0.422959\n0.2    1   True  1000  100   0.000336908   0.00010122   0.300439\n0.2    1   True  1000  1000  0.001022      0.000203274  0.198898\n0.2    1   False 100   100   0.000207532   9.5412e-05   0.459746\n0.2    1   False 100   1000  0.000356127   0.000146824  0.41228\n0.2    1   False 1000  100   0.000322664   0.000100918  0.312764\n0.2    1   False 1000  1000  0.000998987   0.000203442  0.203648\n0.2    10  True  100   100   0.000211692   0.000109903  0.519165\n0.2    10  True  100   1000  0.000372819   0.000164321  0.440753\n0.2    10  True  1000  100   0.000338651   0.000144806  0.427596\n0.2    10  True  1000  1000  0.00108312    0.000758876  0.70064\n0.2    10  False 100   100   0.000215727   0.000110502  0.512231\n0.2    10  False 100   1000  0.000375419   0.0001613    0.429653\n0.2    10  False 1000  100   0.000336999   0.000145628  0.432132\n0.2    10  False 1000  1000  0.00110502    0.000762043  0.689618\n0.2    25  True  100   100   0.000218705   0.000129913  0.594009\n0.2    25  True  100   1000  0.000394794   0.00029428   0.745402\n0.2    25  True  1000  100   0.000404483   0.0002693    0.665788\n0.2    25  True  1000  1000  0.0012002     0.00194494   1.62052\n0.2    25  False 100   100   0.000221494   0.0001306    0.589632\n0.2    25  False 100   1000  0.000396436   0.000297204  0.74969\n0.2    25  False 1000  100   0.000409346   0.000270068  0.659754\n0.2    25  False 1000  1000  0.00121051    0.00193737   1.60046\n0.5    1   True  100   100   0.000214981   9.82111e-05  0.456836\n0.5    1   True  100   1000  0.000415328   0.000223073  0.537101\n0.5    1   True  1000  100   0.000358324   0.00011269   0.314492\n0.5    1   True  1000  1000  0.00137612    0.000437401  0.317851\n0.5    1   False 100   100   0.000224196   0.000101423  0.452386\n0.5    1   False 100   1000  0.000400987   0.000223286  0.556841\n0.5    1   False 1000  100   0.000368825   0.00011224   0.304318\n0.5    1   False 1000  1000  0.00136036    0.000429369  0.31563\n0.5    10  True  100   100   0.000222125   0.000112308  0.505608\n0.5    10  True  100   1000  0.000461088   0.00032357   0.701753\n0.5    10  True  1000  100   0.000394624   0.000225497  0.571422\n0.5    10  True  1000  1000  0.00158027    0.00190898   1.20801\n0.5    10  False 100   100   0.000232083   0.000114978  0.495418\n0.5    10  False 100   1000  0.000454574   0.000324632  0.714146\n0.5    10  False 1000  100   0.000379097   0.000227768  0.600817\n0.5    10  False 1000  1000  0.00160292    0.00190168   1.18638\n0.5    25  True  100   100   0.00023429    0.000151703  0.647501\n0.5    25  True  100   1000  0.000497462   0.000598873  1.20386\n0.5    25  True  1000  100   0.000460778   0.000557038  1.20891\n0.5    25  True  1000  1000  0.00170036    0.00467336   2.74845\n0.5    25  False 100   100   0.000228981   0.000155334  0.678371\n0.5    25  False 100   1000  0.000496139   0.000620789  1.25124\n0.5    25  False 1000  100   0.00045473    0.000551528  1.21287\n0.5    25  False 1000  1000  0.00171793    0.00467152   2.71927\n0.8    1   True  100   100   0.000222037   0.000105301  0.47425\n0.8    1   True  100   1000  0.000410804   0.000329327  0.801664\n0.8    1   True  1000  100   0.000349735   0.000131225  0.375212\n0.8    1   True  1000  1000  0.00139219    0.000677065  0.48633\n0.8    1   False 100   100   0.000214079   0.000107486  0.502085\n0.8    1   False 100   1000  0.000413746   0.000323244  0.781261\n0.8    1   False 1000  100   0.000348983   0.000131983  0.378193\n0.8    1   False 1000  1000  0.00136296    0.000685325  0.50282\n0.8    10  True  100   100   0.000229159   0.00011825   0.516017\n0.8    10  True  100   1000  0.000498845   0.000532618  1.0677\n0.8    10  True  1000  100   0.000383126   0.00029935   0.781336\n0.8    10  True  1000  1000  0.00162866    0.00307312   1.88689\n0.8    10  False 100   100   0.000230783   0.000124958  0.541452\n0.8    10  False 100   1000  0.000493393   0.000550654  1.11606\n0.8    10  False 1000  100   0.000377167   0.000298581  0.791642\n0.8    10  False 1000  1000  0.00165795    0.00305103   1.84024\n0.8    25  True  100   100   0.000233496   0.000175241  0.75051\n0.8    25  True  100   1000  0.00055654    0.00102658   1.84458\n0.8    25  True  1000  100   0.000463814   0.000783267  1.68875\n0.8    25  True  1000  1000  0.00186905    0.00755344   4.04132\n0.8    25  False 100   100   0.000240243   0.000175047  0.728625\n0.8    25  False 100   1000  0.000578102   0.00104499   1.80763\n0.8    25  False 1000  100   0.000485113   0.000776849  1.60138\n0.8    25  False 1000  1000  0.00211448    0.00752736   3.55992\n\n \n\n\n Args\n  sp_a   SparseTensor (or dense Matrix) A, of rank 2.  \n  b   dense Matrix (or SparseTensor) B, with the same dtype as sp_a.  \n  adjoint_a   Use the adjoint of A in the matrix multiply. If A is complex, this is transpose(conj(A)). Otherwise it's transpose(A).  \n  adjoint_b   Use the adjoint of B in the matrix multiply. If B is complex, this is transpose(conj(B)). Otherwise it's transpose(B).  \n  name   A name prefix for the returned tensors (optional)   \n \n\n\n Returns   A dense matrix (pseudo-code in dense np.matrix notation): A = A.H if adjoint_a else A B = B.H if adjoint_b else B return A*B","title":"tensorflow.sparse.sparse_dense_matmul"},{"text":"classmethod path_hook(*loader_details)  \nA class method which returns a closure for use on sys.path_hooks. An instance of FileFinder is returned by the closure using the path argument given to the closure directly and loader_details indirectly. If the argument to the closure is not an existing directory, ImportError is raised.","title":"python.library.importlib#importlib.machinery.FileFinder.path_hook"},{"text":"read([n])  \nReturn a bytes containing up to n bytes starting from the current file position. If the argument is omitted, None or negative, return all bytes from the current file position to the end of the mapping. The file position is updated to point after the bytes that were returned.  Changed in version 3.3: Argument can be omitted or None.","title":"python.library.mmap#mmap.mmap.read"},{"text":"read1([size])  \nIn BytesIO, this is the same as read().  Changed in version 3.7: The size argument is now optional.","title":"python.library.io#io.BytesIO.read1"},{"text":"tf.compat.v1.reduce_sum Computes the sum of elements across dimensions of a tensor. (deprecated arguments)  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.math.reduce_sum  \ntf.compat.v1.reduce_sum(\n    input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None,\n    keep_dims=None\n)\n Warning: SOME ARGUMENTS ARE DEPRECATED: (keep_dims). They will be removed in a future version. Instructions for updating: keep_dims is deprecated, use keepdims instead Reduces input_tensor along the dimensions given in axis. Unless keepdims is true, the rank of the tensor is reduced by 1 for each of the entries in axis, which must be unique. If keepdims is true, the reduced dimensions are retained with length 1. If axis is None, all dimensions are reduced, and a tensor with a single element is returned. For example: x = tf.constant([[1, 1, 1], [1, 1, 1]])\ntf.reduce_sum(x)  # 6\ntf.reduce_sum(x, 0)  # [2, 2, 2]\ntf.reduce_sum(x, 1)  # [3, 3]\ntf.reduce_sum(x, 1, keepdims=True)  # [[3], [3]]\ntf.reduce_sum(x, [0, 1])  # 6\n\n \n\n\n Args\n  input_tensor   The tensor to reduce. Should have numeric type.  \n  axis   The dimensions to reduce. If None (the default), reduces all dimensions. Must be in the range [-rank(input_tensor), rank(input_tensor)).  \n  keepdims   If true, retains reduced dimensions with length 1.  \n  name   A name for the operation (optional).  \n  reduction_indices   The old (deprecated) name for axis.  \n  keep_dims   Deprecated alias for keepdims.   \n \n\n\n Returns   The reduced tensor, of the same dtype as the input_tensor.  \n Numpy Compatibility Equivalent to np.sum apart the fact that numpy upcast uint8 and int32 to int64 while tensorflow returns the same dtype as the input.","title":"tensorflow.compat.v1.reduce_sum"},{"text":"classmethod datetime.combine(date, time, tzinfo=self.tzinfo)  \nReturn a new datetime object whose date components are equal to the given date object\u2019s, and whose time components are equal to the given time object\u2019s. If the tzinfo argument is provided, its value is used to set the tzinfo attribute of the result, otherwise the tzinfo attribute of the time argument is used. For any datetime object d, d == datetime.combine(d.date(), d.time(), d.tzinfo). If date is a datetime object, its time components and tzinfo attributes are ignored.  Changed in version 3.6: Added the tzinfo argument.","title":"python.library.datetime#datetime.datetime.combine"},{"text":"pandas.Timestamp.combine   classmethodTimestamp.combine(date, time)\n \nCombine date, time into datetime with same date and time fields. Examples \n>>> from datetime import date, time\n>>> pd.Timestamp.combine(date(2020, 3, 14), time(15, 30, 15))\nTimestamp('2020-03-14 15:30:15')","title":"pandas.reference.api.pandas.timestamp.combine"},{"text":"read(size=-1)  \nRead up to size bytes from the object and return them. As a convenience, if size is unspecified or -1, all bytes until EOF are returned. Otherwise, only one system call is ever made. Fewer than size bytes may be returned if the operating system call returns fewer than size bytes. If 0 bytes are returned, and size was not 0, this indicates end of file. If the object is in non-blocking mode and no bytes are available, None is returned. The default implementation defers to readall() and readinto().","title":"python.library.io#io.RawIOBase.read"},{"text":"sys.path_hooks  \nA list of callables that take a path argument to try to create a finder for the path. If a finder can be created, it is to be returned by the callable, else raise ImportError. Originally specified in PEP 302.","title":"python.library.sys#sys.path_hooks"},{"text":"pushbutton(name, x, y, width, height, attributes, text, next_control)  \nAdd and return a PushButton control.","title":"python.library.msilib#msilib.Dialog.pushbutton"}]}
{"task_id":3367288,"prompt":"def f_3367288(str1):\n\treturn ","suffix":"","canonical_solution":"'first string is: %s, second one is: %s' % (str1, 'geo.tif')","test_start":"\ndef check(candidate):","test":["\n    assert candidate(\"s001\") == \"first string is: s001, second one is: geo.tif\"\n","\n    assert candidate(\"\") == \"first string is: , second one is: geo.tif\"\n","\n    assert candidate(\"  \") == \"first string is:   , second one is: geo.tif\"\n"],"entry_point":"f_3367288","intent":"use `%s` operator to print variable values `str1` inside a string","library":[],"docs":[]}
{"task_id":3475251,"prompt":"def f_3475251():\n\treturn ","suffix":"","canonical_solution":"[x.strip() for x in '2.MATCHES $$TEXT$$ STRING'.split('$$TEXT$$')]","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == ['2.MATCHES', 'STRING']\n"],"entry_point":"f_3475251","intent":"Split a string '2.MATCHES $$TEXT$$ STRING' by a delimiter '$$TEXT$$'","library":[],"docs":[]}
{"task_id":273192,"prompt":"def f_273192(directory):\n\t","suffix":"\n\treturn ","canonical_solution":"if (not os.path.exists(directory)):\n\t    os.makedirs(directory)","test_start":"\nimport os \n\ndef check(candidate):","test":["\n    candidate(\"hello\")\n    assert os.path.exists(\"hello\")\n","\n    candidate(\"_some_dir\")\n    assert os.path.exists(\"_some_dir\")\n"],"entry_point":"f_273192","intent":"check if directory `directory ` exists and create it if necessary","library":["os"],"docs":[{"text":"turtle.back(distance)  \nturtle.bk(distance)  \nturtle.backward(distance)  \n Parameters \ndistance \u2013 a number   Move the turtle backward by distance, opposite to the direction the turtle is headed. Do not change the turtle\u2019s heading. >>> turtle.position()\n(0.00,0.00)\n>>> turtle.backward(30)\n>>> turtle.position()\n(-30.00,0.00)","title":"python.library.turtle#turtle.backward"},{"text":"turtle.back(distance)  \nturtle.bk(distance)  \nturtle.backward(distance)  \n Parameters \ndistance \u2013 a number   Move the turtle backward by distance, opposite to the direction the turtle is headed. Do not change the turtle\u2019s heading. >>> turtle.position()\n(0.00,0.00)\n>>> turtle.backward(30)\n>>> turtle.position()\n(-30.00,0.00)","title":"python.library.turtle#turtle.back"},{"text":"turtle.back(distance)  \nturtle.bk(distance)  \nturtle.backward(distance)  \n Parameters \ndistance \u2013 a number   Move the turtle backward by distance, opposite to the direction the turtle is headed. Do not change the turtle\u2019s heading. >>> turtle.position()\n(0.00,0.00)\n>>> turtle.backward(30)\n>>> turtle.position()\n(-30.00,0.00)","title":"python.library.turtle#turtle.bk"},{"text":"SSLSocket.version()  \nReturn the actual SSL protocol version negotiated by the connection as a string, or None is no secure connection is established. As of this writing, possible return values include \"SSLv2\", \"SSLv3\", \"TLSv1\", \"TLSv1.1\" and \"TLSv1.2\". Recent OpenSSL versions may define more return values.  New in version 3.5.","title":"python.library.ssl#ssl.SSLSocket.version"},{"text":"urllib.request.pathname2url(path)  \nConvert the pathname path from the local syntax for a path to the form used in the path component of a URL. This does not produce a complete URL. The return value will already be quoted using the quote() function.","title":"python.library.urllib.request#urllib.request.pathname2url"},{"text":"directory  \nIf not specified, the directory to serve is the current working directory.  Changed in version 3.9: Accepts a path-like object.","title":"python.library.http.server#http.server.SimpleHTTPRequestHandler.directory"},{"text":"tf.keras.preprocessing.image_dataset_from_directory Generates a tf.data.Dataset from image files in a directory. \ntf.keras.preprocessing.image_dataset_from_directory(\n    directory, labels='inferred', label_mode='int',\n    class_names=None, color_mode='rgb', batch_size=32, image_size=(256,\n    256), shuffle=True, seed=None, validation_split=None, subset=None,\n    interpolation='bilinear', follow_links=False\n)\n If your directory structure is: main_directory\/\n...class_a\/\n......a_image_1.jpg\n......a_image_2.jpg\n...class_b\/\n......b_image_1.jpg\n......b_image_2.jpg\n Then calling image_dataset_from_directory(main_directory, labels='inferred') will return a tf.data.Dataset that yields batches of images from the subdirectories class_a and class_b, together with labels 0 and 1 (0 corresponding to class_a and 1 corresponding to class_b). Supported image formats: jpeg, png, bmp, gif. Animated gifs are truncated to the first frame.\n \n\n\n Arguments\n  directory   Directory where the data is located. If labels is \"inferred\", it should contain subdirectories, each containing images for a class. Otherwise, the directory structure is ignored.  \n  labels   Either \"inferred\" (labels are generated from the directory structure), or a list\/tuple of integer labels of the same size as the number of image files found in the directory. Labels should be sorted according to the alphanumeric order of the image file paths (obtained via os.walk(directory) in Python).  \n  label_mode    'int': means that the labels are encoded as integers (e.g. for sparse_categorical_crossentropy loss). 'categorical' means that the labels are encoded as a categorical vector (e.g. for categorical_crossentropy loss). 'binary' means that the labels (there can be only 2) are encoded as float32 scalars with values 0 or 1 (e.g. for binary_crossentropy). None (no labels). \n\n \n  class_names   Only valid if \"labels\" is \"inferred\". This is the explict list of class names (must match names of subdirectories). Used to control the order of the classes (otherwise alphanumerical order is used).  \n  color_mode   One of \"grayscale\", \"rgb\", \"rgba\". Default: \"rgb\". Whether the images will be converted to have 1, 3, or 4 channels.  \n  batch_size   Size of the batches of data. Default: 32.  \n  image_size   Size to resize images to after they are read from disk. Defaults to (256, 256). Since the pipeline processes batches of images that must all have the same size, this must be provided.  \n  shuffle   Whether to shuffle the data. Default: True. If set to False, sorts the data in alphanumeric order.  \n  seed   Optional random seed for shuffling and transformations.  \n  validation_split   Optional float between 0 and 1, fraction of data to reserve for validation.  \n  subset   One of \"training\" or \"validation\". Only used if validation_split is set.  \n  interpolation   String, the interpolation method used when resizing images. Defaults to bilinear. Supports bilinear, nearest, bicubic, area, lanczos3, lanczos5, gaussian, mitchellcubic.  \n  follow_links   Whether to visits subdirectories pointed to by symlinks. Defaults to False.   \n \n\n\n Returns   A tf.data.Dataset object.  If label_mode is None, it yields float32 tensors of shape (batch_size, image_size[0], image_size[1], num_channels), encoding images (see below for rules regarding num_channels). Otherwise, it yields a tuple (images, labels), where images has shape (batch_size, image_size[0], image_size[1], num_channels), and labels follows the format described below. \n\n \n Rules regarding labels format:  if label_mode is int, the labels are an int32 tensor of shape (batch_size,). if label_mode is binary, the labels are a float32 tensor of 1s and 0s of shape (batch_size, 1). if label_mode is categorial, the labels are a float32 tensor of shape (batch_size, num_classes), representing a one-hot encoding of the class index.  Rules regarding number of channels in the yielded images:  if color_mode is grayscale, there's 1 channel in the image tensors. if color_mode is rgb, there are 3 channel in the image tensors. if color_mode is rgba, there are 4 channel in the image tensors.","title":"tensorflow.keras.preprocessing.image_dataset_from_directory"},{"text":"class tkinter.filedialog.Directory(master=None, **options)  \nCreate a dialog prompting the user to select a directory.","title":"python.library.dialog#tkinter.filedialog.Directory"},{"text":"class email.headerregistry.SingleAddressHeader  \nA subclass of AddressHeader that adds one additional attribute:  \naddress  \nThe single address encoded by the header value. If the header value actually contains more than one address (which would be a violation of the RFC under the default policy), accessing this attribute will result in a ValueError.","title":"python.library.email.headerregistry#email.headerregistry.SingleAddressHeader"},{"text":"ssl.OPENSSL_VERSION_NUMBER  \nThe raw version number of the OpenSSL library, as a single integer: >>> ssl.OPENSSL_VERSION_NUMBER\n268443839\n>>> hex(ssl.OPENSSL_VERSION_NUMBER)\n'0x100020bf'\n  New in version 3.2.","title":"python.library.ssl#ssl.OPENSSL_VERSION_NUMBER"}]}
{"task_id":273192,"prompt":"def f_273192(path):\n\t","suffix":"\n\treturn ","canonical_solution":"try:\n\t    os.makedirs(path)\n\texcept OSError:\n\t    if (not os.path.isdir(path)):\n\t        raise","test_start":"\nimport os \n\ndef check(candidate):","test":["\n    candidate(\"hello\")\n    assert os.path.exists(\"hello\")\n","\n    candidate(\"_some_dir\")\n    assert os.path.exists(\"_some_dir\")\n"],"entry_point":"f_273192","intent":"check if a directory `path` exists and create it if necessary","library":["os"],"docs":[{"text":"tf.io.gfile.makedirs     View source on GitHub    Creates a directory and all parent\/intermediate directories.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.io.gfile.makedirs  \ntf.io.gfile.makedirs(\n    path\n)\n It succeeds if path already exists and is writable.\n \n\n\n Args\n  path   string, name of the directory to be created   \n \n\n\n Raises\n  errors.OpError   If the operation fails.","title":"tensorflow.io.gfile.makedirs"},{"text":"tf.io.gfile.mkdir     View source on GitHub    Creates a directory with the name given by path.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.io.gfile.mkdir  \ntf.io.gfile.mkdir(\n    path\n)\n\n \n\n\n Args\n  path   string, name of the directory to be created    Notes: The parent directories need to exist. Use tf.io.gfile.makedirs instead if there is the possibility that the parent dirs don't exist.\n \n\n\n Raises\n  errors.OpError   If the operation fails.","title":"tensorflow.io.gfile.mkdir"},{"text":"class asyncio.DatagramTransport(BaseTransport)  \nA transport for datagram (UDP) connections. Instances of the DatagramTransport class are returned from the loop.create_datagram_endpoint() event loop method.","title":"python.library.asyncio-protocol#asyncio.DatagramTransport"},{"text":"os.path.normcase(path)  \nNormalize the case of a pathname. On Windows, convert all characters in the pathname to lowercase, and also convert forward slashes to backward slashes. On other operating systems, return the path unchanged.  Changed in version 3.6: Accepts a path-like object.","title":"python.library.os.path#os.path.normcase"},{"text":"curses.is_term_resized(nlines, ncols)  \nReturn True if resize_term() would modify the window structure, False otherwise.","title":"python.library.curses#curses.is_term_resized"},{"text":"os.path.isdir(path)  \nReturn True if path is an existing directory. This follows symbolic links, so both islink() and isdir() can be true for the same path.  Changed in version 3.6: Accepts a path-like object.","title":"python.library.os.path#os.path.isdir"},{"text":"class asyncio.DatagramProtocol(BaseProtocol)  \nThe base class for implementing datagram (UDP) protocols.","title":"python.library.asyncio-protocol#asyncio.DatagramProtocol"},{"text":"class ast.AnnAssign(target, annotation, value, simple)  \nAn assignment with a type annotation. target is a single node and can be a Name, a Attribute or a Subscript. annotation is the annotation, such as a Constant or Name node. value is a single optional node. simple is a boolean integer set to True for a Name node in target that do not appear in between parenthesis and are hence pure names and not expressions. >>> print(ast.dump(ast.parse('c: int'), indent=4))\nModule(\n    body=[\n        AnnAssign(\n            target=Name(id='c', ctx=Store()),\n            annotation=Name(id='int', ctx=Load()),\n            simple=1)],\n    type_ignores=[])\n\n>>> print(ast.dump(ast.parse('(a): int = 1'), indent=4)) # Annotation with parenthesis\nModule(\n    body=[\n        AnnAssign(\n            target=Name(id='a', ctx=Store()),\n            annotation=Name(id='int', ctx=Load()),\n            value=Constant(value=1),\n            simple=0)],\n    type_ignores=[])\n\n>>> print(ast.dump(ast.parse('a.b: int'), indent=4)) # Attribute annotation\nModule(\n    body=[\n        AnnAssign(\n            target=Attribute(\n                value=Name(id='a', ctx=Load()),\n                attr='b',\n                ctx=Store()),\n            annotation=Name(id='int', ctx=Load()),\n            simple=0)],\n    type_ignores=[])\n\n>>> print(ast.dump(ast.parse('a[1]: int'), indent=4)) # Subscript annotation\nModule(\n    body=[\n        AnnAssign(\n            target=Subscript(\n                value=Name(id='a', ctx=Load()),\n                slice=Constant(value=1),\n                ctx=Store()),\n            annotation=Name(id='int', ctx=Load()),\n            simple=0)],\n    type_ignores=[])","title":"python.library.ast#ast.AnnAssign"},{"text":"tf.io.gfile.isdir     View source on GitHub    Returns whether the path is a directory or not.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.io.gfile.isdir  \ntf.io.gfile.isdir(\n    path\n)\n\n \n\n\n Args\n  path   string, path to a potential directory   \n \n\n\n Returns   True, if the path is a directory; False otherwise","title":"tensorflow.io.gfile.isdir"},{"text":"close()  \nStop serving: close listening sockets and set the sockets attribute to None. The sockets that represent existing incoming client connections are left open. The server is closed asynchronously, use the wait_closed() coroutine to wait until the server is closed.","title":"python.library.asyncio-eventloop#asyncio.Server.close"}]}
{"task_id":273192,"prompt":"def f_273192(path):\n\t","suffix":"\n\treturn ","canonical_solution":"try:\n\t    os.makedirs(path)\n\texcept OSError as exception:\n\t    if (exception.errno != errno.EEXIST):\n\t        raise","test_start":"\nimport os \n\ndef check(candidate):","test":["\n    candidate(\"hello\")\n    assert os.path.exists(\"hello\")\n","\n    candidate(\"_some_dir\")\n    assert os.path.exists(\"_some_dir\")\n"],"entry_point":"f_273192","intent":"check if a directory `path` exists and create it if necessary","library":["os"],"docs":[{"text":"tf.io.gfile.makedirs     View source on GitHub    Creates a directory and all parent\/intermediate directories.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.io.gfile.makedirs  \ntf.io.gfile.makedirs(\n    path\n)\n It succeeds if path already exists and is writable.\n \n\n\n Args\n  path   string, name of the directory to be created   \n \n\n\n Raises\n  errors.OpError   If the operation fails.","title":"tensorflow.io.gfile.makedirs"},{"text":"tf.io.gfile.mkdir     View source on GitHub    Creates a directory with the name given by path.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.io.gfile.mkdir  \ntf.io.gfile.mkdir(\n    path\n)\n\n \n\n\n Args\n  path   string, name of the directory to be created    Notes: The parent directories need to exist. Use tf.io.gfile.makedirs instead if there is the possibility that the parent dirs don't exist.\n \n\n\n Raises\n  errors.OpError   If the operation fails.","title":"tensorflow.io.gfile.mkdir"},{"text":"class asyncio.DatagramTransport(BaseTransport)  \nA transport for datagram (UDP) connections. Instances of the DatagramTransport class are returned from the loop.create_datagram_endpoint() event loop method.","title":"python.library.asyncio-protocol#asyncio.DatagramTransport"},{"text":"os.path.normcase(path)  \nNormalize the case of a pathname. On Windows, convert all characters in the pathname to lowercase, and also convert forward slashes to backward slashes. On other operating systems, return the path unchanged.  Changed in version 3.6: Accepts a path-like object.","title":"python.library.os.path#os.path.normcase"},{"text":"curses.is_term_resized(nlines, ncols)  \nReturn True if resize_term() would modify the window structure, False otherwise.","title":"python.library.curses#curses.is_term_resized"},{"text":"os.path.isdir(path)  \nReturn True if path is an existing directory. This follows symbolic links, so both islink() and isdir() can be true for the same path.  Changed in version 3.6: Accepts a path-like object.","title":"python.library.os.path#os.path.isdir"},{"text":"class asyncio.DatagramProtocol(BaseProtocol)  \nThe base class for implementing datagram (UDP) protocols.","title":"python.library.asyncio-protocol#asyncio.DatagramProtocol"},{"text":"class ast.AnnAssign(target, annotation, value, simple)  \nAn assignment with a type annotation. target is a single node and can be a Name, a Attribute or a Subscript. annotation is the annotation, such as a Constant or Name node. value is a single optional node. simple is a boolean integer set to True for a Name node in target that do not appear in between parenthesis and are hence pure names and not expressions. >>> print(ast.dump(ast.parse('c: int'), indent=4))\nModule(\n    body=[\n        AnnAssign(\n            target=Name(id='c', ctx=Store()),\n            annotation=Name(id='int', ctx=Load()),\n            simple=1)],\n    type_ignores=[])\n\n>>> print(ast.dump(ast.parse('(a): int = 1'), indent=4)) # Annotation with parenthesis\nModule(\n    body=[\n        AnnAssign(\n            target=Name(id='a', ctx=Store()),\n            annotation=Name(id='int', ctx=Load()),\n            value=Constant(value=1),\n            simple=0)],\n    type_ignores=[])\n\n>>> print(ast.dump(ast.parse('a.b: int'), indent=4)) # Attribute annotation\nModule(\n    body=[\n        AnnAssign(\n            target=Attribute(\n                value=Name(id='a', ctx=Load()),\n                attr='b',\n                ctx=Store()),\n            annotation=Name(id='int', ctx=Load()),\n            simple=0)],\n    type_ignores=[])\n\n>>> print(ast.dump(ast.parse('a[1]: int'), indent=4)) # Subscript annotation\nModule(\n    body=[\n        AnnAssign(\n            target=Subscript(\n                value=Name(id='a', ctx=Load()),\n                slice=Constant(value=1),\n                ctx=Store()),\n            annotation=Name(id='int', ctx=Load()),\n            simple=0)],\n    type_ignores=[])","title":"python.library.ast#ast.AnnAssign"},{"text":"tf.io.gfile.isdir     View source on GitHub    Returns whether the path is a directory or not.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.io.gfile.isdir  \ntf.io.gfile.isdir(\n    path\n)\n\n \n\n\n Args\n  path   string, path to a potential directory   \n \n\n\n Returns   True, if the path is a directory; False otherwise","title":"tensorflow.io.gfile.isdir"},{"text":"close()  \nStop serving: close listening sockets and set the sockets attribute to None. The sockets that represent existing incoming client connections are left open. The server is closed asynchronously, use the wait_closed() coroutine to wait until the server is closed.","title":"python.library.asyncio-eventloop#asyncio.Server.close"}]}
{"task_id":18785032,"prompt":"def f_18785032(text):\n\treturn ","suffix":"","canonical_solution":"re.sub('\\\\bH3\\\\b', 'H1', text)","test_start":"\nimport re\n\ndef check(candidate):","test":["\n    assert candidate(\"hello world and H3\") == \"hello world and H1\"\n","\n    assert candidate(\"hello world and H1\") == \"hello world and H1\"\n","\n    assert candidate(\"hello world!\") == \"hello world!\"\n"],"entry_point":"f_18785032","intent":"Replace a separate word 'H3' by 'H1' in a string 'text'","library":["re"],"docs":[{"text":"stringprep.map_table_b3(code)  \nReturn the mapped value for code according to tableB.3 (Mapping for case-folding used with no normalization).","title":"python.library.stringprep#stringprep.map_table_b3"},{"text":"class email.policy.Compat32(**kw)  \nThis concrete Policy is the backward compatibility policy. It replicates the behavior of the email package in Python 3.2. The policy module also defines an instance of this class, compat32, that is used as the default policy. Thus the default behavior of the email package is to maintain compatibility with Python 3.2. The following attributes have values that are different from the Policy default:  \nmangle_from_  \nThe default is True. \n The class provides the following concrete implementations of the abstract methods of Policy:  \nheader_source_parse(sourcelines)  \nThe name is parsed as everything up to the \u2018:\u2019 and returned unmodified. The value is determined by stripping leading whitespace off the remainder of the first line, joining all subsequent lines together, and stripping any trailing carriage return or linefeed characters. \n  \nheader_store_parse(name, value)  \nThe name and value are returned unmodified. \n  \nheader_fetch_parse(name, value)  \nIf the value contains binary data, it is converted into a Header object using the unknown-8bit charset. Otherwise it is returned unmodified. \n  \nfold(name, value)  \nHeaders are folded using the Header folding algorithm, which preserves existing line breaks in the value, and wraps each resulting line to the max_line_length. Non-ASCII binary data are CTE encoded using the unknown-8bit charset. \n  \nfold_binary(name, value)  \nHeaders are folded using the Header folding algorithm, which preserves existing line breaks in the value, and wraps each resulting line to the max_line_length. If cte_type is 7bit, non-ascii binary data is CTE encoded using the unknown-8bit charset. Otherwise the original source header is used, with its existing line breaks and any (RFC invalid) binary data it may contain.","title":"python.library.email.policy#email.policy.Compat32"},{"text":"CookiePolicy.hide_cookie2  \nDon\u2019t add Cookie2 header to requests (the presence of this header indicates to the server that we understand RFC 2965 cookies).","title":"python.library.http.cookiejar#http.cookiejar.CookiePolicy.hide_cookie2"},{"text":"mpl_toolkits.axes_grid1.axes_divider.HBoxDivider   classmpl_toolkits.axes_grid1.axes_divider.HBoxDivider(fig, *args, horizontal=None, vertical=None, aspect=None, anchor='C')[source]\n \nBases: mpl_toolkits.axes_grid1.axes_divider.SubplotDivider A SubplotDivider for laying out axes horizontally, while ensuring that they have equal heights. Examples (Source code, png, pdf)     Parameters \n \nfigmatplotlib.figure.Figure\n\n\n*argstuple (nrows, ncols, index) or int\n\n\nThe array of subplots in the figure has dimensions (nrows,\nncols), and index is the index of the subplot being created. index starts at 1 in the upper left corner and increases to the right. If nrows, ncols, and index are all single digit numbers, then args can be passed as a single 3-digit number (e.g. 234 for (2, 3, 4)).       locate(nx, ny, nx1=None, ny1=None, axes=None, renderer=None)[source]\n \n Parameters \n \nnx, nx1int\n\n\nIntegers specifying the column-position of the cell. When nx1 is None, a single nx-th column is specified. Otherwise location of columns spanning between nx to nx1 (but excluding nx1-th column) is specified.  \nny, ny1int\n\n\nSame as nx and nx1, but for row positions.  axes\nrenderer\n   \n   new_locator(nx, nx1=None)[source]\n \nCreate a new AxesLocator for the specified cell.  Parameters \n \nnx, nx1int\n\n\nIntegers specifying the column-position of the cell. When nx1 is None, a single nx-th column is specified. Otherwise location of columns spanning between nx to nx1 (but excluding nx1-th column) is specified.     \n \n  Examples using mpl_toolkits.axes_grid1.axes_divider.HBoxDivider\n \n   HBoxDivider demo","title":"matplotlib._as_gen.mpl_toolkits.axes_grid1.axes_divider.hboxdivider"},{"text":"email.policy.compat32  \nAn instance of Compat32, providing backward compatibility with the behavior of the email package in Python 3.2.","title":"python.library.email.policy#email.policy.compat32"},{"text":"exception http.client.UnimplementedFileMode  \nA subclass of HTTPException.","title":"python.library.http.client#http.client.UnimplementedFileMode"},{"text":"mpl_toolkits.axisartist.angle_helper.select_step360   mpl_toolkits.axisartist.angle_helper.select_step360(v1, v2, nv, include_last=True, threshold_factor=3600)[source]","title":"matplotlib._as_gen.mpl_toolkits.axisartist.angle_helper.select_step360"},{"text":"int_to_base36(i) [source]\n \nConverts a positive integer to a base 36 string.","title":"django.ref.utils#django.utils.http.int_to_base36"},{"text":"get_command(ctx, name)  \nGiven a context and a command name, this returns a Command object if it exists or returns None.","title":"flask.api.index#flask.cli.FlaskGroup.get_command"},{"text":"new_locator(nx, nx1=None)[source]\n \nCreate a new AxesLocator for the specified cell.  Parameters \n \nnx, nx1int\n\n\nIntegers specifying the column-position of the cell. When nx1 is None, a single nx-th column is specified. Otherwise location of columns spanning between nx to nx1 (but excluding nx1-th column) is specified.","title":"matplotlib._as_gen.mpl_toolkits.axes_grid1.axes_divider.hboxdivider#mpl_toolkits.axes_grid1.axes_divider.HBoxDivider.new_locator"}]}
{"task_id":1450897,"prompt":"def f_1450897():\n\treturn ","suffix":"","canonical_solution":"re.sub('\\\\D', '', 'aas30dsa20')","test_start":"\nimport re\n\ndef check(candidate):","test":["\n    assert candidate() == \"3020\"\n"],"entry_point":"f_1450897","intent":"substitute ASCII letters in string 'aas30dsa20' with empty string ''","library":["re"],"docs":[{"text":"encodings.idna.ToASCII(label)  \nConvert a label to ASCII, as specified in RFC 3490. UseSTD3ASCIIRules is assumed to be false.","title":"python.library.codecs#encodings.idna.ToASCII"},{"text":"string.ascii_letters  \nThe concatenation of the ascii_lowercase and ascii_uppercase constants described below. This value is not locale-dependent.","title":"python.library.string#string.ascii_letters"},{"text":"ascii(object)  \nAs repr(), return a string containing a printable representation of an object, but escape the non-ASCII characters in the string returned by repr() using \\x, \\u or \\U escapes. This generates a string similar to that returned by repr() in Python 2.","title":"python.library.functions#ascii"},{"text":"re.A  \nre.ASCII  \nMake \\w, \\W, \\b, \\B, \\d, \\D, \\s and \\S perform ASCII-only matching instead of full Unicode matching. This is only meaningful for Unicode patterns, and is ignored for byte patterns. Corresponds to the inline flag (?a). Note that for backward compatibility, the re.U flag still exists (as well as its synonym re.UNICODE and its embedded counterpart (?u)), but these are redundant in Python 3 since matches are Unicode by default for strings (and Unicode matching isn\u2019t allowed for bytes).","title":"python.library.re#re.ASCII"},{"text":"string.ascii_lowercase  \nThe lowercase letters 'abcdefghijklmnopqrstuvwxyz'. This value is not locale-dependent and will not change.","title":"python.library.string#string.ascii_lowercase"},{"text":"test.support.FS_NONASCII  \nA non-ASCII character encodable by os.fsencode().","title":"python.library.test#test.support.FS_NONASCII"},{"text":"re.A  \nre.ASCII  \nMake \\w, \\W, \\b, \\B, \\d, \\D, \\s and \\S perform ASCII-only matching instead of full Unicode matching. This is only meaningful for Unicode patterns, and is ignored for byte patterns. Corresponds to the inline flag (?a). Note that for backward compatibility, the re.U flag still exists (as well as its synonym re.UNICODE and its embedded counterpart (?u)), but these are redundant in Python 3 since matches are Unicode by default for strings (and Unicode matching isn\u2019t allowed for bytes).","title":"python.library.re#re.A"},{"text":"test.support.TESTFN_NONASCII  \nSet to a filename containing the FS_NONASCII character.","title":"python.library.test#test.support.TESTFN_NONASCII"},{"text":"string.printable  \nString of ASCII characters which are considered printable. This is a combination of digits, ascii_letters, punctuation, and whitespace.","title":"python.library.string#string.printable"},{"text":"matplotlib.fontconfig_pattern.family_escape(\/, repl, string, count=0)\n \nReturn the string obtained by replacing the leftmost non-overlapping occurrences of pattern in string by the replacement repl.","title":"matplotlib.fontconfig_pattern_api#matplotlib.fontconfig_pattern.family_escape"}]}
{"task_id":1450897,"prompt":"def f_1450897():\n\treturn ","suffix":"","canonical_solution":"\"\"\"\"\"\".join([x for x in 'aas30dsa20' if x.isdigit()])","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == \"3020\"\n"],"entry_point":"f_1450897","intent":"get digits only from a string `aas30dsa20` using lambda function","library":[],"docs":[]}
{"task_id":14435268,"prompt":"def f_14435268(soup):\n\treturn ","suffix":"","canonical_solution":"soup.find('name').string","test_start":"\nfrom bs4 import BeautifulSoup\n\ndef check(candidate):","test":["\n    content = \"<contact><name>LastName<\/name><lastName>FirstName<\/lastName><phone>+90 333 12345<\/phone><\/contact>\"\n    soup = BeautifulSoup(content)\n    assert candidate(soup) == \"LastName\"\n","\n    content = \"<name>hello world!<\/name>\"\n    soup = BeautifulSoup(content)\n    assert candidate(soup) == \"hello world!\"\n"],"entry_point":"f_14435268","intent":"access a tag called \"name\" in beautifulsoup `soup`","library":["bs4"],"docs":[]}
{"task_id":20180210,"prompt":"def f_20180210(A, B):\n\treturn ","suffix":"","canonical_solution":"np.concatenate((A, B))","test_start":"\nimport numpy as np \n\ndef check(candidate):","test":["\n    A = np.array([1,2])\n    B = np.array([3,4])\n    assert np.allclose(candidate(A, B), np.array([1,2,3,4]))\n","\n    A = np.array([[1,2]])\n    B = np.array([[3,4]])\n    assert np.allclose(candidate(A, B), np.array([[1,2],[3,4]]))\n","\n    A = np.array([[1],[2]])\n    B = np.array([[3],[4]])\n    assert np.allclose(candidate(A, B), np.array([[1],[2],[3],[4]]))\n","\n    a = np.array([[1, 3, 4], [4, 5, 6], [6, 0, -1]])\n    b = np.array([[5, 6, 1], [0, 2, -1], [9, 4, 1]])\n    expected = np.array([[ 1, 3, 4], [ 4, 5, 6],\n        [ 6, 0, -1], [ 5, 6, 1], [ 0, 2, -1], [ 9, 4, 1]])\n    assert np.array_equal(candidate(a, b), expected)\n"],"entry_point":"f_20180210","intent":"Create new matrix object  by concatenating data from matrix A and matrix B","library":["numpy"],"docs":[{"text":"concat_matrix=b'cm'[source]","title":"matplotlib.backend_pdf_api#matplotlib.backends.backend_pdf.Op.concat_matrix"},{"text":"pandas.api.extensions.ExtensionArray._concat_same_type   classmethodExtensionArray._concat_same_type(to_concat)[source]\n \nConcatenate multiple array of this dtype.  Parameters \n \nto_concat:sequence of this type\n\n  Returns \n ExtensionArray","title":"pandas.reference.api.pandas.api.extensions.extensionarray._concat_same_type"},{"text":"pandas.DataFrame.join   DataFrame.join(other, on=None, how='left', lsuffix='', rsuffix='', sort=False)[source]\n \nJoin columns of another DataFrame. Join columns with other DataFrame either on index or on a key column. Efficiently join multiple DataFrame objects by index at once by passing a list.  Parameters \n \nother:DataFrame, Series, or list of DataFrame\n\n\nIndex should be similar to one of the columns in this one. If a Series is passed, its name attribute must be set, and that will be used as the column name in the resulting joined DataFrame.  \non:str, list of str, or array-like, optional\n\n\nColumn or index level name(s) in the caller to join on the index in other, otherwise joins index-on-index. If multiple values given, the other DataFrame must have a MultiIndex. Can pass an array as the join key if it is not already contained in the calling DataFrame. Like an Excel VLOOKUP operation.  \nhow:{\u2018left\u2019, \u2018right\u2019, \u2018outer\u2019, \u2018inner\u2019}, default \u2018left\u2019\n\n\nHow to handle the operation of the two objects.  left: use calling frame\u2019s index (or column if on is specified) right: use other\u2019s index. outer: form union of calling frame\u2019s index (or column if on is specified) with other\u2019s index, and sort it. lexicographically. inner: form intersection of calling frame\u2019s index (or column if on is specified) with other\u2019s index, preserving the order of the calling\u2019s one. \ncross: creates the cartesian product from both frames, preserves the order of the left keys.  New in version 1.2.0.     \nlsuffix:str, default \u2018\u2019\n\n\nSuffix to use from left frame\u2019s overlapping columns.  \nrsuffix:str, default \u2018\u2019\n\n\nSuffix to use from right frame\u2019s overlapping columns.  \nsort:bool, default False\n\n\nOrder result DataFrame lexicographically by the join key. If False, the order of the join key depends on the join type (how keyword).    Returns \n DataFrame\n\nA dataframe containing columns from both the caller and other.      See also  DataFrame.merge\n\nFor column(s)-on-column(s) operations.    Notes Parameters on, lsuffix, and rsuffix are not supported when passing a list of DataFrame objects. Support for specifying index levels as the on parameter was added in version 0.23.0. Examples \n>>> df = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3', 'K4', 'K5'],\n...                    'A': ['A0', 'A1', 'A2', 'A3', 'A4', 'A5']})\n  \n>>> df\n  key   A\n0  K0  A0\n1  K1  A1\n2  K2  A2\n3  K3  A3\n4  K4  A4\n5  K5  A5\n  \n>>> other = pd.DataFrame({'key': ['K0', 'K1', 'K2'],\n...                       'B': ['B0', 'B1', 'B2']})\n  \n>>> other\n  key   B\n0  K0  B0\n1  K1  B1\n2  K2  B2\n  Join DataFrames using their indexes. \n>>> df.join(other, lsuffix='_caller', rsuffix='_other')\n  key_caller   A key_other    B\n0         K0  A0        K0   B0\n1         K1  A1        K1   B1\n2         K2  A2        K2   B2\n3         K3  A3       NaN  NaN\n4         K4  A4       NaN  NaN\n5         K5  A5       NaN  NaN\n  If we want to join using the key columns, we need to set key to be the index in both df and other. The joined DataFrame will have key as its index. \n>>> df.set_index('key').join(other.set_index('key'))\n      A    B\nkey\nK0   A0   B0\nK1   A1   B1\nK2   A2   B2\nK3   A3  NaN\nK4   A4  NaN\nK5   A5  NaN\n  Another option to join using the key columns is to use the on parameter. DataFrame.join always uses other\u2019s index but we can use any column in df. This method preserves the original DataFrame\u2019s index in the result. \n>>> df.join(other.set_index('key'), on='key')\n  key   A    B\n0  K0  A0   B0\n1  K1  A1   B1\n2  K2  A2   B2\n3  K3  A3  NaN\n4  K4  A4  NaN\n5  K5  A5  NaN\n  Using non-unique key values shows how they are matched. \n>>> df = pd.DataFrame({'key': ['K0', 'K1', 'K1', 'K3', 'K0', 'K1'],\n...                    'A': ['A0', 'A1', 'A2', 'A3', 'A4', 'A5']})\n  \n>>> df\n  key   A\n0  K0  A0\n1  K1  A1\n2  K1  A2\n3  K3  A3\n4  K0  A4\n5  K1  A5\n  \n>>> df.join(other.set_index('key'), on='key')\n  key   A    B\n0  K0  A0   B0\n1  K1  A1   B1\n2  K1  A2   B1\n3  K3  A3  NaN\n4  K0  A4   B0\n5  K1  A5   B1","title":"pandas.reference.api.pandas.dataframe.join"},{"text":"operator.concat(a, b)  \noperator.__concat__(a, b)  \nReturn a + b for a and b sequences.","title":"python.library.operator#operator.__concat__"},{"text":"classBarAB(widthA=1.0, angleA=0, widthB=1.0, angleB=0)[source]\n \nBases: matplotlib.patches.ArrowStyle._Curve An arrow with vertical bars | at both ends.  Parameters \n \nwidthA, widthBfloat, default: 1.0\n\n\nWidth of the bracket.  \nangleA, angleBfloat, default: 0 degrees\n\n\nOrientation of the bracket, as a counterclockwise angle. 0 degrees means perpendicular to the line.       arrow='|-|'","title":"matplotlib._as_gen.matplotlib.patches.arrowstyle#matplotlib.patches.ArrowStyle.BarAB"},{"text":"operator.concat(a, b)  \noperator.__concat__(a, b)  \nReturn a + b for a and b sequences.","title":"python.library.operator#operator.concat"},{"text":"tf.keras.layers.AveragePooling3D     View source on GitHub    Average pooling operation for 3D data (spatial or spatio-temporal). Inherits From: Layer, Module  View aliases  Main aliases \ntf.keras.layers.AvgPool3D Compat aliases for migration See Migration guide for more details. tf.compat.v1.keras.layers.AveragePooling3D, tf.compat.v1.keras.layers.AvgPool3D  \ntf.keras.layers.AveragePooling3D(\n    pool_size=(2, 2, 2), strides=None, padding='valid', data_format=None,\n    **kwargs\n)\n\n \n\n\n Arguments\n  pool_size   tuple of 3 integers, factors by which to downscale (dim1, dim2, dim3). (2, 2, 2) will halve the size of the 3D input in each dimension.  \n  strides   tuple of 3 integers, or None. Strides values.  \n  padding   One of \"valid\" or \"same\" (case-insensitive). \"valid\" means no padding. \"same\" results in padding evenly to the left\/right or up\/down of the input such that output has the same height\/width dimension as the input.  \n  data_format   A string, one of channels_last (default) or channels_first. The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) while channels_first corresponds to inputs with shape (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3). It defaults to the image_data_format value found in your Keras config file at ~\/.keras\/keras.json. If you never set it, then it will be \"channels_last\".    Input shape:  If data_format='channels_last': 5D tensor with shape: (batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)\n If data_format='channels_first': 5D tensor with shape: (batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)\n  Output shape:  If data_format='channels_last': 5D tensor with shape: (batch_size, pooled_dim1, pooled_dim2, pooled_dim3, channels)\n If data_format='channels_first': 5D tensor with shape: (batch_size, channels, pooled_dim1, pooled_dim2, pooled_dim3)","title":"tensorflow.keras.layers.averagepooling3d"},{"text":"tf.compat.v1.layers.AveragePooling3D Average pooling layer for 3D inputs (e.g. volumes). Inherits From: AveragePooling3D, Layer, Layer, Module \ntf.compat.v1.layers.AveragePooling3D(\n    pool_size, strides, padding='valid',\n    data_format='channels_last', name=None, **kwargs\n)\n\n \n\n\n Arguments\n  pool_size   An integer or tuple\/list of 3 integers: (pool_depth, pool_height, pool_width) specifying the size of the pooling window. Can be a single integer to specify the same value for all spatial dimensions.  \n  strides   An integer or tuple\/list of 3 integers, specifying the strides of the pooling operation. Can be a single integer to specify the same value for all spatial dimensions.  \n  padding   A string. The padding method, either 'valid' or 'same'. Case-insensitive.  \n  data_format   A string. The ordering of the dimensions in the inputs. channels_last (default) and channels_first are supported. channels_last corresponds to inputs with shape (batch, depth, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, depth, height, width).  \n  name   A string, the name of the layer.   \n \n\n\n Attributes\n  graph  \n \n  scope_name","title":"tensorflow.compat.v1.layers.averagepooling3d"},{"text":"classBracketAB(widthA=1.0, lengthA=0.2, angleA=0, widthB=1.0, lengthB=0.2, angleB=0)[source]\n \nBases: matplotlib.patches.ArrowStyle._Curve An arrow with outward square brackets at both ends.  Parameters \n \nwidthA, widthBfloat, default: 1.0\n\n\nWidth of the bracket.  \nlengthA, lengthBfloat, default: 0.2\n\n\nLength of the bracket.  \nangleA, angleBfloat, default: 0 degrees\n\n\nOrientation of the bracket, as a counterclockwise angle. 0 degrees means perpendicular to the line.       arrow=']-['","title":"matplotlib._as_gen.matplotlib.patches.arrowstyle#matplotlib.patches.ArrowStyle.BracketAB"},{"text":"broadcast_to(shape) \u2192 Tensor  \nSee torch.broadcast_to().","title":"torch.tensors#torch.Tensor.broadcast_to"}]}
{"task_id":20180210,"prompt":"def f_20180210(A, B):\n\treturn ","suffix":"","canonical_solution":"np.vstack((A, B))","test_start":"\nimport numpy as np \n\ndef check(candidate):","test":["\n    A = np.array([1,2])\n    B = np.array([3,4])\n    assert np.allclose(candidate(A, B), np.array([[1,2],[3,4]]))\n","\n    A = np.array([[1,2]])\n    B = np.array([[3,4]])\n    assert np.allclose(candidate(A, B), np.array([[1,2],[3,4]]))\n","\n    A = np.array([[1],[2]])\n    B = np.array([[3],[4]])\n    assert np.allclose(candidate(A, B), np.array([[1],[2],[3],[4]]))\n","\n    a = np.array([[1, 3, 4], [4, 5, 6], [6, 0, -1]])\n    b = np.array([[5, 6, 1], [0, 2, -1], [9, 4, 1]])\n    expected = np.array([[ 1, 3, 4], [ 4, 5, 6],\n        [ 6, 0, -1], [ 5, 6, 1], [ 0, 2, -1], [ 9, 4, 1]])\n    assert np.array_equal(candidate(a, b), expected)\n"],"entry_point":"f_20180210","intent":"concat two matrices `A` and `B` in numpy","library":["numpy"],"docs":[{"text":"numpy.c_   numpy.c_ = <numpy.lib.index_tricks.CClass object>\n \nTranslates slice objects to concatenation along the second axis. This is short-hand for np.r_['-1,2,0', index expression], which is useful because of its common occurrence. In particular, arrays will be stacked along their last axis after being upgraded to at least 2-D with 1\u2019s post-pended to the shape (column vectors made out of 1-D arrays).  See also  column_stack\n\nStack 1-D arrays as columns into a 2-D array.  r_\n\nFor more detailed documentation.    Examples >>> np.c_[np.array([1,2,3]), np.array([4,5,6])]\narray([[1, 4],\n       [2, 5],\n       [3, 6]])\n>>> np.c_[np.array([[1,2,3]]), 0, 0, np.array([[4,5,6]])]\narray([[1, 2, 3, ..., 4, 5, 6]])","title":"numpy.reference.generated.numpy.c_"},{"text":"numpy.ma.append   ma.append(a, b, axis=None)[source]\n \nAppend values to the end of an array.  New in version 1.9.0.   Parameters \n \naarray_like\n\n\nValues are appended to a copy of this array.  \nbarray_like\n\n\nThese values are appended to a copy of a. It must be of the correct shape (the same shape as a, excluding axis). If axis is not specified, b can be any shape and will be flattened before use.  \naxisint, optional\n\n\nThe axis along which v are appended. If axis is not given, both a and b are flattened before use.    Returns \n \nappendMaskedArray\n\n\nA copy of a with b appended to axis. Note that append does not occur in-place: a new array is allocated and filled. If axis is None, the result is a flattened array.      See also  numpy.append\n\nEquivalent function in the top-level NumPy module.    Examples >>> import numpy.ma as ma\n>>> a = ma.masked_values([1, 2, 3], 2)\n>>> b = ma.masked_values([[4, 5, 6], [7, 8, 9]], 7)\n>>> ma.append(a, b)\nmasked_array(data=[1, --, 3, 4, 5, 6, --, 8, 9],\n             mask=[False,  True, False, False, False, False,  True, False,\n                   False],\n       fill_value=999999)","title":"numpy.reference.generated.numpy.ma.append"},{"text":"numpy.concatenate   numpy.concatenate((a1, a2, ...), axis=0, out=None, dtype=None, casting=\"same_kind\")\n \nJoin a sequence of arrays along an existing axis.  Parameters \n \na1, a2, \u2026sequence of array_like\n\n\nThe arrays must have the same shape, except in the dimension corresponding to axis (the first, by default).  \naxisint, optional\n\n\nThe axis along which the arrays will be joined. If axis is None, arrays are flattened before use. Default is 0.  \noutndarray, optional\n\n\nIf provided, the destination to place the result. The shape must be correct, matching that of what concatenate would have returned if no out argument were specified.  \ndtypestr or dtype\n\n\nIf provided, the destination array will have this dtype. Cannot be provided together with out.  New in version 1.20.0.   \ncasting{\u2018no\u2019, \u2018equiv\u2019, \u2018safe\u2019, \u2018same_kind\u2019, \u2018unsafe\u2019}, optional\n\n\nControls what kind of data casting may occur. Defaults to \u2018same_kind\u2019.  New in version 1.20.0.     Returns \n \nresndarray\n\n\nThe concatenated array.      See also  ma.concatenate\n\nConcatenate function that preserves input masks.  array_split\n\nSplit an array into multiple sub-arrays of equal or near-equal size.  split\n\nSplit array into a list of multiple sub-arrays of equal size.  hsplit\n\nSplit array into multiple sub-arrays horizontally (column wise).  vsplit\n\nSplit array into multiple sub-arrays vertically (row wise).  dsplit\n\nSplit array into multiple sub-arrays along the 3rd axis (depth).  stack\n\nStack a sequence of arrays along a new axis.  block\n\nAssemble arrays from blocks.  hstack\n\nStack arrays in sequence horizontally (column wise).  vstack\n\nStack arrays in sequence vertically (row wise).  dstack\n\nStack arrays in sequence depth wise (along third dimension).  column_stack\n\nStack 1-D arrays as columns into a 2-D array.    Notes When one or more of the arrays to be concatenated is a MaskedArray, this function will return a MaskedArray object instead of an ndarray, but the input masks are not preserved. In cases where a MaskedArray is expected as input, use the ma.concatenate function from the masked array module instead. Examples >>> a = np.array([[1, 2], [3, 4]])\n>>> b = np.array([[5, 6]])\n>>> np.concatenate((a, b), axis=0)\narray([[1, 2],\n       [3, 4],\n       [5, 6]])\n>>> np.concatenate((a, b.T), axis=1)\narray([[1, 2, 5],\n       [3, 4, 6]])\n>>> np.concatenate((a, b), axis=None)\narray([1, 2, 3, 4, 5, 6])\n This function will not preserve masking of MaskedArray inputs. >>> a = np.ma.arange(3)\n>>> a[1] = np.ma.masked\n>>> b = np.arange(2, 5)\n>>> a\nmasked_array(data=[0, --, 2],\n             mask=[False,  True, False],\n       fill_value=999999)\n>>> b\narray([2, 3, 4])\n>>> np.concatenate([a, b])\nmasked_array(data=[0, 1, 2, 2, 3, 4],\n             mask=False,\n       fill_value=999999)\n>>> np.ma.concatenate([a, b])\nmasked_array(data=[0, --, 2, 2, 3, 4],\n             mask=[False,  True, False, False, False, False],\n       fill_value=999999)","title":"numpy.reference.generated.numpy.concatenate"},{"text":"numpy.ufunc.outer method   ufunc.outer(A, B, \/, **kwargs)\n \nApply the ufunc op to all pairs (a, b) with a in A and b in B. Let M = A.ndim, N = B.ndim. Then the result, C, of op.outer(A, B) is an array of dimension M + N such that:  \\[C[i_0, ..., i_{M-1}, j_0, ..., j_{N-1}] = op(A[i_0, ..., i_{M-1}], B[j_0, ..., j_{N-1}])\\] For A and B one-dimensional, this is equivalent to: r = empty(len(A),len(B))\nfor i in range(len(A)):\n    for j in range(len(B)):\n        r[i,j] = op(A[i], B[j])  # op = ufunc in question\n  Parameters \n \nAarray_like\n\n\nFirst array  \nBarray_like\n\n\nSecond array  \nkwargsany\n\n\nArguments to pass on to the ufunc. Typically dtype or out. See ufunc for a comprehensive overview of all available arguments.    Returns \n \nrndarray\n\n\nOutput array      See also  numpy.outer\n\nA less powerful version of np.multiply.outer that ravels all inputs to 1D. This exists primarily for compatibility with old code.  tensordot\n\nnp.tensordot(a, b, axes=((), ())) and np.multiply.outer(a, b) behave same for all dimensions of a and b.    Examples >>> np.multiply.outer([1, 2, 3], [4, 5, 6])\narray([[ 4,  5,  6],\n       [ 8, 10, 12],\n       [12, 15, 18]])\n A multi-dimensional example: >>> A = np.array([[1, 2, 3], [4, 5, 6]])\n>>> A.shape\n(2, 3)\n>>> B = np.array([[1, 2, 3, 4]])\n>>> B.shape\n(1, 4)\n>>> C = np.multiply.outer(A, B)\n>>> C.shape; C\n(2, 3, 1, 4)\narray([[[[ 1,  2,  3,  4]],\n        [[ 2,  4,  6,  8]],\n        [[ 3,  6,  9, 12]]],\n       [[[ 4,  8, 12, 16]],\n        [[ 5, 10, 15, 20]],\n        [[ 6, 12, 18, 24]]]])","title":"numpy.reference.generated.numpy.ufunc.outer"},{"text":"numpy.ma.concatenate   ma.concatenate(arrays, axis=0)[source]\n \nConcatenate a sequence of arrays along the given axis.  Parameters \n \narrayssequence of array_like\n\n\nThe arrays must have the same shape, except in the dimension corresponding to axis (the first, by default).  \naxisint, optional\n\n\nThe axis along which the arrays will be joined. Default is 0.    Returns \n \nresultMaskedArray\n\n\nThe concatenated array with any masked entries preserved.      See also  numpy.concatenate\n\nEquivalent function in the top-level NumPy module.    Examples >>> import numpy.ma as ma\n>>> a = ma.arange(3)\n>>> a[1] = ma.masked\n>>> b = ma.arange(2, 5)\n>>> a\nmasked_array(data=[0, --, 2],\n             mask=[False,  True, False],\n       fill_value=999999)\n>>> b\nmasked_array(data=[2, 3, 4],\n             mask=False,\n       fill_value=999999)\n>>> ma.concatenate([a, b])\nmasked_array(data=[0, --, 2, 2, 3, 4],\n             mask=[False,  True, False, False, False, False],\n       fill_value=999999)","title":"numpy.reference.generated.numpy.ma.concatenate"},{"text":"tf.experimental.numpy.concatenate TensorFlow variant of NumPy's concatenate. \ntf.experimental.numpy.concatenate(\n    arys, axis=0\n)\n See the NumPy documentation for numpy.concatenate.","title":"tensorflow.experimental.numpy.concatenate"},{"text":"numpy.ma.column_stack   ma.column_stack(*args, **kwargs) = <numpy.ma.extras._fromnxfunction_seq object>\n \nStack 1-D arrays as columns into a 2-D array. Take a sequence of 1-D arrays and stack them as columns to make a single 2-D array. 2-D arrays are stacked as-is, just like with hstack. 1-D arrays are turned into 2-D columns first.  Parameters \n \ntupsequence of 1-D or 2-D arrays.\n\n\nArrays to stack. All of them must have the same first dimension.    Returns \n \nstacked2-D array\n\n\nThe array formed by stacking the given arrays.      See also  \nstack, hstack, vstack, concatenate\n\n  Notes The function is applied to both the _data and the _mask, if any. Examples >>> a = np.array((1,2,3))\n>>> b = np.array((2,3,4))\n>>> np.column_stack((a,b))\narray([[1, 2],\n       [2, 3],\n       [3, 4]])","title":"numpy.reference.generated.numpy.ma.column_stack"},{"text":"numpy.column_stack   numpy.column_stack(tup)[source]\n \nStack 1-D arrays as columns into a 2-D array. Take a sequence of 1-D arrays and stack them as columns to make a single 2-D array. 2-D arrays are stacked as-is, just like with hstack. 1-D arrays are turned into 2-D columns first.  Parameters \n \ntupsequence of 1-D or 2-D arrays.\n\n\nArrays to stack. All of them must have the same first dimension.    Returns \n \nstacked2-D array\n\n\nThe array formed by stacking the given arrays.      See also  \nstack, hstack, vstack, concatenate\n\n  Examples >>> a = np.array((1,2,3))\n>>> b = np.array((2,3,4))\n>>> np.column_stack((a,b))\narray([[1, 2],\n       [2, 3],\n       [3, 4]])","title":"numpy.reference.generated.numpy.column_stack"},{"text":"pandas.api.extensions.ExtensionArray._concat_same_type   classmethodExtensionArray._concat_same_type(to_concat)[source]\n \nConcatenate multiple array of this dtype.  Parameters \n \nto_concat:sequence of this type\n\n  Returns \n ExtensionArray","title":"pandas.reference.api.pandas.api.extensions.extensionarray._concat_same_type"},{"text":"numpy.char.add   char.add(x1, x2)[source]\n \nReturn element-wise string concatenation for two arrays of str or unicode. Arrays x1 and x2 must have the same shape.  Parameters \n \nx1array_like of str or unicode\n\n\nInput array.  \nx2array_like of str or unicode\n\n\nInput array.    Returns \n \naddndarray\n\n\nOutput array of string_ or unicode_, depending on input types of the same shape as x1 and x2.","title":"numpy.reference.generated.numpy.char.add"}]}
{"task_id":2011048,"prompt":"def f_2011048(filepath):\n\treturn ","suffix":"","canonical_solution":"os.stat(filepath).st_size","test_start":"\nimport os\n\ndef check(candidate):","test":["\n    with open(\"tmp.txt\", 'w') as fw: fw.write(\"hello world!\")\n    assert candidate(\"tmp.txt\") == 12\n","\n    with open(\"tmp.txt\", 'w') as fw: fw.write(\"\")\n    assert candidate(\"tmp.txt\") == 0\n","\n    with open(\"tmp.txt\", 'w') as fw: fw.write('\\n')\n    assert candidate(\"tmp.txt\") == 1\n","\n    filename = 'o.txt'\n    with open (filename, 'w') as f:\n        f.write('a')\n    assert candidate(filename) == 1\n"],"entry_point":"f_2011048","intent":"Get the characters count in a file `filepath`","library":["os"],"docs":[{"text":"fileinput.filelineno()  \nReturn the line number in the current file. Before the first line has been read, returns 0. After the last line of the last file has been read, returns the line number of that line within the file.","title":"python.library.fileinput#fileinput.filelineno"},{"text":"lines  \nHeight of the terminal window in characters.","title":"python.library.os#os.terminal_size.lines"},{"text":"tell()  \nReturn the current stream position as an opaque number. The number does not usually represent a number of bytes in the underlying binary storage.","title":"python.library.io#io.TextIOBase.tell"},{"text":"fileinput.lineno()  \nReturn the cumulative line number of the line that has just been read. Before the first line has been read, returns 0. After the last line of the last file has been read, returns the line number of that line.","title":"python.library.fileinput#fileinput.lineno"},{"text":"readfp(fp, filename=None)  \n Deprecated since version 3.2: Use read_file() instead.   Changed in version 3.2: readfp() now iterates on fp instead of calling fp.readline().  For existing code calling readfp() with arguments which don\u2019t support iteration, the following generator may be used as a wrapper around the file-like object: def readline_generator(fp):\n    line = fp.readline()\n    while line:\n        yield line\n        line = fp.readline()\n Instead of parser.readfp(fp) use parser.read_file(readline_generator(fp)).","title":"python.library.configparser#configparser.ConfigParser.readfp"},{"text":"charset()  \nReturn the encoding of the message catalog file.","title":"python.library.gettext#gettext.NullTranslations.charset"},{"text":"matplotlib.cbook.file_requires_unicode(x)[source]\n \nReturn whether the given writable file-like object requires Unicode to be written to it.","title":"matplotlib.cbook_api#matplotlib.cbook.file_requires_unicode"},{"text":"classmatplotlib.afm.CharMetrics(width, name, bbox)[source]\n \nBases: tuple Represents the character metrics of a single character. Notes The fields do currently only describe a subset of character metrics information defined in the AFM standard. Create new instance of CharMetrics(width, name, bbox)   bbox\n \nThe bbox of the character (B) as a tuple (llx, lly, urx, ury). \n   name\n \nThe character name (N). \n   width\n \nThe character width (WX).","title":"matplotlib.afm_api#matplotlib.afm.CharMetrics"},{"text":"clone(fp)  \nReturn an independent clone of this BytesGenerator instance with the exact same option settings, and fp as the new outfp.","title":"python.library.email.generator#email.generator.BytesGenerator.clone"},{"text":"output_charset()  \nReturn the encoding used to return translated messages in lgettext() and lngettext().  Deprecated since version 3.8, will be removed in version 3.10.","title":"python.library.gettext#gettext.NullTranslations.output_charset"}]}
{"task_id":2600191,"prompt":"def f_2600191(l):\n\treturn ","suffix":"","canonical_solution":"l.count('a')","test_start":"\ndef check(candidate):","test":["\n    assert candidate(\"123456asf\") == 1\n","\n    assert candidate(\"123456gyjnccfgsf\") == 0\n","\n    assert candidate(\"aA\"*10) == 10\n"],"entry_point":"f_2600191","intent":"count the occurrences of item \"a\" in list `l`","library":[],"docs":[]}
{"task_id":2600191,"prompt":"def f_2600191(l):\n\treturn ","suffix":"","canonical_solution":"Counter(l)","test_start":"\nfrom collections import Counter \n\ndef check(candidate):","test":["\n    assert dict(candidate(\"123456asf\")) == {'1': 1, '2': 1, '3': 1, '4': 1, '5': 1, '6': 1, 'a': 1, 's': 1, 'f': 1}\n","\n    assert candidate(\"123456gyjnccfgsf\") == {'1': 1,'2': 1,'3': 1,'4': 1,'5': 1,'6': 1,'g': 2,'y': 1,'j': 1,'n': 1,'c': 2,'f': 2,'s': 1}\n","\n    assert candidate(\"aA\"*10) == {'a': 10, 'A': 10}\n","\n    y = candidate([1, 6])\n    assert y[1] == 1\n    assert y[6] == 1\n"],"entry_point":"f_2600191","intent":"count the occurrences of items in list `l`","library":["collections"],"docs":[{"text":"count(value)  \nReturns the number of occurrences of value.","title":"python.library.multiprocessing.shared_memory#multiprocessing.shared_memory.ShareableList.count"},{"text":"operator.countOf(a, b)  \nReturn the number of occurrences of b in a.","title":"python.library.operator#operator.countOf"},{"text":"tf.raw_ops.BatchCholeskyGrad  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.raw_ops.BatchCholeskyGrad  \ntf.raw_ops.BatchCholeskyGrad(\n    l, grad, name=None\n)\n\n \n\n\n Args\n  l   A Tensor. Must be one of the following types: float32, float64.  \n  grad   A Tensor. Must have the same type as l.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor. Has the same type as l.","title":"tensorflow.raw_ops.batchcholeskygrad"},{"text":"pandas.Series.str.count   Series.str.count(pat, flags=0)[source]\n \nCount occurrences of pattern in each string of the Series\/Index. This function is used to count the number of times a particular regex pattern is repeated in each of the string elements of the Series.  Parameters \n \npat:str\n\n\nValid regular expression.  \nflags:int, default 0, meaning no flags\n\n\nFlags for the re module. For a complete list, see here.  **kwargs\n\nFor compatibility with other string methods. Not used.    Returns \n Series or Index\n\nSame type as the calling object containing the integer counts.      See also  re\n\nStandard library module for regular expressions.  str.count\n\nStandard library version, without regular expression support.    Notes Some characters need to be escaped when passing in pat. eg. '$' has a special meaning in regex and must be escaped when finding this literal character. Examples \n>>> s = pd.Series(['A', 'B', 'Aaba', 'Baca', np.nan, 'CABA', 'cat'])\n>>> s.str.count('a')\n0    0.0\n1    0.0\n2    2.0\n3    2.0\n4    NaN\n5    0.0\n6    1.0\ndtype: float64\n  Escape '$' to find the literal dollar sign. \n>>> s = pd.Series(['$', 'B', 'Aab$', '$$ca', 'C$B$', 'cat'])\n>>> s.str.count('\\\\$')\n0    1\n1    0\n2    1\n3    2\n4    2\n5    0\ndtype: int64\n  This is also available on Index \n>>> pd.Index(['A', 'A', 'Aaba', 'cat']).str.count('a')\nInt64Index([0, 0, 2, 1], dtype='int64')","title":"pandas.reference.api.pandas.series.str.count"},{"text":"torch.cuda.device_count() [source]\n \nReturns the number of GPUs available.","title":"torch.cuda#torch.cuda.device_count"},{"text":"class sklearn.linear_model.TheilSenRegressor(*, fit_intercept=True, copy_X=True, max_subpopulation=10000.0, n_subsamples=None, max_iter=300, tol=0.001, random_state=None, n_jobs=None, verbose=False) [source]\n \nTheil-Sen Estimator: robust multivariate regression model. The algorithm calculates least square solutions on subsets with size n_subsamples of the samples in X. Any value of n_subsamples between the number of features and samples leads to an estimator with a compromise between robustness and efficiency. Since the number of least square solutions is \u201cn_samples choose n_subsamples\u201d, it can be extremely large and can therefore be limited with max_subpopulation. If this limit is reached, the subsets are chosen randomly. In a final step, the spatial median (or L1 median) is calculated of all least square solutions. Read more in the User Guide.  Parameters \n \nfit_interceptbool, default=True \n\nWhether to calculate the intercept for this model. If set to false, no intercept will be used in calculations.  \ncopy_Xbool, default=True \n\nIf True, X will be copied; else, it may be overwritten.  \nmax_subpopulationint, default=1e4 \n\nInstead of computing with a set of cardinality \u2018n choose k\u2019, where n is the number of samples and k is the number of subsamples (at least number of features), consider only a stochastic subpopulation of a given maximal size if \u2018n choose k\u2019 is larger than max_subpopulation. For other than small problem sizes this parameter will determine memory usage and runtime if n_subsamples is not changed.  \nn_subsamplesint, default=None \n\nNumber of samples to calculate the parameters. This is at least the number of features (plus 1 if fit_intercept=True) and the number of samples as a maximum. A lower number leads to a higher breakdown point and a low efficiency while a high number leads to a low breakdown point and a high efficiency. If None, take the minimum number of subsamples leading to maximal robustness. If n_subsamples is set to n_samples, Theil-Sen is identical to least squares.  \nmax_iterint, default=300 \n\nMaximum number of iterations for the calculation of spatial median.  \ntolfloat, default=1.e-3 \n\nTolerance when calculating spatial median.  \nrandom_stateint, RandomState instance or None, default=None \n\nA random number generator instance to define the state of the random permutations generator. Pass an int for reproducible output across multiple function calls. See Glossary  \nn_jobsint, default=None \n\nNumber of CPUs to use during the cross validation. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details.  \nverbosebool, default=False \n\nVerbose mode when fitting the model.    Attributes \n \ncoef_ndarray of shape (n_features,) \n\nCoefficients of the regression model (median of distribution).  \nintercept_float \n\nEstimated intercept of regression model.  \nbreakdown_float \n\nApproximated breakdown point.  \nn_iter_int \n\nNumber of iterations needed for the spatial median.  \nn_subpopulation_int \n\nNumber of combinations taken into account from \u2018n choose k\u2019, where n is the number of samples and k is the number of subsamples.     References  Theil-Sen Estimators in a Multiple Linear Regression Model, 2009 Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang http:\/\/home.olemiss.edu\/~xdang\/papers\/MTSE.pdf\n  Examples >>> from sklearn.linear_model import TheilSenRegressor\n>>> from sklearn.datasets import make_regression\n>>> X, y = make_regression(\n...     n_samples=200, n_features=2, noise=4.0, random_state=0)\n>>> reg = TheilSenRegressor(random_state=0).fit(X, y)\n>>> reg.score(X, y)\n0.9884...\n>>> reg.predict(X[:1,])\narray([-31.5871...])\n Methods  \nfit(X, y) Fit linear model.  \nget_params([deep]) Get parameters for this estimator.  \npredict(X) Predict using the linear model.  \nscore(X, y[, sample_weight]) Return the coefficient of determination \\(R^2\\) of the prediction.  \nset_params(**params) Set the parameters of this estimator.    \nfit(X, y) [source]\n \nFit linear model.  Parameters \n \nXndarray of shape (n_samples, n_features) \n\nTraining data.  \nyndarray of shape (n_samples,) \n\nTarget values.    Returns \n \nselfreturns an instance of self. \n   \n  \nget_params(deep=True) [source]\n \nGet parameters for this estimator.  Parameters \n \ndeepbool, default=True \n\nIf True, will return the parameters for this estimator and contained subobjects that are estimators.    Returns \n \nparamsdict \n\nParameter names mapped to their values.     \n  \npredict(X) [source]\n \nPredict using the linear model.  Parameters \n \nXarray-like or sparse matrix, shape (n_samples, n_features) \n\nSamples.    Returns \n \nCarray, shape (n_samples,) \n\nReturns predicted values.     \n  \nscore(X, y, sample_weight=None) [source]\n \nReturn the coefficient of determination \\(R^2\\) of the prediction. The coefficient \\(R^2\\) is defined as \\((1 - \\frac{u}{v})\\), where \\(u\\) is the residual sum of squares ((y_true - y_pred)\n** 2).sum() and \\(v\\) is the total sum of squares ((y_true -\ny_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a \\(R^2\\) score of 0.0.  Parameters \n \nXarray-like of shape (n_samples, n_features) \n\nTest samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead with shape (n_samples, n_samples_fitted), where n_samples_fitted is the number of samples used in the fitting for the estimator.  \nyarray-like of shape (n_samples,) or (n_samples, n_outputs) \n\nTrue values for X.  \nsample_weightarray-like of shape (n_samples,), default=None \n\nSample weights.    Returns \n \nscorefloat \n\n\\(R^2\\) of self.predict(X) wrt. y.     Notes The \\(R^2\\) score used when calling score on a regressor uses multioutput='uniform_average' from version 0.23 to keep consistent with default value of r2_score. This influences the score method of all the multioutput regressors (except for MultiOutputRegressor). \n  \nset_params(**params) [source]\n \nSet the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as Pipeline). The latter have parameters of the form <component>__<parameter> so that it\u2019s possible to update each component of a nested object.  Parameters \n \n**paramsdict \n\nEstimator parameters.    Returns \n \nselfestimator instance \n\nEstimator instance.","title":"sklearn.modules.generated.sklearn.linear_model.theilsenregressor#sklearn.linear_model.TheilSenRegressor"},{"text":"sklearn.linear_model.TheilSenRegressor  \nclass sklearn.linear_model.TheilSenRegressor(*, fit_intercept=True, copy_X=True, max_subpopulation=10000.0, n_subsamples=None, max_iter=300, tol=0.001, random_state=None, n_jobs=None, verbose=False) [source]\n \nTheil-Sen Estimator: robust multivariate regression model. The algorithm calculates least square solutions on subsets with size n_subsamples of the samples in X. Any value of n_subsamples between the number of features and samples leads to an estimator with a compromise between robustness and efficiency. Since the number of least square solutions is \u201cn_samples choose n_subsamples\u201d, it can be extremely large and can therefore be limited with max_subpopulation. If this limit is reached, the subsets are chosen randomly. In a final step, the spatial median (or L1 median) is calculated of all least square solutions. Read more in the User Guide.  Parameters \n \nfit_interceptbool, default=True \n\nWhether to calculate the intercept for this model. If set to false, no intercept will be used in calculations.  \ncopy_Xbool, default=True \n\nIf True, X will be copied; else, it may be overwritten.  \nmax_subpopulationint, default=1e4 \n\nInstead of computing with a set of cardinality \u2018n choose k\u2019, where n is the number of samples and k is the number of subsamples (at least number of features), consider only a stochastic subpopulation of a given maximal size if \u2018n choose k\u2019 is larger than max_subpopulation. For other than small problem sizes this parameter will determine memory usage and runtime if n_subsamples is not changed.  \nn_subsamplesint, default=None \n\nNumber of samples to calculate the parameters. This is at least the number of features (plus 1 if fit_intercept=True) and the number of samples as a maximum. A lower number leads to a higher breakdown point and a low efficiency while a high number leads to a low breakdown point and a high efficiency. If None, take the minimum number of subsamples leading to maximal robustness. If n_subsamples is set to n_samples, Theil-Sen is identical to least squares.  \nmax_iterint, default=300 \n\nMaximum number of iterations for the calculation of spatial median.  \ntolfloat, default=1.e-3 \n\nTolerance when calculating spatial median.  \nrandom_stateint, RandomState instance or None, default=None \n\nA random number generator instance to define the state of the random permutations generator. Pass an int for reproducible output across multiple function calls. See Glossary  \nn_jobsint, default=None \n\nNumber of CPUs to use during the cross validation. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details.  \nverbosebool, default=False \n\nVerbose mode when fitting the model.    Attributes \n \ncoef_ndarray of shape (n_features,) \n\nCoefficients of the regression model (median of distribution).  \nintercept_float \n\nEstimated intercept of regression model.  \nbreakdown_float \n\nApproximated breakdown point.  \nn_iter_int \n\nNumber of iterations needed for the spatial median.  \nn_subpopulation_int \n\nNumber of combinations taken into account from \u2018n choose k\u2019, where n is the number of samples and k is the number of subsamples.     References  Theil-Sen Estimators in a Multiple Linear Regression Model, 2009 Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang http:\/\/home.olemiss.edu\/~xdang\/papers\/MTSE.pdf\n  Examples >>> from sklearn.linear_model import TheilSenRegressor\n>>> from sklearn.datasets import make_regression\n>>> X, y = make_regression(\n...     n_samples=200, n_features=2, noise=4.0, random_state=0)\n>>> reg = TheilSenRegressor(random_state=0).fit(X, y)\n>>> reg.score(X, y)\n0.9884...\n>>> reg.predict(X[:1,])\narray([-31.5871...])\n Methods  \nfit(X, y) Fit linear model.  \nget_params([deep]) Get parameters for this estimator.  \npredict(X) Predict using the linear model.  \nscore(X, y[, sample_weight]) Return the coefficient of determination \\(R^2\\) of the prediction.  \nset_params(**params) Set the parameters of this estimator.    \nfit(X, y) [source]\n \nFit linear model.  Parameters \n \nXndarray of shape (n_samples, n_features) \n\nTraining data.  \nyndarray of shape (n_samples,) \n\nTarget values.    Returns \n \nselfreturns an instance of self. \n   \n  \nget_params(deep=True) [source]\n \nGet parameters for this estimator.  Parameters \n \ndeepbool, default=True \n\nIf True, will return the parameters for this estimator and contained subobjects that are estimators.    Returns \n \nparamsdict \n\nParameter names mapped to their values.     \n  \npredict(X) [source]\n \nPredict using the linear model.  Parameters \n \nXarray-like or sparse matrix, shape (n_samples, n_features) \n\nSamples.    Returns \n \nCarray, shape (n_samples,) \n\nReturns predicted values.     \n  \nscore(X, y, sample_weight=None) [source]\n \nReturn the coefficient of determination \\(R^2\\) of the prediction. The coefficient \\(R^2\\) is defined as \\((1 - \\frac{u}{v})\\), where \\(u\\) is the residual sum of squares ((y_true - y_pred)\n** 2).sum() and \\(v\\) is the total sum of squares ((y_true -\ny_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a \\(R^2\\) score of 0.0.  Parameters \n \nXarray-like of shape (n_samples, n_features) \n\nTest samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead with shape (n_samples, n_samples_fitted), where n_samples_fitted is the number of samples used in the fitting for the estimator.  \nyarray-like of shape (n_samples,) or (n_samples, n_outputs) \n\nTrue values for X.  \nsample_weightarray-like of shape (n_samples,), default=None \n\nSample weights.    Returns \n \nscorefloat \n\n\\(R^2\\) of self.predict(X) wrt. y.     Notes The \\(R^2\\) score used when calling score on a regressor uses multioutput='uniform_average' from version 0.23 to keep consistent with default value of r2_score. This influences the score method of all the multioutput regressors (except for MultiOutputRegressor). \n  \nset_params(**params) [source]\n \nSet the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as Pipeline). The latter have parameters of the form <component>__<parameter> so that it\u2019s possible to update each component of a nested object.  Parameters \n \n**paramsdict \n\nEstimator parameters.    Returns \n \nselfestimator instance \n\nEstimator instance.     \n \n Examples using sklearn.linear_model.TheilSenRegressor\n \n  Theil-Sen Regression  \n\n  Robust linear estimator fitting","title":"sklearn.modules.generated.sklearn.linear_model.theilsenregressor"},{"text":"numpy.cross   numpy.cross(a, b, axisa=- 1, axisb=- 1, axisc=- 1, axis=None)[source]\n \nReturn the cross product of two (arrays of) vectors. The cross product of a and b in \\(R^3\\) is a vector perpendicular to both a and b. If a and b are arrays of vectors, the vectors are defined by the last axis of a and b by default, and these axes can have dimensions 2 or 3. Where the dimension of either a or b is 2, the third component of the input vector is assumed to be zero and the cross product calculated accordingly. In cases where both input vectors have dimension 2, the z-component of the cross product is returned.  Parameters \n \naarray_like\n\n\nComponents of the first vector(s).  \nbarray_like\n\n\nComponents of the second vector(s).  \naxisaint, optional\n\n\nAxis of a that defines the vector(s). By default, the last axis.  \naxisbint, optional\n\n\nAxis of b that defines the vector(s). By default, the last axis.  \naxiscint, optional\n\n\nAxis of c containing the cross product vector(s). Ignored if both input vectors have dimension 2, as the return is scalar. By default, the last axis.  \naxisint, optional\n\n\nIf defined, the axis of a, b and c that defines the vector(s) and cross product(s). Overrides axisa, axisb and axisc.    Returns \n \ncndarray\n\n\nVector cross product(s).    Raises \n ValueError\n\nWhen the dimension of the vector(s) in a and\/or b does not equal 2 or 3.      See also  inner\n\nInner product  outer\n\nOuter product.  ix_\n\nConstruct index arrays.    Notes  New in version 1.9.0.  Supports full broadcasting of the inputs. Examples Vector cross-product. >>> x = [1, 2, 3]\n>>> y = [4, 5, 6]\n>>> np.cross(x, y)\narray([-3,  6, -3])\n One vector with dimension 2. >>> x = [1, 2]\n>>> y = [4, 5, 6]\n>>> np.cross(x, y)\narray([12, -6, -3])\n Equivalently: >>> x = [1, 2, 0]\n>>> y = [4, 5, 6]\n>>> np.cross(x, y)\narray([12, -6, -3])\n Both vectors with dimension 2. >>> x = [1,2]\n>>> y = [4,5]\n>>> np.cross(x, y)\narray(-3)\n Multiple vector cross-products. Note that the direction of the cross product vector is defined by the right-hand rule. >>> x = np.array([[1,2,3], [4,5,6]])\n>>> y = np.array([[4,5,6], [1,2,3]])\n>>> np.cross(x, y)\narray([[-3,  6, -3],\n       [ 3, -6,  3]])\n The orientation of c can be changed using the axisc keyword. >>> np.cross(x, y, axisc=0)\narray([[-3,  3],\n       [ 6, -6],\n       [-3,  3]])\n Change the vector definition of x and y using axisa and axisb. >>> x = np.array([[1,2,3], [4,5,6], [7, 8, 9]])\n>>> y = np.array([[7, 8, 9], [4,5,6], [1,2,3]])\n>>> np.cross(x, y)\narray([[ -6,  12,  -6],\n       [  0,   0,   0],\n       [  6, -12,   6]])\n>>> np.cross(x, y, axisa=0, axisb=0)\narray([[-24,  48, -24],\n       [-30,  60, -30],\n       [-36,  72, -36]])","title":"numpy.reference.generated.numpy.cross"},{"text":"sklearn.pipeline.make_pipeline  \nsklearn.pipeline.make_pipeline(*steps, memory=None, verbose=False) [source]\n \nConstruct a Pipeline from the given estimators. This is a shorthand for the Pipeline constructor; it does not require, and does not permit, naming the estimators. Instead, their names will be set to the lowercase of their types automatically.  Parameters \n \n*stepslist of estimators. \n\nmemorystr or object with the joblib.Memory interface, default=None \n\nUsed to cache the fitted transformers of the pipeline. By default, no caching is performed. If a string is given, it is the path to the caching directory. Enabling caching triggers a clone of the transformers before fitting. Therefore, the transformer instance given to the pipeline cannot be inspected directly. Use the attribute named_steps or steps to inspect estimators within the pipeline. Caching the transformers is advantageous when fitting is time consuming.  \nverbosebool, default=False \n\nIf True, the time elapsed while fitting each step will be printed as it is completed.    Returns \n \npPipeline \n    See also  \nPipeline\n\n\nClass for creating a pipeline of transforms with a final estimator.    Examples >>> from sklearn.naive_bayes import GaussianNB\n>>> from sklearn.preprocessing import StandardScaler\n>>> make_pipeline(StandardScaler(), GaussianNB(priors=None))\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('gaussiannb', GaussianNB())])\n \n Examples using sklearn.pipeline.make_pipeline\n \n  Release Highlights for scikit-learn 0.23  \n\n  Release Highlights for scikit-learn 0.24  \n\n  Release Highlights for scikit-learn 0.22  \n\n  A demo of K-Means clustering on the handwritten digits data  \n\n  Principal Component Regression vs Partial Least Squares Regression  \n\n  Feature transformations with ensembles of trees  \n\n  Categorical Feature Support in Gradient Boosting  \n\n  Combine predictors using stacking  \n\n  Pipeline Anova SVM  \n\n  Univariate Feature Selection  \n\n  Polynomial interpolation  \n\n  Robust linear estimator fitting  \n\n  Poisson regression and non-normal loss  \n\n  Tweedie regression on insurance claims  \n\n  Partial Dependence and Individual Conditional Expectation Plots  \n\n  Common pitfalls in interpretation of coefficients of linear models  \n\n  Scalable learning with polynomial kernel aproximation  \n\n  Visualizations with Display Objects  \n\n  Advanced Plotting With Partial Dependence  \n\n  Imputing missing values with variants of IterativeImputer  \n\n  Imputing missing values before building an estimator  \n\n  Detection error tradeoff (DET) curve  \n\n  Dimensionality Reduction with Neighborhood Components Analysis  \n\n  Approximate nearest neighbors in TSNE  \n\n  Varying regularization in Multi-layer Perceptron  \n\n  Importance of Feature Scaling  \n\n  Feature discretization  \n\n  Clustering text documents using k-means","title":"sklearn.modules.generated.sklearn.pipeline.make_pipeline"},{"text":"sklearn.ensemble.VotingRegressor  \nclass sklearn.ensemble.VotingRegressor(estimators, *, weights=None, n_jobs=None, verbose=False) [source]\n \nPrediction voting regressor for unfitted estimators. A voting regressor is an ensemble meta-estimator that fits several base regressors, each on the whole dataset. Then it averages the individual predictions to form a final prediction. Read more in the User Guide.  New in version 0.21.   Parameters \n \nestimatorslist of (str, estimator) tuples \n\nInvoking the fit method on the VotingRegressor will fit clones of those original estimators that will be stored in the class attribute self.estimators_. An estimator can be set to 'drop' using set_params.  Changed in version 0.21: 'drop' is accepted. Using None was deprecated in 0.22 and support was removed in 0.24.   \nweightsarray-like of shape (n_regressors,), default=None \n\nSequence of weights (float or int) to weight the occurrences of predicted values before averaging. Uses uniform weights if None.  \nn_jobsint, default=None \n\nThe number of jobs to run in parallel for fit. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details.  \nverbosebool, default=False \n\nIf True, the time elapsed while fitting will be printed as it is completed.  New in version 0.23.     Attributes \n \nestimators_list of regressors \n\nThe collection of fitted sub-estimators as defined in estimators that are not \u2018drop\u2019.  \nnamed_estimators_Bunch \n\nAttribute to access any fitted sub-estimators by name.  New in version 0.20.       See also  \nVotingClassifier\n\n\nSoft Voting\/Majority Rule classifier.    Examples >>> import numpy as np\n>>> from sklearn.linear_model import LinearRegression\n>>> from sklearn.ensemble import RandomForestRegressor\n>>> from sklearn.ensemble import VotingRegressor\n>>> r1 = LinearRegression()\n>>> r2 = RandomForestRegressor(n_estimators=10, random_state=1)\n>>> X = np.array([[1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36]])\n>>> y = np.array([2, 6, 12, 20, 30, 42])\n>>> er = VotingRegressor([('lr', r1), ('rf', r2)])\n>>> print(er.fit(X, y).predict(X))\n[ 3.3  5.7 11.8 19.7 28.  40.3]\n Methods  \nfit(X, y[, sample_weight]) Fit the estimators.  \nfit_transform(X[, y]) Return class labels or probabilities for each estimator.  \nget_params([deep]) Get the parameters of an estimator from the ensemble.  \npredict(X) Predict regression target for X.  \nscore(X, y[, sample_weight]) Return the coefficient of determination \\(R^2\\) of the prediction.  \nset_params(**params) Set the parameters of an estimator from the ensemble.  \ntransform(X) Return predictions for X for each estimator.    \nfit(X, y, sample_weight=None) [source]\n \nFit the estimators.  Parameters \n \nX{array-like, sparse matrix} of shape (n_samples, n_features) \n\nTraining vectors, where n_samples is the number of samples and n_features is the number of features.  \nyarray-like of shape (n_samples,) \n\nTarget values.  \nsample_weightarray-like of shape (n_samples,), default=None \n\nSample weights. If None, then samples are equally weighted. Note that this is supported only if all underlying estimators support sample weights.    Returns \n \nselfobject \n\nFitted estimator.     \n  \nfit_transform(X, y=None, **fit_params) [source]\n \nReturn class labels or probabilities for each estimator. Return predictions for X for each estimator.  Parameters \n \nX{array-like, sparse matrix, dataframe} of shape (n_samples, n_features) \n\nInput samples  \nyndarray of shape (n_samples,), default=None \n\nTarget values (None for unsupervised transformations).  \n**fit_paramsdict \n\nAdditional fit parameters.    Returns \n \nX_newndarray array of shape (n_samples, n_features_new) \n\nTransformed array.     \n  \nget_params(deep=True) [source]\n \nGet the parameters of an estimator from the ensemble. Returns the parameters given in the constructor as well as the estimators contained within the estimators parameter.  Parameters \n \ndeepbool, default=True \n\nSetting it to True gets the various estimators and the parameters of the estimators as well.     \n  \npredict(X) [source]\n \nPredict regression target for X. The predicted regression target of an input sample is computed as the mean predicted regression targets of the estimators in the ensemble.  Parameters \n \nX{array-like, sparse matrix} of shape (n_samples, n_features) \n\nThe input samples.    Returns \n \nyndarray of shape (n_samples,) \n\nThe predicted values.     \n  \nscore(X, y, sample_weight=None) [source]\n \nReturn the coefficient of determination \\(R^2\\) of the prediction. The coefficient \\(R^2\\) is defined as \\((1 - \\frac{u}{v})\\), where \\(u\\) is the residual sum of squares ((y_true - y_pred)\n** 2).sum() and \\(v\\) is the total sum of squares ((y_true -\ny_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a \\(R^2\\) score of 0.0.  Parameters \n \nXarray-like of shape (n_samples, n_features) \n\nTest samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead with shape (n_samples, n_samples_fitted), where n_samples_fitted is the number of samples used in the fitting for the estimator.  \nyarray-like of shape (n_samples,) or (n_samples, n_outputs) \n\nTrue values for X.  \nsample_weightarray-like of shape (n_samples,), default=None \n\nSample weights.    Returns \n \nscorefloat \n\n\\(R^2\\) of self.predict(X) wrt. y.     Notes The \\(R^2\\) score used when calling score on a regressor uses multioutput='uniform_average' from version 0.23 to keep consistent with default value of r2_score. This influences the score method of all the multioutput regressors (except for MultiOutputRegressor). \n  \nset_params(**params) [source]\n \nSet the parameters of an estimator from the ensemble. Valid parameter keys can be listed with get_params(). Note that you can directly set the parameters of the estimators contained in estimators.  Parameters \n \n**paramskeyword arguments \n\nSpecific parameters using e.g. set_params(parameter_name=new_value). In addition, to setting the parameters of the estimator, the individual estimator of the estimators can also be set, or can be removed by setting them to \u2018drop\u2019.     \n  \ntransform(X) [source]\n \nReturn predictions for X for each estimator.  Parameters \n \nX{array-like, sparse matrix} of shape (n_samples, n_features) \n\nThe input samples.    Returns \n predictions: ndarray of shape (n_samples, n_classifiers)\n\nValues predicted by each regressor.     \n \n Examples using sklearn.ensemble.VotingRegressor\n \n  Plot individual and voting regression predictions","title":"sklearn.modules.generated.sklearn.ensemble.votingregressor"}]}
{"task_id":2600191,"prompt":"def f_2600191(l):\n\treturn ","suffix":"","canonical_solution":"[[x, l.count(x)] for x in set(l)]","test_start":"\nfrom collections import Counter \n\ndef check(candidate):","test":["\n    assert sorted(candidate(\"123456asf\")) == [['1', 1],['2', 1],['3', 1],['4', 1],['5', 1],['6', 1],['a', 1],['f', 1],['s', 1]]\n","\n    assert sorted(candidate(\"aA\"*10)) == [['A', 10], ['a', 10]]\n"],"entry_point":"f_2600191","intent":"count the occurrences of items in list `l`","library":["collections"],"docs":[{"text":"count(value)  \nReturns the number of occurrences of value.","title":"python.library.multiprocessing.shared_memory#multiprocessing.shared_memory.ShareableList.count"},{"text":"operator.countOf(a, b)  \nReturn the number of occurrences of b in a.","title":"python.library.operator#operator.countOf"},{"text":"tf.raw_ops.BatchCholeskyGrad  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.raw_ops.BatchCholeskyGrad  \ntf.raw_ops.BatchCholeskyGrad(\n    l, grad, name=None\n)\n\n \n\n\n Args\n  l   A Tensor. Must be one of the following types: float32, float64.  \n  grad   A Tensor. Must have the same type as l.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor. Has the same type as l.","title":"tensorflow.raw_ops.batchcholeskygrad"},{"text":"pandas.Series.str.count   Series.str.count(pat, flags=0)[source]\n \nCount occurrences of pattern in each string of the Series\/Index. This function is used to count the number of times a particular regex pattern is repeated in each of the string elements of the Series.  Parameters \n \npat:str\n\n\nValid regular expression.  \nflags:int, default 0, meaning no flags\n\n\nFlags for the re module. For a complete list, see here.  **kwargs\n\nFor compatibility with other string methods. Not used.    Returns \n Series or Index\n\nSame type as the calling object containing the integer counts.      See also  re\n\nStandard library module for regular expressions.  str.count\n\nStandard library version, without regular expression support.    Notes Some characters need to be escaped when passing in pat. eg. '$' has a special meaning in regex and must be escaped when finding this literal character. Examples \n>>> s = pd.Series(['A', 'B', 'Aaba', 'Baca', np.nan, 'CABA', 'cat'])\n>>> s.str.count('a')\n0    0.0\n1    0.0\n2    2.0\n3    2.0\n4    NaN\n5    0.0\n6    1.0\ndtype: float64\n  Escape '$' to find the literal dollar sign. \n>>> s = pd.Series(['$', 'B', 'Aab$', '$$ca', 'C$B$', 'cat'])\n>>> s.str.count('\\\\$')\n0    1\n1    0\n2    1\n3    2\n4    2\n5    0\ndtype: int64\n  This is also available on Index \n>>> pd.Index(['A', 'A', 'Aaba', 'cat']).str.count('a')\nInt64Index([0, 0, 2, 1], dtype='int64')","title":"pandas.reference.api.pandas.series.str.count"},{"text":"torch.cuda.device_count() [source]\n \nReturns the number of GPUs available.","title":"torch.cuda#torch.cuda.device_count"},{"text":"class sklearn.linear_model.TheilSenRegressor(*, fit_intercept=True, copy_X=True, max_subpopulation=10000.0, n_subsamples=None, max_iter=300, tol=0.001, random_state=None, n_jobs=None, verbose=False) [source]\n \nTheil-Sen Estimator: robust multivariate regression model. The algorithm calculates least square solutions on subsets with size n_subsamples of the samples in X. Any value of n_subsamples between the number of features and samples leads to an estimator with a compromise between robustness and efficiency. Since the number of least square solutions is \u201cn_samples choose n_subsamples\u201d, it can be extremely large and can therefore be limited with max_subpopulation. If this limit is reached, the subsets are chosen randomly. In a final step, the spatial median (or L1 median) is calculated of all least square solutions. Read more in the User Guide.  Parameters \n \nfit_interceptbool, default=True \n\nWhether to calculate the intercept for this model. If set to false, no intercept will be used in calculations.  \ncopy_Xbool, default=True \n\nIf True, X will be copied; else, it may be overwritten.  \nmax_subpopulationint, default=1e4 \n\nInstead of computing with a set of cardinality \u2018n choose k\u2019, where n is the number of samples and k is the number of subsamples (at least number of features), consider only a stochastic subpopulation of a given maximal size if \u2018n choose k\u2019 is larger than max_subpopulation. For other than small problem sizes this parameter will determine memory usage and runtime if n_subsamples is not changed.  \nn_subsamplesint, default=None \n\nNumber of samples to calculate the parameters. This is at least the number of features (plus 1 if fit_intercept=True) and the number of samples as a maximum. A lower number leads to a higher breakdown point and a low efficiency while a high number leads to a low breakdown point and a high efficiency. If None, take the minimum number of subsamples leading to maximal robustness. If n_subsamples is set to n_samples, Theil-Sen is identical to least squares.  \nmax_iterint, default=300 \n\nMaximum number of iterations for the calculation of spatial median.  \ntolfloat, default=1.e-3 \n\nTolerance when calculating spatial median.  \nrandom_stateint, RandomState instance or None, default=None \n\nA random number generator instance to define the state of the random permutations generator. Pass an int for reproducible output across multiple function calls. See Glossary  \nn_jobsint, default=None \n\nNumber of CPUs to use during the cross validation. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details.  \nverbosebool, default=False \n\nVerbose mode when fitting the model.    Attributes \n \ncoef_ndarray of shape (n_features,) \n\nCoefficients of the regression model (median of distribution).  \nintercept_float \n\nEstimated intercept of regression model.  \nbreakdown_float \n\nApproximated breakdown point.  \nn_iter_int \n\nNumber of iterations needed for the spatial median.  \nn_subpopulation_int \n\nNumber of combinations taken into account from \u2018n choose k\u2019, where n is the number of samples and k is the number of subsamples.     References  Theil-Sen Estimators in a Multiple Linear Regression Model, 2009 Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang http:\/\/home.olemiss.edu\/~xdang\/papers\/MTSE.pdf\n  Examples >>> from sklearn.linear_model import TheilSenRegressor\n>>> from sklearn.datasets import make_regression\n>>> X, y = make_regression(\n...     n_samples=200, n_features=2, noise=4.0, random_state=0)\n>>> reg = TheilSenRegressor(random_state=0).fit(X, y)\n>>> reg.score(X, y)\n0.9884...\n>>> reg.predict(X[:1,])\narray([-31.5871...])\n Methods  \nfit(X, y) Fit linear model.  \nget_params([deep]) Get parameters for this estimator.  \npredict(X) Predict using the linear model.  \nscore(X, y[, sample_weight]) Return the coefficient of determination \\(R^2\\) of the prediction.  \nset_params(**params) Set the parameters of this estimator.    \nfit(X, y) [source]\n \nFit linear model.  Parameters \n \nXndarray of shape (n_samples, n_features) \n\nTraining data.  \nyndarray of shape (n_samples,) \n\nTarget values.    Returns \n \nselfreturns an instance of self. \n   \n  \nget_params(deep=True) [source]\n \nGet parameters for this estimator.  Parameters \n \ndeepbool, default=True \n\nIf True, will return the parameters for this estimator and contained subobjects that are estimators.    Returns \n \nparamsdict \n\nParameter names mapped to their values.     \n  \npredict(X) [source]\n \nPredict using the linear model.  Parameters \n \nXarray-like or sparse matrix, shape (n_samples, n_features) \n\nSamples.    Returns \n \nCarray, shape (n_samples,) \n\nReturns predicted values.     \n  \nscore(X, y, sample_weight=None) [source]\n \nReturn the coefficient of determination \\(R^2\\) of the prediction. The coefficient \\(R^2\\) is defined as \\((1 - \\frac{u}{v})\\), where \\(u\\) is the residual sum of squares ((y_true - y_pred)\n** 2).sum() and \\(v\\) is the total sum of squares ((y_true -\ny_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a \\(R^2\\) score of 0.0.  Parameters \n \nXarray-like of shape (n_samples, n_features) \n\nTest samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead with shape (n_samples, n_samples_fitted), where n_samples_fitted is the number of samples used in the fitting for the estimator.  \nyarray-like of shape (n_samples,) or (n_samples, n_outputs) \n\nTrue values for X.  \nsample_weightarray-like of shape (n_samples,), default=None \n\nSample weights.    Returns \n \nscorefloat \n\n\\(R^2\\) of self.predict(X) wrt. y.     Notes The \\(R^2\\) score used when calling score on a regressor uses multioutput='uniform_average' from version 0.23 to keep consistent with default value of r2_score. This influences the score method of all the multioutput regressors (except for MultiOutputRegressor). \n  \nset_params(**params) [source]\n \nSet the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as Pipeline). The latter have parameters of the form <component>__<parameter> so that it\u2019s possible to update each component of a nested object.  Parameters \n \n**paramsdict \n\nEstimator parameters.    Returns \n \nselfestimator instance \n\nEstimator instance.","title":"sklearn.modules.generated.sklearn.linear_model.theilsenregressor#sklearn.linear_model.TheilSenRegressor"},{"text":"sklearn.linear_model.TheilSenRegressor  \nclass sklearn.linear_model.TheilSenRegressor(*, fit_intercept=True, copy_X=True, max_subpopulation=10000.0, n_subsamples=None, max_iter=300, tol=0.001, random_state=None, n_jobs=None, verbose=False) [source]\n \nTheil-Sen Estimator: robust multivariate regression model. The algorithm calculates least square solutions on subsets with size n_subsamples of the samples in X. Any value of n_subsamples between the number of features and samples leads to an estimator with a compromise between robustness and efficiency. Since the number of least square solutions is \u201cn_samples choose n_subsamples\u201d, it can be extremely large and can therefore be limited with max_subpopulation. If this limit is reached, the subsets are chosen randomly. In a final step, the spatial median (or L1 median) is calculated of all least square solutions. Read more in the User Guide.  Parameters \n \nfit_interceptbool, default=True \n\nWhether to calculate the intercept for this model. If set to false, no intercept will be used in calculations.  \ncopy_Xbool, default=True \n\nIf True, X will be copied; else, it may be overwritten.  \nmax_subpopulationint, default=1e4 \n\nInstead of computing with a set of cardinality \u2018n choose k\u2019, where n is the number of samples and k is the number of subsamples (at least number of features), consider only a stochastic subpopulation of a given maximal size if \u2018n choose k\u2019 is larger than max_subpopulation. For other than small problem sizes this parameter will determine memory usage and runtime if n_subsamples is not changed.  \nn_subsamplesint, default=None \n\nNumber of samples to calculate the parameters. This is at least the number of features (plus 1 if fit_intercept=True) and the number of samples as a maximum. A lower number leads to a higher breakdown point and a low efficiency while a high number leads to a low breakdown point and a high efficiency. If None, take the minimum number of subsamples leading to maximal robustness. If n_subsamples is set to n_samples, Theil-Sen is identical to least squares.  \nmax_iterint, default=300 \n\nMaximum number of iterations for the calculation of spatial median.  \ntolfloat, default=1.e-3 \n\nTolerance when calculating spatial median.  \nrandom_stateint, RandomState instance or None, default=None \n\nA random number generator instance to define the state of the random permutations generator. Pass an int for reproducible output across multiple function calls. See Glossary  \nn_jobsint, default=None \n\nNumber of CPUs to use during the cross validation. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details.  \nverbosebool, default=False \n\nVerbose mode when fitting the model.    Attributes \n \ncoef_ndarray of shape (n_features,) \n\nCoefficients of the regression model (median of distribution).  \nintercept_float \n\nEstimated intercept of regression model.  \nbreakdown_float \n\nApproximated breakdown point.  \nn_iter_int \n\nNumber of iterations needed for the spatial median.  \nn_subpopulation_int \n\nNumber of combinations taken into account from \u2018n choose k\u2019, where n is the number of samples and k is the number of subsamples.     References  Theil-Sen Estimators in a Multiple Linear Regression Model, 2009 Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang http:\/\/home.olemiss.edu\/~xdang\/papers\/MTSE.pdf\n  Examples >>> from sklearn.linear_model import TheilSenRegressor\n>>> from sklearn.datasets import make_regression\n>>> X, y = make_regression(\n...     n_samples=200, n_features=2, noise=4.0, random_state=0)\n>>> reg = TheilSenRegressor(random_state=0).fit(X, y)\n>>> reg.score(X, y)\n0.9884...\n>>> reg.predict(X[:1,])\narray([-31.5871...])\n Methods  \nfit(X, y) Fit linear model.  \nget_params([deep]) Get parameters for this estimator.  \npredict(X) Predict using the linear model.  \nscore(X, y[, sample_weight]) Return the coefficient of determination \\(R^2\\) of the prediction.  \nset_params(**params) Set the parameters of this estimator.    \nfit(X, y) [source]\n \nFit linear model.  Parameters \n \nXndarray of shape (n_samples, n_features) \n\nTraining data.  \nyndarray of shape (n_samples,) \n\nTarget values.    Returns \n \nselfreturns an instance of self. \n   \n  \nget_params(deep=True) [source]\n \nGet parameters for this estimator.  Parameters \n \ndeepbool, default=True \n\nIf True, will return the parameters for this estimator and contained subobjects that are estimators.    Returns \n \nparamsdict \n\nParameter names mapped to their values.     \n  \npredict(X) [source]\n \nPredict using the linear model.  Parameters \n \nXarray-like or sparse matrix, shape (n_samples, n_features) \n\nSamples.    Returns \n \nCarray, shape (n_samples,) \n\nReturns predicted values.     \n  \nscore(X, y, sample_weight=None) [source]\n \nReturn the coefficient of determination \\(R^2\\) of the prediction. The coefficient \\(R^2\\) is defined as \\((1 - \\frac{u}{v})\\), where \\(u\\) is the residual sum of squares ((y_true - y_pred)\n** 2).sum() and \\(v\\) is the total sum of squares ((y_true -\ny_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a \\(R^2\\) score of 0.0.  Parameters \n \nXarray-like of shape (n_samples, n_features) \n\nTest samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead with shape (n_samples, n_samples_fitted), where n_samples_fitted is the number of samples used in the fitting for the estimator.  \nyarray-like of shape (n_samples,) or (n_samples, n_outputs) \n\nTrue values for X.  \nsample_weightarray-like of shape (n_samples,), default=None \n\nSample weights.    Returns \n \nscorefloat \n\n\\(R^2\\) of self.predict(X) wrt. y.     Notes The \\(R^2\\) score used when calling score on a regressor uses multioutput='uniform_average' from version 0.23 to keep consistent with default value of r2_score. This influences the score method of all the multioutput regressors (except for MultiOutputRegressor). \n  \nset_params(**params) [source]\n \nSet the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as Pipeline). The latter have parameters of the form <component>__<parameter> so that it\u2019s possible to update each component of a nested object.  Parameters \n \n**paramsdict \n\nEstimator parameters.    Returns \n \nselfestimator instance \n\nEstimator instance.     \n \n Examples using sklearn.linear_model.TheilSenRegressor\n \n  Theil-Sen Regression  \n\n  Robust linear estimator fitting","title":"sklearn.modules.generated.sklearn.linear_model.theilsenregressor"},{"text":"numpy.cross   numpy.cross(a, b, axisa=- 1, axisb=- 1, axisc=- 1, axis=None)[source]\n \nReturn the cross product of two (arrays of) vectors. The cross product of a and b in \\(R^3\\) is a vector perpendicular to both a and b. If a and b are arrays of vectors, the vectors are defined by the last axis of a and b by default, and these axes can have dimensions 2 or 3. Where the dimension of either a or b is 2, the third component of the input vector is assumed to be zero and the cross product calculated accordingly. In cases where both input vectors have dimension 2, the z-component of the cross product is returned.  Parameters \n \naarray_like\n\n\nComponents of the first vector(s).  \nbarray_like\n\n\nComponents of the second vector(s).  \naxisaint, optional\n\n\nAxis of a that defines the vector(s). By default, the last axis.  \naxisbint, optional\n\n\nAxis of b that defines the vector(s). By default, the last axis.  \naxiscint, optional\n\n\nAxis of c containing the cross product vector(s). Ignored if both input vectors have dimension 2, as the return is scalar. By default, the last axis.  \naxisint, optional\n\n\nIf defined, the axis of a, b and c that defines the vector(s) and cross product(s). Overrides axisa, axisb and axisc.    Returns \n \ncndarray\n\n\nVector cross product(s).    Raises \n ValueError\n\nWhen the dimension of the vector(s) in a and\/or b does not equal 2 or 3.      See also  inner\n\nInner product  outer\n\nOuter product.  ix_\n\nConstruct index arrays.    Notes  New in version 1.9.0.  Supports full broadcasting of the inputs. Examples Vector cross-product. >>> x = [1, 2, 3]\n>>> y = [4, 5, 6]\n>>> np.cross(x, y)\narray([-3,  6, -3])\n One vector with dimension 2. >>> x = [1, 2]\n>>> y = [4, 5, 6]\n>>> np.cross(x, y)\narray([12, -6, -3])\n Equivalently: >>> x = [1, 2, 0]\n>>> y = [4, 5, 6]\n>>> np.cross(x, y)\narray([12, -6, -3])\n Both vectors with dimension 2. >>> x = [1,2]\n>>> y = [4,5]\n>>> np.cross(x, y)\narray(-3)\n Multiple vector cross-products. Note that the direction of the cross product vector is defined by the right-hand rule. >>> x = np.array([[1,2,3], [4,5,6]])\n>>> y = np.array([[4,5,6], [1,2,3]])\n>>> np.cross(x, y)\narray([[-3,  6, -3],\n       [ 3, -6,  3]])\n The orientation of c can be changed using the axisc keyword. >>> np.cross(x, y, axisc=0)\narray([[-3,  3],\n       [ 6, -6],\n       [-3,  3]])\n Change the vector definition of x and y using axisa and axisb. >>> x = np.array([[1,2,3], [4,5,6], [7, 8, 9]])\n>>> y = np.array([[7, 8, 9], [4,5,6], [1,2,3]])\n>>> np.cross(x, y)\narray([[ -6,  12,  -6],\n       [  0,   0,   0],\n       [  6, -12,   6]])\n>>> np.cross(x, y, axisa=0, axisb=0)\narray([[-24,  48, -24],\n       [-30,  60, -30],\n       [-36,  72, -36]])","title":"numpy.reference.generated.numpy.cross"},{"text":"sklearn.pipeline.make_pipeline  \nsklearn.pipeline.make_pipeline(*steps, memory=None, verbose=False) [source]\n \nConstruct a Pipeline from the given estimators. This is a shorthand for the Pipeline constructor; it does not require, and does not permit, naming the estimators. Instead, their names will be set to the lowercase of their types automatically.  Parameters \n \n*stepslist of estimators. \n\nmemorystr or object with the joblib.Memory interface, default=None \n\nUsed to cache the fitted transformers of the pipeline. By default, no caching is performed. If a string is given, it is the path to the caching directory. Enabling caching triggers a clone of the transformers before fitting. Therefore, the transformer instance given to the pipeline cannot be inspected directly. Use the attribute named_steps or steps to inspect estimators within the pipeline. Caching the transformers is advantageous when fitting is time consuming.  \nverbosebool, default=False \n\nIf True, the time elapsed while fitting each step will be printed as it is completed.    Returns \n \npPipeline \n    See also  \nPipeline\n\n\nClass for creating a pipeline of transforms with a final estimator.    Examples >>> from sklearn.naive_bayes import GaussianNB\n>>> from sklearn.preprocessing import StandardScaler\n>>> make_pipeline(StandardScaler(), GaussianNB(priors=None))\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('gaussiannb', GaussianNB())])\n \n Examples using sklearn.pipeline.make_pipeline\n \n  Release Highlights for scikit-learn 0.23  \n\n  Release Highlights for scikit-learn 0.24  \n\n  Release Highlights for scikit-learn 0.22  \n\n  A demo of K-Means clustering on the handwritten digits data  \n\n  Principal Component Regression vs Partial Least Squares Regression  \n\n  Feature transformations with ensembles of trees  \n\n  Categorical Feature Support in Gradient Boosting  \n\n  Combine predictors using stacking  \n\n  Pipeline Anova SVM  \n\n  Univariate Feature Selection  \n\n  Polynomial interpolation  \n\n  Robust linear estimator fitting  \n\n  Poisson regression and non-normal loss  \n\n  Tweedie regression on insurance claims  \n\n  Partial Dependence and Individual Conditional Expectation Plots  \n\n  Common pitfalls in interpretation of coefficients of linear models  \n\n  Scalable learning with polynomial kernel aproximation  \n\n  Visualizations with Display Objects  \n\n  Advanced Plotting With Partial Dependence  \n\n  Imputing missing values with variants of IterativeImputer  \n\n  Imputing missing values before building an estimator  \n\n  Detection error tradeoff (DET) curve  \n\n  Dimensionality Reduction with Neighborhood Components Analysis  \n\n  Approximate nearest neighbors in TSNE  \n\n  Varying regularization in Multi-layer Perceptron  \n\n  Importance of Feature Scaling  \n\n  Feature discretization  \n\n  Clustering text documents using k-means","title":"sklearn.modules.generated.sklearn.pipeline.make_pipeline"},{"text":"sklearn.ensemble.VotingRegressor  \nclass sklearn.ensemble.VotingRegressor(estimators, *, weights=None, n_jobs=None, verbose=False) [source]\n \nPrediction voting regressor for unfitted estimators. A voting regressor is an ensemble meta-estimator that fits several base regressors, each on the whole dataset. Then it averages the individual predictions to form a final prediction. Read more in the User Guide.  New in version 0.21.   Parameters \n \nestimatorslist of (str, estimator) tuples \n\nInvoking the fit method on the VotingRegressor will fit clones of those original estimators that will be stored in the class attribute self.estimators_. An estimator can be set to 'drop' using set_params.  Changed in version 0.21: 'drop' is accepted. Using None was deprecated in 0.22 and support was removed in 0.24.   \nweightsarray-like of shape (n_regressors,), default=None \n\nSequence of weights (float or int) to weight the occurrences of predicted values before averaging. Uses uniform weights if None.  \nn_jobsint, default=None \n\nThe number of jobs to run in parallel for fit. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details.  \nverbosebool, default=False \n\nIf True, the time elapsed while fitting will be printed as it is completed.  New in version 0.23.     Attributes \n \nestimators_list of regressors \n\nThe collection of fitted sub-estimators as defined in estimators that are not \u2018drop\u2019.  \nnamed_estimators_Bunch \n\nAttribute to access any fitted sub-estimators by name.  New in version 0.20.       See also  \nVotingClassifier\n\n\nSoft Voting\/Majority Rule classifier.    Examples >>> import numpy as np\n>>> from sklearn.linear_model import LinearRegression\n>>> from sklearn.ensemble import RandomForestRegressor\n>>> from sklearn.ensemble import VotingRegressor\n>>> r1 = LinearRegression()\n>>> r2 = RandomForestRegressor(n_estimators=10, random_state=1)\n>>> X = np.array([[1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36]])\n>>> y = np.array([2, 6, 12, 20, 30, 42])\n>>> er = VotingRegressor([('lr', r1), ('rf', r2)])\n>>> print(er.fit(X, y).predict(X))\n[ 3.3  5.7 11.8 19.7 28.  40.3]\n Methods  \nfit(X, y[, sample_weight]) Fit the estimators.  \nfit_transform(X[, y]) Return class labels or probabilities for each estimator.  \nget_params([deep]) Get the parameters of an estimator from the ensemble.  \npredict(X) Predict regression target for X.  \nscore(X, y[, sample_weight]) Return the coefficient of determination \\(R^2\\) of the prediction.  \nset_params(**params) Set the parameters of an estimator from the ensemble.  \ntransform(X) Return predictions for X for each estimator.    \nfit(X, y, sample_weight=None) [source]\n \nFit the estimators.  Parameters \n \nX{array-like, sparse matrix} of shape (n_samples, n_features) \n\nTraining vectors, where n_samples is the number of samples and n_features is the number of features.  \nyarray-like of shape (n_samples,) \n\nTarget values.  \nsample_weightarray-like of shape (n_samples,), default=None \n\nSample weights. If None, then samples are equally weighted. Note that this is supported only if all underlying estimators support sample weights.    Returns \n \nselfobject \n\nFitted estimator.     \n  \nfit_transform(X, y=None, **fit_params) [source]\n \nReturn class labels or probabilities for each estimator. Return predictions for X for each estimator.  Parameters \n \nX{array-like, sparse matrix, dataframe} of shape (n_samples, n_features) \n\nInput samples  \nyndarray of shape (n_samples,), default=None \n\nTarget values (None for unsupervised transformations).  \n**fit_paramsdict \n\nAdditional fit parameters.    Returns \n \nX_newndarray array of shape (n_samples, n_features_new) \n\nTransformed array.     \n  \nget_params(deep=True) [source]\n \nGet the parameters of an estimator from the ensemble. Returns the parameters given in the constructor as well as the estimators contained within the estimators parameter.  Parameters \n \ndeepbool, default=True \n\nSetting it to True gets the various estimators and the parameters of the estimators as well.     \n  \npredict(X) [source]\n \nPredict regression target for X. The predicted regression target of an input sample is computed as the mean predicted regression targets of the estimators in the ensemble.  Parameters \n \nX{array-like, sparse matrix} of shape (n_samples, n_features) \n\nThe input samples.    Returns \n \nyndarray of shape (n_samples,) \n\nThe predicted values.     \n  \nscore(X, y, sample_weight=None) [source]\n \nReturn the coefficient of determination \\(R^2\\) of the prediction. The coefficient \\(R^2\\) is defined as \\((1 - \\frac{u}{v})\\), where \\(u\\) is the residual sum of squares ((y_true - y_pred)\n** 2).sum() and \\(v\\) is the total sum of squares ((y_true -\ny_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a \\(R^2\\) score of 0.0.  Parameters \n \nXarray-like of shape (n_samples, n_features) \n\nTest samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead with shape (n_samples, n_samples_fitted), where n_samples_fitted is the number of samples used in the fitting for the estimator.  \nyarray-like of shape (n_samples,) or (n_samples, n_outputs) \n\nTrue values for X.  \nsample_weightarray-like of shape (n_samples,), default=None \n\nSample weights.    Returns \n \nscorefloat \n\n\\(R^2\\) of self.predict(X) wrt. y.     Notes The \\(R^2\\) score used when calling score on a regressor uses multioutput='uniform_average' from version 0.23 to keep consistent with default value of r2_score. This influences the score method of all the multioutput regressors (except for MultiOutputRegressor). \n  \nset_params(**params) [source]\n \nSet the parameters of an estimator from the ensemble. Valid parameter keys can be listed with get_params(). Note that you can directly set the parameters of the estimators contained in estimators.  Parameters \n \n**paramskeyword arguments \n\nSpecific parameters using e.g. set_params(parameter_name=new_value). In addition, to setting the parameters of the estimator, the individual estimator of the estimators can also be set, or can be removed by setting them to \u2018drop\u2019.     \n  \ntransform(X) [source]\n \nReturn predictions for X for each estimator.  Parameters \n \nX{array-like, sparse matrix} of shape (n_samples, n_features) \n\nThe input samples.    Returns \n predictions: ndarray of shape (n_samples, n_classifiers)\n\nValues predicted by each regressor.     \n \n Examples using sklearn.ensemble.VotingRegressor\n \n  Plot individual and voting regression predictions","title":"sklearn.modules.generated.sklearn.ensemble.votingregressor"}]}
{"task_id":2600191,"prompt":"def f_2600191(l):\n\treturn ","suffix":"","canonical_solution":"dict(((x, l.count(x)) for x in set(l)))","test_start":"\nfrom collections import Counter \n\ndef check(candidate):","test":["\n    assert candidate(\"123456asf\") == {'4': 1, 'a': 1, '1': 1, 's': 1, '6': 1, 'f': 1, '3': 1, '5': 1, '2': 1}\n","\n    assert candidate(\"aA\"*10) == {'A': 10, 'a': 10}\n","\n    assert candidate([1, 6]) == {1: 1, 6: 1}\n"],"entry_point":"f_2600191","intent":"count the occurrences of items in list `l`","library":["collections"],"docs":[]}
{"task_id":2600191,"prompt":"def f_2600191(l):\n\treturn ","suffix":"","canonical_solution":"l.count('b')","test_start":"\ndef check(candidate):","test":["\n    assert candidate(\"123456abbbsf\") == 3\n","\n    assert candidate(\"123456gyjnccfgsf\") == 0\n","\n    assert candidate(\"Ab\"*10) == 10\n"],"entry_point":"f_2600191","intent":"count the occurrences of item \"b\" in list `l`","library":[],"docs":[]}
{"task_id":12842997,"prompt":"def f_12842997(srcfile, dstdir):\n\t","suffix":"\n\treturn ","canonical_solution":"shutil.copy(srcfile, dstdir)","test_start":"\nimport shutil\nfrom unittest.mock import Mock\n\ndef check(candidate):","test":["\n    shutil.copy = Mock()\n    try:\n        candidate('opera.txt', '\/')\n    except:\n        return False \n"],"entry_point":"f_12842997","intent":"copy file `srcfile` to directory `dstdir`","library":["shutil"],"docs":[{"text":"shutil.copyfile(src, dst, *, follow_symlinks=True)  \nCopy the contents (no metadata) of the file named src to a file named dst and return dst in the most efficient way possible. src and dst are path-like objects or path names given as strings. dst must be the complete target file name; look at copy() for a copy that accepts a target directory path. If src and dst specify the same file, SameFileError is raised. The destination location must be writable; otherwise, an OSError exception will be raised. If dst already exists, it will be replaced. Special files such as character or block devices and pipes cannot be copied with this function. If follow_symlinks is false and src is a symbolic link, a new symbolic link will be created instead of copying the file src points to. Raises an auditing event shutil.copyfile with arguments src, dst.  Changed in version 3.3: IOError used to be raised instead of OSError. Added follow_symlinks argument. Now returns dst.   Changed in version 3.4: Raise SameFileError instead of Error. Since the former is a subclass of the latter, this change is backward compatible.   Changed in version 3.8: Platform-specific fast-copy syscalls may be used internally in order to copy the file more efficiently. See Platform-dependent efficient copy operations section.","title":"python.library.shutil#shutil.copyfile"},{"text":"Template.copy(infile, outfile)  \nCopy infile to outfile through the pipe.","title":"python.library.pipes#pipes.Template.copy"},{"text":"exception shutil.SameFileError  \nThis exception is raised if source and destination in copyfile() are the same file.  New in version 3.4.","title":"python.library.shutil#shutil.SameFileError"},{"text":"shutil.copyfileobj(fsrc, fdst[, length])  \nCopy the contents of the file-like object fsrc to the file-like object fdst. The integer length, if given, is the buffer size. In particular, a negative length value means to copy the data without looping over the source data in chunks; by default the data is read in chunks to avoid uncontrolled memory consumption. Note that if the current file position of the fsrc object is not 0, only the contents from the current file position to the end of the file will be copied.","title":"python.library.shutil#shutil.copyfileobj"},{"text":"test.support.findfile(filename, subdir=None)  \nReturn the path to the file named filename. If no match is found filename is returned. This does not equal a failure since it could be the path to the file. Setting subdir indicates a relative path to use to find the file rather than looking directly in the path directories.","title":"python.library.test#test.support.findfile"},{"text":"tf.io.gfile.copy     View source on GitHub    Copies data from src to dst.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.io.gfile.copy  \ntf.io.gfile.copy(\n    src, dst, overwrite=False\n)\n\n \n\n\n Args\n  src   string, name of the file whose contents need to be copied  \n  dst   string, name of the file to which to copy to  \n  overwrite   boolean, if false it's an error for dst to be occupied by an existing file.   \n \n\n\n Raises\n  errors.OpError   If the operation fails.","title":"tensorflow.io.gfile.copy"},{"text":"shutil \u2014 High-level file operations Source code: Lib\/shutil.py The shutil module offers a number of high-level operations on files and collections of files. In particular, functions are provided which support file copying and removal. For operations on individual files, see also the os module.  Warning Even the higher-level file copying functions (shutil.copy(), shutil.copy2()) cannot copy all file metadata. On POSIX platforms, this means that file owner and group are lost as well as ACLs. On Mac OS, the resource fork and other metadata are not used. This means that resources will be lost and file type and creator codes will not be correct. On Windows, file owners, ACLs and alternate data streams are not copied.  Directory and files operations  \nshutil.copyfileobj(fsrc, fdst[, length])  \nCopy the contents of the file-like object fsrc to the file-like object fdst. The integer length, if given, is the buffer size. In particular, a negative length value means to copy the data without looping over the source data in chunks; by default the data is read in chunks to avoid uncontrolled memory consumption. Note that if the current file position of the fsrc object is not 0, only the contents from the current file position to the end of the file will be copied. \n  \nshutil.copyfile(src, dst, *, follow_symlinks=True)  \nCopy the contents (no metadata) of the file named src to a file named dst and return dst in the most efficient way possible. src and dst are path-like objects or path names given as strings. dst must be the complete target file name; look at copy() for a copy that accepts a target directory path. If src and dst specify the same file, SameFileError is raised. The destination location must be writable; otherwise, an OSError exception will be raised. If dst already exists, it will be replaced. Special files such as character or block devices and pipes cannot be copied with this function. If follow_symlinks is false and src is a symbolic link, a new symbolic link will be created instead of copying the file src points to. Raises an auditing event shutil.copyfile with arguments src, dst.  Changed in version 3.3: IOError used to be raised instead of OSError. Added follow_symlinks argument. Now returns dst.   Changed in version 3.4: Raise SameFileError instead of Error. Since the former is a subclass of the latter, this change is backward compatible.   Changed in version 3.8: Platform-specific fast-copy syscalls may be used internally in order to copy the file more efficiently. See Platform-dependent efficient copy operations section.  \n  \nexception shutil.SameFileError  \nThis exception is raised if source and destination in copyfile() are the same file.  New in version 3.4.  \n  \nshutil.copymode(src, dst, *, follow_symlinks=True)  \nCopy the permission bits from src to dst. The file contents, owner, and group are unaffected. src and dst are path-like objects or path names given as strings. If follow_symlinks is false, and both src and dst are symbolic links, copymode() will attempt to modify the mode of dst itself (rather than the file it points to). This functionality is not available on every platform; please see copystat() for more information. If copymode() cannot modify symbolic links on the local platform, and it is asked to do so, it will do nothing and return. Raises an auditing event shutil.copymode with arguments src, dst.  Changed in version 3.3: Added follow_symlinks argument.  \n  \nshutil.copystat(src, dst, *, follow_symlinks=True)  \nCopy the permission bits, last access time, last modification time, and flags from src to dst. On Linux, copystat() also copies the \u201cextended attributes\u201d where possible. The file contents, owner, and group are unaffected. src and dst are path-like objects or path names given as strings. If follow_symlinks is false, and src and dst both refer to symbolic links, copystat() will operate on the symbolic links themselves rather than the files the symbolic links refer to\u2014reading the information from the src symbolic link, and writing the information to the dst symbolic link.  Note Not all platforms provide the ability to examine and modify symbolic links. Python itself can tell you what functionality is locally available.  If os.chmod in os.supports_follow_symlinks is True, copystat() can modify the permission bits of a symbolic link. If os.utime in os.supports_follow_symlinks is True, copystat() can modify the last access and modification times of a symbolic link. If os.chflags in os.supports_follow_symlinks is True, copystat() can modify the flags of a symbolic link. (os.chflags is not available on all platforms.)  On platforms where some or all of this functionality is unavailable, when asked to modify a symbolic link, copystat() will copy everything it can. copystat() never returns failure. Please see os.supports_follow_symlinks for more information.  Raises an auditing event shutil.copystat with arguments src, dst.  Changed in version 3.3: Added follow_symlinks argument and support for Linux extended attributes.  \n  \nshutil.copy(src, dst, *, follow_symlinks=True)  \nCopies the file src to the file or directory dst. src and dst should be path-like objects or strings. If dst specifies a directory, the file will be copied into dst using the base filename from src. Returns the path to the newly created file. If follow_symlinks is false, and src is a symbolic link, dst will be created as a symbolic link. If follow_symlinks is true and src is a symbolic link, dst will be a copy of the file src refers to. copy() copies the file data and the file\u2019s permission mode (see os.chmod()). Other metadata, like the file\u2019s creation and modification times, is not preserved. To preserve all file metadata from the original, use copy2() instead. Raises an auditing event shutil.copyfile with arguments src, dst. Raises an auditing event shutil.copymode with arguments src, dst.  Changed in version 3.3: Added follow_symlinks argument. Now returns path to the newly created file.   Changed in version 3.8: Platform-specific fast-copy syscalls may be used internally in order to copy the file more efficiently. See Platform-dependent efficient copy operations section.  \n  \nshutil.copy2(src, dst, *, follow_symlinks=True)  \nIdentical to copy() except that copy2() also attempts to preserve file metadata. When follow_symlinks is false, and src is a symbolic link, copy2() attempts to copy all metadata from the src symbolic link to the newly-created dst symbolic link. However, this functionality is not available on all platforms. On platforms where some or all of this functionality is unavailable, copy2() will preserve all the metadata it can; copy2() never raises an exception because it cannot preserve file metadata. copy2() uses copystat() to copy the file metadata. Please see copystat() for more information about platform support for modifying symbolic link metadata. Raises an auditing event shutil.copyfile with arguments src, dst. Raises an auditing event shutil.copystat with arguments src, dst.  Changed in version 3.3: Added follow_symlinks argument, try to copy extended file system attributes too (currently Linux only). Now returns path to the newly created file.   Changed in version 3.8: Platform-specific fast-copy syscalls may be used internally in order to copy the file more efficiently. See Platform-dependent efficient copy operations section.  \n  \nshutil.ignore_patterns(*patterns)  \nThis factory function creates a function that can be used as a callable for copytree()\u2019s ignore argument, ignoring files and directories that match one of the glob-style patterns provided. See the example below. \n  \nshutil.copytree(src, dst, symlinks=False, ignore=None, copy_function=copy2, ignore_dangling_symlinks=False, dirs_exist_ok=False)  \nRecursively copy an entire directory tree rooted at src to a directory named dst and return the destination directory. dirs_exist_ok dictates whether to raise an exception in case dst or any missing parent directory already exists. Permissions and times of directories are copied with copystat(), individual files are copied using copy2(). If symlinks is true, symbolic links in the source tree are represented as symbolic links in the new tree and the metadata of the original links will be copied as far as the platform allows; if false or omitted, the contents and metadata of the linked files are copied to the new tree. When symlinks is false, if the file pointed by the symlink doesn\u2019t exist, an exception will be added in the list of errors raised in an Error exception at the end of the copy process. You can set the optional ignore_dangling_symlinks flag to true if you want to silence this exception. Notice that this option has no effect on platforms that don\u2019t support os.symlink(). If ignore is given, it must be a callable that will receive as its arguments the directory being visited by copytree(), and a list of its contents, as returned by os.listdir(). Since copytree() is called recursively, the ignore callable will be called once for each directory that is copied. The callable must return a sequence of directory and file names relative to the current directory (i.e. a subset of the items in its second argument); these names will then be ignored in the copy process. ignore_patterns() can be used to create such a callable that ignores names based on glob-style patterns. If exception(s) occur, an Error is raised with a list of reasons. If copy_function is given, it must be a callable that will be used to copy each file. It will be called with the source path and the destination path as arguments. By default, copy2() is used, but any function that supports the same signature (like copy()) can be used. Raises an auditing event shutil.copytree with arguments src, dst.  Changed in version 3.3: Copy metadata when symlinks is false. Now returns dst.   Changed in version 3.2: Added the copy_function argument to be able to provide a custom copy function. Added the ignore_dangling_symlinks argument to silent dangling symlinks errors when symlinks is false.   Changed in version 3.8: Platform-specific fast-copy syscalls may be used internally in order to copy the file more efficiently. See Platform-dependent efficient copy operations section.   New in version 3.8: The dirs_exist_ok parameter.  \n  \nshutil.rmtree(path, ignore_errors=False, onerror=None)  \nDelete an entire directory tree; path must point to a directory (but not a symbolic link to a directory). If ignore_errors is true, errors resulting from failed removals will be ignored; if false or omitted, such errors are handled by calling a handler specified by onerror or, if that is omitted, they raise an exception.  Note On platforms that support the necessary fd-based functions a symlink attack resistant version of rmtree() is used by default. On other platforms, the rmtree() implementation is susceptible to a symlink attack: given proper timing and circumstances, attackers can manipulate symlinks on the filesystem to delete files they wouldn\u2019t be able to access otherwise. Applications can use the rmtree.avoids_symlink_attacks function attribute to determine which case applies.  If onerror is provided, it must be a callable that accepts three parameters: function, path, and excinfo. The first parameter, function, is the function which raised the exception; it depends on the platform and implementation. The second parameter, path, will be the path name passed to function. The third parameter, excinfo, will be the exception information returned by sys.exc_info(). Exceptions raised by onerror will not be caught. Raises an auditing event shutil.rmtree with argument path.  Changed in version 3.3: Added a symlink attack resistant version that is used automatically if platform supports fd-based functions.   Changed in version 3.8: On Windows, will no longer delete the contents of a directory junction before removing the junction.   \nrmtree.avoids_symlink_attacks  \nIndicates whether the current platform and implementation provides a symlink attack resistant version of rmtree(). Currently this is only true for platforms supporting fd-based directory access functions.  New in version 3.3.  \n \n  \nshutil.move(src, dst, copy_function=copy2)  \nRecursively move a file or directory (src) to another location (dst) and return the destination. If the destination is an existing directory, then src is moved inside that directory. If the destination already exists but is not a directory, it may be overwritten depending on os.rename() semantics. If the destination is on the current filesystem, then os.rename() is used. Otherwise, src is copied to dst using copy_function and then removed. In case of symlinks, a new symlink pointing to the target of src will be created in or as dst and src will be removed. If copy_function is given, it must be a callable that takes two arguments src and dst, and will be used to copy src to dst if os.rename() cannot be used. If the source is a directory, copytree() is called, passing it the copy_function(). The default copy_function is copy2(). Using copy() as the copy_function allows the move to succeed when it is not possible to also copy the metadata, at the expense of not copying any of the metadata. Raises an auditing event shutil.move with arguments src, dst.  Changed in version 3.3: Added explicit symlink handling for foreign filesystems, thus adapting it to the behavior of GNU\u2019s mv. Now returns dst.   Changed in version 3.5: Added the copy_function keyword argument.   Changed in version 3.8: Platform-specific fast-copy syscalls may be used internally in order to copy the file more efficiently. See Platform-dependent efficient copy operations section.   Changed in version 3.9: Accepts a path-like object for both src and dst.  \n  \nshutil.disk_usage(path)  \nReturn disk usage statistics about the given path as a named tuple with the attributes total, used and free, which are the amount of total, used and free space, in bytes. path may be a file or a directory.  New in version 3.3.   Changed in version 3.8: On Windows, path can now be a file or directory.  Availability: Unix, Windows. \n  \nshutil.chown(path, user=None, group=None)  \nChange owner user and\/or group of the given path. user can be a system user name or a uid; the same applies to group. At least one argument is required. See also os.chown(), the underlying function. Raises an auditing event shutil.chown with arguments path, user, group. Availability: Unix.  New in version 3.3.  \n  \nshutil.which(cmd, mode=os.F_OK | os.X_OK, path=None)  \nReturn the path to an executable which would be run if the given cmd was called. If no cmd would be called, return None. mode is a permission mask passed to os.access(), by default determining if the file exists and executable. When no path is specified, the results of os.environ() are used, returning either the \u201cPATH\u201d value or a fallback of os.defpath. On Windows, the current directory is always prepended to the path whether or not you use the default or provide your own, which is the behavior the command shell uses when finding executables. Additionally, when finding the cmd in the path, the PATHEXT environment variable is checked. For example, if you call shutil.which(\"python\"), which() will search PATHEXT to know that it should look for python.exe within the path directories. For example, on Windows: >>> shutil.which(\"python\")\n'C:\\\\Python33\\\\python.EXE'\n  New in version 3.3.   Changed in version 3.8: The bytes type is now accepted. If cmd type is bytes, the result type is also bytes.  \n  \nexception shutil.Error  \nThis exception collects exceptions that are raised during a multi-file operation. For copytree(), the exception argument is a list of 3-tuples (srcname, dstname, exception). \n Platform-dependent efficient copy operations Starting from Python 3.8, all functions involving a file copy (copyfile(), copy(), copy2(), copytree(), and move()) may use platform-specific \u201cfast-copy\u201d syscalls in order to copy the file more efficiently (see bpo-33671). \u201cfast-copy\u201d means that the copying operation occurs within the kernel, avoiding the use of userspace buffers in Python as in \u201coutfd.write(infd.read())\u201d. On macOS fcopyfile is used to copy the file content (not metadata). On Linux os.sendfile() is used. On Windows shutil.copyfile() uses a bigger default buffer size (1 MiB instead of 64 KiB) and a memoryview()-based variant of shutil.copyfileobj() is used. If the fast-copy operation fails and no data was written in the destination file then shutil will silently fallback on using less efficient copyfileobj() function internally.  Changed in version 3.8.  copytree example This example is the implementation of the copytree() function, described above, with the docstring omitted. It demonstrates many of the other functions provided by this module. def copytree(src, dst, symlinks=False):\n    names = os.listdir(src)\n    os.makedirs(dst)\n    errors = []\n    for name in names:\n        srcname = os.path.join(src, name)\n        dstname = os.path.join(dst, name)\n        try:\n            if symlinks and os.path.islink(srcname):\n                linkto = os.readlink(srcname)\n                os.symlink(linkto, dstname)\n            elif os.path.isdir(srcname):\n                copytree(srcname, dstname, symlinks)\n            else:\n                copy2(srcname, dstname)\n            # XXX What about devices, sockets etc.?\n        except OSError as why:\n            errors.append((srcname, dstname, str(why)))\n        # catch the Error from the recursive copytree so that we can\n        # continue with other files\n        except Error as err:\n            errors.extend(err.args[0])\n    try:\n        copystat(src, dst)\n    except OSError as why:\n        # can't copy file access times on Windows\n        if why.winerror is None:\n            errors.extend((src, dst, str(why)))\n    if errors:\n        raise Error(errors)\n Another example that uses the ignore_patterns() helper: from shutil import copytree, ignore_patterns\n\ncopytree(source, destination, ignore=ignore_patterns('*.pyc', 'tmp*'))\n This will copy everything except .pyc files and files or directories whose name starts with tmp. Another example that uses the ignore argument to add a logging call: from shutil import copytree\nimport logging\n\ndef _logpath(path, names):\n    logging.info('Working in %s', path)\n    return []   # nothing will be ignored\n\ncopytree(source, destination, ignore=_logpath)\n rmtree example This example shows how to remove a directory tree on Windows where some of the files have their read-only bit set. It uses the onerror callback to clear the readonly bit and reattempt the remove. Any subsequent failure will propagate. import os, stat\nimport shutil\n\ndef remove_readonly(func, path, _):\n    \"Clear the readonly bit and reattempt the removal\"\n    os.chmod(path, stat.S_IWRITE)\n    func(path)\n\nshutil.rmtree(directory, onerror=remove_readonly)\n Archiving operations  New in version 3.2.   Changed in version 3.5: Added support for the xztar format.  High-level utilities to create and read compressed and archived files are also provided. They rely on the zipfile and tarfile modules.  \nshutil.make_archive(base_name, format[, root_dir[, base_dir[, verbose[, dry_run[, owner[, group[, logger]]]]]]])  \nCreate an archive file (such as zip or tar) and return its name. base_name is the name of the file to create, including the path, minus any format-specific extension. format is the archive format: one of \u201czip\u201d (if the zlib module is available), \u201ctar\u201d, \u201cgztar\u201d (if the zlib module is available), \u201cbztar\u201d (if the bz2 module is available), or \u201cxztar\u201d (if the lzma module is available). root_dir is a directory that will be the root directory of the archive, all paths in the archive will be relative to it; for example, we typically chdir into root_dir before creating the archive. base_dir is the directory where we start archiving from; i.e. base_dir will be the common prefix of all files and directories in the archive. base_dir must be given relative to root_dir. See Archiving example with base_dir for how to use base_dir and root_dir together. root_dir and base_dir both default to the current directory. If dry_run is true, no archive is created, but the operations that would be executed are logged to logger. owner and group are used when creating a tar archive. By default, uses the current owner and group. logger must be an object compatible with PEP 282, usually an instance of logging.Logger. The verbose argument is unused and deprecated. Raises an auditing event shutil.make_archive with arguments base_name, format, root_dir, base_dir.  Changed in version 3.8: The modern pax (POSIX.1-2001) format is now used instead of the legacy GNU format for archives created with format=\"tar\".  \n  \nshutil.get_archive_formats()  \nReturn a list of supported formats for archiving. Each element of the returned sequence is a tuple (name, description). By default shutil provides these formats:  \nzip: ZIP file (if the zlib module is available). \ntar: Uncompressed tar file. Uses POSIX.1-2001 pax format for new archives. \ngztar: gzip\u2019ed tar-file (if the zlib module is available). \nbztar: bzip2\u2019ed tar-file (if the bz2 module is available). \nxztar: xz\u2019ed tar-file (if the lzma module is available).  You can register new formats or provide your own archiver for any existing formats, by using register_archive_format(). \n  \nshutil.register_archive_format(name, function[, extra_args[, description]])  \nRegister an archiver for the format name. function is the callable that will be used to unpack archives. The callable will receive the base_name of the file to create, followed by the base_dir (which defaults to os.curdir) to start archiving from. Further arguments are passed as keyword arguments: owner, group, dry_run and logger (as passed in make_archive()). If given, extra_args is a sequence of (name, value) pairs that will be used as extra keywords arguments when the archiver callable is used. description is used by get_archive_formats() which returns the list of archivers. Defaults to an empty string. \n  \nshutil.unregister_archive_format(name)  \nRemove the archive format name from the list of supported formats. \n  \nshutil.unpack_archive(filename[, extract_dir[, format]])  \nUnpack an archive. filename is the full path of the archive. extract_dir is the name of the target directory where the archive is unpacked. If not provided, the current working directory is used. format is the archive format: one of \u201czip\u201d, \u201ctar\u201d, \u201cgztar\u201d, \u201cbztar\u201d, or \u201cxztar\u201d. Or any other format registered with register_unpack_format(). If not provided, unpack_archive() will use the archive file name extension and see if an unpacker was registered for that extension. In case none is found, a ValueError is raised. Raises an auditing event shutil.unpack_archive with arguments filename, extract_dir, format.  Changed in version 3.7: Accepts a path-like object for filename and extract_dir.  \n  \nshutil.register_unpack_format(name, extensions, function[, extra_args[, description]])  \nRegisters an unpack format. name is the name of the format and extensions is a list of extensions corresponding to the format, like .zip for Zip files. function is the callable that will be used to unpack archives. The callable will receive the path of the archive, followed by the directory the archive must be extracted to. When provided, extra_args is a sequence of (name, value) tuples that will be passed as keywords arguments to the callable. description can be provided to describe the format, and will be returned by the get_unpack_formats() function. \n  \nshutil.unregister_unpack_format(name)  \nUnregister an unpack format. name is the name of the format. \n  \nshutil.get_unpack_formats()  \nReturn a list of all registered formats for unpacking. Each element of the returned sequence is a tuple (name, extensions, description). By default shutil provides these formats:  \nzip: ZIP file (unpacking compressed files works only if the corresponding module is available). \ntar: uncompressed tar file. \ngztar: gzip\u2019ed tar-file (if the zlib module is available). \nbztar: bzip2\u2019ed tar-file (if the bz2 module is available). \nxztar: xz\u2019ed tar-file (if the lzma module is available).  You can register new formats or provide your own unpacker for any existing formats, by using register_unpack_format(). \n Archiving example In this example, we create a gzip\u2019ed tar-file archive containing all files found in the .ssh directory of the user: >>> from shutil import make_archive\n>>> import os\n>>> archive_name = os.path.expanduser(os.path.join('~', 'myarchive'))\n>>> root_dir = os.path.expanduser(os.path.join('~', '.ssh'))\n>>> make_archive(archive_name, 'gztar', root_dir)\n'\/Users\/tarek\/myarchive.tar.gz'\n The resulting archive contains: $ tar -tzvf \/Users\/tarek\/myarchive.tar.gz\ndrwx------ tarek\/staff       0 2010-02-01 16:23:40 .\/\n-rw-r--r-- tarek\/staff     609 2008-06-09 13:26:54 .\/authorized_keys\n-rwxr-xr-x tarek\/staff      65 2008-06-09 13:26:54 .\/config\n-rwx------ tarek\/staff     668 2008-06-09 13:26:54 .\/id_dsa\n-rwxr-xr-x tarek\/staff     609 2008-06-09 13:26:54 .\/id_dsa.pub\n-rw------- tarek\/staff    1675 2008-06-09 13:26:54 .\/id_rsa\n-rw-r--r-- tarek\/staff     397 2008-06-09 13:26:54 .\/id_rsa.pub\n-rw-r--r-- tarek\/staff   37192 2010-02-06 18:23:10 .\/known_hosts\n Archiving example with base_dir\n In this example, similar to the one above, we show how to use make_archive(), but this time with the usage of base_dir. We now have the following directory structure: $ tree tmp\ntmp\n\u2514\u2500\u2500 root\n    \u2514\u2500\u2500 structure\n        \u251c\u2500\u2500 content\n            \u2514\u2500\u2500 please_add.txt\n        \u2514\u2500\u2500 do_not_add.txt\n In the final archive, please_add.txt should be included, but do_not_add.txt should not. Therefore we use the following: >>> from shutil import make_archive\n>>> import os\n>>> archive_name = os.path.expanduser(os.path.join('~', 'myarchive'))\n>>> make_archive(\n...     archive_name,\n...     'tar',\n...     root_dir='tmp\/root',\n...     base_dir='structure\/content',\n... )\n'\/Users\/tarek\/my_archive.tar'\n Listing the files in the resulting archive gives us: $ python -m tarfile -l \/Users\/tarek\/myarchive.tar\nstructure\/content\/\nstructure\/content\/please_add.txt\n Querying the size of the output terminal  \nshutil.get_terminal_size(fallback=(columns, lines))  \nGet the size of the terminal window. For each of the two dimensions, the environment variable, COLUMNS and LINES respectively, is checked. If the variable is defined and the value is a positive integer, it is used. When COLUMNS or LINES is not defined, which is the common case, the terminal connected to sys.__stdout__ is queried by invoking os.get_terminal_size(). If the terminal size cannot be successfully queried, either because the system doesn\u2019t support querying, or because we are not connected to a terminal, the value given in fallback parameter is used. fallback defaults to (80, 24) which is the default size used by many terminal emulators. The value returned is a named tuple of type os.terminal_size. See also: The Single UNIX Specification, Version 2, Other Environment Variables.  New in version 3.3.","title":"python.library.shutil"},{"text":"numpy.distutils.misc_util.exec_mod_from_location(modname, modfile)[source]\n \nUse importlib machinery to import a module modname from the file modfile. Depending on the spec.loader, the module may not be registered in sys.modules.","title":"numpy.reference.distutils.misc_util#numpy.distutils.misc_util.exec_mod_from_location"},{"text":"shutil.copy(src, dst, *, follow_symlinks=True)  \nCopies the file src to the file or directory dst. src and dst should be path-like objects or strings. If dst specifies a directory, the file will be copied into dst using the base filename from src. Returns the path to the newly created file. If follow_symlinks is false, and src is a symbolic link, dst will be created as a symbolic link. If follow_symlinks is true and src is a symbolic link, dst will be a copy of the file src refers to. copy() copies the file data and the file\u2019s permission mode (see os.chmod()). Other metadata, like the file\u2019s creation and modification times, is not preserved. To preserve all file metadata from the original, use copy2() instead. Raises an auditing event shutil.copyfile with arguments src, dst. Raises an auditing event shutil.copymode with arguments src, dst.  Changed in version 3.3: Added follow_symlinks argument. Now returns path to the newly created file.   Changed in version 3.8: Platform-specific fast-copy syscalls may be used internally in order to copy the file more efficiently. See Platform-dependent efficient copy operations section.","title":"python.library.shutil#shutil.copy"},{"text":"matplotlib.image.thumbnail(infile, thumbfile, scale=0.1, interpolation='bilinear', preview=False)[source]\n \nMake a thumbnail of image in infile with output filename thumbfile. See Image Thumbnail.  Parameters \n \ninfilestr or file-like\n\n\nThe image file. Matplotlib relies on Pillow for image reading, and thus supports a wide range of file formats, including PNG, JPG, TIFF and others.  \nthumbfilestr or file-like\n\n\nThe thumbnail filename.  \nscalefloat, default: 0.1\n\n\nThe scale factor for the thumbnail.  \ninterpolationstr, default: 'bilinear'\n\n\nThe interpolation scheme used in the resampling. See the interpolation parameter of imshow for possible values.  \npreviewbool, default: False\n\n\nIf True, the default backend (presumably a user interface backend) will be used which will cause a figure to be raised if show is called. If it is False, the figure is created using FigureCanvasBase and the drawing backend is selected as Figure.savefig would normally do.    Returns \n Figure\n\nThe figure instance containing the thumbnail.","title":"matplotlib.image_api#matplotlib.image.thumbnail"}]}
{"task_id":1555968,"prompt":"def f_1555968(x):\n\treturn ","suffix":"","canonical_solution":"max(k for k, v in x.items() if v != 0)","test_start":"\ndef check(candidate):","test":["\n    assert candidate({'a': 1, 'b': 2, 'c': 2000}) == 'c'\n","\n    assert candidate({'a': 0., 'b': 0, 'c': 200.02}) == 'c'\n","\n    assert candidate({'key1': -100, 'key2': 0.}) == 'key1'\n","\n    x = {1:\"g\", 2:\"a\", 5:\"er\", -4:\"dr\"}\n    assert candidate(x) == 5\n"],"entry_point":"f_1555968","intent":"find the key associated with the largest value in dictionary `x` whilst key is non-zero value","library":[],"docs":[]}
{"task_id":17021863,"prompt":"def f_17021863(file):\n\t","suffix":"\n\treturn ","canonical_solution":"file.seek(0)","test_start":"\ndef check(candidate):","test":["\n    with open ('a.txt', 'w') as f:\n        f.write('kangaroo\\nkoala\\noxford\\n')\n    f = open('a.txt', 'r')\n    f.read()\n    candidate(f)\n    assert f.readline() == 'kangaroo\\n'\n"],"entry_point":"f_17021863","intent":"Put the curser at beginning of the file","library":[],"docs":[]}
{"task_id":38152389,"prompt":"def f_38152389(df):\n\t","suffix":"\n\treturn df","canonical_solution":"df['c'] = np.where(df['a'].isnull, df['b'], df['a'])","test_start":"\nimport numpy as np \nimport pandas as pd \n\ndef check(candidate):","test":["\n    df = pd.DataFrame({'a': [1,2,3], 'b': [0,0,0]})\n    assert np.allclose(candidate(df), pd.DataFrame({'a': [1,2,3], 'b': [0,0,0], 'c': [0,0,0]}))\n","\n    df = pd.DataFrame({'a': [0,2,3], 'b': [4,5,6]})\n    assert np.allclose(candidate(df), pd.DataFrame({'a': [0,2,3], 'b': [4,5,6], 'c': [4,5,6]}))\n"],"entry_point":"f_38152389","intent":"combine values from column 'b' and column 'a' of dataframe `df`  into column 'c' of datafram `df`","library":["numpy","pandas"],"docs":[{"text":"pandas.DataFrame.combine_first   DataFrame.combine_first(other)[source]\n \nUpdate null elements with value in the same location in other. Combine two DataFrame objects by filling null values in one DataFrame with non-null values from other DataFrame. The row and column indexes of the resulting DataFrame will be the union of the two.  Parameters \n \nother:DataFrame\n\n\nProvided DataFrame to use to fill null values.    Returns \n DataFrame\n\nThe result of combining the provided DataFrame with the other object.      See also  DataFrame.combine\n\nPerform series-wise operation on two DataFrames using a given function.    Examples \n>>> df1 = pd.DataFrame({'A': [None, 0], 'B': [None, 4]})\n>>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})\n>>> df1.combine_first(df2)\n     A    B\n0  1.0  3.0\n1  0.0  4.0\n  Null values still persist if the location of that null value does not exist in other \n>>> df1 = pd.DataFrame({'A': [None, 0], 'B': [4, None]})\n>>> df2 = pd.DataFrame({'B': [3, 3], 'C': [1, 1]}, index=[1, 2])\n>>> df1.combine_first(df2)\n     A    B    C\n0  NaN  4.0  NaN\n1  0.0  3.0  1.0\n2  NaN  3.0  1.0","title":"pandas.reference.api.pandas.dataframe.combine_first"},{"text":"pandas.DataFrame.combine   DataFrame.combine(other, func, fill_value=None, overwrite=True)[source]\n \nPerform column-wise combine with another DataFrame. Combines a DataFrame with other DataFrame using func to element-wise combine columns. The row and column indexes of the resulting DataFrame will be the union of the two.  Parameters \n \nother:DataFrame\n\n\nThe DataFrame to merge column-wise.  \nfunc:function\n\n\nFunction that takes two series as inputs and return a Series or a scalar. Used to merge the two dataframes column by columns.  \nfill_value:scalar value, default None\n\n\nThe value to fill NaNs with prior to passing any column to the merge func.  \noverwrite:bool, default True\n\n\nIf True, columns in self that do not exist in other will be overwritten with NaNs.    Returns \n DataFrame\n\nCombination of the provided DataFrames.      See also  DataFrame.combine_first\n\nCombine two DataFrame objects and default to non-null values in frame calling the method.    Examples Combine using a simple function that chooses the smaller column. \n>>> df1 = pd.DataFrame({'A': [0, 0], 'B': [4, 4]})\n>>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})\n>>> take_smaller = lambda s1, s2: s1 if s1.sum() < s2.sum() else s2\n>>> df1.combine(df2, take_smaller)\n   A  B\n0  0  3\n1  0  3\n  Example using a true element-wise combine function. \n>>> df1 = pd.DataFrame({'A': [5, 0], 'B': [2, 4]})\n>>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})\n>>> df1.combine(df2, np.minimum)\n   A  B\n0  1  2\n1  0  3\n  Using fill_value fills Nones prior to passing the column to the merge function. \n>>> df1 = pd.DataFrame({'A': [0, 0], 'B': [None, 4]})\n>>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})\n>>> df1.combine(df2, take_smaller, fill_value=-5)\n   A    B\n0  0 -5.0\n1  0  4.0\n  However, if the same element in both dataframes is None, that None is preserved \n>>> df1 = pd.DataFrame({'A': [0, 0], 'B': [None, 4]})\n>>> df2 = pd.DataFrame({'A': [1, 1], 'B': [None, 3]})\n>>> df1.combine(df2, take_smaller, fill_value=-5)\n    A    B\n0  0 -5.0\n1  0  3.0\n  Example that demonstrates the use of overwrite and behavior when the axis differ between the dataframes. \n>>> df1 = pd.DataFrame({'A': [0, 0], 'B': [4, 4]})\n>>> df2 = pd.DataFrame({'B': [3, 3], 'C': [-10, 1], }, index=[1, 2])\n>>> df1.combine(df2, take_smaller)\n     A    B     C\n0  NaN  NaN   NaN\n1  NaN  3.0 -10.0\n2  NaN  3.0   1.0\n  \n>>> df1.combine(df2, take_smaller, overwrite=False)\n     A    B     C\n0  0.0  NaN   NaN\n1  0.0  3.0 -10.0\n2  NaN  3.0   1.0\n  Demonstrating the preference of the passed in dataframe. \n>>> df2 = pd.DataFrame({'B': [3, 3], 'C': [1, 1], }, index=[1, 2])\n>>> df2.combine(df1, take_smaller)\n   A    B   C\n0  0.0  NaN NaN\n1  0.0  3.0 NaN\n2  NaN  3.0 NaN\n  \n>>> df2.combine(df1, take_smaller, overwrite=False)\n     A    B   C\n0  0.0  NaN NaN\n1  0.0  3.0 1.0\n2  NaN  3.0 1.0","title":"pandas.reference.api.pandas.dataframe.combine"},{"text":"Cookbook This is a repository for short and sweet examples and links for useful pandas recipes. We encourage users to add to this documentation. Adding interesting links and\/or inline examples to this section is a great First Pull Request. Simplified, condensed, new-user friendly, in-line examples have been inserted where possible to augment the Stack-Overflow and GitHub links. Many of the links contain expanded information, above what the in-line examples offer. pandas (pd) and NumPy (np) are the only two abbreviated imported modules. The rest are kept explicitly imported for newer users.  Idioms These are some neat pandas idioms if-then\/if-then-else on one column, and assignment to another one or more columns: \nIn [1]: df = pd.DataFrame(\n   ...:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ...: )\n   ...: \n\nIn [2]: df\nOut[2]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n   if-then\u2026 An if-then on one column \nIn [3]: df.loc[df.AAA >= 5, \"BBB\"] = -1\n\nIn [4]: df\nOut[4]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   -1   50\n2    6   -1  -30\n3    7   -1  -50\n  An if-then with assignment to 2 columns: \nIn [5]: df.loc[df.AAA >= 5, [\"BBB\", \"CCC\"]] = 555\n\nIn [6]: df\nOut[6]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5  555  555\n2    6  555  555\n3    7  555  555\n  Add another line with different logic, to do the -else \nIn [7]: df.loc[df.AAA < 5, [\"BBB\", \"CCC\"]] = 2000\n\nIn [8]: df\nOut[8]: \n   AAA   BBB   CCC\n0    4  2000  2000\n1    5   555   555\n2    6   555   555\n3    7   555   555\n  Or use pandas where after you\u2019ve set up a mask \nIn [9]: df_mask = pd.DataFrame(\n   ...:     {\"AAA\": [True] * 4, \"BBB\": [False] * 4, \"CCC\": [True, False] * 2}\n   ...: )\n   ...: \n\nIn [10]: df.where(df_mask, -1000)\nOut[10]: \n   AAA   BBB   CCC\n0    4 -1000  2000\n1    5 -1000 -1000\n2    6 -1000   555\n3    7 -1000 -1000\n  if-then-else using NumPy\u2019s where() \nIn [11]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [12]: df\nOut[12]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [13]: df[\"logic\"] = np.where(df[\"AAA\"] > 5, \"high\", \"low\")\n\nIn [14]: df\nOut[14]: \n   AAA  BBB  CCC logic\n0    4   10  100   low\n1    5   20   50   low\n2    6   30  -30  high\n3    7   40  -50  high\n    Splitting Split a frame with a boolean criterion \nIn [15]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [16]: df\nOut[16]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [17]: df[df.AAA <= 5]\nOut[17]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n\nIn [18]: df[df.AAA > 5]\nOut[18]: \n   AAA  BBB  CCC\n2    6   30  -30\n3    7   40  -50\n    Building criteria Select with multi-column criteria \nIn [19]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [20]: df\nOut[20]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n  \u2026and (without assignment returns a Series) \nIn [21]: df.loc[(df[\"BBB\"] < 25) & (df[\"CCC\"] >= -40), \"AAA\"]\nOut[21]: \n0    4\n1    5\nName: AAA, dtype: int64\n  \u2026or (without assignment returns a Series) \nIn [22]: df.loc[(df[\"BBB\"] > 25) | (df[\"CCC\"] >= -40), \"AAA\"]\nOut[22]: \n0    4\n1    5\n2    6\n3    7\nName: AAA, dtype: int64\n  \u2026or (with assignment modifies the DataFrame.) \nIn [23]: df.loc[(df[\"BBB\"] > 25) | (df[\"CCC\"] >= 75), \"AAA\"] = 0.1\n\nIn [24]: df\nOut[24]: \n   AAA  BBB  CCC\n0  0.1   10  100\n1  5.0   20   50\n2  0.1   30  -30\n3  0.1   40  -50\n  Select rows with data closest to certain value using argsort \nIn [25]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [26]: df\nOut[26]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [27]: aValue = 43.0\n\nIn [28]: df.loc[(df.CCC - aValue).abs().argsort()]\nOut[28]: \n   AAA  BBB  CCC\n1    5   20   50\n0    4   10  100\n2    6   30  -30\n3    7   40  -50\n  Dynamically reduce a list of criteria using a binary operators \nIn [29]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [30]: df\nOut[30]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [31]: Crit1 = df.AAA <= 5.5\n\nIn [32]: Crit2 = df.BBB == 10.0\n\nIn [33]: Crit3 = df.CCC > -40.0\n  One could hard code: \nIn [34]: AllCrit = Crit1 & Crit2 & Crit3\n  \u2026Or it can be done with a list of dynamically built criteria \nIn [35]: import functools\n\nIn [36]: CritList = [Crit1, Crit2, Crit3]\n\nIn [37]: AllCrit = functools.reduce(lambda x, y: x & y, CritList)\n\nIn [38]: df[AllCrit]\nOut[38]: \n   AAA  BBB  CCC\n0    4   10  100\n     Selection  Dataframes The indexing docs. Using both row labels and value conditionals \nIn [39]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [40]: df\nOut[40]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [41]: df[(df.AAA <= 6) & (df.index.isin([0, 2, 4]))]\nOut[41]: \n   AAA  BBB  CCC\n0    4   10  100\n2    6   30  -30\n  Use loc for label-oriented slicing and iloc positional slicing GH2904 \nIn [42]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]},\n   ....:     index=[\"foo\", \"bar\", \"boo\", \"kar\"],\n   ....: )\n   ....: \n  There are 2 explicit slicing methods, with a third general case  Positional-oriented (Python slicing style : exclusive of end) Label-oriented (Non-Python slicing style : inclusive of end) General (Either slicing style : depends on if the slice contains labels or positions)  \nIn [43]: df.loc[\"bar\":\"kar\"]  # Label\nOut[43]: \n     AAA  BBB  CCC\nbar    5   20   50\nboo    6   30  -30\nkar    7   40  -50\n\n# Generic\nIn [44]: df[0:3]\nOut[44]: \n     AAA  BBB  CCC\nfoo    4   10  100\nbar    5   20   50\nboo    6   30  -30\n\nIn [45]: df[\"bar\":\"kar\"]\nOut[45]: \n     AAA  BBB  CCC\nbar    5   20   50\nboo    6   30  -30\nkar    7   40  -50\n  Ambiguity arises when an index consists of integers with a non-zero start or non-unit increment. \nIn [46]: data = {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n\nIn [47]: df2 = pd.DataFrame(data=data, index=[1, 2, 3, 4])  # Note index starts at 1.\n\nIn [48]: df2.iloc[1:3]  # Position-oriented\nOut[48]: \n   AAA  BBB  CCC\n2    5   20   50\n3    6   30  -30\n\nIn [49]: df2.loc[1:3]  # Label-oriented\nOut[49]: \n   AAA  BBB  CCC\n1    4   10  100\n2    5   20   50\n3    6   30  -30\n  Using inverse operator (~) to take the complement of a mask \nIn [50]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [51]: df\nOut[51]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [52]: df[~((df.AAA <= 6) & (df.index.isin([0, 2, 4])))]\nOut[52]: \n   AAA  BBB  CCC\n1    5   20   50\n3    7   40  -50\n    New columns Efficiently and dynamically creating new columns using applymap \nIn [53]: df = pd.DataFrame({\"AAA\": [1, 2, 1, 3], \"BBB\": [1, 1, 2, 2], \"CCC\": [2, 1, 3, 1]})\n\nIn [54]: df\nOut[54]: \n   AAA  BBB  CCC\n0    1    1    2\n1    2    1    1\n2    1    2    3\n3    3    2    1\n\nIn [55]: source_cols = df.columns  # Or some subset would work too\n\nIn [56]: new_cols = [str(x) + \"_cat\" for x in source_cols]\n\nIn [57]: categories = {1: \"Alpha\", 2: \"Beta\", 3: \"Charlie\"}\n\nIn [58]: df[new_cols] = df[source_cols].applymap(categories.get)\n\nIn [59]: df\nOut[59]: \n   AAA  BBB  CCC  AAA_cat BBB_cat  CCC_cat\n0    1    1    2    Alpha   Alpha     Beta\n1    2    1    1     Beta   Alpha    Alpha\n2    1    2    3    Alpha    Beta  Charlie\n3    3    2    1  Charlie    Beta    Alpha\n  Keep other columns when using min() with groupby \nIn [60]: df = pd.DataFrame(\n   ....:     {\"AAA\": [1, 1, 1, 2, 2, 2, 3, 3], \"BBB\": [2, 1, 3, 4, 5, 1, 2, 3]}\n   ....: )\n   ....: \n\nIn [61]: df\nOut[61]: \n   AAA  BBB\n0    1    2\n1    1    1\n2    1    3\n3    2    4\n4    2    5\n5    2    1\n6    3    2\n7    3    3\n  Method 1 : idxmin() to get the index of the minimums \nIn [62]: df.loc[df.groupby(\"AAA\")[\"BBB\"].idxmin()]\nOut[62]: \n   AAA  BBB\n1    1    1\n5    2    1\n6    3    2\n  Method 2 : sort then take first of each \nIn [63]: df.sort_values(by=\"BBB\").groupby(\"AAA\", as_index=False).first()\nOut[63]: \n   AAA  BBB\n0    1    1\n1    2    1\n2    3    2\n  Notice the same results, with the exception of the index.    Multiindexing The multindexing docs. Creating a MultiIndex from a labeled frame \nIn [64]: df = pd.DataFrame(\n   ....:     {\n   ....:         \"row\": [0, 1, 2],\n   ....:         \"One_X\": [1.1, 1.1, 1.1],\n   ....:         \"One_Y\": [1.2, 1.2, 1.2],\n   ....:         \"Two_X\": [1.11, 1.11, 1.11],\n   ....:         \"Two_Y\": [1.22, 1.22, 1.22],\n   ....:     }\n   ....: )\n   ....: \n\nIn [65]: df\nOut[65]: \n   row  One_X  One_Y  Two_X  Two_Y\n0    0    1.1    1.2   1.11   1.22\n1    1    1.1    1.2   1.11   1.22\n2    2    1.1    1.2   1.11   1.22\n\n# As Labelled Index\nIn [66]: df = df.set_index(\"row\")\n\nIn [67]: df\nOut[67]: \n     One_X  One_Y  Two_X  Two_Y\nrow                            \n0      1.1    1.2   1.11   1.22\n1      1.1    1.2   1.11   1.22\n2      1.1    1.2   1.11   1.22\n\n# With Hierarchical Columns\nIn [68]: df.columns = pd.MultiIndex.from_tuples([tuple(c.split(\"_\")) for c in df.columns])\n\nIn [69]: df\nOut[69]: \n     One        Two      \n       X    Y     X     Y\nrow                      \n0    1.1  1.2  1.11  1.22\n1    1.1  1.2  1.11  1.22\n2    1.1  1.2  1.11  1.22\n\n# Now stack & Reset\nIn [70]: df = df.stack(0).reset_index(1)\n\nIn [71]: df\nOut[71]: \n    level_1     X     Y\nrow                    \n0       One  1.10  1.20\n0       Two  1.11  1.22\n1       One  1.10  1.20\n1       Two  1.11  1.22\n2       One  1.10  1.20\n2       Two  1.11  1.22\n\n# And fix the labels (Notice the label 'level_1' got added automatically)\nIn [72]: df.columns = [\"Sample\", \"All_X\", \"All_Y\"]\n\nIn [73]: df\nOut[73]: \n    Sample  All_X  All_Y\nrow                     \n0      One   1.10   1.20\n0      Two   1.11   1.22\n1      One   1.10   1.20\n1      Two   1.11   1.22\n2      One   1.10   1.20\n2      Two   1.11   1.22\n   Arithmetic Performing arithmetic with a MultiIndex that needs broadcasting \nIn [74]: cols = pd.MultiIndex.from_tuples(\n   ....:     [(x, y) for x in [\"A\", \"B\", \"C\"] for y in [\"O\", \"I\"]]\n   ....: )\n   ....: \n\nIn [75]: df = pd.DataFrame(np.random.randn(2, 6), index=[\"n\", \"m\"], columns=cols)\n\nIn [76]: df\nOut[76]: \n          A                   B                   C          \n          O         I         O         I         O         I\nn  0.469112 -0.282863 -1.509059 -1.135632  1.212112 -0.173215\nm  0.119209 -1.044236 -0.861849 -2.104569 -0.494929  1.071804\n\nIn [77]: df = df.div(df[\"C\"], level=1)\n\nIn [78]: df\nOut[78]: \n          A                   B              C     \n          O         I         O         I    O    I\nn  0.387021  1.633022 -1.244983  6.556214  1.0  1.0\nm -0.240860 -0.974279  1.741358 -1.963577  1.0  1.0\n    Slicing Slicing a MultiIndex with xs \nIn [79]: coords = [(\"AA\", \"one\"), (\"AA\", \"six\"), (\"BB\", \"one\"), (\"BB\", \"two\"), (\"BB\", \"six\")]\n\nIn [80]: index = pd.MultiIndex.from_tuples(coords)\n\nIn [81]: df = pd.DataFrame([11, 22, 33, 44, 55], index, [\"MyData\"])\n\nIn [82]: df\nOut[82]: \n        MyData\nAA one      11\n   six      22\nBB one      33\n   two      44\n   six      55\n  To take the cross section of the 1st level and 1st axis the index: \n# Note : level and axis are optional, and default to zero\nIn [83]: df.xs(\"BB\", level=0, axis=0)\nOut[83]: \n     MyData\none      33\ntwo      44\nsix      55\n  \u2026and now the 2nd level of the 1st axis. \nIn [84]: df.xs(\"six\", level=1, axis=0)\nOut[84]: \n    MyData\nAA      22\nBB      55\n  Slicing a MultiIndex with xs, method #2 \nIn [85]: import itertools\n\nIn [86]: index = list(itertools.product([\"Ada\", \"Quinn\", \"Violet\"], [\"Comp\", \"Math\", \"Sci\"]))\n\nIn [87]: headr = list(itertools.product([\"Exams\", \"Labs\"], [\"I\", \"II\"]))\n\nIn [88]: indx = pd.MultiIndex.from_tuples(index, names=[\"Student\", \"Course\"])\n\nIn [89]: cols = pd.MultiIndex.from_tuples(headr)  # Notice these are un-named\n\nIn [90]: data = [[70 + x + y + (x * y) % 3 for x in range(4)] for y in range(9)]\n\nIn [91]: df = pd.DataFrame(data, indx, cols)\n\nIn [92]: df\nOut[92]: \n               Exams     Labs    \n                   I  II    I  II\nStudent Course                   \nAda     Comp      70  71   72  73\n        Math      71  73   75  74\n        Sci       72  75   75  75\nQuinn   Comp      73  74   75  76\n        Math      74  76   78  77\n        Sci       75  78   78  78\nViolet  Comp      76  77   78  79\n        Math      77  79   81  80\n        Sci       78  81   81  81\n\nIn [93]: All = slice(None)\n\nIn [94]: df.loc[\"Violet\"]\nOut[94]: \n       Exams     Labs    \n           I  II    I  II\nCourse                   \nComp      76  77   78  79\nMath      77  79   81  80\nSci       78  81   81  81\n\nIn [95]: df.loc[(All, \"Math\"), All]\nOut[95]: \n               Exams     Labs    \n                   I  II    I  II\nStudent Course                   \nAda     Math      71  73   75  74\nQuinn   Math      74  76   78  77\nViolet  Math      77  79   81  80\n\nIn [96]: df.loc[(slice(\"Ada\", \"Quinn\"), \"Math\"), All]\nOut[96]: \n               Exams     Labs    \n                   I  II    I  II\nStudent Course                   \nAda     Math      71  73   75  74\nQuinn   Math      74  76   78  77\n\nIn [97]: df.loc[(All, \"Math\"), (\"Exams\")]\nOut[97]: \n                 I  II\nStudent Course        \nAda     Math    71  73\nQuinn   Math    74  76\nViolet  Math    77  79\n\nIn [98]: df.loc[(All, \"Math\"), (All, \"II\")]\nOut[98]: \n               Exams Labs\n                  II   II\nStudent Course           \nAda     Math      73   74\nQuinn   Math      76   77\nViolet  Math      79   80\n  Setting portions of a MultiIndex with xs   Sorting Sort by specific column or an ordered list of columns, with a MultiIndex \nIn [99]: df.sort_values(by=(\"Labs\", \"II\"), ascending=False)\nOut[99]: \n               Exams     Labs    \n                   I  II    I  II\nStudent Course                   \nViolet  Sci       78  81   81  81\n        Math      77  79   81  80\n        Comp      76  77   78  79\nQuinn   Sci       75  78   78  78\n        Math      74  76   78  77\n        Comp      73  74   75  76\nAda     Sci       72  75   75  75\n        Math      71  73   75  74\n        Comp      70  71   72  73\n  Partial selection, the need for sortedness GH2995   Levels Prepending a level to a multiindex Flatten Hierarchical columns    Missing data The missing data docs. Fill forward a reversed timeseries \nIn [100]: df = pd.DataFrame(\n   .....:     np.random.randn(6, 1),\n   .....:     index=pd.date_range(\"2013-08-01\", periods=6, freq=\"B\"),\n   .....:     columns=list(\"A\"),\n   .....: )\n   .....: \n\nIn [101]: df.loc[df.index[3], \"A\"] = np.nan\n\nIn [102]: df\nOut[102]: \n                   A\n2013-08-01  0.721555\n2013-08-02 -0.706771\n2013-08-05 -1.039575\n2013-08-06       NaN\n2013-08-07 -0.424972\n2013-08-08  0.567020\n\nIn [103]: df.reindex(df.index[::-1]).ffill()\nOut[103]: \n                   A\n2013-08-08  0.567020\n2013-08-07 -0.424972\n2013-08-06 -0.424972\n2013-08-05 -1.039575\n2013-08-02 -0.706771\n2013-08-01  0.721555\n  cumsum reset at NaN values  Replace Using replace with backrefs    Grouping The grouping docs. Basic grouping with apply Unlike agg, apply\u2019s callable is passed a sub-DataFrame which gives you access to all the columns \nIn [104]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"animal\": \"cat dog cat fish dog cat cat\".split(),\n   .....:         \"size\": list(\"SSMMMLL\"),\n   .....:         \"weight\": [8, 10, 11, 1, 20, 12, 12],\n   .....:         \"adult\": [False] * 5 + [True] * 2,\n   .....:     }\n   .....: )\n   .....: \n\nIn [105]: df\nOut[105]: \n  animal size  weight  adult\n0    cat    S       8  False\n1    dog    S      10  False\n2    cat    M      11  False\n3   fish    M       1  False\n4    dog    M      20  False\n5    cat    L      12   True\n6    cat    L      12   True\n\n# List the size of the animals with the highest weight.\nIn [106]: df.groupby(\"animal\").apply(lambda subf: subf[\"size\"][subf[\"weight\"].idxmax()])\nOut[106]: \nanimal\ncat     L\ndog     M\nfish    M\ndtype: object\n  Using get_group \nIn [107]: gb = df.groupby([\"animal\"])\n\nIn [108]: gb.get_group(\"cat\")\nOut[108]: \n  animal size  weight  adult\n0    cat    S       8  False\n2    cat    M      11  False\n5    cat    L      12   True\n6    cat    L      12   True\n  Apply to different items in a group \nIn [109]: def GrowUp(x):\n   .....:     avg_weight = sum(x[x[\"size\"] == \"S\"].weight * 1.5)\n   .....:     avg_weight += sum(x[x[\"size\"] == \"M\"].weight * 1.25)\n   .....:     avg_weight += sum(x[x[\"size\"] == \"L\"].weight)\n   .....:     avg_weight \/= len(x)\n   .....:     return pd.Series([\"L\", avg_weight, True], index=[\"size\", \"weight\", \"adult\"])\n   .....: \n\nIn [110]: expected_df = gb.apply(GrowUp)\n\nIn [111]: expected_df\nOut[111]: \n       size   weight  adult\nanimal                     \ncat       L  12.4375   True\ndog       L  20.0000   True\nfish      L   1.2500   True\n  Expanding apply \nIn [112]: S = pd.Series([i \/ 100.0 for i in range(1, 11)])\n\nIn [113]: def cum_ret(x, y):\n   .....:     return x * (1 + y)\n   .....: \n\nIn [114]: def red(x):\n   .....:     return functools.reduce(cum_ret, x, 1.0)\n   .....: \n\nIn [115]: S.expanding().apply(red, raw=True)\nOut[115]: \n0    1.010000\n1    1.030200\n2    1.061106\n3    1.103550\n4    1.158728\n5    1.228251\n6    1.314229\n7    1.419367\n8    1.547110\n9    1.701821\ndtype: float64\n  Replacing some values with mean of the rest of a group \nIn [116]: df = pd.DataFrame({\"A\": [1, 1, 2, 2], \"B\": [1, -1, 1, 2]})\n\nIn [117]: gb = df.groupby(\"A\")\n\nIn [118]: def replace(g):\n   .....:     mask = g < 0\n   .....:     return g.where(mask, g[~mask].mean())\n   .....: \n\nIn [119]: gb.transform(replace)\nOut[119]: \n     B\n0  1.0\n1 -1.0\n2  1.5\n3  1.5\n  Sort groups by aggregated data \nIn [120]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"code\": [\"foo\", \"bar\", \"baz\"] * 2,\n   .....:         \"data\": [0.16, -0.21, 0.33, 0.45, -0.59, 0.62],\n   .....:         \"flag\": [False, True] * 3,\n   .....:     }\n   .....: )\n   .....: \n\nIn [121]: code_groups = df.groupby(\"code\")\n\nIn [122]: agg_n_sort_order = code_groups[[\"data\"]].transform(sum).sort_values(by=\"data\")\n\nIn [123]: sorted_df = df.loc[agg_n_sort_order.index]\n\nIn [124]: sorted_df\nOut[124]: \n  code  data   flag\n1  bar -0.21   True\n4  bar -0.59  False\n0  foo  0.16  False\n3  foo  0.45   True\n2  baz  0.33  False\n5  baz  0.62   True\n  Create multiple aggregated columns \nIn [125]: rng = pd.date_range(start=\"2014-10-07\", periods=10, freq=\"2min\")\n\nIn [126]: ts = pd.Series(data=list(range(10)), index=rng)\n\nIn [127]: def MyCust(x):\n   .....:     if len(x) > 2:\n   .....:         return x[1] * 1.234\n   .....:     return pd.NaT\n   .....: \n\nIn [128]: mhc = {\"Mean\": np.mean, \"Max\": np.max, \"Custom\": MyCust}\n\nIn [129]: ts.resample(\"5min\").apply(mhc)\nOut[129]: \n                     Mean  Max Custom\n2014-10-07 00:00:00   1.0    2  1.234\n2014-10-07 00:05:00   3.5    4    NaT\n2014-10-07 00:10:00   6.0    7  7.404\n2014-10-07 00:15:00   8.5    9    NaT\n\nIn [130]: ts\nOut[130]: \n2014-10-07 00:00:00    0\n2014-10-07 00:02:00    1\n2014-10-07 00:04:00    2\n2014-10-07 00:06:00    3\n2014-10-07 00:08:00    4\n2014-10-07 00:10:00    5\n2014-10-07 00:12:00    6\n2014-10-07 00:14:00    7\n2014-10-07 00:16:00    8\n2014-10-07 00:18:00    9\nFreq: 2T, dtype: int64\n  Create a value counts column and reassign back to the DataFrame \nIn [131]: df = pd.DataFrame(\n   .....:     {\"Color\": \"Red Red Red Blue\".split(), \"Value\": [100, 150, 50, 50]}\n   .....: )\n   .....: \n\nIn [132]: df\nOut[132]: \n  Color  Value\n0   Red    100\n1   Red    150\n2   Red     50\n3  Blue     50\n\nIn [133]: df[\"Counts\"] = df.groupby([\"Color\"]).transform(len)\n\nIn [134]: df\nOut[134]: \n  Color  Value  Counts\n0   Red    100       3\n1   Red    150       3\n2   Red     50       3\n3  Blue     50       1\n  Shift groups of the values in a column based on the index \nIn [135]: df = pd.DataFrame(\n   .....:     {\"line_race\": [10, 10, 8, 10, 10, 8], \"beyer\": [99, 102, 103, 103, 88, 100]},\n   .....:     index=[\n   .....:         \"Last Gunfighter\",\n   .....:         \"Last Gunfighter\",\n   .....:         \"Last Gunfighter\",\n   .....:         \"Paynter\",\n   .....:         \"Paynter\",\n   .....:         \"Paynter\",\n   .....:     ],\n   .....: )\n   .....: \n\nIn [136]: df\nOut[136]: \n                 line_race  beyer\nLast Gunfighter         10     99\nLast Gunfighter         10    102\nLast Gunfighter          8    103\nPaynter                 10    103\nPaynter                 10     88\nPaynter                  8    100\n\nIn [137]: df[\"beyer_shifted\"] = df.groupby(level=0)[\"beyer\"].shift(1)\n\nIn [138]: df\nOut[138]: \n                 line_race  beyer  beyer_shifted\nLast Gunfighter         10     99            NaN\nLast Gunfighter         10    102           99.0\nLast Gunfighter          8    103          102.0\nPaynter                 10    103            NaN\nPaynter                 10     88          103.0\nPaynter                  8    100           88.0\n  Select row with maximum value from each group \nIn [139]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"host\": [\"other\", \"other\", \"that\", \"this\", \"this\"],\n   .....:         \"service\": [\"mail\", \"web\", \"mail\", \"mail\", \"web\"],\n   .....:         \"no\": [1, 2, 1, 2, 1],\n   .....:     }\n   .....: ).set_index([\"host\", \"service\"])\n   .....: \n\nIn [140]: mask = df.groupby(level=0).agg(\"idxmax\")\n\nIn [141]: df_count = df.loc[mask[\"no\"]].reset_index()\n\nIn [142]: df_count\nOut[142]: \n    host service  no\n0  other     web   2\n1   that    mail   1\n2   this    mail   2\n  Grouping like Python\u2019s itertools.groupby \nIn [143]: df = pd.DataFrame([0, 1, 0, 1, 1, 1, 0, 1, 1], columns=[\"A\"])\n\nIn [144]: df[\"A\"].groupby((df[\"A\"] != df[\"A\"].shift()).cumsum()).groups\nOut[144]: {1: [0], 2: [1], 3: [2], 4: [3, 4, 5], 5: [6], 6: [7, 8]}\n\nIn [145]: df[\"A\"].groupby((df[\"A\"] != df[\"A\"].shift()).cumsum()).cumsum()\nOut[145]: \n0    0\n1    1\n2    0\n3    1\n4    2\n5    3\n6    0\n7    1\n8    2\nName: A, dtype: int64\n   Expanding data Alignment and to-date Rolling Computation window based on values instead of counts Rolling Mean by Time Interval   Splitting Splitting a frame Create a list of dataframes, split using a delineation based on logic included in rows. \nIn [146]: df = pd.DataFrame(\n   .....:     data={\n   .....:         \"Case\": [\"A\", \"A\", \"A\", \"B\", \"A\", \"A\", \"B\", \"A\", \"A\"],\n   .....:         \"Data\": np.random.randn(9),\n   .....:     }\n   .....: )\n   .....: \n\nIn [147]: dfs = list(\n   .....:     zip(\n   .....:         *df.groupby(\n   .....:             (1 * (df[\"Case\"] == \"B\"))\n   .....:             .cumsum()\n   .....:             .rolling(window=3, min_periods=1)\n   .....:             .median()\n   .....:         )\n   .....:     )\n   .....: )[-1]\n   .....: \n\nIn [148]: dfs[0]\nOut[148]: \n  Case      Data\n0    A  0.276232\n1    A -1.087401\n2    A -0.673690\n3    B  0.113648\n\nIn [149]: dfs[1]\nOut[149]: \n  Case      Data\n4    A -1.478427\n5    A  0.524988\n6    B  0.404705\n\nIn [150]: dfs[2]\nOut[150]: \n  Case      Data\n7    A  0.577046\n8    A -1.715002\n    Pivot The Pivot docs. Partial sums and subtotals \nIn [151]: df = pd.DataFrame(\n   .....:     data={\n   .....:         \"Province\": [\"ON\", \"QC\", \"BC\", \"AL\", \"AL\", \"MN\", \"ON\"],\n   .....:         \"City\": [\n   .....:             \"Toronto\",\n   .....:             \"Montreal\",\n   .....:             \"Vancouver\",\n   .....:             \"Calgary\",\n   .....:             \"Edmonton\",\n   .....:             \"Winnipeg\",\n   .....:             \"Windsor\",\n   .....:         ],\n   .....:         \"Sales\": [13, 6, 16, 8, 4, 3, 1],\n   .....:     }\n   .....: )\n   .....: \n\nIn [152]: table = pd.pivot_table(\n   .....:     df,\n   .....:     values=[\"Sales\"],\n   .....:     index=[\"Province\"],\n   .....:     columns=[\"City\"],\n   .....:     aggfunc=np.sum,\n   .....:     margins=True,\n   .....: )\n   .....: \n\nIn [153]: table.stack(\"City\")\nOut[153]: \n                    Sales\nProvince City            \nAL       All         12.0\n         Calgary      8.0\n         Edmonton     4.0\nBC       All         16.0\n         Vancouver   16.0\n...                   ...\nAll      Montreal     6.0\n         Toronto     13.0\n         Vancouver   16.0\n         Windsor      1.0\n         Winnipeg     3.0\n\n[20 rows x 1 columns]\n  Frequency table like plyr in R \nIn [154]: grades = [48, 99, 75, 80, 42, 80, 72, 68, 36, 78]\n\nIn [155]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"ID\": [\"x%d\" % r for r in range(10)],\n   .....:         \"Gender\": [\"F\", \"M\", \"F\", \"M\", \"F\", \"M\", \"F\", \"M\", \"M\", \"M\"],\n   .....:         \"ExamYear\": [\n   .....:             \"2007\",\n   .....:             \"2007\",\n   .....:             \"2007\",\n   .....:             \"2008\",\n   .....:             \"2008\",\n   .....:             \"2008\",\n   .....:             \"2008\",\n   .....:             \"2009\",\n   .....:             \"2009\",\n   .....:             \"2009\",\n   .....:         ],\n   .....:         \"Class\": [\n   .....:             \"algebra\",\n   .....:             \"stats\",\n   .....:             \"bio\",\n   .....:             \"algebra\",\n   .....:             \"algebra\",\n   .....:             \"stats\",\n   .....:             \"stats\",\n   .....:             \"algebra\",\n   .....:             \"bio\",\n   .....:             \"bio\",\n   .....:         ],\n   .....:         \"Participated\": [\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"no\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:         ],\n   .....:         \"Passed\": [\"yes\" if x > 50 else \"no\" for x in grades],\n   .....:         \"Employed\": [\n   .....:             True,\n   .....:             True,\n   .....:             True,\n   .....:             False,\n   .....:             False,\n   .....:             False,\n   .....:             False,\n   .....:             True,\n   .....:             True,\n   .....:             False,\n   .....:         ],\n   .....:         \"Grade\": grades,\n   .....:     }\n   .....: )\n   .....: \n\nIn [156]: df.groupby(\"ExamYear\").agg(\n   .....:     {\n   .....:         \"Participated\": lambda x: x.value_counts()[\"yes\"],\n   .....:         \"Passed\": lambda x: sum(x == \"yes\"),\n   .....:         \"Employed\": lambda x: sum(x),\n   .....:         \"Grade\": lambda x: sum(x) \/ len(x),\n   .....:     }\n   .....: )\n   .....: \nOut[156]: \n          Participated  Passed  Employed      Grade\nExamYear                                           \n2007                 3       2         3  74.000000\n2008                 3       3         0  68.500000\n2009                 3       2         2  60.666667\n  Plot pandas DataFrame with year over year data To create year and month cross tabulation: \nIn [157]: df = pd.DataFrame(\n   .....:     {\"value\": np.random.randn(36)},\n   .....:     index=pd.date_range(\"2011-01-01\", freq=\"M\", periods=36),\n   .....: )\n   .....: \n\nIn [158]: pd.pivot_table(\n   .....:     df, index=df.index.month, columns=df.index.year, values=\"value\", aggfunc=\"sum\"\n   .....: )\n   .....: \nOut[158]: \n        2011      2012      2013\n1  -1.039268 -0.968914  2.565646\n2  -0.370647 -1.294524  1.431256\n3  -1.157892  0.413738  1.340309\n4  -1.344312  0.276662 -1.170299\n5   0.844885 -0.472035 -0.226169\n6   1.075770 -0.013960  0.410835\n7  -0.109050 -0.362543  0.813850\n8   1.643563 -0.006154  0.132003\n9  -1.469388 -0.923061 -0.827317\n10  0.357021  0.895717 -0.076467\n11 -0.674600  0.805244 -1.187678\n12 -1.776904 -1.206412  1.130127\n    Apply Rolling apply to organize - Turning embedded lists into a MultiIndex frame \nIn [159]: df = pd.DataFrame(\n   .....:     data={\n   .....:         \"A\": [[2, 4, 8, 16], [100, 200], [10, 20, 30]],\n   .....:         \"B\": [[\"a\", \"b\", \"c\"], [\"jj\", \"kk\"], [\"ccc\"]],\n   .....:     },\n   .....:     index=[\"I\", \"II\", \"III\"],\n   .....: )\n   .....: \n\nIn [160]: def SeriesFromSubList(aList):\n   .....:     return pd.Series(aList)\n   .....: \n\nIn [161]: df_orgz = pd.concat(\n   .....:     {ind: row.apply(SeriesFromSubList) for ind, row in df.iterrows()}\n   .....: )\n   .....: \n\nIn [162]: df_orgz\nOut[162]: \n         0     1     2     3\nI   A    2     4     8  16.0\n    B    a     b     c   NaN\nII  A  100   200   NaN   NaN\n    B   jj    kk   NaN   NaN\nIII A   10  20.0  30.0   NaN\n    B  ccc   NaN   NaN   NaN\n  Rolling apply with a DataFrame returning a Series Rolling Apply to multiple columns where function calculates a Series before a Scalar from the Series is returned \nIn [163]: df = pd.DataFrame(\n   .....:     data=np.random.randn(2000, 2) \/ 10000,\n   .....:     index=pd.date_range(\"2001-01-01\", periods=2000),\n   .....:     columns=[\"A\", \"B\"],\n   .....: )\n   .....: \n\nIn [164]: df\nOut[164]: \n                   A         B\n2001-01-01 -0.000144 -0.000141\n2001-01-02  0.000161  0.000102\n2001-01-03  0.000057  0.000088\n2001-01-04 -0.000221  0.000097\n2001-01-05 -0.000201 -0.000041\n...              ...       ...\n2006-06-19  0.000040 -0.000235\n2006-06-20 -0.000123 -0.000021\n2006-06-21 -0.000113  0.000114\n2006-06-22  0.000136  0.000109\n2006-06-23  0.000027  0.000030\n\n[2000 rows x 2 columns]\n\nIn [165]: def gm(df, const):\n   .....:     v = ((((df[\"A\"] + df[\"B\"]) + 1).cumprod()) - 1) * const\n   .....:     return v.iloc[-1]\n   .....: \n\nIn [166]: s = pd.Series(\n   .....:     {\n   .....:         df.index[i]: gm(df.iloc[i: min(i + 51, len(df) - 1)], 5)\n   .....:         for i in range(len(df) - 50)\n   .....:     }\n   .....: )\n   .....: \n\nIn [167]: s\nOut[167]: \n2001-01-01    0.000930\n2001-01-02    0.002615\n2001-01-03    0.001281\n2001-01-04    0.001117\n2001-01-05    0.002772\n                ...   \n2006-04-30    0.003296\n2006-05-01    0.002629\n2006-05-02    0.002081\n2006-05-03    0.004247\n2006-05-04    0.003928\nLength: 1950, dtype: float64\n  Rolling apply with a DataFrame returning a Scalar Rolling Apply to multiple columns where function returns a Scalar (Volume Weighted Average Price) \nIn [168]: rng = pd.date_range(start=\"2014-01-01\", periods=100)\n\nIn [169]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"Open\": np.random.randn(len(rng)),\n   .....:         \"Close\": np.random.randn(len(rng)),\n   .....:         \"Volume\": np.random.randint(100, 2000, len(rng)),\n   .....:     },\n   .....:     index=rng,\n   .....: )\n   .....: \n\nIn [170]: df\nOut[170]: \n                Open     Close  Volume\n2014-01-01 -1.611353 -0.492885    1219\n2014-01-02 -3.000951  0.445794    1054\n2014-01-03 -0.138359 -0.076081    1381\n2014-01-04  0.301568  1.198259    1253\n2014-01-05  0.276381 -0.669831    1728\n...              ...       ...     ...\n2014-04-06 -0.040338  0.937843    1188\n2014-04-07  0.359661 -0.285908    1864\n2014-04-08  0.060978  1.714814     941\n2014-04-09  1.759055 -0.455942    1065\n2014-04-10  0.138185 -1.147008    1453\n\n[100 rows x 3 columns]\n\nIn [171]: def vwap(bars):\n   .....:     return (bars.Close * bars.Volume).sum() \/ bars.Volume.sum()\n   .....: \n\nIn [172]: window = 5\n\nIn [173]: s = pd.concat(\n   .....:     [\n   .....:         (pd.Series(vwap(df.iloc[i: i + window]), index=[df.index[i + window]]))\n   .....:         for i in range(len(df) - window)\n   .....:     ]\n   .....: )\n   .....: \n\nIn [174]: s.round(2)\nOut[174]: \n2014-01-06    0.02\n2014-01-07    0.11\n2014-01-08    0.10\n2014-01-09    0.07\n2014-01-10   -0.29\n              ... \n2014-04-06   -0.63\n2014-04-07   -0.02\n2014-04-08   -0.03\n2014-04-09    0.34\n2014-04-10    0.29\nLength: 95, dtype: float64\n     Timeseries Between times Using indexer between time Constructing a datetime range that excludes weekends and includes only certain times Vectorized Lookup Aggregation and plotting time series Turn a matrix with hours in columns and days in rows into a continuous row sequence in the form of a time series. How to rearrange a Python pandas DataFrame? Dealing with duplicates when reindexing a timeseries to a specified frequency Calculate the first day of the month for each entry in a DatetimeIndex \nIn [175]: dates = pd.date_range(\"2000-01-01\", periods=5)\n\nIn [176]: dates.to_period(freq=\"M\").to_timestamp()\nOut[176]: \nDatetimeIndex(['2000-01-01', '2000-01-01', '2000-01-01', '2000-01-01',\n               '2000-01-01'],\n              dtype='datetime64[ns]', freq=None)\n   Resampling The Resample docs. Using Grouper instead of TimeGrouper for time grouping of values Time grouping with some missing values Valid frequency arguments to Grouper Timeseries Grouping using a MultiIndex Using TimeGrouper and another grouping to create subgroups, then apply a custom function GH3791 Resampling with custom periods Resample intraday frame without adding new days Resample minute data Resample with groupby    Merge The Join docs. Concatenate two dataframes with overlapping index (emulate R rbind) \nIn [177]: rng = pd.date_range(\"2000-01-01\", periods=6)\n\nIn [178]: df1 = pd.DataFrame(np.random.randn(6, 3), index=rng, columns=[\"A\", \"B\", \"C\"])\n\nIn [179]: df2 = df1.copy()\n  Depending on df construction, ignore_index may be needed \nIn [180]: df = pd.concat([df1, df2], ignore_index=True)\n\nIn [181]: df\nOut[181]: \n           A         B         C\n0  -0.870117 -0.479265 -0.790855\n1   0.144817  1.726395 -0.464535\n2  -0.821906  1.597605  0.187307\n3  -0.128342 -1.511638 -0.289858\n4   0.399194 -1.430030 -0.639760\n5   1.115116 -2.012600  1.810662\n6  -0.870117 -0.479265 -0.790855\n7   0.144817  1.726395 -0.464535\n8  -0.821906  1.597605  0.187307\n9  -0.128342 -1.511638 -0.289858\n10  0.399194 -1.430030 -0.639760\n11  1.115116 -2.012600  1.810662\n  Self Join of a DataFrame GH2996 \nIn [182]: df = pd.DataFrame(\n   .....:     data={\n   .....:         \"Area\": [\"A\"] * 5 + [\"C\"] * 2,\n   .....:         \"Bins\": [110] * 2 + [160] * 3 + [40] * 2,\n   .....:         \"Test_0\": [0, 1, 0, 1, 2, 0, 1],\n   .....:         \"Data\": np.random.randn(7),\n   .....:     }\n   .....: )\n   .....: \n\nIn [183]: df\nOut[183]: \n  Area  Bins  Test_0      Data\n0    A   110       0 -0.433937\n1    A   110       1 -0.160552\n2    A   160       0  0.744434\n3    A   160       1  1.754213\n4    A   160       2  0.000850\n5    C    40       0  0.342243\n6    C    40       1  1.070599\n\nIn [184]: df[\"Test_1\"] = df[\"Test_0\"] - 1\n\nIn [185]: pd.merge(\n   .....:     df,\n   .....:     df,\n   .....:     left_on=[\"Bins\", \"Area\", \"Test_0\"],\n   .....:     right_on=[\"Bins\", \"Area\", \"Test_1\"],\n   .....:     suffixes=(\"_L\", \"_R\"),\n   .....: )\n   .....: \nOut[185]: \n  Area  Bins  Test_0_L    Data_L  Test_1_L  Test_0_R    Data_R  Test_1_R\n0    A   110         0 -0.433937        -1         1 -0.160552         0\n1    A   160         0  0.744434        -1         1  1.754213         0\n2    A   160         1  1.754213         0         2  0.000850         1\n3    C    40         0  0.342243        -1         1  1.070599         0\n  How to set the index and join KDB like asof join Join with a criteria based on the values Using searchsorted to merge based on values inside a range   Plotting The Plotting docs. Make Matplotlib look like R Setting x-axis major and minor labels Plotting multiple charts in an IPython Jupyter notebook Creating a multi-line plot Plotting a heatmap Annotate a time-series plot Annotate a time-series plot #2 Generate Embedded plots in excel files using Pandas, Vincent and xlsxwriter Boxplot for each quartile of a stratifying variable \nIn [186]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"stratifying_var\": np.random.uniform(0, 100, 20),\n   .....:         \"price\": np.random.normal(100, 5, 20),\n   .....:     }\n   .....: )\n   .....: \n\nIn [187]: df[\"quartiles\"] = pd.qcut(\n   .....:     df[\"stratifying_var\"], 4, labels=[\"0-25%\", \"25-50%\", \"50-75%\", \"75-100%\"]\n   .....: )\n   .....: \n\nIn [188]: df.boxplot(column=\"price\", by=\"quartiles\")\nOut[188]: <AxesSubplot:title={'center':'price'}, xlabel='quartiles'>\n     Data in\/out Performance comparison of SQL vs HDF5  CSV The CSV docs read_csv in action appending to a csv Reading a csv chunk-by-chunk Reading only certain rows of a csv chunk-by-chunk Reading the first few lines of a frame Reading a file that is compressed but not by gzip\/bz2 (the native compressed formats which read_csv understands). This example shows a WinZipped file, but is a general application of opening the file within a context manager and using that handle to read. See here Inferring dtypes from a file Dealing with bad lines GH2886 Write a multi-row index CSV without writing duplicates  Reading multiple files to create a single DataFrame The best way to combine multiple files into a single DataFrame is to read the individual frames one by one, put all of the individual frames into a list, and then combine the frames in the list using pd.concat(): \nIn [189]: for i in range(3):\n   .....:     data = pd.DataFrame(np.random.randn(10, 4))\n   .....:     data.to_csv(\"file_{}.csv\".format(i))\n   .....: \n\nIn [190]: files = [\"file_0.csv\", \"file_1.csv\", \"file_2.csv\"]\n\nIn [191]: result = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n  You can use the same approach to read all files matching a pattern. Here is an example using glob: \nIn [192]: import glob\n\nIn [193]: import os\n\nIn [194]: files = glob.glob(\"file_*.csv\")\n\nIn [195]: result = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n  Finally, this strategy will work with the other pd.read_*(...) functions described in the io docs.   Parsing date components in multi-columns Parsing date components in multi-columns is faster with a format \nIn [196]: i = pd.date_range(\"20000101\", periods=10000)\n\nIn [197]: df = pd.DataFrame({\"year\": i.year, \"month\": i.month, \"day\": i.day})\n\nIn [198]: df.head()\nOut[198]: \n   year  month  day\n0  2000      1    1\n1  2000      1    2\n2  2000      1    3\n3  2000      1    4\n4  2000      1    5\n\nIn [199]: %timeit pd.to_datetime(df.year * 10000 + df.month * 100 + df.day, format='%Y%m%d')\n   .....: ds = df.apply(lambda x: \"%04d%02d%02d\" % (x[\"year\"], x[\"month\"], x[\"day\"]), axis=1)\n   .....: ds.head()\n   .....: %timeit pd.to_datetime(ds)\n   .....: \n8.7 ms +- 765 us per loop (mean +- std. dev. of 7 runs, 100 loops each)\n2.1 ms +- 419 us per loop (mean +- std. dev. of 7 runs, 100 loops each)\n    Skip row between header and data \nIn [200]: data = \"\"\";;;;\n   .....:  ;;;;\n   .....:  ;;;;\n   .....:  ;;;;\n   .....:  ;;;;\n   .....:  ;;;;\n   .....: ;;;;\n   .....:  ;;;;\n   .....:  ;;;;\n   .....: ;;;;\n   .....: date;Param1;Param2;Param4;Param5\n   .....:     ;m\u00b2;\u00b0C;m\u00b2;m\n   .....: ;;;;\n   .....: 01.01.1990 00:00;1;1;2;3\n   .....: 01.01.1990 01:00;5;3;4;5\n   .....: 01.01.1990 02:00;9;5;6;7\n   .....: 01.01.1990 03:00;13;7;8;9\n   .....: 01.01.1990 04:00;17;9;10;11\n   .....: 01.01.1990 05:00;21;11;12;13\n   .....: \"\"\"\n   .....: \n   Option 1: pass rows explicitly to skip rows \nIn [201]: from io import StringIO\n\nIn [202]: pd.read_csv(\n   .....:     StringIO(data),\n   .....:     sep=\";\",\n   .....:     skiprows=[11, 12],\n   .....:     index_col=0,\n   .....:     parse_dates=True,\n   .....:     header=10,\n   .....: )\n   .....: \nOut[202]: \n                     Param1  Param2  Param4  Param5\ndate                                               \n1990-01-01 00:00:00       1       1       2       3\n1990-01-01 01:00:00       5       3       4       5\n1990-01-01 02:00:00       9       5       6       7\n1990-01-01 03:00:00      13       7       8       9\n1990-01-01 04:00:00      17       9      10      11\n1990-01-01 05:00:00      21      11      12      13\n    Option 2: read column names and then data \nIn [203]: pd.read_csv(StringIO(data), sep=\";\", header=10, nrows=10).columns\nOut[203]: Index(['date', 'Param1', 'Param2', 'Param4', 'Param5'], dtype='object')\n\nIn [204]: columns = pd.read_csv(StringIO(data), sep=\";\", header=10, nrows=10).columns\n\nIn [205]: pd.read_csv(\n   .....:     StringIO(data), sep=\";\", index_col=0, header=12, parse_dates=True, names=columns\n   .....: )\n   .....: \nOut[205]: \n                     Param1  Param2  Param4  Param5\ndate                                               \n1990-01-01 00:00:00       1       1       2       3\n1990-01-01 01:00:00       5       3       4       5\n1990-01-01 02:00:00       9       5       6       7\n1990-01-01 03:00:00      13       7       8       9\n1990-01-01 04:00:00      17       9      10      11\n1990-01-01 05:00:00      21      11      12      13\n      SQL The SQL docs Reading from databases with SQL   Excel The Excel docs Reading from a filelike handle Modifying formatting in XlsxWriter output Loading only visible sheets GH19842#issuecomment-892150745   HTML Reading HTML tables from a server that cannot handle the default request header   HDFStore The HDFStores docs Simple queries with a Timestamp Index Managing heterogeneous data using a linked multiple table hierarchy GH3032 Merging on-disk tables with millions of rows Avoiding inconsistencies when writing to a store from multiple processes\/threads De-duplicating a large store by chunks, essentially a recursive reduction operation. Shows a function for taking in data from csv file and creating a store by chunks, with date parsing as well. See here Creating a store chunk-by-chunk from a csv file Appending to a store, while creating a unique index Large Data work flows Reading in a sequence of files, then providing a global unique index to a store while appending Groupby on a HDFStore with low group density Groupby on a HDFStore with high group density Hierarchical queries on a HDFStore Counting with a HDFStore Troubleshoot HDFStore exceptions Setting min_itemsize with strings Using ptrepack to create a completely-sorted-index on a store Storing Attributes to a group node \nIn [206]: df = pd.DataFrame(np.random.randn(8, 3))\n\nIn [207]: store = pd.HDFStore(\"test.h5\")\n\nIn [208]: store.put(\"df\", df)\n\n# you can store an arbitrary Python object via pickle\nIn [209]: store.get_storer(\"df\").attrs.my_attribute = {\"A\": 10}\n\nIn [210]: store.get_storer(\"df\").attrs.my_attribute\nOut[210]: {'A': 10}\n  You can create or load a HDFStore in-memory by passing the driver parameter to PyTables. Changes are only written to disk when the HDFStore is closed. \nIn [211]: store = pd.HDFStore(\"test.h5\", \"w\", driver=\"H5FD_CORE\")\n\nIn [212]: df = pd.DataFrame(np.random.randn(8, 3))\n\nIn [213]: store[\"test\"] = df\n\n# only after closing the store, data is written to disk:\nIn [214]: store.close()\n    Binary files pandas readily accepts NumPy record arrays, if you need to read in a binary file consisting of an array of C structs. For example, given this C program in a file called main.c compiled with gcc main.c -std=gnu99 on a 64-bit machine, \n#include <stdio.h>\n#include <stdint.h>\n\ntypedef struct _Data\n{\n    int32_t count;\n    double avg;\n    float scale;\n} Data;\n\nint main(int argc, const char *argv[])\n{\n    size_t n = 10;\n    Data d[n];\n\n    for (int i = 0; i < n; ++i)\n    {\n        d[i].count = i;\n        d[i].avg = i + 1.0;\n        d[i].scale = (float) i + 2.0f;\n    }\n\n    FILE *file = fopen(\"binary.dat\", \"wb\");\n    fwrite(&d, sizeof(Data), n, file);\n    fclose(file);\n\n    return 0;\n}\n  the following Python code will read the binary file 'binary.dat' into a pandas DataFrame, where each element of the struct corresponds to a column in the frame: \nnames = \"count\", \"avg\", \"scale\"\n\n# note that the offsets are larger than the size of the type because of\n# struct padding\noffsets = 0, 8, 16\nformats = \"i4\", \"f8\", \"f4\"\ndt = np.dtype({\"names\": names, \"offsets\": offsets, \"formats\": formats}, align=True)\ndf = pd.DataFrame(np.fromfile(\"binary.dat\", dt))\n   Note The offsets of the structure elements may be different depending on the architecture of the machine on which the file was created. Using a raw binary file format like this for general data storage is not recommended, as it is not cross platform. We recommended either HDF5 or parquet, both of which are supported by pandas\u2019 IO facilities.     Computation Numerical integration (sample-based) of a time series  Correlation Often it\u2019s useful to obtain the lower (or upper) triangular form of a correlation matrix calculated from DataFrame.corr(). This can be achieved by passing a boolean mask to where as follows: \nIn [215]: df = pd.DataFrame(np.random.random(size=(100, 5)))\n\nIn [216]: corr_mat = df.corr()\n\nIn [217]: mask = np.tril(np.ones_like(corr_mat, dtype=np.bool_), k=-1)\n\nIn [218]: corr_mat.where(mask)\nOut[218]: \n          0         1         2        3   4\n0       NaN       NaN       NaN      NaN NaN\n1 -0.079861       NaN       NaN      NaN NaN\n2 -0.236573  0.183801       NaN      NaN NaN\n3 -0.013795 -0.051975  0.037235      NaN NaN\n4 -0.031974  0.118342 -0.073499 -0.02063 NaN\n  The method argument within DataFrame.corr can accept a callable in addition to the named correlation types. Here we compute the distance correlation matrix for a DataFrame object. \nIn [219]: def distcorr(x, y):\n   .....:     n = len(x)\n   .....:     a = np.zeros(shape=(n, n))\n   .....:     b = np.zeros(shape=(n, n))\n   .....:     for i in range(n):\n   .....:         for j in range(i + 1, n):\n   .....:             a[i, j] = abs(x[i] - x[j])\n   .....:             b[i, j] = abs(y[i] - y[j])\n   .....:     a += a.T\n   .....:     b += b.T\n   .....:     a_bar = np.vstack([np.nanmean(a, axis=0)] * n)\n   .....:     b_bar = np.vstack([np.nanmean(b, axis=0)] * n)\n   .....:     A = a - a_bar - a_bar.T + np.full(shape=(n, n), fill_value=a_bar.mean())\n   .....:     B = b - b_bar - b_bar.T + np.full(shape=(n, n), fill_value=b_bar.mean())\n   .....:     cov_ab = np.sqrt(np.nansum(A * B)) \/ n\n   .....:     std_a = np.sqrt(np.sqrt(np.nansum(A ** 2)) \/ n)\n   .....:     std_b = np.sqrt(np.sqrt(np.nansum(B ** 2)) \/ n)\n   .....:     return cov_ab \/ std_a \/ std_b\n   .....: \n\nIn [220]: df = pd.DataFrame(np.random.normal(size=(100, 3)))\n\nIn [221]: df.corr(method=distcorr)\nOut[221]: \n          0         1         2\n0  1.000000  0.197613  0.216328\n1  0.197613  1.000000  0.208749\n2  0.216328  0.208749  1.000000\n     Timedeltas The Timedeltas docs. Using timedeltas \nIn [222]: import datetime\n\nIn [223]: s = pd.Series(pd.date_range(\"2012-1-1\", periods=3, freq=\"D\"))\n\nIn [224]: s - s.max()\nOut[224]: \n0   -2 days\n1   -1 days\n2    0 days\ndtype: timedelta64[ns]\n\nIn [225]: s.max() - s\nOut[225]: \n0   2 days\n1   1 days\n2   0 days\ndtype: timedelta64[ns]\n\nIn [226]: s - datetime.datetime(2011, 1, 1, 3, 5)\nOut[226]: \n0   364 days 20:55:00\n1   365 days 20:55:00\n2   366 days 20:55:00\ndtype: timedelta64[ns]\n\nIn [227]: s + datetime.timedelta(minutes=5)\nOut[227]: \n0   2012-01-01 00:05:00\n1   2012-01-02 00:05:00\n2   2012-01-03 00:05:00\ndtype: datetime64[ns]\n\nIn [228]: datetime.datetime(2011, 1, 1, 3, 5) - s\nOut[228]: \n0   -365 days +03:05:00\n1   -366 days +03:05:00\n2   -367 days +03:05:00\ndtype: timedelta64[ns]\n\nIn [229]: datetime.timedelta(minutes=5) + s\nOut[229]: \n0   2012-01-01 00:05:00\n1   2012-01-02 00:05:00\n2   2012-01-03 00:05:00\ndtype: datetime64[ns]\n  Adding and subtracting deltas and dates \nIn [230]: deltas = pd.Series([datetime.timedelta(days=i) for i in range(3)])\n\nIn [231]: df = pd.DataFrame({\"A\": s, \"B\": deltas})\n\nIn [232]: df\nOut[232]: \n           A      B\n0 2012-01-01 0 days\n1 2012-01-02 1 days\n2 2012-01-03 2 days\n\nIn [233]: df[\"New Dates\"] = df[\"A\"] + df[\"B\"]\n\nIn [234]: df[\"Delta\"] = df[\"A\"] - df[\"New Dates\"]\n\nIn [235]: df\nOut[235]: \n           A      B  New Dates   Delta\n0 2012-01-01 0 days 2012-01-01  0 days\n1 2012-01-02 1 days 2012-01-03 -1 days\n2 2012-01-03 2 days 2012-01-05 -2 days\n\nIn [236]: df.dtypes\nOut[236]: \nA             datetime64[ns]\nB            timedelta64[ns]\nNew Dates     datetime64[ns]\nDelta        timedelta64[ns]\ndtype: object\n  Another example Values can be set to NaT using np.nan, similar to datetime \nIn [237]: y = s - s.shift()\n\nIn [238]: y\nOut[238]: \n0      NaT\n1   1 days\n2   1 days\ndtype: timedelta64[ns]\n\nIn [239]: y[1] = np.nan\n\nIn [240]: y\nOut[240]: \n0      NaT\n1      NaT\n2   1 days\ndtype: timedelta64[ns]\n    Creating example data To create a dataframe from every combination of some given values, like R\u2019s expand.grid() function, we can create a dict where the keys are column names and the values are lists of the data values: \nIn [241]: def expand_grid(data_dict):\n   .....:     rows = itertools.product(*data_dict.values())\n   .....:     return pd.DataFrame.from_records(rows, columns=data_dict.keys())\n   .....: \n\nIn [242]: df = expand_grid(\n   .....:     {\"height\": [60, 70], \"weight\": [100, 140, 180], \"sex\": [\"Male\", \"Female\"]}\n   .....: )\n   .....: \n\nIn [243]: df\nOut[243]: \n    height  weight     sex\n0       60     100    Male\n1       60     100  Female\n2       60     140    Male\n3       60     140  Female\n4       60     180    Male\n5       60     180  Female\n6       70     100    Male\n7       70     100  Female\n8       70     140    Male\n9       70     140  Female\n10      70     180    Male\n11      70     180  Female","title":"pandas.user_guide.cookbook"},{"text":"pandas.Series.combine_first   Series.combine_first(other)[source]\n \nUpdate null elements with value in the same location in \u2018other\u2019. Combine two Series objects by filling null values in one Series with non-null values from the other Series. Result index will be the union of the two indexes.  Parameters \n \nother:Series\n\n\nThe value(s) to be used for filling null values.    Returns \n Series\n\nThe result of combining the provided Series with the other object.      See also  Series.combine\n\nPerform element-wise operation on two Series using a given function.    Examples \n>>> s1 = pd.Series([1, np.nan])\n>>> s2 = pd.Series([3, 4, 5])\n>>> s1.combine_first(s2)\n0    1.0\n1    4.0\n2    5.0\ndtype: float64\n  Null values still persist if the location of that null value does not exist in other \n>>> s1 = pd.Series({'falcon': np.nan, 'eagle': 160.0})\n>>> s2 = pd.Series({'eagle': 200.0, 'duck': 30.0})\n>>> s1.combine_first(s2)\nduck       30.0\neagle     160.0\nfalcon      NaN\ndtype: float64","title":"pandas.reference.api.pandas.series.combine_first"},{"text":"pandas.wide_to_long   pandas.wide_to_long(df, stubnames, i, j, sep='', suffix='\\\\d+')[source]\n \nUnpivot a DataFrame from wide to long format. Less flexible but more user-friendly than melt. With stubnames [\u2018A\u2019, \u2018B\u2019], this function expects to find one or more group of columns with format A-suffix1, A-suffix2,\u2026, B-suffix1, B-suffix2,\u2026 You specify what you want to call this suffix in the resulting long format with j (for example j=\u2019year\u2019) Each row of these wide variables are assumed to be uniquely identified by i (can be a single column name or a list of column names) All remaining variables in the data frame are left intact.  Parameters \n \ndf:DataFrame\n\n\nThe wide-format DataFrame.  \nstubnames:str or list-like\n\n\nThe stub name(s). The wide format variables are assumed to start with the stub names.  \ni:str or list-like\n\n\nColumn(s) to use as id variable(s).  \nj:str\n\n\nThe name of the sub-observation variable. What you wish to name your suffix in the long format.  \nsep:str, default \u201c\u201d\n\n\nA character indicating the separation of the variable names in the wide format, to be stripped from the names in the long format. For example, if your column names are A-suffix1, A-suffix2, you can strip the hyphen by specifying sep=\u2019-\u2019.  \nsuffix:str, default \u2018\\d+\u2019\n\n\nA regular expression capturing the wanted suffixes. \u2018\\d+\u2019 captures numeric suffixes. Suffixes with no numbers could be specified with the negated character class \u2018\\D+\u2019. You can also further disambiguate suffixes, for example, if your wide variables are of the form A-one, B-two,.., and you have an unrelated column A-rating, you can ignore the last one by specifying suffix=\u2019(!?one|two)\u2019. When all suffixes are numeric, they are cast to int64\/float64.    Returns \n DataFrame\n\nA DataFrame that contains each stub name as a variable, with new index (i, j).      See also  melt\n\nUnpivot a DataFrame from wide to long format, optionally leaving identifiers set.  pivot\n\nCreate a spreadsheet-style pivot table as a DataFrame.  DataFrame.pivot\n\nPivot without aggregation that can handle non-numeric data.  DataFrame.pivot_table\n\nGeneralization of pivot that can handle duplicate values for one index\/column pair.  DataFrame.unstack\n\nPivot based on the index values instead of a column.    Notes All extra variables are left untouched. This simply uses pandas.melt under the hood, but is hard-coded to \u201cdo the right thing\u201d in a typical case. Examples \n>>> np.random.seed(123)\n>>> df = pd.DataFrame({\"A1970\" : {0 : \"a\", 1 : \"b\", 2 : \"c\"},\n...                    \"A1980\" : {0 : \"d\", 1 : \"e\", 2 : \"f\"},\n...                    \"B1970\" : {0 : 2.5, 1 : 1.2, 2 : .7},\n...                    \"B1980\" : {0 : 3.2, 1 : 1.3, 2 : .1},\n...                    \"X\"     : dict(zip(range(3), np.random.randn(3)))\n...                   })\n>>> df[\"id\"] = df.index\n>>> df\n  A1970 A1980  B1970  B1980         X  id\n0     a     d    2.5    3.2 -1.085631   0\n1     b     e    1.2    1.3  0.997345   1\n2     c     f    0.7    0.1  0.282978   2\n>>> pd.wide_to_long(df, [\"A\", \"B\"], i=\"id\", j=\"year\")\n... \n                X  A    B\nid year\n0  1970 -1.085631  a  2.5\n1  1970  0.997345  b  1.2\n2  1970  0.282978  c  0.7\n0  1980 -1.085631  d  3.2\n1  1980  0.997345  e  1.3\n2  1980  0.282978  f  0.1\n  With multiple id columns \n>>> df = pd.DataFrame({\n...     'famid': [1, 1, 1, 2, 2, 2, 3, 3, 3],\n...     'birth': [1, 2, 3, 1, 2, 3, 1, 2, 3],\n...     'ht1': [2.8, 2.9, 2.2, 2, 1.8, 1.9, 2.2, 2.3, 2.1],\n...     'ht2': [3.4, 3.8, 2.9, 3.2, 2.8, 2.4, 3.3, 3.4, 2.9]\n... })\n>>> df\n   famid  birth  ht1  ht2\n0      1      1  2.8  3.4\n1      1      2  2.9  3.8\n2      1      3  2.2  2.9\n3      2      1  2.0  3.2\n4      2      2  1.8  2.8\n5      2      3  1.9  2.4\n6      3      1  2.2  3.3\n7      3      2  2.3  3.4\n8      3      3  2.1  2.9\n>>> l = pd.wide_to_long(df, stubnames='ht', i=['famid', 'birth'], j='age')\n>>> l\n... \n                  ht\nfamid birth age\n1     1     1    2.8\n            2    3.4\n      2     1    2.9\n            2    3.8\n      3     1    2.2\n            2    2.9\n2     1     1    2.0\n            2    3.2\n      2     1    1.8\n            2    2.8\n      3     1    1.9\n            2    2.4\n3     1     1    2.2\n            2    3.3\n      2     1    2.3\n            2    3.4\n      3     1    2.1\n            2    2.9\n  Going from long back to wide just takes some creative use of unstack \n>>> w = l.unstack()\n>>> w.columns = w.columns.map('{0[0]}{0[1]}'.format)\n>>> w.reset_index()\n   famid  birth  ht1  ht2\n0      1      1  2.8  3.4\n1      1      2  2.9  3.8\n2      1      3  2.2  2.9\n3      2      1  2.0  3.2\n4      2      2  1.8  2.8\n5      2      3  1.9  2.4\n6      3      1  2.2  3.3\n7      3      2  2.3  3.4\n8      3      3  2.1  2.9\n  Less wieldy column names are also handled \n>>> np.random.seed(0)\n>>> df = pd.DataFrame({'A(weekly)-2010': np.random.rand(3),\n...                    'A(weekly)-2011': np.random.rand(3),\n...                    'B(weekly)-2010': np.random.rand(3),\n...                    'B(weekly)-2011': np.random.rand(3),\n...                    'X' : np.random.randint(3, size=3)})\n>>> df['id'] = df.index\n>>> df \n   A(weekly)-2010  A(weekly)-2011  B(weekly)-2010  B(weekly)-2011  X  id\n0        0.548814        0.544883        0.437587        0.383442  0   0\n1        0.715189        0.423655        0.891773        0.791725  1   1\n2        0.602763        0.645894        0.963663        0.528895  1   2\n  \n>>> pd.wide_to_long(df, ['A(weekly)', 'B(weekly)'], i='id',\n...                 j='year', sep='-')\n... \n         X  A(weekly)  B(weekly)\nid year\n0  2010  0   0.548814   0.437587\n1  2010  1   0.715189   0.891773\n2  2010  1   0.602763   0.963663\n0  2011  0   0.544883   0.383442\n1  2011  1   0.423655   0.791725\n2  2011  1   0.645894   0.528895\n  If we have many columns, we could also use a regex to find our stubnames and pass that list on to wide_to_long \n>>> stubnames = sorted(\n...     set([match[0] for match in df.columns.str.findall(\n...         r'[A-B]\\(.*\\)').values if match != []])\n... )\n>>> list(stubnames)\n['A(weekly)', 'B(weekly)']\n  All of the above examples have integers as suffixes. It is possible to have non-integers as suffixes. \n>>> df = pd.DataFrame({\n...     'famid': [1, 1, 1, 2, 2, 2, 3, 3, 3],\n...     'birth': [1, 2, 3, 1, 2, 3, 1, 2, 3],\n...     'ht_one': [2.8, 2.9, 2.2, 2, 1.8, 1.9, 2.2, 2.3, 2.1],\n...     'ht_two': [3.4, 3.8, 2.9, 3.2, 2.8, 2.4, 3.3, 3.4, 2.9]\n... })\n>>> df\n   famid  birth  ht_one  ht_two\n0      1      1     2.8     3.4\n1      1      2     2.9     3.8\n2      1      3     2.2     2.9\n3      2      1     2.0     3.2\n4      2      2     1.8     2.8\n5      2      3     1.9     2.4\n6      3      1     2.2     3.3\n7      3      2     2.3     3.4\n8      3      3     2.1     2.9\n  \n>>> l = pd.wide_to_long(df, stubnames='ht', i=['famid', 'birth'], j='age',\n...                     sep='_', suffix=r'\\w+')\n>>> l\n... \n                  ht\nfamid birth age\n1     1     one  2.8\n            two  3.4\n      2     one  2.9\n            two  3.8\n      3     one  2.2\n            two  2.9\n2     1     one  2.0\n            two  3.2\n      2     one  1.8\n            two  2.8\n      3     one  1.9\n            two  2.4\n3     1     one  2.2\n            two  3.3\n      2     one  2.3\n            two  3.4\n      3     one  2.1\n            two  2.9","title":"pandas.reference.api.pandas.wide_to_long"},{"text":"pandas.Series.combine   Series.combine(other, func, fill_value=None)[source]\n \nCombine the Series with a Series or scalar according to func. Combine the Series and other using func to perform elementwise selection for combined Series. fill_value is assumed when value is missing at some index from one of the two objects being combined.  Parameters \n \nother:Series or scalar\n\n\nThe value(s) to be combined with the Series.  \nfunc:function\n\n\nFunction that takes two scalars as inputs and returns an element.  \nfill_value:scalar, optional\n\n\nThe value to assume when an index is missing from one Series or the other. The default specifies to use the appropriate NaN value for the underlying dtype of the Series.    Returns \n Series\n\nThe result of combining the Series with the other object.      See also  Series.combine_first\n\nCombine Series values, choosing the calling Series\u2019 values first.    Examples Consider 2 Datasets s1 and s2 containing highest clocked speeds of different birds. \n>>> s1 = pd.Series({'falcon': 330.0, 'eagle': 160.0})\n>>> s1\nfalcon    330.0\neagle     160.0\ndtype: float64\n>>> s2 = pd.Series({'falcon': 345.0, 'eagle': 200.0, 'duck': 30.0})\n>>> s2\nfalcon    345.0\neagle     200.0\nduck       30.0\ndtype: float64\n  Now, to combine the two datasets and view the highest speeds of the birds across the two datasets \n>>> s1.combine(s2, max)\nduck        NaN\neagle     200.0\nfalcon    345.0\ndtype: float64\n  In the previous example, the resulting value for duck is missing, because the maximum of a NaN and a float is a NaN. So, in the example, we set fill_value=0, so the maximum value returned will be the value from some dataset. \n>>> s1.combine(s2, max, fill_value=0)\nduck       30.0\neagle     200.0\nfalcon    345.0\ndtype: float64","title":"pandas.reference.api.pandas.series.combine"},{"text":"operator.setitem(a, b, c)  \noperator.__setitem__(a, b, c)  \nSet the value of a at index b to c.","title":"python.library.operator#operator.setitem"},{"text":"tf.raw_ops.Betainc   Compute the regularized incomplete beta integral \\(I_x(a, b)\\).  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.raw_ops.Betainc  \ntf.raw_ops.Betainc(\n    a, b, x, name=None\n)\n The regularized incomplete beta integral is defined as: \\(I_x(a, b) = \\frac{B(x; a, b)}{B(a, b)}\\) where \\(B(x; a, b) = \\int_0^x t^{a-1} (1 - t)^{b-1} dt\\) is the incomplete beta function and \\(B(a, b)\\) is the complete beta function.\n \n\n\n Args\n  a   A Tensor. Must be one of the following types: float32, float64.  \n  b   A Tensor. Must have the same type as a.  \n  x   A Tensor. Must have the same type as a.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor. Has the same type as a.","title":"tensorflow.raw_ops.betainc"},{"text":"setup_python(context)  \nCreates a copy or symlink to the Python executable in the environment. On POSIX systems, if a specific executable python3.x was used, symlinks to python and python3 will be created pointing to that executable, unless files with those names already exist.","title":"python.library.venv#venv.EnvBuilder.setup_python"},{"text":"operator.setitem(a, b, c)  \noperator.__setitem__(a, b, c)  \nSet the value of a at index b to c.","title":"python.library.operator#operator.__setitem__"}]}
{"task_id":4175686,"prompt":"def f_4175686(d):\n\t","suffix":"\n\treturn d","canonical_solution":"del d['ele']","test_start":"\ndef check(candidate):","test":["\n    assert candidate({\"ale\":1, \"ele\": 2}) == {\"ale\": 1}\n"],"entry_point":"f_4175686","intent":"remove key 'ele' from dictionary `d`","library":[],"docs":[]}
{"task_id":11574195,"prompt":"def f_11574195():\n\treturn ","suffix":"","canonical_solution":"['it'] + ['was'] + ['annoying']","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == ['it', 'was', 'annoying']\n"],"entry_point":"f_11574195","intent":"merge list `['it']` and list `['was']` and list `['annoying']` into one list","library":[],"docs":[]}
{"task_id":587647,"prompt":"def f_587647(x):\n\treturn ","suffix":"","canonical_solution":"str(int(x) + 1).zfill(len(x))","test_start":"\ndef check(candidate):","test":["\n    assert candidate(\"001\") == \"002\"\n","\n    assert candidate(\"100\") == \"101\"\n"],"entry_point":"f_587647","intent":"increment a value with leading zeroes in a number `x`","library":[],"docs":[]}
{"task_id":17315881,"prompt":"def f_17315881(df):\n\treturn ","suffix":"","canonical_solution":"all(df.index[:-1] <= df.index[1:])","test_start":"\nimport pandas as pd \n\ndef check(candidate):","test":["\n    df1 = pd.DataFrame({'a': [1,2], 'bb': [0,2]})\n    assert candidate(df1) == True\n","\n    df2 = pd.DataFrame({'a': [1,2,3,4,5], 'bb': [0,3,5,7,9]})\n    shuffled = df2.sample(frac=3, replace=True)\n    assert candidate(shuffled) == False\n","\n    df = pd.DataFrame([[1, 2], [5, 4]], columns=['a', 'b'])\n    assert candidate(df)\n"],"entry_point":"f_17315881","intent":"check if a pandas dataframe `df`'s index is sorted","library":["pandas"],"docs":[{"text":"pandas.Index.is_monotonic_increasing   propertyIndex.is_monotonic_increasing\n \nReturn if the index is monotonic increasing (only equal or increasing) values. Examples \n>>> Index([1, 2, 3]).is_monotonic_increasing\nTrue\n>>> Index([1, 2, 2]).is_monotonic_increasing\nTrue\n>>> Index([1, 3, 2]).is_monotonic_increasing\nFalse","title":"pandas.reference.api.pandas.index.is_monotonic_increasing"},{"text":"pandas.Index.is_monotonic_decreasing   propertyIndex.is_monotonic_decreasing\n \nReturn if the index is monotonic decreasing (only equal or decreasing) values. Examples \n>>> Index([3, 2, 1]).is_monotonic_decreasing\nTrue\n>>> Index([3, 2, 2]).is_monotonic_decreasing\nTrue\n>>> Index([3, 1, 2]).is_monotonic_decreasing\nFalse","title":"pandas.reference.api.pandas.index.is_monotonic_decreasing"},{"text":"pandas.Index.is_unique   Index.is_unique\n \nReturn if the index has unique values.","title":"pandas.reference.api.pandas.index.is_unique"},{"text":"pandas.Index.is_monotonic   propertyIndex.is_monotonic\n \nAlias for is_monotonic_increasing.","title":"pandas.reference.api.pandas.index.is_monotonic"},{"text":"pandas.Series.is_monotonic_increasing   propertySeries.is_monotonic_increasing\n \nAlias for is_monotonic.","title":"pandas.reference.api.pandas.series.is_monotonic_increasing"},{"text":"pandas.Series.is_monotonic_decreasing   propertySeries.is_monotonic_decreasing\n \nReturn boolean if values in the object are monotonic_decreasing.  Returns \n bool","title":"pandas.reference.api.pandas.series.is_monotonic_decreasing"},{"text":"pandas.core.groupby.SeriesGroupBy.is_monotonic_increasing   propertySeriesGroupBy.is_monotonic_increasing\n \nAlias for is_monotonic.","title":"pandas.reference.api.pandas.core.groupby.seriesgroupby.is_monotonic_increasing"},{"text":"pandas.core.groupby.SeriesGroupBy.is_monotonic_decreasing   propertySeriesGroupBy.is_monotonic_decreasing\n \nReturn boolean if values in the object are monotonic_decreasing.  Returns \n bool","title":"pandas.reference.api.pandas.core.groupby.seriesgroupby.is_monotonic_decreasing"},{"text":"pandas.CategoricalIndex.ordered   propertyCategoricalIndex.ordered\n \nWhether the categories have an ordered relationship.","title":"pandas.reference.api.pandas.categoricalindex.ordered"},{"text":"pandas.Series.is_monotonic   propertySeries.is_monotonic\n \nReturn boolean if values in the object are monotonic_increasing.  Returns \n bool","title":"pandas.reference.api.pandas.series.is_monotonic"}]}
{"task_id":16296643,"prompt":"def f_16296643(t):\n\treturn ","suffix":"","canonical_solution":"list(t)","test_start":"\ndef check(candidate):","test":["\n    assert candidate((0, 1, 2)) == [0,1,2]\n","\n    assert candidate(('a', [], 100)) == ['a', [], 100]\n"],"entry_point":"f_16296643","intent":"Convert tuple `t` to list","library":[],"docs":[]}
{"task_id":16296643,"prompt":"def f_16296643(t):\n\treturn ","suffix":"","canonical_solution":"tuple(t)","test_start":"\ndef check(candidate):","test":["\n    assert candidate([0,1,2]) == (0, 1, 2)\n","\n    assert candidate(['a', [], 100]) == ('a', [], 100)\n"],"entry_point":"f_16296643","intent":"Convert list `t` to tuple","library":[],"docs":[]}
{"task_id":16296643,"prompt":"def f_16296643(level1):\n\t","suffix":"\n\treturn level1","canonical_solution":"level1 = map(list, level1)","test_start":"\ndef check(candidate):","test":["\n    t = ((1, 2), (3, 4))\n    t = candidate(t)\n    assert list(t) == [[1, 2], [3, 4]]\n"],"entry_point":"f_16296643","intent":"Convert tuple `level1` to list","library":[],"docs":[]}
{"task_id":3880399,"prompt":"def f_3880399(dataobject, logFile):\n\treturn ","suffix":"","canonical_solution":"pprint.pprint(dataobject, logFile)","test_start":"\nimport pprint \n\ndef check(candidate):","test":["\n    f = open('kkk.txt', 'w')\n    candidate('hello', f)\n    f.close()\n    with open('kkk.txt', 'r') as f:\n        assert 'hello' in f.readline()\n"],"entry_point":"f_3880399","intent":"send the output of pprint object `dataobject` to file `logFile`","library":["pprint"],"docs":[{"text":"PrettyPrinter.pprint(object)  \nPrint the formatted representation of object on the configured stream, followed by a newline.","title":"python.library.pprint#pprint.PrettyPrinter.pprint"},{"text":"trigger(*args, **kwargs)[source]\n \nCalled when this tool gets used. This method is called by ToolManager.trigger_tool.  Parameters \n \neventEvent\n\n\nThe canvas event that caused this tool to be called.  \nsenderobject\n\n\nObject that requested the tool to be triggered.  \ndataobject\n\n\nExtra data.","title":"matplotlib.backend_tools_api#matplotlib.backend_tools.ToolCopyToClipboardBase.trigger"},{"text":"use_xobject=b'Do'[source]","title":"matplotlib.backend_pdf_api#matplotlib.backends.backend_pdf.Op.use_xobject"},{"text":"numpy.ma.masked_object   ma.masked_object(x, value, copy=True, shrink=True)[source]\n \nMask the array x where the data are exactly equal to value. This function is similar to masked_values, but only suitable for object arrays: for floating point, use masked_values instead.  Parameters \n \nxarray_like\n\n\nArray to mask  \nvalueobject\n\n\nComparison value  \ncopy{True, False}, optional\n\n\nWhether to return a copy of x.  \nshrink{True, False}, optional\n\n\nWhether to collapse a mask full of False to nomask    Returns \n \nresultMaskedArray\n\n\nThe result of masking x where equal to value.      See also  masked_where\n\nMask where a condition is met.  masked_equal\n\nMask where equal to a given value (integers).  masked_values\n\nMask using floating point equality.    Examples >>> import numpy.ma as ma\n>>> food = np.array(['green_eggs', 'ham'], dtype=object)\n>>> # don't eat spoiled food\n>>> eat = ma.masked_object(food, 'green_eggs')\n>>> eat\nmasked_array(data=[--, 'ham'],\n             mask=[ True, False],\n       fill_value='green_eggs',\n            dtype=object)\n>>> # plain ol` ham is boring\n>>> fresh_food = np.array(['cheese', 'ham', 'pineapple'], dtype=object)\n>>> eat = ma.masked_object(fresh_food, 'green_eggs')\n>>> eat\nmasked_array(data=['cheese', 'ham', 'pineapple'],\n             mask=False,\n       fill_value='green_eggs',\n            dtype=object)\n Note that mask is set to nomask if possible. >>> eat\nmasked_array(data=['cheese', 'ham', 'pineapple'],\n             mask=False,\n       fill_value='green_eggs',\n            dtype=object)","title":"numpy.reference.generated.numpy.ma.masked_object"},{"text":"trigger(sender, event, data=None)[source]\n \nCalled when this tool gets used. This method is called by ToolManager.trigger_tool.  Parameters \n \neventEvent\n\n\nThe canvas event that caused this tool to be called.  \nsenderobject\n\n\nObject that requested the tool to be triggered.  \ndataobject\n\n\nExtra data.","title":"matplotlib.backend_tools_api#matplotlib.backend_tools.ToolBase.trigger"},{"text":"emit()  \nPickles the record\u2019s attribute dictionary and writes it to the socket in binary format. If there is an error with the socket, silently drops the packet. To unpickle the record at the receiving end into a LogRecord, use the makeLogRecord() function.","title":"python.library.logging.handlers#logging.handlers.DatagramHandler.emit"},{"text":"fileobj  \nFile object registered.","title":"python.library.selectors#selectors.SelectorKey.fileobj"},{"text":"numpy.who   numpy.who(vardict=None)[source]\n \nPrint the NumPy arrays in the given dictionary. If there is no dictionary passed in or vardict is None then returns NumPy arrays in the globals() dictionary (all NumPy arrays in the namespace).  Parameters \n \nvardictdict, optional\n\n\nA dictionary possibly containing ndarrays. Default is globals().    Returns \n \noutNone\n\n\nReturns \u2018None\u2019.     Notes Prints out the name, shape, bytes and type of all of the ndarrays present in vardict. Examples >>> a = np.arange(10)\n>>> b = np.ones(20)\n>>> np.who()\nName            Shape            Bytes            Type\n===========================================================\na               10               80               int64\nb               20               160              float64\nUpper bound on total bytes  =       240\n >>> d = {'x': np.arange(2.0), 'y': np.arange(3.0), 'txt': 'Some str',\n... 'idx':5}\n>>> np.who(d)\nName            Shape            Bytes            Type\n===========================================================\nx               2                16               float64\ny               3                24               float64\nUpper bound on total bytes  =       40","title":"numpy.reference.generated.numpy.who"},{"text":"skimage.filters.threshold_isodata(image=None, nbins=256, return_all=False, *, hist=None) [source]\n \nReturn threshold value(s) based on ISODATA method. Histogram-based threshold, known as Ridler-Calvard method or inter-means. Threshold values returned satisfy the following equality: threshold = (image[image <= threshold].mean() +\n             image[image > threshold].mean()) \/ 2.0\n That is, returned thresholds are intensities that separate the image into two groups of pixels, where the threshold intensity is midway between the mean intensities of these groups. For integer images, the above equality holds to within one; for floating- point images, the equality holds to within the histogram bin-width. Either image or hist must be provided. In case hist is given, the actual histogram of the image is ignored.  Parameters \n \nimage(N, M) ndarray, optional \n\nInput image.  \nnbinsint, optional \n\nNumber of bins used to calculate histogram. This value is ignored for integer arrays.  \nreturn_allbool, optional \n\nIf False (default), return only the lowest threshold that satisfies the above equality. If True, return all valid thresholds.  \nhistarray, or 2-tuple of arrays, optional \n\nHistogram to determine the threshold from and a corresponding array of bin center intensities. Alternatively, only the histogram can be passed.    Returns \n \nthresholdfloat or int or array \n\nThreshold value(s).     References  \n1  \nRidler, TW & Calvard, S (1978), \u201cPicture thresholding using an iterative selection method\u201d IEEE Transactions on Systems, Man and Cybernetics 8: 630-632, DOI:10.1109\/TSMC.1978.4310039  \n2  \nSezgin M. and Sankur B. (2004) \u201cSurvey over Image Thresholding Techniques and Quantitative Performance Evaluation\u201d Journal of Electronic Imaging, 13(1): 146-165, http:\/\/www.busim.ee.boun.edu.tr\/~sankur\/SankurFolder\/Threshold_survey.pdf DOI:10.1117\/1.1631315  \n3  \nImageJ AutoThresholder code, http:\/\/fiji.sc\/wiki\/index.php\/Auto_Threshold   Examples >>> from skimage.data import coins\n>>> image = coins()\n>>> thresh = threshold_isodata(image)\n>>> binary = image > thresh","title":"skimage.api.skimage.filters#skimage.filters.threshold_isodata"},{"text":"trigger(sender, event, data=None)[source]\n \nCalled when this tool gets used. This method is called by ToolManager.trigger_tool.  Parameters \n \neventEvent\n\n\nThe canvas event that caused this tool to be called.  \nsenderobject\n\n\nObject that requested the tool to be triggered.  \ndataobject\n\n\nExtra data.","title":"matplotlib.backend_tools_api#matplotlib.backend_tools.ToolGrid.trigger"}]}
{"task_id":21800169,"prompt":"def f_21800169(df):\n\treturn ","suffix":"","canonical_solution":"df.loc[df['BoolCol']]","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    df = pd.DataFrame([[True, 2, 3], [False, 5, 6]], columns = ['BoolCol', 'a', 'b'])\n    y = candidate(df)\n    assert y['a'][0] == 2\n    assert y['b'][0] == 3\n"],"entry_point":"f_21800169","intent":"get index of rows in column 'BoolCol'","library":["pandas"],"docs":[{"text":"pandas.BooleanDtype   classpandas.BooleanDtype[source]\n \nExtension dtype for boolean data.  New in version 1.0.0.   Warning BooleanDtype is considered experimental. The implementation and parts of the API may change without warning.  Examples \n>>> pd.BooleanDtype()\nBooleanDtype\n  Attributes       \nNone     Methods       \nNone","title":"pandas.reference.api.pandas.booleandtype"},{"text":"numpy.matrix.nonzero method   matrix.nonzero()\n \nReturn the indices of the elements that are non-zero. Refer to numpy.nonzero for full documentation.  See also  numpy.nonzero\n\nequivalent function","title":"numpy.reference.generated.numpy.matrix.nonzero"},{"text":"numpy.recarray.nonzero method   recarray.nonzero()\n \nReturn the indices of the elements that are non-zero. Refer to numpy.nonzero for full documentation.  See also  numpy.nonzero\n\nequivalent function","title":"numpy.reference.generated.numpy.recarray.nonzero"},{"text":"numpy.ndarray.nonzero method   ndarray.nonzero()\n \nReturn the indices of the elements that are non-zero. Refer to numpy.nonzero for full documentation.  See also  numpy.nonzero\n\nequivalent function","title":"numpy.reference.generated.numpy.ndarray.nonzero"},{"text":"numpy.ma.nonzero   ma.nonzero(self) = <numpy.ma.core._frommethod object>\n \nReturn the indices of unmasked elements that are not zero. Returns a tuple of arrays, one for each dimension, containing the indices of the non-zero elements in that dimension. The corresponding non-zero values can be obtained with: a[a.nonzero()]\n To group the indices by element, rather than dimension, use instead: np.transpose(a.nonzero())\n The result of this is always a 2d array, with a row for each non-zero element.  Parameters \n None\n  Returns \n \ntuple_of_arraystuple\n\n\nIndices of elements that are non-zero.      See also  numpy.nonzero\n\nFunction operating on ndarrays.  flatnonzero\n\nReturn indices that are non-zero in the flattened version of the input array.  numpy.ndarray.nonzero\n\nEquivalent ndarray method.  count_nonzero\n\nCounts the number of non-zero elements in the input array.    Examples >>> import numpy.ma as ma\n>>> x = ma.array(np.eye(3))\n>>> x\nmasked_array(\n  data=[[1., 0., 0.],\n        [0., 1., 0.],\n        [0., 0., 1.]],\n  mask=False,\n  fill_value=1e+20)\n>>> x.nonzero()\n(array([0, 1, 2]), array([0, 1, 2]))\n Masked elements are ignored. >>> x[1, 1] = ma.masked\n>>> x\nmasked_array(\n  data=[[1.0, 0.0, 0.0],\n        [0.0, --, 0.0],\n        [0.0, 0.0, 1.0]],\n  mask=[[False, False, False],\n        [False,  True, False],\n        [False, False, False]],\n  fill_value=1e+20)\n>>> x.nonzero()\n(array([0, 2]), array([0, 2]))\n Indices can also be grouped by element. >>> np.transpose(x.nonzero())\narray([[0, 0],\n       [2, 2]])\n A common use for nonzero is to find the indices of an array, where a condition is True. Given an array a, the condition a > 3 is a boolean array and since False is interpreted as 0, ma.nonzero(a > 3) yields the indices of the a where the condition is true. >>> a = ma.array([[1,2,3],[4,5,6],[7,8,9]])\n>>> a > 3\nmasked_array(\n  data=[[False, False, False],\n        [ True,  True,  True],\n        [ True,  True,  True]],\n  mask=False,\n  fill_value=True)\n>>> ma.nonzero(a > 3)\n(array([1, 1, 1, 2, 2, 2]), array([0, 1, 2, 0, 1, 2]))\n The nonzero method of the condition array can also be called. >>> (a > 3).nonzero()\n(array([1, 1, 1, 2, 2, 2]), array([0, 1, 2, 0, 1, 2]))","title":"numpy.reference.generated.numpy.ma.nonzero"},{"text":"numpy.ma.MaskedArray.nonzero method   ma.MaskedArray.nonzero()[source]\n \nReturn the indices of unmasked elements that are not zero. Returns a tuple of arrays, one for each dimension, containing the indices of the non-zero elements in that dimension. The corresponding non-zero values can be obtained with: a[a.nonzero()]\n To group the indices by element, rather than dimension, use instead: np.transpose(a.nonzero())\n The result of this is always a 2d array, with a row for each non-zero element.  Parameters \n None\n  Returns \n \ntuple_of_arraystuple\n\n\nIndices of elements that are non-zero.      See also  numpy.nonzero\n\nFunction operating on ndarrays.  flatnonzero\n\nReturn indices that are non-zero in the flattened version of the input array.  numpy.ndarray.nonzero\n\nEquivalent ndarray method.  count_nonzero\n\nCounts the number of non-zero elements in the input array.    Examples >>> import numpy.ma as ma\n>>> x = ma.array(np.eye(3))\n>>> x\nmasked_array(\n  data=[[1., 0., 0.],\n        [0., 1., 0.],\n        [0., 0., 1.]],\n  mask=False,\n  fill_value=1e+20)\n>>> x.nonzero()\n(array([0, 1, 2]), array([0, 1, 2]))\n Masked elements are ignored. >>> x[1, 1] = ma.masked\n>>> x\nmasked_array(\n  data=[[1.0, 0.0, 0.0],\n        [0.0, --, 0.0],\n        [0.0, 0.0, 1.0]],\n  mask=[[False, False, False],\n        [False,  True, False],\n        [False, False, False]],\n  fill_value=1e+20)\n>>> x.nonzero()\n(array([0, 2]), array([0, 2]))\n Indices can also be grouped by element. >>> np.transpose(x.nonzero())\narray([[0, 0],\n       [2, 2]])\n A common use for nonzero is to find the indices of an array, where a condition is True. Given an array a, the condition a > 3 is a boolean array and since False is interpreted as 0, ma.nonzero(a > 3) yields the indices of the a where the condition is true. >>> a = ma.array([[1,2,3],[4,5,6],[7,8,9]])\n>>> a > 3\nmasked_array(\n  data=[[False, False, False],\n        [ True,  True,  True],\n        [ True,  True,  True]],\n  mask=False,\n  fill_value=True)\n>>> ma.nonzero(a > 3)\n(array([1, 1, 1, 2, 2, 2]), array([0, 1, 2, 0, 1, 2]))\n The nonzero method of the condition array can also be called. >>> (a > 3).nonzero()\n(array([1, 1, 1, 2, 2, 2]), array([0, 1, 2, 0, 1, 2]))","title":"numpy.reference.generated.numpy.ma.maskedarray.nonzero"},{"text":"pandas.Index.asof_locs   Index.asof_locs(where, mask)[source]\n \nReturn the locations (indices) of labels in the index. As in the asof function, if the label (a particular entry in where) is not in the index, the latest index label up to the passed label is chosen and its index returned. If all of the labels in the index are later than a label in where, -1 is returned. mask is used to ignore NA values in the index during calculation.  Parameters \n \nwhere:Index\n\n\nAn Index consisting of an array of timestamps.  \nmask:np.ndarray[bool]\n\n\nArray of booleans denoting where values in the original data are not NA.    Returns \n np.ndarray[np.intp]\n\nAn array of locations (indices) of the labels from the Index which correspond to the return values of the asof function for every element in where.","title":"pandas.reference.api.pandas.index.asof_locs"},{"text":"pandas.Index.is_boolean   finalIndex.is_boolean()[source]\n \nCheck if the Index only consists of booleans.  Returns \n bool\n\nWhether or not the Index only consists of booleans.      See also  is_integer\n\nCheck if the Index only consists of integers.  is_floating\n\nCheck if the Index is a floating type.  is_numeric\n\nCheck if the Index only consists of numeric data.  is_object\n\nCheck if the Index is of the object dtype.  is_categorical\n\nCheck if the Index holds categorical data.  is_interval\n\nCheck if the Index holds Interval objects.  is_mixed\n\nCheck if the Index holds data with mixed data types.    Examples \n>>> idx = pd.Index([True, False, True])\n>>> idx.is_boolean()\nTrue\n  \n>>> idx = pd.Index([\"True\", \"False\", \"True\"])\n>>> idx.is_boolean()\nFalse\n  \n>>> idx = pd.Index([True, False, \"True\"])\n>>> idx.is_boolean()\nFalse","title":"pandas.reference.api.pandas.index.is_boolean"},{"text":"pandas.Series.bool   Series.bool()[source]\n \nReturn the bool of a single element Series or DataFrame. This must be a boolean scalar value, either True or False. It will raise a ValueError if the Series or DataFrame does not have exactly 1 element, or that element is not boolean (integer values 0 and 1 will also raise an exception).  Returns \n bool\n\nThe value in the Series or DataFrame.      See also  Series.astype\n\nChange the data type of a Series, including to boolean.  DataFrame.astype\n\nChange the data type of a DataFrame, including to boolean.  numpy.bool_\n\nNumPy boolean data type, used by pandas for boolean values.    Examples The method will only work for single element objects with a boolean value: \n>>> pd.Series([True]).bool()\nTrue\n>>> pd.Series([False]).bool()\nFalse\n  \n>>> pd.DataFrame({'col': [True]}).bool()\nTrue\n>>> pd.DataFrame({'col': [False]}).bool()\nFalse","title":"pandas.reference.api.pandas.series.bool"},{"text":"pandas.DataFrame.bool   DataFrame.bool()[source]\n \nReturn the bool of a single element Series or DataFrame. This must be a boolean scalar value, either True or False. It will raise a ValueError if the Series or DataFrame does not have exactly 1 element, or that element is not boolean (integer values 0 and 1 will also raise an exception).  Returns \n bool\n\nThe value in the Series or DataFrame.      See also  Series.astype\n\nChange the data type of a Series, including to boolean.  DataFrame.astype\n\nChange the data type of a DataFrame, including to boolean.  numpy.bool_\n\nNumPy boolean data type, used by pandas for boolean values.    Examples The method will only work for single element objects with a boolean value: \n>>> pd.Series([True]).bool()\nTrue\n>>> pd.Series([False]).bool()\nFalse\n  \n>>> pd.DataFrame({'col': [True]}).bool()\nTrue\n>>> pd.DataFrame({'col': [False]}).bool()\nFalse","title":"pandas.reference.api.pandas.dataframe.bool"}]}
{"task_id":21800169,"prompt":"def f_21800169(df):\n\treturn ","suffix":"","canonical_solution":"df.iloc[np.flatnonzero(df['BoolCol'])]","test_start":"\nimport numpy as np\nimport pandas as pd\n\ndef check(candidate):","test":["\n    df = pd.DataFrame([[True, 2, 3], [False, 5, 6]], columns = ['BoolCol', 'a', 'b'])\n    y = candidate(df)\n    assert y['a'][0] == 2\n    assert y['b'][0] == 3\n"],"entry_point":"f_21800169","intent":"Create a list containing the indexes of rows where the value of column 'BoolCol' in dataframe `df` are equal to True","library":["numpy","pandas"],"docs":[{"text":"pandas.Index.asof_locs   Index.asof_locs(where, mask)[source]\n \nReturn the locations (indices) of labels in the index. As in the asof function, if the label (a particular entry in where) is not in the index, the latest index label up to the passed label is chosen and its index returned. If all of the labels in the index are later than a label in where, -1 is returned. mask is used to ignore NA values in the index during calculation.  Parameters \n \nwhere:Index\n\n\nAn Index consisting of an array of timestamps.  \nmask:np.ndarray[bool]\n\n\nArray of booleans denoting where values in the original data are not NA.    Returns \n np.ndarray[np.intp]\n\nAn array of locations (indices) of the labels from the Index which correspond to the return values of the asof function for every element in where.","title":"pandas.reference.api.pandas.index.asof_locs"},{"text":"pandas.DataFrame.lookup   DataFrame.lookup(row_labels, col_labels)[source]\n \nLabel-based \u201cfancy indexing\u201d function for DataFrame. Given equal-length arrays of row and column labels, return an array of the values corresponding to each (row, col) pair.  Deprecated since version 1.2.0: DataFrame.lookup is deprecated, use DataFrame.melt and DataFrame.loc instead. For further details see Looking up values by index\/column labels.   Parameters \n \nrow_labels:sequence\n\n\nThe row labels to use for lookup.  \ncol_labels:sequence\n\n\nThe column labels to use for lookup.    Returns \n numpy.ndarray\n\nThe found values.","title":"pandas.reference.api.pandas.dataframe.lookup"},{"text":"numpy.matrix.nonzero method   matrix.nonzero()\n \nReturn the indices of the elements that are non-zero. Refer to numpy.nonzero for full documentation.  See also  numpy.nonzero\n\nequivalent function","title":"numpy.reference.generated.numpy.matrix.nonzero"},{"text":"numpy.ma.nonzero   ma.nonzero(self) = <numpy.ma.core._frommethod object>\n \nReturn the indices of unmasked elements that are not zero. Returns a tuple of arrays, one for each dimension, containing the indices of the non-zero elements in that dimension. The corresponding non-zero values can be obtained with: a[a.nonzero()]\n To group the indices by element, rather than dimension, use instead: np.transpose(a.nonzero())\n The result of this is always a 2d array, with a row for each non-zero element.  Parameters \n None\n  Returns \n \ntuple_of_arraystuple\n\n\nIndices of elements that are non-zero.      See also  numpy.nonzero\n\nFunction operating on ndarrays.  flatnonzero\n\nReturn indices that are non-zero in the flattened version of the input array.  numpy.ndarray.nonzero\n\nEquivalent ndarray method.  count_nonzero\n\nCounts the number of non-zero elements in the input array.    Examples >>> import numpy.ma as ma\n>>> x = ma.array(np.eye(3))\n>>> x\nmasked_array(\n  data=[[1., 0., 0.],\n        [0., 1., 0.],\n        [0., 0., 1.]],\n  mask=False,\n  fill_value=1e+20)\n>>> x.nonzero()\n(array([0, 1, 2]), array([0, 1, 2]))\n Masked elements are ignored. >>> x[1, 1] = ma.masked\n>>> x\nmasked_array(\n  data=[[1.0, 0.0, 0.0],\n        [0.0, --, 0.0],\n        [0.0, 0.0, 1.0]],\n  mask=[[False, False, False],\n        [False,  True, False],\n        [False, False, False]],\n  fill_value=1e+20)\n>>> x.nonzero()\n(array([0, 2]), array([0, 2]))\n Indices can also be grouped by element. >>> np.transpose(x.nonzero())\narray([[0, 0],\n       [2, 2]])\n A common use for nonzero is to find the indices of an array, where a condition is True. Given an array a, the condition a > 3 is a boolean array and since False is interpreted as 0, ma.nonzero(a > 3) yields the indices of the a where the condition is true. >>> a = ma.array([[1,2,3],[4,5,6],[7,8,9]])\n>>> a > 3\nmasked_array(\n  data=[[False, False, False],\n        [ True,  True,  True],\n        [ True,  True,  True]],\n  mask=False,\n  fill_value=True)\n>>> ma.nonzero(a > 3)\n(array([1, 1, 1, 2, 2, 2]), array([0, 1, 2, 0, 1, 2]))\n The nonzero method of the condition array can also be called. >>> (a > 3).nonzero()\n(array([1, 1, 1, 2, 2, 2]), array([0, 1, 2, 0, 1, 2]))","title":"numpy.reference.generated.numpy.ma.nonzero"},{"text":"pandas.DataFrame.bool   DataFrame.bool()[source]\n \nReturn the bool of a single element Series or DataFrame. This must be a boolean scalar value, either True or False. It will raise a ValueError if the Series or DataFrame does not have exactly 1 element, or that element is not boolean (integer values 0 and 1 will also raise an exception).  Returns \n bool\n\nThe value in the Series or DataFrame.      See also  Series.astype\n\nChange the data type of a Series, including to boolean.  DataFrame.astype\n\nChange the data type of a DataFrame, including to boolean.  numpy.bool_\n\nNumPy boolean data type, used by pandas for boolean values.    Examples The method will only work for single element objects with a boolean value: \n>>> pd.Series([True]).bool()\nTrue\n>>> pd.Series([False]).bool()\nFalse\n  \n>>> pd.DataFrame({'col': [True]}).bool()\nTrue\n>>> pd.DataFrame({'col': [False]}).bool()\nFalse","title":"pandas.reference.api.pandas.dataframe.bool"},{"text":"numpy.ma.MaskedArray.nonzero method   ma.MaskedArray.nonzero()[source]\n \nReturn the indices of unmasked elements that are not zero. Returns a tuple of arrays, one for each dimension, containing the indices of the non-zero elements in that dimension. The corresponding non-zero values can be obtained with: a[a.nonzero()]\n To group the indices by element, rather than dimension, use instead: np.transpose(a.nonzero())\n The result of this is always a 2d array, with a row for each non-zero element.  Parameters \n None\n  Returns \n \ntuple_of_arraystuple\n\n\nIndices of elements that are non-zero.      See also  numpy.nonzero\n\nFunction operating on ndarrays.  flatnonzero\n\nReturn indices that are non-zero in the flattened version of the input array.  numpy.ndarray.nonzero\n\nEquivalent ndarray method.  count_nonzero\n\nCounts the number of non-zero elements in the input array.    Examples >>> import numpy.ma as ma\n>>> x = ma.array(np.eye(3))\n>>> x\nmasked_array(\n  data=[[1., 0., 0.],\n        [0., 1., 0.],\n        [0., 0., 1.]],\n  mask=False,\n  fill_value=1e+20)\n>>> x.nonzero()\n(array([0, 1, 2]), array([0, 1, 2]))\n Masked elements are ignored. >>> x[1, 1] = ma.masked\n>>> x\nmasked_array(\n  data=[[1.0, 0.0, 0.0],\n        [0.0, --, 0.0],\n        [0.0, 0.0, 1.0]],\n  mask=[[False, False, False],\n        [False,  True, False],\n        [False, False, False]],\n  fill_value=1e+20)\n>>> x.nonzero()\n(array([0, 2]), array([0, 2]))\n Indices can also be grouped by element. >>> np.transpose(x.nonzero())\narray([[0, 0],\n       [2, 2]])\n A common use for nonzero is to find the indices of an array, where a condition is True. Given an array a, the condition a > 3 is a boolean array and since False is interpreted as 0, ma.nonzero(a > 3) yields the indices of the a where the condition is true. >>> a = ma.array([[1,2,3],[4,5,6],[7,8,9]])\n>>> a > 3\nmasked_array(\n  data=[[False, False, False],\n        [ True,  True,  True],\n        [ True,  True,  True]],\n  mask=False,\n  fill_value=True)\n>>> ma.nonzero(a > 3)\n(array([1, 1, 1, 2, 2, 2]), array([0, 1, 2, 0, 1, 2]))\n The nonzero method of the condition array can also be called. >>> (a > 3).nonzero()\n(array([1, 1, 1, 2, 2, 2]), array([0, 1, 2, 0, 1, 2]))","title":"numpy.reference.generated.numpy.ma.maskedarray.nonzero"},{"text":"numpy.ndarray.nonzero method   ndarray.nonzero()\n \nReturn the indices of the elements that are non-zero. Refer to numpy.nonzero for full documentation.  See also  numpy.nonzero\n\nequivalent function","title":"numpy.reference.generated.numpy.ndarray.nonzero"},{"text":"pandas.BooleanDtype   classpandas.BooleanDtype[source]\n \nExtension dtype for boolean data.  New in version 1.0.0.   Warning BooleanDtype is considered experimental. The implementation and parts of the API may change without warning.  Examples \n>>> pd.BooleanDtype()\nBooleanDtype\n  Attributes       \nNone     Methods       \nNone","title":"pandas.reference.api.pandas.booleandtype"},{"text":"asyncio.run(coro, *, debug=False)  \nExecute the coroutine coro and return the result. This function runs the passed coroutine, taking care of managing the asyncio event loop, finalizing asynchronous generators, and closing the threadpool. This function cannot be called when another asyncio event loop is running in the same thread. If debug is True, the event loop will be run in debug mode. This function always creates a new event loop and closes it at the end. It should be used as a main entry point for asyncio programs, and should ideally only be called once. Example: async def main():\n    await asyncio.sleep(1)\n    print('hello')\n\nasyncio.run(main())\n  New in version 3.7.   Changed in version 3.9: Updated to use loop.shutdown_default_executor().   Note The source code for asyncio.run() can be found in Lib\/asyncio\/runners.py.","title":"python.library.asyncio-task#asyncio.run"},{"text":"pandas.arrays.BooleanArray   classpandas.arrays.BooleanArray(values, mask, copy=False)[source]\n \nArray of boolean (True\/False) data with missing values. This is a pandas Extension array for boolean data, under the hood represented by 2 numpy arrays: a boolean array with the data and a boolean array with the mask (True indicating missing). BooleanArray implements Kleene logic (sometimes called three-value logic) for logical operations. See Kleene logical operations for more. To construct an BooleanArray from generic array-like input, use pandas.array() specifying dtype=\"boolean\" (see examples below).  New in version 1.0.0.   Warning BooleanArray is considered experimental. The implementation and parts of the API may change without warning.   Parameters \n \nvalues:numpy.ndarray\n\n\nA 1-d boolean-dtype array with the data.  \nmask:numpy.ndarray\n\n\nA 1-d boolean-dtype array indicating missing values (True indicates missing).  \ncopy:bool, default False\n\n\nWhether to copy the values and mask arrays.    Returns \n BooleanArray\n   Examples Create an BooleanArray with pandas.array(): \n>>> pd.array([True, False, None], dtype=\"boolean\")\n<BooleanArray>\n[True, False, <NA>]\nLength: 3, dtype: boolean\n  Attributes       \nNone     Methods       \nNone","title":"pandas.reference.api.pandas.arrays.booleanarray"}]}
{"task_id":21800169,"prompt":"def f_21800169(df):\n\treturn ","suffix":"","canonical_solution":"df[df['BoolCol'] == True].index.tolist()","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    df = pd.DataFrame([[True, 2, 3], [False, 5, 6]], columns = ['BoolCol', 'a', 'b'])\n    y = candidate(df)\n    assert y == [0]\n"],"entry_point":"f_21800169","intent":"from dataframe `df` get list of indexes of rows where column 'BoolCol' values match True","library":["pandas"],"docs":[{"text":"pandas.DataFrame.lookup   DataFrame.lookup(row_labels, col_labels)[source]\n \nLabel-based \u201cfancy indexing\u201d function for DataFrame. Given equal-length arrays of row and column labels, return an array of the values corresponding to each (row, col) pair.  Deprecated since version 1.2.0: DataFrame.lookup is deprecated, use DataFrame.melt and DataFrame.loc instead. For further details see Looking up values by index\/column labels.   Parameters \n \nrow_labels:sequence\n\n\nThe row labels to use for lookup.  \ncol_labels:sequence\n\n\nThe column labels to use for lookup.    Returns \n numpy.ndarray\n\nThe found values.","title":"pandas.reference.api.pandas.dataframe.lookup"},{"text":"pandas.Index.asof_locs   Index.asof_locs(where, mask)[source]\n \nReturn the locations (indices) of labels in the index. As in the asof function, if the label (a particular entry in where) is not in the index, the latest index label up to the passed label is chosen and its index returned. If all of the labels in the index are later than a label in where, -1 is returned. mask is used to ignore NA values in the index during calculation.  Parameters \n \nwhere:Index\n\n\nAn Index consisting of an array of timestamps.  \nmask:np.ndarray[bool]\n\n\nArray of booleans denoting where values in the original data are not NA.    Returns \n np.ndarray[np.intp]\n\nAn array of locations (indices) of the labels from the Index which correspond to the return values of the asof function for every element in where.","title":"pandas.reference.api.pandas.index.asof_locs"},{"text":"Cookbook This is a repository for short and sweet examples and links for useful pandas recipes. We encourage users to add to this documentation. Adding interesting links and\/or inline examples to this section is a great First Pull Request. Simplified, condensed, new-user friendly, in-line examples have been inserted where possible to augment the Stack-Overflow and GitHub links. Many of the links contain expanded information, above what the in-line examples offer. pandas (pd) and NumPy (np) are the only two abbreviated imported modules. The rest are kept explicitly imported for newer users.  Idioms These are some neat pandas idioms if-then\/if-then-else on one column, and assignment to another one or more columns: \nIn [1]: df = pd.DataFrame(\n   ...:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ...: )\n   ...: \n\nIn [2]: df\nOut[2]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n   if-then\u2026 An if-then on one column \nIn [3]: df.loc[df.AAA >= 5, \"BBB\"] = -1\n\nIn [4]: df\nOut[4]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   -1   50\n2    6   -1  -30\n3    7   -1  -50\n  An if-then with assignment to 2 columns: \nIn [5]: df.loc[df.AAA >= 5, [\"BBB\", \"CCC\"]] = 555\n\nIn [6]: df\nOut[6]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5  555  555\n2    6  555  555\n3    7  555  555\n  Add another line with different logic, to do the -else \nIn [7]: df.loc[df.AAA < 5, [\"BBB\", \"CCC\"]] = 2000\n\nIn [8]: df\nOut[8]: \n   AAA   BBB   CCC\n0    4  2000  2000\n1    5   555   555\n2    6   555   555\n3    7   555   555\n  Or use pandas where after you\u2019ve set up a mask \nIn [9]: df_mask = pd.DataFrame(\n   ...:     {\"AAA\": [True] * 4, \"BBB\": [False] * 4, \"CCC\": [True, False] * 2}\n   ...: )\n   ...: \n\nIn [10]: df.where(df_mask, -1000)\nOut[10]: \n   AAA   BBB   CCC\n0    4 -1000  2000\n1    5 -1000 -1000\n2    6 -1000   555\n3    7 -1000 -1000\n  if-then-else using NumPy\u2019s where() \nIn [11]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [12]: df\nOut[12]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [13]: df[\"logic\"] = np.where(df[\"AAA\"] > 5, \"high\", \"low\")\n\nIn [14]: df\nOut[14]: \n   AAA  BBB  CCC logic\n0    4   10  100   low\n1    5   20   50   low\n2    6   30  -30  high\n3    7   40  -50  high\n    Splitting Split a frame with a boolean criterion \nIn [15]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [16]: df\nOut[16]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [17]: df[df.AAA <= 5]\nOut[17]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n\nIn [18]: df[df.AAA > 5]\nOut[18]: \n   AAA  BBB  CCC\n2    6   30  -30\n3    7   40  -50\n    Building criteria Select with multi-column criteria \nIn [19]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [20]: df\nOut[20]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n  \u2026and (without assignment returns a Series) \nIn [21]: df.loc[(df[\"BBB\"] < 25) & (df[\"CCC\"] >= -40), \"AAA\"]\nOut[21]: \n0    4\n1    5\nName: AAA, dtype: int64\n  \u2026or (without assignment returns a Series) \nIn [22]: df.loc[(df[\"BBB\"] > 25) | (df[\"CCC\"] >= -40), \"AAA\"]\nOut[22]: \n0    4\n1    5\n2    6\n3    7\nName: AAA, dtype: int64\n  \u2026or (with assignment modifies the DataFrame.) \nIn [23]: df.loc[(df[\"BBB\"] > 25) | (df[\"CCC\"] >= 75), \"AAA\"] = 0.1\n\nIn [24]: df\nOut[24]: \n   AAA  BBB  CCC\n0  0.1   10  100\n1  5.0   20   50\n2  0.1   30  -30\n3  0.1   40  -50\n  Select rows with data closest to certain value using argsort \nIn [25]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [26]: df\nOut[26]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [27]: aValue = 43.0\n\nIn [28]: df.loc[(df.CCC - aValue).abs().argsort()]\nOut[28]: \n   AAA  BBB  CCC\n1    5   20   50\n0    4   10  100\n2    6   30  -30\n3    7   40  -50\n  Dynamically reduce a list of criteria using a binary operators \nIn [29]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [30]: df\nOut[30]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [31]: Crit1 = df.AAA <= 5.5\n\nIn [32]: Crit2 = df.BBB == 10.0\n\nIn [33]: Crit3 = df.CCC > -40.0\n  One could hard code: \nIn [34]: AllCrit = Crit1 & Crit2 & Crit3\n  \u2026Or it can be done with a list of dynamically built criteria \nIn [35]: import functools\n\nIn [36]: CritList = [Crit1, Crit2, Crit3]\n\nIn [37]: AllCrit = functools.reduce(lambda x, y: x & y, CritList)\n\nIn [38]: df[AllCrit]\nOut[38]: \n   AAA  BBB  CCC\n0    4   10  100\n     Selection  Dataframes The indexing docs. Using both row labels and value conditionals \nIn [39]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [40]: df\nOut[40]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [41]: df[(df.AAA <= 6) & (df.index.isin([0, 2, 4]))]\nOut[41]: \n   AAA  BBB  CCC\n0    4   10  100\n2    6   30  -30\n  Use loc for label-oriented slicing and iloc positional slicing GH2904 \nIn [42]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]},\n   ....:     index=[\"foo\", \"bar\", \"boo\", \"kar\"],\n   ....: )\n   ....: \n  There are 2 explicit slicing methods, with a third general case  Positional-oriented (Python slicing style : exclusive of end) Label-oriented (Non-Python slicing style : inclusive of end) General (Either slicing style : depends on if the slice contains labels or positions)  \nIn [43]: df.loc[\"bar\":\"kar\"]  # Label\nOut[43]: \n     AAA  BBB  CCC\nbar    5   20   50\nboo    6   30  -30\nkar    7   40  -50\n\n# Generic\nIn [44]: df[0:3]\nOut[44]: \n     AAA  BBB  CCC\nfoo    4   10  100\nbar    5   20   50\nboo    6   30  -30\n\nIn [45]: df[\"bar\":\"kar\"]\nOut[45]: \n     AAA  BBB  CCC\nbar    5   20   50\nboo    6   30  -30\nkar    7   40  -50\n  Ambiguity arises when an index consists of integers with a non-zero start or non-unit increment. \nIn [46]: data = {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n\nIn [47]: df2 = pd.DataFrame(data=data, index=[1, 2, 3, 4])  # Note index starts at 1.\n\nIn [48]: df2.iloc[1:3]  # Position-oriented\nOut[48]: \n   AAA  BBB  CCC\n2    5   20   50\n3    6   30  -30\n\nIn [49]: df2.loc[1:3]  # Label-oriented\nOut[49]: \n   AAA  BBB  CCC\n1    4   10  100\n2    5   20   50\n3    6   30  -30\n  Using inverse operator (~) to take the complement of a mask \nIn [50]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [51]: df\nOut[51]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [52]: df[~((df.AAA <= 6) & (df.index.isin([0, 2, 4])))]\nOut[52]: \n   AAA  BBB  CCC\n1    5   20   50\n3    7   40  -50\n    New columns Efficiently and dynamically creating new columns using applymap \nIn [53]: df = pd.DataFrame({\"AAA\": [1, 2, 1, 3], \"BBB\": [1, 1, 2, 2], \"CCC\": [2, 1, 3, 1]})\n\nIn [54]: df\nOut[54]: \n   AAA  BBB  CCC\n0    1    1    2\n1    2    1    1\n2    1    2    3\n3    3    2    1\n\nIn [55]: source_cols = df.columns  # Or some subset would work too\n\nIn [56]: new_cols = [str(x) + \"_cat\" for x in source_cols]\n\nIn [57]: categories = {1: \"Alpha\", 2: \"Beta\", 3: \"Charlie\"}\n\nIn [58]: df[new_cols] = df[source_cols].applymap(categories.get)\n\nIn [59]: df\nOut[59]: \n   AAA  BBB  CCC  AAA_cat BBB_cat  CCC_cat\n0    1    1    2    Alpha   Alpha     Beta\n1    2    1    1     Beta   Alpha    Alpha\n2    1    2    3    Alpha    Beta  Charlie\n3    3    2    1  Charlie    Beta    Alpha\n  Keep other columns when using min() with groupby \nIn [60]: df = pd.DataFrame(\n   ....:     {\"AAA\": [1, 1, 1, 2, 2, 2, 3, 3], \"BBB\": [2, 1, 3, 4, 5, 1, 2, 3]}\n   ....: )\n   ....: \n\nIn [61]: df\nOut[61]: \n   AAA  BBB\n0    1    2\n1    1    1\n2    1    3\n3    2    4\n4    2    5\n5    2    1\n6    3    2\n7    3    3\n  Method 1 : idxmin() to get the index of the minimums \nIn [62]: df.loc[df.groupby(\"AAA\")[\"BBB\"].idxmin()]\nOut[62]: \n   AAA  BBB\n1    1    1\n5    2    1\n6    3    2\n  Method 2 : sort then take first of each \nIn [63]: df.sort_values(by=\"BBB\").groupby(\"AAA\", as_index=False).first()\nOut[63]: \n   AAA  BBB\n0    1    1\n1    2    1\n2    3    2\n  Notice the same results, with the exception of the index.    Multiindexing The multindexing docs. Creating a MultiIndex from a labeled frame \nIn [64]: df = pd.DataFrame(\n   ....:     {\n   ....:         \"row\": [0, 1, 2],\n   ....:         \"One_X\": [1.1, 1.1, 1.1],\n   ....:         \"One_Y\": [1.2, 1.2, 1.2],\n   ....:         \"Two_X\": [1.11, 1.11, 1.11],\n   ....:         \"Two_Y\": [1.22, 1.22, 1.22],\n   ....:     }\n   ....: )\n   ....: \n\nIn [65]: df\nOut[65]: \n   row  One_X  One_Y  Two_X  Two_Y\n0    0    1.1    1.2   1.11   1.22\n1    1    1.1    1.2   1.11   1.22\n2    2    1.1    1.2   1.11   1.22\n\n# As Labelled Index\nIn [66]: df = df.set_index(\"row\")\n\nIn [67]: df\nOut[67]: \n     One_X  One_Y  Two_X  Two_Y\nrow                            \n0      1.1    1.2   1.11   1.22\n1      1.1    1.2   1.11   1.22\n2      1.1    1.2   1.11   1.22\n\n# With Hierarchical Columns\nIn [68]: df.columns = pd.MultiIndex.from_tuples([tuple(c.split(\"_\")) for c in df.columns])\n\nIn [69]: df\nOut[69]: \n     One        Two      \n       X    Y     X     Y\nrow                      \n0    1.1  1.2  1.11  1.22\n1    1.1  1.2  1.11  1.22\n2    1.1  1.2  1.11  1.22\n\n# Now stack & Reset\nIn [70]: df = df.stack(0).reset_index(1)\n\nIn [71]: df\nOut[71]: \n    level_1     X     Y\nrow                    \n0       One  1.10  1.20\n0       Two  1.11  1.22\n1       One  1.10  1.20\n1       Two  1.11  1.22\n2       One  1.10  1.20\n2       Two  1.11  1.22\n\n# And fix the labels (Notice the label 'level_1' got added automatically)\nIn [72]: df.columns = [\"Sample\", \"All_X\", \"All_Y\"]\n\nIn [73]: df\nOut[73]: \n    Sample  All_X  All_Y\nrow                     \n0      One   1.10   1.20\n0      Two   1.11   1.22\n1      One   1.10   1.20\n1      Two   1.11   1.22\n2      One   1.10   1.20\n2      Two   1.11   1.22\n   Arithmetic Performing arithmetic with a MultiIndex that needs broadcasting \nIn [74]: cols = pd.MultiIndex.from_tuples(\n   ....:     [(x, y) for x in [\"A\", \"B\", \"C\"] for y in [\"O\", \"I\"]]\n   ....: )\n   ....: \n\nIn [75]: df = pd.DataFrame(np.random.randn(2, 6), index=[\"n\", \"m\"], columns=cols)\n\nIn [76]: df\nOut[76]: \n          A                   B                   C          \n          O         I         O         I         O         I\nn  0.469112 -0.282863 -1.509059 -1.135632  1.212112 -0.173215\nm  0.119209 -1.044236 -0.861849 -2.104569 -0.494929  1.071804\n\nIn [77]: df = df.div(df[\"C\"], level=1)\n\nIn [78]: df\nOut[78]: \n          A                   B              C     \n          O         I         O         I    O    I\nn  0.387021  1.633022 -1.244983  6.556214  1.0  1.0\nm -0.240860 -0.974279  1.741358 -1.963577  1.0  1.0\n    Slicing Slicing a MultiIndex with xs \nIn [79]: coords = [(\"AA\", \"one\"), (\"AA\", \"six\"), (\"BB\", \"one\"), (\"BB\", \"two\"), (\"BB\", \"six\")]\n\nIn [80]: index = pd.MultiIndex.from_tuples(coords)\n\nIn [81]: df = pd.DataFrame([11, 22, 33, 44, 55], index, [\"MyData\"])\n\nIn [82]: df\nOut[82]: \n        MyData\nAA one      11\n   six      22\nBB one      33\n   two      44\n   six      55\n  To take the cross section of the 1st level and 1st axis the index: \n# Note : level and axis are optional, and default to zero\nIn [83]: df.xs(\"BB\", level=0, axis=0)\nOut[83]: \n     MyData\none      33\ntwo      44\nsix      55\n  \u2026and now the 2nd level of the 1st axis. \nIn [84]: df.xs(\"six\", level=1, axis=0)\nOut[84]: \n    MyData\nAA      22\nBB      55\n  Slicing a MultiIndex with xs, method #2 \nIn [85]: import itertools\n\nIn [86]: index = list(itertools.product([\"Ada\", \"Quinn\", \"Violet\"], [\"Comp\", \"Math\", \"Sci\"]))\n\nIn [87]: headr = list(itertools.product([\"Exams\", \"Labs\"], [\"I\", \"II\"]))\n\nIn [88]: indx = pd.MultiIndex.from_tuples(index, names=[\"Student\", \"Course\"])\n\nIn [89]: cols = pd.MultiIndex.from_tuples(headr)  # Notice these are un-named\n\nIn [90]: data = [[70 + x + y + (x * y) % 3 for x in range(4)] for y in range(9)]\n\nIn [91]: df = pd.DataFrame(data, indx, cols)\n\nIn [92]: df\nOut[92]: \n               Exams     Labs    \n                   I  II    I  II\nStudent Course                   \nAda     Comp      70  71   72  73\n        Math      71  73   75  74\n        Sci       72  75   75  75\nQuinn   Comp      73  74   75  76\n        Math      74  76   78  77\n        Sci       75  78   78  78\nViolet  Comp      76  77   78  79\n        Math      77  79   81  80\n        Sci       78  81   81  81\n\nIn [93]: All = slice(None)\n\nIn [94]: df.loc[\"Violet\"]\nOut[94]: \n       Exams     Labs    \n           I  II    I  II\nCourse                   \nComp      76  77   78  79\nMath      77  79   81  80\nSci       78  81   81  81\n\nIn [95]: df.loc[(All, \"Math\"), All]\nOut[95]: \n               Exams     Labs    \n                   I  II    I  II\nStudent Course                   \nAda     Math      71  73   75  74\nQuinn   Math      74  76   78  77\nViolet  Math      77  79   81  80\n\nIn [96]: df.loc[(slice(\"Ada\", \"Quinn\"), \"Math\"), All]\nOut[96]: \n               Exams     Labs    \n                   I  II    I  II\nStudent Course                   \nAda     Math      71  73   75  74\nQuinn   Math      74  76   78  77\n\nIn [97]: df.loc[(All, \"Math\"), (\"Exams\")]\nOut[97]: \n                 I  II\nStudent Course        \nAda     Math    71  73\nQuinn   Math    74  76\nViolet  Math    77  79\n\nIn [98]: df.loc[(All, \"Math\"), (All, \"II\")]\nOut[98]: \n               Exams Labs\n                  II   II\nStudent Course           \nAda     Math      73   74\nQuinn   Math      76   77\nViolet  Math      79   80\n  Setting portions of a MultiIndex with xs   Sorting Sort by specific column or an ordered list of columns, with a MultiIndex \nIn [99]: df.sort_values(by=(\"Labs\", \"II\"), ascending=False)\nOut[99]: \n               Exams     Labs    \n                   I  II    I  II\nStudent Course                   \nViolet  Sci       78  81   81  81\n        Math      77  79   81  80\n        Comp      76  77   78  79\nQuinn   Sci       75  78   78  78\n        Math      74  76   78  77\n        Comp      73  74   75  76\nAda     Sci       72  75   75  75\n        Math      71  73   75  74\n        Comp      70  71   72  73\n  Partial selection, the need for sortedness GH2995   Levels Prepending a level to a multiindex Flatten Hierarchical columns    Missing data The missing data docs. Fill forward a reversed timeseries \nIn [100]: df = pd.DataFrame(\n   .....:     np.random.randn(6, 1),\n   .....:     index=pd.date_range(\"2013-08-01\", periods=6, freq=\"B\"),\n   .....:     columns=list(\"A\"),\n   .....: )\n   .....: \n\nIn [101]: df.loc[df.index[3], \"A\"] = np.nan\n\nIn [102]: df\nOut[102]: \n                   A\n2013-08-01  0.721555\n2013-08-02 -0.706771\n2013-08-05 -1.039575\n2013-08-06       NaN\n2013-08-07 -0.424972\n2013-08-08  0.567020\n\nIn [103]: df.reindex(df.index[::-1]).ffill()\nOut[103]: \n                   A\n2013-08-08  0.567020\n2013-08-07 -0.424972\n2013-08-06 -0.424972\n2013-08-05 -1.039575\n2013-08-02 -0.706771\n2013-08-01  0.721555\n  cumsum reset at NaN values  Replace Using replace with backrefs    Grouping The grouping docs. Basic grouping with apply Unlike agg, apply\u2019s callable is passed a sub-DataFrame which gives you access to all the columns \nIn [104]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"animal\": \"cat dog cat fish dog cat cat\".split(),\n   .....:         \"size\": list(\"SSMMMLL\"),\n   .....:         \"weight\": [8, 10, 11, 1, 20, 12, 12],\n   .....:         \"adult\": [False] * 5 + [True] * 2,\n   .....:     }\n   .....: )\n   .....: \n\nIn [105]: df\nOut[105]: \n  animal size  weight  adult\n0    cat    S       8  False\n1    dog    S      10  False\n2    cat    M      11  False\n3   fish    M       1  False\n4    dog    M      20  False\n5    cat    L      12   True\n6    cat    L      12   True\n\n# List the size of the animals with the highest weight.\nIn [106]: df.groupby(\"animal\").apply(lambda subf: subf[\"size\"][subf[\"weight\"].idxmax()])\nOut[106]: \nanimal\ncat     L\ndog     M\nfish    M\ndtype: object\n  Using get_group \nIn [107]: gb = df.groupby([\"animal\"])\n\nIn [108]: gb.get_group(\"cat\")\nOut[108]: \n  animal size  weight  adult\n0    cat    S       8  False\n2    cat    M      11  False\n5    cat    L      12   True\n6    cat    L      12   True\n  Apply to different items in a group \nIn [109]: def GrowUp(x):\n   .....:     avg_weight = sum(x[x[\"size\"] == \"S\"].weight * 1.5)\n   .....:     avg_weight += sum(x[x[\"size\"] == \"M\"].weight * 1.25)\n   .....:     avg_weight += sum(x[x[\"size\"] == \"L\"].weight)\n   .....:     avg_weight \/= len(x)\n   .....:     return pd.Series([\"L\", avg_weight, True], index=[\"size\", \"weight\", \"adult\"])\n   .....: \n\nIn [110]: expected_df = gb.apply(GrowUp)\n\nIn [111]: expected_df\nOut[111]: \n       size   weight  adult\nanimal                     \ncat       L  12.4375   True\ndog       L  20.0000   True\nfish      L   1.2500   True\n  Expanding apply \nIn [112]: S = pd.Series([i \/ 100.0 for i in range(1, 11)])\n\nIn [113]: def cum_ret(x, y):\n   .....:     return x * (1 + y)\n   .....: \n\nIn [114]: def red(x):\n   .....:     return functools.reduce(cum_ret, x, 1.0)\n   .....: \n\nIn [115]: S.expanding().apply(red, raw=True)\nOut[115]: \n0    1.010000\n1    1.030200\n2    1.061106\n3    1.103550\n4    1.158728\n5    1.228251\n6    1.314229\n7    1.419367\n8    1.547110\n9    1.701821\ndtype: float64\n  Replacing some values with mean of the rest of a group \nIn [116]: df = pd.DataFrame({\"A\": [1, 1, 2, 2], \"B\": [1, -1, 1, 2]})\n\nIn [117]: gb = df.groupby(\"A\")\n\nIn [118]: def replace(g):\n   .....:     mask = g < 0\n   .....:     return g.where(mask, g[~mask].mean())\n   .....: \n\nIn [119]: gb.transform(replace)\nOut[119]: \n     B\n0  1.0\n1 -1.0\n2  1.5\n3  1.5\n  Sort groups by aggregated data \nIn [120]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"code\": [\"foo\", \"bar\", \"baz\"] * 2,\n   .....:         \"data\": [0.16, -0.21, 0.33, 0.45, -0.59, 0.62],\n   .....:         \"flag\": [False, True] * 3,\n   .....:     }\n   .....: )\n   .....: \n\nIn [121]: code_groups = df.groupby(\"code\")\n\nIn [122]: agg_n_sort_order = code_groups[[\"data\"]].transform(sum).sort_values(by=\"data\")\n\nIn [123]: sorted_df = df.loc[agg_n_sort_order.index]\n\nIn [124]: sorted_df\nOut[124]: \n  code  data   flag\n1  bar -0.21   True\n4  bar -0.59  False\n0  foo  0.16  False\n3  foo  0.45   True\n2  baz  0.33  False\n5  baz  0.62   True\n  Create multiple aggregated columns \nIn [125]: rng = pd.date_range(start=\"2014-10-07\", periods=10, freq=\"2min\")\n\nIn [126]: ts = pd.Series(data=list(range(10)), index=rng)\n\nIn [127]: def MyCust(x):\n   .....:     if len(x) > 2:\n   .....:         return x[1] * 1.234\n   .....:     return pd.NaT\n   .....: \n\nIn [128]: mhc = {\"Mean\": np.mean, \"Max\": np.max, \"Custom\": MyCust}\n\nIn [129]: ts.resample(\"5min\").apply(mhc)\nOut[129]: \n                     Mean  Max Custom\n2014-10-07 00:00:00   1.0    2  1.234\n2014-10-07 00:05:00   3.5    4    NaT\n2014-10-07 00:10:00   6.0    7  7.404\n2014-10-07 00:15:00   8.5    9    NaT\n\nIn [130]: ts\nOut[130]: \n2014-10-07 00:00:00    0\n2014-10-07 00:02:00    1\n2014-10-07 00:04:00    2\n2014-10-07 00:06:00    3\n2014-10-07 00:08:00    4\n2014-10-07 00:10:00    5\n2014-10-07 00:12:00    6\n2014-10-07 00:14:00    7\n2014-10-07 00:16:00    8\n2014-10-07 00:18:00    9\nFreq: 2T, dtype: int64\n  Create a value counts column and reassign back to the DataFrame \nIn [131]: df = pd.DataFrame(\n   .....:     {\"Color\": \"Red Red Red Blue\".split(), \"Value\": [100, 150, 50, 50]}\n   .....: )\n   .....: \n\nIn [132]: df\nOut[132]: \n  Color  Value\n0   Red    100\n1   Red    150\n2   Red     50\n3  Blue     50\n\nIn [133]: df[\"Counts\"] = df.groupby([\"Color\"]).transform(len)\n\nIn [134]: df\nOut[134]: \n  Color  Value  Counts\n0   Red    100       3\n1   Red    150       3\n2   Red     50       3\n3  Blue     50       1\n  Shift groups of the values in a column based on the index \nIn [135]: df = pd.DataFrame(\n   .....:     {\"line_race\": [10, 10, 8, 10, 10, 8], \"beyer\": [99, 102, 103, 103, 88, 100]},\n   .....:     index=[\n   .....:         \"Last Gunfighter\",\n   .....:         \"Last Gunfighter\",\n   .....:         \"Last Gunfighter\",\n   .....:         \"Paynter\",\n   .....:         \"Paynter\",\n   .....:         \"Paynter\",\n   .....:     ],\n   .....: )\n   .....: \n\nIn [136]: df\nOut[136]: \n                 line_race  beyer\nLast Gunfighter         10     99\nLast Gunfighter         10    102\nLast Gunfighter          8    103\nPaynter                 10    103\nPaynter                 10     88\nPaynter                  8    100\n\nIn [137]: df[\"beyer_shifted\"] = df.groupby(level=0)[\"beyer\"].shift(1)\n\nIn [138]: df\nOut[138]: \n                 line_race  beyer  beyer_shifted\nLast Gunfighter         10     99            NaN\nLast Gunfighter         10    102           99.0\nLast Gunfighter          8    103          102.0\nPaynter                 10    103            NaN\nPaynter                 10     88          103.0\nPaynter                  8    100           88.0\n  Select row with maximum value from each group \nIn [139]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"host\": [\"other\", \"other\", \"that\", \"this\", \"this\"],\n   .....:         \"service\": [\"mail\", \"web\", \"mail\", \"mail\", \"web\"],\n   .....:         \"no\": [1, 2, 1, 2, 1],\n   .....:     }\n   .....: ).set_index([\"host\", \"service\"])\n   .....: \n\nIn [140]: mask = df.groupby(level=0).agg(\"idxmax\")\n\nIn [141]: df_count = df.loc[mask[\"no\"]].reset_index()\n\nIn [142]: df_count\nOut[142]: \n    host service  no\n0  other     web   2\n1   that    mail   1\n2   this    mail   2\n  Grouping like Python\u2019s itertools.groupby \nIn [143]: df = pd.DataFrame([0, 1, 0, 1, 1, 1, 0, 1, 1], columns=[\"A\"])\n\nIn [144]: df[\"A\"].groupby((df[\"A\"] != df[\"A\"].shift()).cumsum()).groups\nOut[144]: {1: [0], 2: [1], 3: [2], 4: [3, 4, 5], 5: [6], 6: [7, 8]}\n\nIn [145]: df[\"A\"].groupby((df[\"A\"] != df[\"A\"].shift()).cumsum()).cumsum()\nOut[145]: \n0    0\n1    1\n2    0\n3    1\n4    2\n5    3\n6    0\n7    1\n8    2\nName: A, dtype: int64\n   Expanding data Alignment and to-date Rolling Computation window based on values instead of counts Rolling Mean by Time Interval   Splitting Splitting a frame Create a list of dataframes, split using a delineation based on logic included in rows. \nIn [146]: df = pd.DataFrame(\n   .....:     data={\n   .....:         \"Case\": [\"A\", \"A\", \"A\", \"B\", \"A\", \"A\", \"B\", \"A\", \"A\"],\n   .....:         \"Data\": np.random.randn(9),\n   .....:     }\n   .....: )\n   .....: \n\nIn [147]: dfs = list(\n   .....:     zip(\n   .....:         *df.groupby(\n   .....:             (1 * (df[\"Case\"] == \"B\"))\n   .....:             .cumsum()\n   .....:             .rolling(window=3, min_periods=1)\n   .....:             .median()\n   .....:         )\n   .....:     )\n   .....: )[-1]\n   .....: \n\nIn [148]: dfs[0]\nOut[148]: \n  Case      Data\n0    A  0.276232\n1    A -1.087401\n2    A -0.673690\n3    B  0.113648\n\nIn [149]: dfs[1]\nOut[149]: \n  Case      Data\n4    A -1.478427\n5    A  0.524988\n6    B  0.404705\n\nIn [150]: dfs[2]\nOut[150]: \n  Case      Data\n7    A  0.577046\n8    A -1.715002\n    Pivot The Pivot docs. Partial sums and subtotals \nIn [151]: df = pd.DataFrame(\n   .....:     data={\n   .....:         \"Province\": [\"ON\", \"QC\", \"BC\", \"AL\", \"AL\", \"MN\", \"ON\"],\n   .....:         \"City\": [\n   .....:             \"Toronto\",\n   .....:             \"Montreal\",\n   .....:             \"Vancouver\",\n   .....:             \"Calgary\",\n   .....:             \"Edmonton\",\n   .....:             \"Winnipeg\",\n   .....:             \"Windsor\",\n   .....:         ],\n   .....:         \"Sales\": [13, 6, 16, 8, 4, 3, 1],\n   .....:     }\n   .....: )\n   .....: \n\nIn [152]: table = pd.pivot_table(\n   .....:     df,\n   .....:     values=[\"Sales\"],\n   .....:     index=[\"Province\"],\n   .....:     columns=[\"City\"],\n   .....:     aggfunc=np.sum,\n   .....:     margins=True,\n   .....: )\n   .....: \n\nIn [153]: table.stack(\"City\")\nOut[153]: \n                    Sales\nProvince City            \nAL       All         12.0\n         Calgary      8.0\n         Edmonton     4.0\nBC       All         16.0\n         Vancouver   16.0\n...                   ...\nAll      Montreal     6.0\n         Toronto     13.0\n         Vancouver   16.0\n         Windsor      1.0\n         Winnipeg     3.0\n\n[20 rows x 1 columns]\n  Frequency table like plyr in R \nIn [154]: grades = [48, 99, 75, 80, 42, 80, 72, 68, 36, 78]\n\nIn [155]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"ID\": [\"x%d\" % r for r in range(10)],\n   .....:         \"Gender\": [\"F\", \"M\", \"F\", \"M\", \"F\", \"M\", \"F\", \"M\", \"M\", \"M\"],\n   .....:         \"ExamYear\": [\n   .....:             \"2007\",\n   .....:             \"2007\",\n   .....:             \"2007\",\n   .....:             \"2008\",\n   .....:             \"2008\",\n   .....:             \"2008\",\n   .....:             \"2008\",\n   .....:             \"2009\",\n   .....:             \"2009\",\n   .....:             \"2009\",\n   .....:         ],\n   .....:         \"Class\": [\n   .....:             \"algebra\",\n   .....:             \"stats\",\n   .....:             \"bio\",\n   .....:             \"algebra\",\n   .....:             \"algebra\",\n   .....:             \"stats\",\n   .....:             \"stats\",\n   .....:             \"algebra\",\n   .....:             \"bio\",\n   .....:             \"bio\",\n   .....:         ],\n   .....:         \"Participated\": [\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"no\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:         ],\n   .....:         \"Passed\": [\"yes\" if x > 50 else \"no\" for x in grades],\n   .....:         \"Employed\": [\n   .....:             True,\n   .....:             True,\n   .....:             True,\n   .....:             False,\n   .....:             False,\n   .....:             False,\n   .....:             False,\n   .....:             True,\n   .....:             True,\n   .....:             False,\n   .....:         ],\n   .....:         \"Grade\": grades,\n   .....:     }\n   .....: )\n   .....: \n\nIn [156]: df.groupby(\"ExamYear\").agg(\n   .....:     {\n   .....:         \"Participated\": lambda x: x.value_counts()[\"yes\"],\n   .....:         \"Passed\": lambda x: sum(x == \"yes\"),\n   .....:         \"Employed\": lambda x: sum(x),\n   .....:         \"Grade\": lambda x: sum(x) \/ len(x),\n   .....:     }\n   .....: )\n   .....: \nOut[156]: \n          Participated  Passed  Employed      Grade\nExamYear                                           \n2007                 3       2         3  74.000000\n2008                 3       3         0  68.500000\n2009                 3       2         2  60.666667\n  Plot pandas DataFrame with year over year data To create year and month cross tabulation: \nIn [157]: df = pd.DataFrame(\n   .....:     {\"value\": np.random.randn(36)},\n   .....:     index=pd.date_range(\"2011-01-01\", freq=\"M\", periods=36),\n   .....: )\n   .....: \n\nIn [158]: pd.pivot_table(\n   .....:     df, index=df.index.month, columns=df.index.year, values=\"value\", aggfunc=\"sum\"\n   .....: )\n   .....: \nOut[158]: \n        2011      2012      2013\n1  -1.039268 -0.968914  2.565646\n2  -0.370647 -1.294524  1.431256\n3  -1.157892  0.413738  1.340309\n4  -1.344312  0.276662 -1.170299\n5   0.844885 -0.472035 -0.226169\n6   1.075770 -0.013960  0.410835\n7  -0.109050 -0.362543  0.813850\n8   1.643563 -0.006154  0.132003\n9  -1.469388 -0.923061 -0.827317\n10  0.357021  0.895717 -0.076467\n11 -0.674600  0.805244 -1.187678\n12 -1.776904 -1.206412  1.130127\n    Apply Rolling apply to organize - Turning embedded lists into a MultiIndex frame \nIn [159]: df = pd.DataFrame(\n   .....:     data={\n   .....:         \"A\": [[2, 4, 8, 16], [100, 200], [10, 20, 30]],\n   .....:         \"B\": [[\"a\", \"b\", \"c\"], [\"jj\", \"kk\"], [\"ccc\"]],\n   .....:     },\n   .....:     index=[\"I\", \"II\", \"III\"],\n   .....: )\n   .....: \n\nIn [160]: def SeriesFromSubList(aList):\n   .....:     return pd.Series(aList)\n   .....: \n\nIn [161]: df_orgz = pd.concat(\n   .....:     {ind: row.apply(SeriesFromSubList) for ind, row in df.iterrows()}\n   .....: )\n   .....: \n\nIn [162]: df_orgz\nOut[162]: \n         0     1     2     3\nI   A    2     4     8  16.0\n    B    a     b     c   NaN\nII  A  100   200   NaN   NaN\n    B   jj    kk   NaN   NaN\nIII A   10  20.0  30.0   NaN\n    B  ccc   NaN   NaN   NaN\n  Rolling apply with a DataFrame returning a Series Rolling Apply to multiple columns where function calculates a Series before a Scalar from the Series is returned \nIn [163]: df = pd.DataFrame(\n   .....:     data=np.random.randn(2000, 2) \/ 10000,\n   .....:     index=pd.date_range(\"2001-01-01\", periods=2000),\n   .....:     columns=[\"A\", \"B\"],\n   .....: )\n   .....: \n\nIn [164]: df\nOut[164]: \n                   A         B\n2001-01-01 -0.000144 -0.000141\n2001-01-02  0.000161  0.000102\n2001-01-03  0.000057  0.000088\n2001-01-04 -0.000221  0.000097\n2001-01-05 -0.000201 -0.000041\n...              ...       ...\n2006-06-19  0.000040 -0.000235\n2006-06-20 -0.000123 -0.000021\n2006-06-21 -0.000113  0.000114\n2006-06-22  0.000136  0.000109\n2006-06-23  0.000027  0.000030\n\n[2000 rows x 2 columns]\n\nIn [165]: def gm(df, const):\n   .....:     v = ((((df[\"A\"] + df[\"B\"]) + 1).cumprod()) - 1) * const\n   .....:     return v.iloc[-1]\n   .....: \n\nIn [166]: s = pd.Series(\n   .....:     {\n   .....:         df.index[i]: gm(df.iloc[i: min(i + 51, len(df) - 1)], 5)\n   .....:         for i in range(len(df) - 50)\n   .....:     }\n   .....: )\n   .....: \n\nIn [167]: s\nOut[167]: \n2001-01-01    0.000930\n2001-01-02    0.002615\n2001-01-03    0.001281\n2001-01-04    0.001117\n2001-01-05    0.002772\n                ...   \n2006-04-30    0.003296\n2006-05-01    0.002629\n2006-05-02    0.002081\n2006-05-03    0.004247\n2006-05-04    0.003928\nLength: 1950, dtype: float64\n  Rolling apply with a DataFrame returning a Scalar Rolling Apply to multiple columns where function returns a Scalar (Volume Weighted Average Price) \nIn [168]: rng = pd.date_range(start=\"2014-01-01\", periods=100)\n\nIn [169]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"Open\": np.random.randn(len(rng)),\n   .....:         \"Close\": np.random.randn(len(rng)),\n   .....:         \"Volume\": np.random.randint(100, 2000, len(rng)),\n   .....:     },\n   .....:     index=rng,\n   .....: )\n   .....: \n\nIn [170]: df\nOut[170]: \n                Open     Close  Volume\n2014-01-01 -1.611353 -0.492885    1219\n2014-01-02 -3.000951  0.445794    1054\n2014-01-03 -0.138359 -0.076081    1381\n2014-01-04  0.301568  1.198259    1253\n2014-01-05  0.276381 -0.669831    1728\n...              ...       ...     ...\n2014-04-06 -0.040338  0.937843    1188\n2014-04-07  0.359661 -0.285908    1864\n2014-04-08  0.060978  1.714814     941\n2014-04-09  1.759055 -0.455942    1065\n2014-04-10  0.138185 -1.147008    1453\n\n[100 rows x 3 columns]\n\nIn [171]: def vwap(bars):\n   .....:     return (bars.Close * bars.Volume).sum() \/ bars.Volume.sum()\n   .....: \n\nIn [172]: window = 5\n\nIn [173]: s = pd.concat(\n   .....:     [\n   .....:         (pd.Series(vwap(df.iloc[i: i + window]), index=[df.index[i + window]]))\n   .....:         for i in range(len(df) - window)\n   .....:     ]\n   .....: )\n   .....: \n\nIn [174]: s.round(2)\nOut[174]: \n2014-01-06    0.02\n2014-01-07    0.11\n2014-01-08    0.10\n2014-01-09    0.07\n2014-01-10   -0.29\n              ... \n2014-04-06   -0.63\n2014-04-07   -0.02\n2014-04-08   -0.03\n2014-04-09    0.34\n2014-04-10    0.29\nLength: 95, dtype: float64\n     Timeseries Between times Using indexer between time Constructing a datetime range that excludes weekends and includes only certain times Vectorized Lookup Aggregation and plotting time series Turn a matrix with hours in columns and days in rows into a continuous row sequence in the form of a time series. How to rearrange a Python pandas DataFrame? Dealing with duplicates when reindexing a timeseries to a specified frequency Calculate the first day of the month for each entry in a DatetimeIndex \nIn [175]: dates = pd.date_range(\"2000-01-01\", periods=5)\n\nIn [176]: dates.to_period(freq=\"M\").to_timestamp()\nOut[176]: \nDatetimeIndex(['2000-01-01', '2000-01-01', '2000-01-01', '2000-01-01',\n               '2000-01-01'],\n              dtype='datetime64[ns]', freq=None)\n   Resampling The Resample docs. Using Grouper instead of TimeGrouper for time grouping of values Time grouping with some missing values Valid frequency arguments to Grouper Timeseries Grouping using a MultiIndex Using TimeGrouper and another grouping to create subgroups, then apply a custom function GH3791 Resampling with custom periods Resample intraday frame without adding new days Resample minute data Resample with groupby    Merge The Join docs. Concatenate two dataframes with overlapping index (emulate R rbind) \nIn [177]: rng = pd.date_range(\"2000-01-01\", periods=6)\n\nIn [178]: df1 = pd.DataFrame(np.random.randn(6, 3), index=rng, columns=[\"A\", \"B\", \"C\"])\n\nIn [179]: df2 = df1.copy()\n  Depending on df construction, ignore_index may be needed \nIn [180]: df = pd.concat([df1, df2], ignore_index=True)\n\nIn [181]: df\nOut[181]: \n           A         B         C\n0  -0.870117 -0.479265 -0.790855\n1   0.144817  1.726395 -0.464535\n2  -0.821906  1.597605  0.187307\n3  -0.128342 -1.511638 -0.289858\n4   0.399194 -1.430030 -0.639760\n5   1.115116 -2.012600  1.810662\n6  -0.870117 -0.479265 -0.790855\n7   0.144817  1.726395 -0.464535\n8  -0.821906  1.597605  0.187307\n9  -0.128342 -1.511638 -0.289858\n10  0.399194 -1.430030 -0.639760\n11  1.115116 -2.012600  1.810662\n  Self Join of a DataFrame GH2996 \nIn [182]: df = pd.DataFrame(\n   .....:     data={\n   .....:         \"Area\": [\"A\"] * 5 + [\"C\"] * 2,\n   .....:         \"Bins\": [110] * 2 + [160] * 3 + [40] * 2,\n   .....:         \"Test_0\": [0, 1, 0, 1, 2, 0, 1],\n   .....:         \"Data\": np.random.randn(7),\n   .....:     }\n   .....: )\n   .....: \n\nIn [183]: df\nOut[183]: \n  Area  Bins  Test_0      Data\n0    A   110       0 -0.433937\n1    A   110       1 -0.160552\n2    A   160       0  0.744434\n3    A   160       1  1.754213\n4    A   160       2  0.000850\n5    C    40       0  0.342243\n6    C    40       1  1.070599\n\nIn [184]: df[\"Test_1\"] = df[\"Test_0\"] - 1\n\nIn [185]: pd.merge(\n   .....:     df,\n   .....:     df,\n   .....:     left_on=[\"Bins\", \"Area\", \"Test_0\"],\n   .....:     right_on=[\"Bins\", \"Area\", \"Test_1\"],\n   .....:     suffixes=(\"_L\", \"_R\"),\n   .....: )\n   .....: \nOut[185]: \n  Area  Bins  Test_0_L    Data_L  Test_1_L  Test_0_R    Data_R  Test_1_R\n0    A   110         0 -0.433937        -1         1 -0.160552         0\n1    A   160         0  0.744434        -1         1  1.754213         0\n2    A   160         1  1.754213         0         2  0.000850         1\n3    C    40         0  0.342243        -1         1  1.070599         0\n  How to set the index and join KDB like asof join Join with a criteria based on the values Using searchsorted to merge based on values inside a range   Plotting The Plotting docs. Make Matplotlib look like R Setting x-axis major and minor labels Plotting multiple charts in an IPython Jupyter notebook Creating a multi-line plot Plotting a heatmap Annotate a time-series plot Annotate a time-series plot #2 Generate Embedded plots in excel files using Pandas, Vincent and xlsxwriter Boxplot for each quartile of a stratifying variable \nIn [186]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"stratifying_var\": np.random.uniform(0, 100, 20),\n   .....:         \"price\": np.random.normal(100, 5, 20),\n   .....:     }\n   .....: )\n   .....: \n\nIn [187]: df[\"quartiles\"] = pd.qcut(\n   .....:     df[\"stratifying_var\"], 4, labels=[\"0-25%\", \"25-50%\", \"50-75%\", \"75-100%\"]\n   .....: )\n   .....: \n\nIn [188]: df.boxplot(column=\"price\", by=\"quartiles\")\nOut[188]: <AxesSubplot:title={'center':'price'}, xlabel='quartiles'>\n     Data in\/out Performance comparison of SQL vs HDF5  CSV The CSV docs read_csv in action appending to a csv Reading a csv chunk-by-chunk Reading only certain rows of a csv chunk-by-chunk Reading the first few lines of a frame Reading a file that is compressed but not by gzip\/bz2 (the native compressed formats which read_csv understands). This example shows a WinZipped file, but is a general application of opening the file within a context manager and using that handle to read. See here Inferring dtypes from a file Dealing with bad lines GH2886 Write a multi-row index CSV without writing duplicates  Reading multiple files to create a single DataFrame The best way to combine multiple files into a single DataFrame is to read the individual frames one by one, put all of the individual frames into a list, and then combine the frames in the list using pd.concat(): \nIn [189]: for i in range(3):\n   .....:     data = pd.DataFrame(np.random.randn(10, 4))\n   .....:     data.to_csv(\"file_{}.csv\".format(i))\n   .....: \n\nIn [190]: files = [\"file_0.csv\", \"file_1.csv\", \"file_2.csv\"]\n\nIn [191]: result = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n  You can use the same approach to read all files matching a pattern. Here is an example using glob: \nIn [192]: import glob\n\nIn [193]: import os\n\nIn [194]: files = glob.glob(\"file_*.csv\")\n\nIn [195]: result = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n  Finally, this strategy will work with the other pd.read_*(...) functions described in the io docs.   Parsing date components in multi-columns Parsing date components in multi-columns is faster with a format \nIn [196]: i = pd.date_range(\"20000101\", periods=10000)\n\nIn [197]: df = pd.DataFrame({\"year\": i.year, \"month\": i.month, \"day\": i.day})\n\nIn [198]: df.head()\nOut[198]: \n   year  month  day\n0  2000      1    1\n1  2000      1    2\n2  2000      1    3\n3  2000      1    4\n4  2000      1    5\n\nIn [199]: %timeit pd.to_datetime(df.year * 10000 + df.month * 100 + df.day, format='%Y%m%d')\n   .....: ds = df.apply(lambda x: \"%04d%02d%02d\" % (x[\"year\"], x[\"month\"], x[\"day\"]), axis=1)\n   .....: ds.head()\n   .....: %timeit pd.to_datetime(ds)\n   .....: \n8.7 ms +- 765 us per loop (mean +- std. dev. of 7 runs, 100 loops each)\n2.1 ms +- 419 us per loop (mean +- std. dev. of 7 runs, 100 loops each)\n    Skip row between header and data \nIn [200]: data = \"\"\";;;;\n   .....:  ;;;;\n   .....:  ;;;;\n   .....:  ;;;;\n   .....:  ;;;;\n   .....:  ;;;;\n   .....: ;;;;\n   .....:  ;;;;\n   .....:  ;;;;\n   .....: ;;;;\n   .....: date;Param1;Param2;Param4;Param5\n   .....:     ;m\u00b2;\u00b0C;m\u00b2;m\n   .....: ;;;;\n   .....: 01.01.1990 00:00;1;1;2;3\n   .....: 01.01.1990 01:00;5;3;4;5\n   .....: 01.01.1990 02:00;9;5;6;7\n   .....: 01.01.1990 03:00;13;7;8;9\n   .....: 01.01.1990 04:00;17;9;10;11\n   .....: 01.01.1990 05:00;21;11;12;13\n   .....: \"\"\"\n   .....: \n   Option 1: pass rows explicitly to skip rows \nIn [201]: from io import StringIO\n\nIn [202]: pd.read_csv(\n   .....:     StringIO(data),\n   .....:     sep=\";\",\n   .....:     skiprows=[11, 12],\n   .....:     index_col=0,\n   .....:     parse_dates=True,\n   .....:     header=10,\n   .....: )\n   .....: \nOut[202]: \n                     Param1  Param2  Param4  Param5\ndate                                               \n1990-01-01 00:00:00       1       1       2       3\n1990-01-01 01:00:00       5       3       4       5\n1990-01-01 02:00:00       9       5       6       7\n1990-01-01 03:00:00      13       7       8       9\n1990-01-01 04:00:00      17       9      10      11\n1990-01-01 05:00:00      21      11      12      13\n    Option 2: read column names and then data \nIn [203]: pd.read_csv(StringIO(data), sep=\";\", header=10, nrows=10).columns\nOut[203]: Index(['date', 'Param1', 'Param2', 'Param4', 'Param5'], dtype='object')\n\nIn [204]: columns = pd.read_csv(StringIO(data), sep=\";\", header=10, nrows=10).columns\n\nIn [205]: pd.read_csv(\n   .....:     StringIO(data), sep=\";\", index_col=0, header=12, parse_dates=True, names=columns\n   .....: )\n   .....: \nOut[205]: \n                     Param1  Param2  Param4  Param5\ndate                                               \n1990-01-01 00:00:00       1       1       2       3\n1990-01-01 01:00:00       5       3       4       5\n1990-01-01 02:00:00       9       5       6       7\n1990-01-01 03:00:00      13       7       8       9\n1990-01-01 04:00:00      17       9      10      11\n1990-01-01 05:00:00      21      11      12      13\n      SQL The SQL docs Reading from databases with SQL   Excel The Excel docs Reading from a filelike handle Modifying formatting in XlsxWriter output Loading only visible sheets GH19842#issuecomment-892150745   HTML Reading HTML tables from a server that cannot handle the default request header   HDFStore The HDFStores docs Simple queries with a Timestamp Index Managing heterogeneous data using a linked multiple table hierarchy GH3032 Merging on-disk tables with millions of rows Avoiding inconsistencies when writing to a store from multiple processes\/threads De-duplicating a large store by chunks, essentially a recursive reduction operation. Shows a function for taking in data from csv file and creating a store by chunks, with date parsing as well. See here Creating a store chunk-by-chunk from a csv file Appending to a store, while creating a unique index Large Data work flows Reading in a sequence of files, then providing a global unique index to a store while appending Groupby on a HDFStore with low group density Groupby on a HDFStore with high group density Hierarchical queries on a HDFStore Counting with a HDFStore Troubleshoot HDFStore exceptions Setting min_itemsize with strings Using ptrepack to create a completely-sorted-index on a store Storing Attributes to a group node \nIn [206]: df = pd.DataFrame(np.random.randn(8, 3))\n\nIn [207]: store = pd.HDFStore(\"test.h5\")\n\nIn [208]: store.put(\"df\", df)\n\n# you can store an arbitrary Python object via pickle\nIn [209]: store.get_storer(\"df\").attrs.my_attribute = {\"A\": 10}\n\nIn [210]: store.get_storer(\"df\").attrs.my_attribute\nOut[210]: {'A': 10}\n  You can create or load a HDFStore in-memory by passing the driver parameter to PyTables. Changes are only written to disk when the HDFStore is closed. \nIn [211]: store = pd.HDFStore(\"test.h5\", \"w\", driver=\"H5FD_CORE\")\n\nIn [212]: df = pd.DataFrame(np.random.randn(8, 3))\n\nIn [213]: store[\"test\"] = df\n\n# only after closing the store, data is written to disk:\nIn [214]: store.close()\n    Binary files pandas readily accepts NumPy record arrays, if you need to read in a binary file consisting of an array of C structs. For example, given this C program in a file called main.c compiled with gcc main.c -std=gnu99 on a 64-bit machine, \n#include <stdio.h>\n#include <stdint.h>\n\ntypedef struct _Data\n{\n    int32_t count;\n    double avg;\n    float scale;\n} Data;\n\nint main(int argc, const char *argv[])\n{\n    size_t n = 10;\n    Data d[n];\n\n    for (int i = 0; i < n; ++i)\n    {\n        d[i].count = i;\n        d[i].avg = i + 1.0;\n        d[i].scale = (float) i + 2.0f;\n    }\n\n    FILE *file = fopen(\"binary.dat\", \"wb\");\n    fwrite(&d, sizeof(Data), n, file);\n    fclose(file);\n\n    return 0;\n}\n  the following Python code will read the binary file 'binary.dat' into a pandas DataFrame, where each element of the struct corresponds to a column in the frame: \nnames = \"count\", \"avg\", \"scale\"\n\n# note that the offsets are larger than the size of the type because of\n# struct padding\noffsets = 0, 8, 16\nformats = \"i4\", \"f8\", \"f4\"\ndt = np.dtype({\"names\": names, \"offsets\": offsets, \"formats\": formats}, align=True)\ndf = pd.DataFrame(np.fromfile(\"binary.dat\", dt))\n   Note The offsets of the structure elements may be different depending on the architecture of the machine on which the file was created. Using a raw binary file format like this for general data storage is not recommended, as it is not cross platform. We recommended either HDF5 or parquet, both of which are supported by pandas\u2019 IO facilities.     Computation Numerical integration (sample-based) of a time series  Correlation Often it\u2019s useful to obtain the lower (or upper) triangular form of a correlation matrix calculated from DataFrame.corr(). This can be achieved by passing a boolean mask to where as follows: \nIn [215]: df = pd.DataFrame(np.random.random(size=(100, 5)))\n\nIn [216]: corr_mat = df.corr()\n\nIn [217]: mask = np.tril(np.ones_like(corr_mat, dtype=np.bool_), k=-1)\n\nIn [218]: corr_mat.where(mask)\nOut[218]: \n          0         1         2        3   4\n0       NaN       NaN       NaN      NaN NaN\n1 -0.079861       NaN       NaN      NaN NaN\n2 -0.236573  0.183801       NaN      NaN NaN\n3 -0.013795 -0.051975  0.037235      NaN NaN\n4 -0.031974  0.118342 -0.073499 -0.02063 NaN\n  The method argument within DataFrame.corr can accept a callable in addition to the named correlation types. Here we compute the distance correlation matrix for a DataFrame object. \nIn [219]: def distcorr(x, y):\n   .....:     n = len(x)\n   .....:     a = np.zeros(shape=(n, n))\n   .....:     b = np.zeros(shape=(n, n))\n   .....:     for i in range(n):\n   .....:         for j in range(i + 1, n):\n   .....:             a[i, j] = abs(x[i] - x[j])\n   .....:             b[i, j] = abs(y[i] - y[j])\n   .....:     a += a.T\n   .....:     b += b.T\n   .....:     a_bar = np.vstack([np.nanmean(a, axis=0)] * n)\n   .....:     b_bar = np.vstack([np.nanmean(b, axis=0)] * n)\n   .....:     A = a - a_bar - a_bar.T + np.full(shape=(n, n), fill_value=a_bar.mean())\n   .....:     B = b - b_bar - b_bar.T + np.full(shape=(n, n), fill_value=b_bar.mean())\n   .....:     cov_ab = np.sqrt(np.nansum(A * B)) \/ n\n   .....:     std_a = np.sqrt(np.sqrt(np.nansum(A ** 2)) \/ n)\n   .....:     std_b = np.sqrt(np.sqrt(np.nansum(B ** 2)) \/ n)\n   .....:     return cov_ab \/ std_a \/ std_b\n   .....: \n\nIn [220]: df = pd.DataFrame(np.random.normal(size=(100, 3)))\n\nIn [221]: df.corr(method=distcorr)\nOut[221]: \n          0         1         2\n0  1.000000  0.197613  0.216328\n1  0.197613  1.000000  0.208749\n2  0.216328  0.208749  1.000000\n     Timedeltas The Timedeltas docs. Using timedeltas \nIn [222]: import datetime\n\nIn [223]: s = pd.Series(pd.date_range(\"2012-1-1\", periods=3, freq=\"D\"))\n\nIn [224]: s - s.max()\nOut[224]: \n0   -2 days\n1   -1 days\n2    0 days\ndtype: timedelta64[ns]\n\nIn [225]: s.max() - s\nOut[225]: \n0   2 days\n1   1 days\n2   0 days\ndtype: timedelta64[ns]\n\nIn [226]: s - datetime.datetime(2011, 1, 1, 3, 5)\nOut[226]: \n0   364 days 20:55:00\n1   365 days 20:55:00\n2   366 days 20:55:00\ndtype: timedelta64[ns]\n\nIn [227]: s + datetime.timedelta(minutes=5)\nOut[227]: \n0   2012-01-01 00:05:00\n1   2012-01-02 00:05:00\n2   2012-01-03 00:05:00\ndtype: datetime64[ns]\n\nIn [228]: datetime.datetime(2011, 1, 1, 3, 5) - s\nOut[228]: \n0   -365 days +03:05:00\n1   -366 days +03:05:00\n2   -367 days +03:05:00\ndtype: timedelta64[ns]\n\nIn [229]: datetime.timedelta(minutes=5) + s\nOut[229]: \n0   2012-01-01 00:05:00\n1   2012-01-02 00:05:00\n2   2012-01-03 00:05:00\ndtype: datetime64[ns]\n  Adding and subtracting deltas and dates \nIn [230]: deltas = pd.Series([datetime.timedelta(days=i) for i in range(3)])\n\nIn [231]: df = pd.DataFrame({\"A\": s, \"B\": deltas})\n\nIn [232]: df\nOut[232]: \n           A      B\n0 2012-01-01 0 days\n1 2012-01-02 1 days\n2 2012-01-03 2 days\n\nIn [233]: df[\"New Dates\"] = df[\"A\"] + df[\"B\"]\n\nIn [234]: df[\"Delta\"] = df[\"A\"] - df[\"New Dates\"]\n\nIn [235]: df\nOut[235]: \n           A      B  New Dates   Delta\n0 2012-01-01 0 days 2012-01-01  0 days\n1 2012-01-02 1 days 2012-01-03 -1 days\n2 2012-01-03 2 days 2012-01-05 -2 days\n\nIn [236]: df.dtypes\nOut[236]: \nA             datetime64[ns]\nB            timedelta64[ns]\nNew Dates     datetime64[ns]\nDelta        timedelta64[ns]\ndtype: object\n  Another example Values can be set to NaT using np.nan, similar to datetime \nIn [237]: y = s - s.shift()\n\nIn [238]: y\nOut[238]: \n0      NaT\n1   1 days\n2   1 days\ndtype: timedelta64[ns]\n\nIn [239]: y[1] = np.nan\n\nIn [240]: y\nOut[240]: \n0      NaT\n1      NaT\n2   1 days\ndtype: timedelta64[ns]\n    Creating example data To create a dataframe from every combination of some given values, like R\u2019s expand.grid() function, we can create a dict where the keys are column names and the values are lists of the data values: \nIn [241]: def expand_grid(data_dict):\n   .....:     rows = itertools.product(*data_dict.values())\n   .....:     return pd.DataFrame.from_records(rows, columns=data_dict.keys())\n   .....: \n\nIn [242]: df = expand_grid(\n   .....:     {\"height\": [60, 70], \"weight\": [100, 140, 180], \"sex\": [\"Male\", \"Female\"]}\n   .....: )\n   .....: \n\nIn [243]: df\nOut[243]: \n    height  weight     sex\n0       60     100    Male\n1       60     100  Female\n2       60     140    Male\n3       60     140  Female\n4       60     180    Male\n5       60     180  Female\n6       70     100    Male\n7       70     100  Female\n8       70     140    Male\n9       70     140  Female\n10      70     180    Male\n11      70     180  Female","title":"pandas.user_guide.cookbook"},{"text":"pandas.DataFrame.bool   DataFrame.bool()[source]\n \nReturn the bool of a single element Series or DataFrame. This must be a boolean scalar value, either True or False. It will raise a ValueError if the Series or DataFrame does not have exactly 1 element, or that element is not boolean (integer values 0 and 1 will also raise an exception).  Returns \n bool\n\nThe value in the Series or DataFrame.      See also  Series.astype\n\nChange the data type of a Series, including to boolean.  DataFrame.astype\n\nChange the data type of a DataFrame, including to boolean.  numpy.bool_\n\nNumPy boolean data type, used by pandas for boolean values.    Examples The method will only work for single element objects with a boolean value: \n>>> pd.Series([True]).bool()\nTrue\n>>> pd.Series([False]).bool()\nFalse\n  \n>>> pd.DataFrame({'col': [True]}).bool()\nTrue\n>>> pd.DataFrame({'col': [False]}).bool()\nFalse","title":"pandas.reference.api.pandas.dataframe.bool"},{"text":"numpy.matrix.nonzero method   matrix.nonzero()\n \nReturn the indices of the elements that are non-zero. Refer to numpy.nonzero for full documentation.  See also  numpy.nonzero\n\nequivalent function","title":"numpy.reference.generated.numpy.matrix.nonzero"},{"text":"pandas.BooleanDtype   classpandas.BooleanDtype[source]\n \nExtension dtype for boolean data.  New in version 1.0.0.   Warning BooleanDtype is considered experimental. The implementation and parts of the API may change without warning.  Examples \n>>> pd.BooleanDtype()\nBooleanDtype\n  Attributes       \nNone     Methods       \nNone","title":"pandas.reference.api.pandas.booleandtype"},{"text":"pandas.arrays.BooleanArray   classpandas.arrays.BooleanArray(values, mask, copy=False)[source]\n \nArray of boolean (True\/False) data with missing values. This is a pandas Extension array for boolean data, under the hood represented by 2 numpy arrays: a boolean array with the data and a boolean array with the mask (True indicating missing). BooleanArray implements Kleene logic (sometimes called three-value logic) for logical operations. See Kleene logical operations for more. To construct an BooleanArray from generic array-like input, use pandas.array() specifying dtype=\"boolean\" (see examples below).  New in version 1.0.0.   Warning BooleanArray is considered experimental. The implementation and parts of the API may change without warning.   Parameters \n \nvalues:numpy.ndarray\n\n\nA 1-d boolean-dtype array with the data.  \nmask:numpy.ndarray\n\n\nA 1-d boolean-dtype array indicating missing values (True indicates missing).  \ncopy:bool, default False\n\n\nWhether to copy the values and mask arrays.    Returns \n BooleanArray\n   Examples Create an BooleanArray with pandas.array(): \n>>> pd.array([True, False, None], dtype=\"boolean\")\n<BooleanArray>\n[True, False, <NA>]\nLength: 3, dtype: boolean\n  Attributes       \nNone     Methods       \nNone","title":"pandas.reference.api.pandas.arrays.booleanarray"},{"text":"numpy.ndarray.nonzero method   ndarray.nonzero()\n \nReturn the indices of the elements that are non-zero. Refer to numpy.nonzero for full documentation.  See also  numpy.nonzero\n\nequivalent function","title":"numpy.reference.generated.numpy.ndarray.nonzero"},{"text":"asyncio.run(coro, *, debug=False)  \nExecute the coroutine coro and return the result. This function runs the passed coroutine, taking care of managing the asyncio event loop, finalizing asynchronous generators, and closing the threadpool. This function cannot be called when another asyncio event loop is running in the same thread. If debug is True, the event loop will be run in debug mode. This function always creates a new event loop and closes it at the end. It should be used as a main entry point for asyncio programs, and should ideally only be called once. Example: async def main():\n    await asyncio.sleep(1)\n    print('hello')\n\nasyncio.run(main())\n  New in version 3.7.   Changed in version 3.9: Updated to use loop.shutdown_default_executor().   Note The source code for asyncio.run() can be found in Lib\/asyncio\/runners.py.","title":"python.library.asyncio-task#asyncio.run"},{"text":"class torch.distributions.studentT.StudentT(df, loc=0.0, scale=1.0, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a Student\u2019s t-distribution parameterized by degree of freedom df, mean loc and scale scale. Example: >>> m = StudentT(torch.tensor([2.0]))\n>>> m.sample()  # Student's t-distributed with degrees of freedom=2\ntensor([ 0.1046])\n  Parameters \n \ndf (float or Tensor) \u2013 degrees of freedom \nloc (float or Tensor) \u2013 mean of the distribution \nscale (float or Tensor) \u2013 scale of the distribution     \narg_constraints = {'df': GreaterThan(lower_bound=0.0), 'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} \n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = Real() \n  \nproperty variance","title":"torch.distributions#torch.distributions.studentT.StudentT"}]}
{"task_id":21800169,"prompt":"def f_21800169(df):\n\treturn ","suffix":"","canonical_solution":"df[df['BoolCol']].index.tolist()","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    df = pd.DataFrame([[True, 2, 3], [False, 5, 6]], columns = ['BoolCol', 'a', 'b'])\n    y = candidate(df)\n    assert y == [0]\n"],"entry_point":"f_21800169","intent":"get index of rows in dataframe `df` which column 'BoolCol' matches value True","library":["pandas"],"docs":[{"text":"pandas.DataFrame.lookup   DataFrame.lookup(row_labels, col_labels)[source]\n \nLabel-based \u201cfancy indexing\u201d function for DataFrame. Given equal-length arrays of row and column labels, return an array of the values corresponding to each (row, col) pair.  Deprecated since version 1.2.0: DataFrame.lookup is deprecated, use DataFrame.melt and DataFrame.loc instead. For further details see Looking up values by index\/column labels.   Parameters \n \nrow_labels:sequence\n\n\nThe row labels to use for lookup.  \ncol_labels:sequence\n\n\nThe column labels to use for lookup.    Returns \n numpy.ndarray\n\nThe found values.","title":"pandas.reference.api.pandas.dataframe.lookup"},{"text":"pandas.DataFrame.bool   DataFrame.bool()[source]\n \nReturn the bool of a single element Series or DataFrame. This must be a boolean scalar value, either True or False. It will raise a ValueError if the Series or DataFrame does not have exactly 1 element, or that element is not boolean (integer values 0 and 1 will also raise an exception).  Returns \n bool\n\nThe value in the Series or DataFrame.      See also  Series.astype\n\nChange the data type of a Series, including to boolean.  DataFrame.astype\n\nChange the data type of a DataFrame, including to boolean.  numpy.bool_\n\nNumPy boolean data type, used by pandas for boolean values.    Examples The method will only work for single element objects with a boolean value: \n>>> pd.Series([True]).bool()\nTrue\n>>> pd.Series([False]).bool()\nFalse\n  \n>>> pd.DataFrame({'col': [True]}).bool()\nTrue\n>>> pd.DataFrame({'col': [False]}).bool()\nFalse","title":"pandas.reference.api.pandas.dataframe.bool"},{"text":"pandas.Index.asof_locs   Index.asof_locs(where, mask)[source]\n \nReturn the locations (indices) of labels in the index. As in the asof function, if the label (a particular entry in where) is not in the index, the latest index label up to the passed label is chosen and its index returned. If all of the labels in the index are later than a label in where, -1 is returned. mask is used to ignore NA values in the index during calculation.  Parameters \n \nwhere:Index\n\n\nAn Index consisting of an array of timestamps.  \nmask:np.ndarray[bool]\n\n\nArray of booleans denoting where values in the original data are not NA.    Returns \n np.ndarray[np.intp]\n\nAn array of locations (indices) of the labels from the Index which correspond to the return values of the asof function for every element in where.","title":"pandas.reference.api.pandas.index.asof_locs"},{"text":"Cookbook This is a repository for short and sweet examples and links for useful pandas recipes. We encourage users to add to this documentation. Adding interesting links and\/or inline examples to this section is a great First Pull Request. Simplified, condensed, new-user friendly, in-line examples have been inserted where possible to augment the Stack-Overflow and GitHub links. Many of the links contain expanded information, above what the in-line examples offer. pandas (pd) and NumPy (np) are the only two abbreviated imported modules. The rest are kept explicitly imported for newer users.  Idioms These are some neat pandas idioms if-then\/if-then-else on one column, and assignment to another one or more columns: \nIn [1]: df = pd.DataFrame(\n   ...:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ...: )\n   ...: \n\nIn [2]: df\nOut[2]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n   if-then\u2026 An if-then on one column \nIn [3]: df.loc[df.AAA >= 5, \"BBB\"] = -1\n\nIn [4]: df\nOut[4]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   -1   50\n2    6   -1  -30\n3    7   -1  -50\n  An if-then with assignment to 2 columns: \nIn [5]: df.loc[df.AAA >= 5, [\"BBB\", \"CCC\"]] = 555\n\nIn [6]: df\nOut[6]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5  555  555\n2    6  555  555\n3    7  555  555\n  Add another line with different logic, to do the -else \nIn [7]: df.loc[df.AAA < 5, [\"BBB\", \"CCC\"]] = 2000\n\nIn [8]: df\nOut[8]: \n   AAA   BBB   CCC\n0    4  2000  2000\n1    5   555   555\n2    6   555   555\n3    7   555   555\n  Or use pandas where after you\u2019ve set up a mask \nIn [9]: df_mask = pd.DataFrame(\n   ...:     {\"AAA\": [True] * 4, \"BBB\": [False] * 4, \"CCC\": [True, False] * 2}\n   ...: )\n   ...: \n\nIn [10]: df.where(df_mask, -1000)\nOut[10]: \n   AAA   BBB   CCC\n0    4 -1000  2000\n1    5 -1000 -1000\n2    6 -1000   555\n3    7 -1000 -1000\n  if-then-else using NumPy\u2019s where() \nIn [11]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [12]: df\nOut[12]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [13]: df[\"logic\"] = np.where(df[\"AAA\"] > 5, \"high\", \"low\")\n\nIn [14]: df\nOut[14]: \n   AAA  BBB  CCC logic\n0    4   10  100   low\n1    5   20   50   low\n2    6   30  -30  high\n3    7   40  -50  high\n    Splitting Split a frame with a boolean criterion \nIn [15]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [16]: df\nOut[16]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [17]: df[df.AAA <= 5]\nOut[17]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n\nIn [18]: df[df.AAA > 5]\nOut[18]: \n   AAA  BBB  CCC\n2    6   30  -30\n3    7   40  -50\n    Building criteria Select with multi-column criteria \nIn [19]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [20]: df\nOut[20]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n  \u2026and (without assignment returns a Series) \nIn [21]: df.loc[(df[\"BBB\"] < 25) & (df[\"CCC\"] >= -40), \"AAA\"]\nOut[21]: \n0    4\n1    5\nName: AAA, dtype: int64\n  \u2026or (without assignment returns a Series) \nIn [22]: df.loc[(df[\"BBB\"] > 25) | (df[\"CCC\"] >= -40), \"AAA\"]\nOut[22]: \n0    4\n1    5\n2    6\n3    7\nName: AAA, dtype: int64\n  \u2026or (with assignment modifies the DataFrame.) \nIn [23]: df.loc[(df[\"BBB\"] > 25) | (df[\"CCC\"] >= 75), \"AAA\"] = 0.1\n\nIn [24]: df\nOut[24]: \n   AAA  BBB  CCC\n0  0.1   10  100\n1  5.0   20   50\n2  0.1   30  -30\n3  0.1   40  -50\n  Select rows with data closest to certain value using argsort \nIn [25]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [26]: df\nOut[26]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [27]: aValue = 43.0\n\nIn [28]: df.loc[(df.CCC - aValue).abs().argsort()]\nOut[28]: \n   AAA  BBB  CCC\n1    5   20   50\n0    4   10  100\n2    6   30  -30\n3    7   40  -50\n  Dynamically reduce a list of criteria using a binary operators \nIn [29]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [30]: df\nOut[30]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [31]: Crit1 = df.AAA <= 5.5\n\nIn [32]: Crit2 = df.BBB == 10.0\n\nIn [33]: Crit3 = df.CCC > -40.0\n  One could hard code: \nIn [34]: AllCrit = Crit1 & Crit2 & Crit3\n  \u2026Or it can be done with a list of dynamically built criteria \nIn [35]: import functools\n\nIn [36]: CritList = [Crit1, Crit2, Crit3]\n\nIn [37]: AllCrit = functools.reduce(lambda x, y: x & y, CritList)\n\nIn [38]: df[AllCrit]\nOut[38]: \n   AAA  BBB  CCC\n0    4   10  100\n     Selection  Dataframes The indexing docs. Using both row labels and value conditionals \nIn [39]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [40]: df\nOut[40]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [41]: df[(df.AAA <= 6) & (df.index.isin([0, 2, 4]))]\nOut[41]: \n   AAA  BBB  CCC\n0    4   10  100\n2    6   30  -30\n  Use loc for label-oriented slicing and iloc positional slicing GH2904 \nIn [42]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]},\n   ....:     index=[\"foo\", \"bar\", \"boo\", \"kar\"],\n   ....: )\n   ....: \n  There are 2 explicit slicing methods, with a third general case  Positional-oriented (Python slicing style : exclusive of end) Label-oriented (Non-Python slicing style : inclusive of end) General (Either slicing style : depends on if the slice contains labels or positions)  \nIn [43]: df.loc[\"bar\":\"kar\"]  # Label\nOut[43]: \n     AAA  BBB  CCC\nbar    5   20   50\nboo    6   30  -30\nkar    7   40  -50\n\n# Generic\nIn [44]: df[0:3]\nOut[44]: \n     AAA  BBB  CCC\nfoo    4   10  100\nbar    5   20   50\nboo    6   30  -30\n\nIn [45]: df[\"bar\":\"kar\"]\nOut[45]: \n     AAA  BBB  CCC\nbar    5   20   50\nboo    6   30  -30\nkar    7   40  -50\n  Ambiguity arises when an index consists of integers with a non-zero start or non-unit increment. \nIn [46]: data = {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n\nIn [47]: df2 = pd.DataFrame(data=data, index=[1, 2, 3, 4])  # Note index starts at 1.\n\nIn [48]: df2.iloc[1:3]  # Position-oriented\nOut[48]: \n   AAA  BBB  CCC\n2    5   20   50\n3    6   30  -30\n\nIn [49]: df2.loc[1:3]  # Label-oriented\nOut[49]: \n   AAA  BBB  CCC\n1    4   10  100\n2    5   20   50\n3    6   30  -30\n  Using inverse operator (~) to take the complement of a mask \nIn [50]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [51]: df\nOut[51]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [52]: df[~((df.AAA <= 6) & (df.index.isin([0, 2, 4])))]\nOut[52]: \n   AAA  BBB  CCC\n1    5   20   50\n3    7   40  -50\n    New columns Efficiently and dynamically creating new columns using applymap \nIn [53]: df = pd.DataFrame({\"AAA\": [1, 2, 1, 3], \"BBB\": [1, 1, 2, 2], \"CCC\": [2, 1, 3, 1]})\n\nIn [54]: df\nOut[54]: \n   AAA  BBB  CCC\n0    1    1    2\n1    2    1    1\n2    1    2    3\n3    3    2    1\n\nIn [55]: source_cols = df.columns  # Or some subset would work too\n\nIn [56]: new_cols = [str(x) + \"_cat\" for x in source_cols]\n\nIn [57]: categories = {1: \"Alpha\", 2: \"Beta\", 3: \"Charlie\"}\n\nIn [58]: df[new_cols] = df[source_cols].applymap(categories.get)\n\nIn [59]: df\nOut[59]: \n   AAA  BBB  CCC  AAA_cat BBB_cat  CCC_cat\n0    1    1    2    Alpha   Alpha     Beta\n1    2    1    1     Beta   Alpha    Alpha\n2    1    2    3    Alpha    Beta  Charlie\n3    3    2    1  Charlie    Beta    Alpha\n  Keep other columns when using min() with groupby \nIn [60]: df = pd.DataFrame(\n   ....:     {\"AAA\": [1, 1, 1, 2, 2, 2, 3, 3], \"BBB\": [2, 1, 3, 4, 5, 1, 2, 3]}\n   ....: )\n   ....: \n\nIn [61]: df\nOut[61]: \n   AAA  BBB\n0    1    2\n1    1    1\n2    1    3\n3    2    4\n4    2    5\n5    2    1\n6    3    2\n7    3    3\n  Method 1 : idxmin() to get the index of the minimums \nIn [62]: df.loc[df.groupby(\"AAA\")[\"BBB\"].idxmin()]\nOut[62]: \n   AAA  BBB\n1    1    1\n5    2    1\n6    3    2\n  Method 2 : sort then take first of each \nIn [63]: df.sort_values(by=\"BBB\").groupby(\"AAA\", as_index=False).first()\nOut[63]: \n   AAA  BBB\n0    1    1\n1    2    1\n2    3    2\n  Notice the same results, with the exception of the index.    Multiindexing The multindexing docs. Creating a MultiIndex from a labeled frame \nIn [64]: df = pd.DataFrame(\n   ....:     {\n   ....:         \"row\": [0, 1, 2],\n   ....:         \"One_X\": [1.1, 1.1, 1.1],\n   ....:         \"One_Y\": [1.2, 1.2, 1.2],\n   ....:         \"Two_X\": [1.11, 1.11, 1.11],\n   ....:         \"Two_Y\": [1.22, 1.22, 1.22],\n   ....:     }\n   ....: )\n   ....: \n\nIn [65]: df\nOut[65]: \n   row  One_X  One_Y  Two_X  Two_Y\n0    0    1.1    1.2   1.11   1.22\n1    1    1.1    1.2   1.11   1.22\n2    2    1.1    1.2   1.11   1.22\n\n# As Labelled Index\nIn [66]: df = df.set_index(\"row\")\n\nIn [67]: df\nOut[67]: \n     One_X  One_Y  Two_X  Two_Y\nrow                            \n0      1.1    1.2   1.11   1.22\n1      1.1    1.2   1.11   1.22\n2      1.1    1.2   1.11   1.22\n\n# With Hierarchical Columns\nIn [68]: df.columns = pd.MultiIndex.from_tuples([tuple(c.split(\"_\")) for c in df.columns])\n\nIn [69]: df\nOut[69]: \n     One        Two      \n       X    Y     X     Y\nrow                      \n0    1.1  1.2  1.11  1.22\n1    1.1  1.2  1.11  1.22\n2    1.1  1.2  1.11  1.22\n\n# Now stack & Reset\nIn [70]: df = df.stack(0).reset_index(1)\n\nIn [71]: df\nOut[71]: \n    level_1     X     Y\nrow                    \n0       One  1.10  1.20\n0       Two  1.11  1.22\n1       One  1.10  1.20\n1       Two  1.11  1.22\n2       One  1.10  1.20\n2       Two  1.11  1.22\n\n# And fix the labels (Notice the label 'level_1' got added automatically)\nIn [72]: df.columns = [\"Sample\", \"All_X\", \"All_Y\"]\n\nIn [73]: df\nOut[73]: \n    Sample  All_X  All_Y\nrow                     \n0      One   1.10   1.20\n0      Two   1.11   1.22\n1      One   1.10   1.20\n1      Two   1.11   1.22\n2      One   1.10   1.20\n2      Two   1.11   1.22\n   Arithmetic Performing arithmetic with a MultiIndex that needs broadcasting \nIn [74]: cols = pd.MultiIndex.from_tuples(\n   ....:     [(x, y) for x in [\"A\", \"B\", \"C\"] for y in [\"O\", \"I\"]]\n   ....: )\n   ....: \n\nIn [75]: df = pd.DataFrame(np.random.randn(2, 6), index=[\"n\", \"m\"], columns=cols)\n\nIn [76]: df\nOut[76]: \n          A                   B                   C          \n          O         I         O         I         O         I\nn  0.469112 -0.282863 -1.509059 -1.135632  1.212112 -0.173215\nm  0.119209 -1.044236 -0.861849 -2.104569 -0.494929  1.071804\n\nIn [77]: df = df.div(df[\"C\"], level=1)\n\nIn [78]: df\nOut[78]: \n          A                   B              C     \n          O         I         O         I    O    I\nn  0.387021  1.633022 -1.244983  6.556214  1.0  1.0\nm -0.240860 -0.974279  1.741358 -1.963577  1.0  1.0\n    Slicing Slicing a MultiIndex with xs \nIn [79]: coords = [(\"AA\", \"one\"), (\"AA\", \"six\"), (\"BB\", \"one\"), (\"BB\", \"two\"), (\"BB\", \"six\")]\n\nIn [80]: index = pd.MultiIndex.from_tuples(coords)\n\nIn [81]: df = pd.DataFrame([11, 22, 33, 44, 55], index, [\"MyData\"])\n\nIn [82]: df\nOut[82]: \n        MyData\nAA one      11\n   six      22\nBB one      33\n   two      44\n   six      55\n  To take the cross section of the 1st level and 1st axis the index: \n# Note : level and axis are optional, and default to zero\nIn [83]: df.xs(\"BB\", level=0, axis=0)\nOut[83]: \n     MyData\none      33\ntwo      44\nsix      55\n  \u2026and now the 2nd level of the 1st axis. \nIn [84]: df.xs(\"six\", level=1, axis=0)\nOut[84]: \n    MyData\nAA      22\nBB      55\n  Slicing a MultiIndex with xs, method #2 \nIn [85]: import itertools\n\nIn [86]: index = list(itertools.product([\"Ada\", \"Quinn\", \"Violet\"], [\"Comp\", \"Math\", \"Sci\"]))\n\nIn [87]: headr = list(itertools.product([\"Exams\", \"Labs\"], [\"I\", \"II\"]))\n\nIn [88]: indx = pd.MultiIndex.from_tuples(index, names=[\"Student\", \"Course\"])\n\nIn [89]: cols = pd.MultiIndex.from_tuples(headr)  # Notice these are un-named\n\nIn [90]: data = [[70 + x + y + (x * y) % 3 for x in range(4)] for y in range(9)]\n\nIn [91]: df = pd.DataFrame(data, indx, cols)\n\nIn [92]: df\nOut[92]: \n               Exams     Labs    \n                   I  II    I  II\nStudent Course                   \nAda     Comp      70  71   72  73\n        Math      71  73   75  74\n        Sci       72  75   75  75\nQuinn   Comp      73  74   75  76\n        Math      74  76   78  77\n        Sci       75  78   78  78\nViolet  Comp      76  77   78  79\n        Math      77  79   81  80\n        Sci       78  81   81  81\n\nIn [93]: All = slice(None)\n\nIn [94]: df.loc[\"Violet\"]\nOut[94]: \n       Exams     Labs    \n           I  II    I  II\nCourse                   \nComp      76  77   78  79\nMath      77  79   81  80\nSci       78  81   81  81\n\nIn [95]: df.loc[(All, \"Math\"), All]\nOut[95]: \n               Exams     Labs    \n                   I  II    I  II\nStudent Course                   \nAda     Math      71  73   75  74\nQuinn   Math      74  76   78  77\nViolet  Math      77  79   81  80\n\nIn [96]: df.loc[(slice(\"Ada\", \"Quinn\"), \"Math\"), All]\nOut[96]: \n               Exams     Labs    \n                   I  II    I  II\nStudent Course                   \nAda     Math      71  73   75  74\nQuinn   Math      74  76   78  77\n\nIn [97]: df.loc[(All, \"Math\"), (\"Exams\")]\nOut[97]: \n                 I  II\nStudent Course        \nAda     Math    71  73\nQuinn   Math    74  76\nViolet  Math    77  79\n\nIn [98]: df.loc[(All, \"Math\"), (All, \"II\")]\nOut[98]: \n               Exams Labs\n                  II   II\nStudent Course           \nAda     Math      73   74\nQuinn   Math      76   77\nViolet  Math      79   80\n  Setting portions of a MultiIndex with xs   Sorting Sort by specific column or an ordered list of columns, with a MultiIndex \nIn [99]: df.sort_values(by=(\"Labs\", \"II\"), ascending=False)\nOut[99]: \n               Exams     Labs    \n                   I  II    I  II\nStudent Course                   \nViolet  Sci       78  81   81  81\n        Math      77  79   81  80\n        Comp      76  77   78  79\nQuinn   Sci       75  78   78  78\n        Math      74  76   78  77\n        Comp      73  74   75  76\nAda     Sci       72  75   75  75\n        Math      71  73   75  74\n        Comp      70  71   72  73\n  Partial selection, the need for sortedness GH2995   Levels Prepending a level to a multiindex Flatten Hierarchical columns    Missing data The missing data docs. Fill forward a reversed timeseries \nIn [100]: df = pd.DataFrame(\n   .....:     np.random.randn(6, 1),\n   .....:     index=pd.date_range(\"2013-08-01\", periods=6, freq=\"B\"),\n   .....:     columns=list(\"A\"),\n   .....: )\n   .....: \n\nIn [101]: df.loc[df.index[3], \"A\"] = np.nan\n\nIn [102]: df\nOut[102]: \n                   A\n2013-08-01  0.721555\n2013-08-02 -0.706771\n2013-08-05 -1.039575\n2013-08-06       NaN\n2013-08-07 -0.424972\n2013-08-08  0.567020\n\nIn [103]: df.reindex(df.index[::-1]).ffill()\nOut[103]: \n                   A\n2013-08-08  0.567020\n2013-08-07 -0.424972\n2013-08-06 -0.424972\n2013-08-05 -1.039575\n2013-08-02 -0.706771\n2013-08-01  0.721555\n  cumsum reset at NaN values  Replace Using replace with backrefs    Grouping The grouping docs. Basic grouping with apply Unlike agg, apply\u2019s callable is passed a sub-DataFrame which gives you access to all the columns \nIn [104]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"animal\": \"cat dog cat fish dog cat cat\".split(),\n   .....:         \"size\": list(\"SSMMMLL\"),\n   .....:         \"weight\": [8, 10, 11, 1, 20, 12, 12],\n   .....:         \"adult\": [False] * 5 + [True] * 2,\n   .....:     }\n   .....: )\n   .....: \n\nIn [105]: df\nOut[105]: \n  animal size  weight  adult\n0    cat    S       8  False\n1    dog    S      10  False\n2    cat    M      11  False\n3   fish    M       1  False\n4    dog    M      20  False\n5    cat    L      12   True\n6    cat    L      12   True\n\n# List the size of the animals with the highest weight.\nIn [106]: df.groupby(\"animal\").apply(lambda subf: subf[\"size\"][subf[\"weight\"].idxmax()])\nOut[106]: \nanimal\ncat     L\ndog     M\nfish    M\ndtype: object\n  Using get_group \nIn [107]: gb = df.groupby([\"animal\"])\n\nIn [108]: gb.get_group(\"cat\")\nOut[108]: \n  animal size  weight  adult\n0    cat    S       8  False\n2    cat    M      11  False\n5    cat    L      12   True\n6    cat    L      12   True\n  Apply to different items in a group \nIn [109]: def GrowUp(x):\n   .....:     avg_weight = sum(x[x[\"size\"] == \"S\"].weight * 1.5)\n   .....:     avg_weight += sum(x[x[\"size\"] == \"M\"].weight * 1.25)\n   .....:     avg_weight += sum(x[x[\"size\"] == \"L\"].weight)\n   .....:     avg_weight \/= len(x)\n   .....:     return pd.Series([\"L\", avg_weight, True], index=[\"size\", \"weight\", \"adult\"])\n   .....: \n\nIn [110]: expected_df = gb.apply(GrowUp)\n\nIn [111]: expected_df\nOut[111]: \n       size   weight  adult\nanimal                     \ncat       L  12.4375   True\ndog       L  20.0000   True\nfish      L   1.2500   True\n  Expanding apply \nIn [112]: S = pd.Series([i \/ 100.0 for i in range(1, 11)])\n\nIn [113]: def cum_ret(x, y):\n   .....:     return x * (1 + y)\n   .....: \n\nIn [114]: def red(x):\n   .....:     return functools.reduce(cum_ret, x, 1.0)\n   .....: \n\nIn [115]: S.expanding().apply(red, raw=True)\nOut[115]: \n0    1.010000\n1    1.030200\n2    1.061106\n3    1.103550\n4    1.158728\n5    1.228251\n6    1.314229\n7    1.419367\n8    1.547110\n9    1.701821\ndtype: float64\n  Replacing some values with mean of the rest of a group \nIn [116]: df = pd.DataFrame({\"A\": [1, 1, 2, 2], \"B\": [1, -1, 1, 2]})\n\nIn [117]: gb = df.groupby(\"A\")\n\nIn [118]: def replace(g):\n   .....:     mask = g < 0\n   .....:     return g.where(mask, g[~mask].mean())\n   .....: \n\nIn [119]: gb.transform(replace)\nOut[119]: \n     B\n0  1.0\n1 -1.0\n2  1.5\n3  1.5\n  Sort groups by aggregated data \nIn [120]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"code\": [\"foo\", \"bar\", \"baz\"] * 2,\n   .....:         \"data\": [0.16, -0.21, 0.33, 0.45, -0.59, 0.62],\n   .....:         \"flag\": [False, True] * 3,\n   .....:     }\n   .....: )\n   .....: \n\nIn [121]: code_groups = df.groupby(\"code\")\n\nIn [122]: agg_n_sort_order = code_groups[[\"data\"]].transform(sum).sort_values(by=\"data\")\n\nIn [123]: sorted_df = df.loc[agg_n_sort_order.index]\n\nIn [124]: sorted_df\nOut[124]: \n  code  data   flag\n1  bar -0.21   True\n4  bar -0.59  False\n0  foo  0.16  False\n3  foo  0.45   True\n2  baz  0.33  False\n5  baz  0.62   True\n  Create multiple aggregated columns \nIn [125]: rng = pd.date_range(start=\"2014-10-07\", periods=10, freq=\"2min\")\n\nIn [126]: ts = pd.Series(data=list(range(10)), index=rng)\n\nIn [127]: def MyCust(x):\n   .....:     if len(x) > 2:\n   .....:         return x[1] * 1.234\n   .....:     return pd.NaT\n   .....: \n\nIn [128]: mhc = {\"Mean\": np.mean, \"Max\": np.max, \"Custom\": MyCust}\n\nIn [129]: ts.resample(\"5min\").apply(mhc)\nOut[129]: \n                     Mean  Max Custom\n2014-10-07 00:00:00   1.0    2  1.234\n2014-10-07 00:05:00   3.5    4    NaT\n2014-10-07 00:10:00   6.0    7  7.404\n2014-10-07 00:15:00   8.5    9    NaT\n\nIn [130]: ts\nOut[130]: \n2014-10-07 00:00:00    0\n2014-10-07 00:02:00    1\n2014-10-07 00:04:00    2\n2014-10-07 00:06:00    3\n2014-10-07 00:08:00    4\n2014-10-07 00:10:00    5\n2014-10-07 00:12:00    6\n2014-10-07 00:14:00    7\n2014-10-07 00:16:00    8\n2014-10-07 00:18:00    9\nFreq: 2T, dtype: int64\n  Create a value counts column and reassign back to the DataFrame \nIn [131]: df = pd.DataFrame(\n   .....:     {\"Color\": \"Red Red Red Blue\".split(), \"Value\": [100, 150, 50, 50]}\n   .....: )\n   .....: \n\nIn [132]: df\nOut[132]: \n  Color  Value\n0   Red    100\n1   Red    150\n2   Red     50\n3  Blue     50\n\nIn [133]: df[\"Counts\"] = df.groupby([\"Color\"]).transform(len)\n\nIn [134]: df\nOut[134]: \n  Color  Value  Counts\n0   Red    100       3\n1   Red    150       3\n2   Red     50       3\n3  Blue     50       1\n  Shift groups of the values in a column based on the index \nIn [135]: df = pd.DataFrame(\n   .....:     {\"line_race\": [10, 10, 8, 10, 10, 8], \"beyer\": [99, 102, 103, 103, 88, 100]},\n   .....:     index=[\n   .....:         \"Last Gunfighter\",\n   .....:         \"Last Gunfighter\",\n   .....:         \"Last Gunfighter\",\n   .....:         \"Paynter\",\n   .....:         \"Paynter\",\n   .....:         \"Paynter\",\n   .....:     ],\n   .....: )\n   .....: \n\nIn [136]: df\nOut[136]: \n                 line_race  beyer\nLast Gunfighter         10     99\nLast Gunfighter         10    102\nLast Gunfighter          8    103\nPaynter                 10    103\nPaynter                 10     88\nPaynter                  8    100\n\nIn [137]: df[\"beyer_shifted\"] = df.groupby(level=0)[\"beyer\"].shift(1)\n\nIn [138]: df\nOut[138]: \n                 line_race  beyer  beyer_shifted\nLast Gunfighter         10     99            NaN\nLast Gunfighter         10    102           99.0\nLast Gunfighter          8    103          102.0\nPaynter                 10    103            NaN\nPaynter                 10     88          103.0\nPaynter                  8    100           88.0\n  Select row with maximum value from each group \nIn [139]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"host\": [\"other\", \"other\", \"that\", \"this\", \"this\"],\n   .....:         \"service\": [\"mail\", \"web\", \"mail\", \"mail\", \"web\"],\n   .....:         \"no\": [1, 2, 1, 2, 1],\n   .....:     }\n   .....: ).set_index([\"host\", \"service\"])\n   .....: \n\nIn [140]: mask = df.groupby(level=0).agg(\"idxmax\")\n\nIn [141]: df_count = df.loc[mask[\"no\"]].reset_index()\n\nIn [142]: df_count\nOut[142]: \n    host service  no\n0  other     web   2\n1   that    mail   1\n2   this    mail   2\n  Grouping like Python\u2019s itertools.groupby \nIn [143]: df = pd.DataFrame([0, 1, 0, 1, 1, 1, 0, 1, 1], columns=[\"A\"])\n\nIn [144]: df[\"A\"].groupby((df[\"A\"] != df[\"A\"].shift()).cumsum()).groups\nOut[144]: {1: [0], 2: [1], 3: [2], 4: [3, 4, 5], 5: [6], 6: [7, 8]}\n\nIn [145]: df[\"A\"].groupby((df[\"A\"] != df[\"A\"].shift()).cumsum()).cumsum()\nOut[145]: \n0    0\n1    1\n2    0\n3    1\n4    2\n5    3\n6    0\n7    1\n8    2\nName: A, dtype: int64\n   Expanding data Alignment and to-date Rolling Computation window based on values instead of counts Rolling Mean by Time Interval   Splitting Splitting a frame Create a list of dataframes, split using a delineation based on logic included in rows. \nIn [146]: df = pd.DataFrame(\n   .....:     data={\n   .....:         \"Case\": [\"A\", \"A\", \"A\", \"B\", \"A\", \"A\", \"B\", \"A\", \"A\"],\n   .....:         \"Data\": np.random.randn(9),\n   .....:     }\n   .....: )\n   .....: \n\nIn [147]: dfs = list(\n   .....:     zip(\n   .....:         *df.groupby(\n   .....:             (1 * (df[\"Case\"] == \"B\"))\n   .....:             .cumsum()\n   .....:             .rolling(window=3, min_periods=1)\n   .....:             .median()\n   .....:         )\n   .....:     )\n   .....: )[-1]\n   .....: \n\nIn [148]: dfs[0]\nOut[148]: \n  Case      Data\n0    A  0.276232\n1    A -1.087401\n2    A -0.673690\n3    B  0.113648\n\nIn [149]: dfs[1]\nOut[149]: \n  Case      Data\n4    A -1.478427\n5    A  0.524988\n6    B  0.404705\n\nIn [150]: dfs[2]\nOut[150]: \n  Case      Data\n7    A  0.577046\n8    A -1.715002\n    Pivot The Pivot docs. Partial sums and subtotals \nIn [151]: df = pd.DataFrame(\n   .....:     data={\n   .....:         \"Province\": [\"ON\", \"QC\", \"BC\", \"AL\", \"AL\", \"MN\", \"ON\"],\n   .....:         \"City\": [\n   .....:             \"Toronto\",\n   .....:             \"Montreal\",\n   .....:             \"Vancouver\",\n   .....:             \"Calgary\",\n   .....:             \"Edmonton\",\n   .....:             \"Winnipeg\",\n   .....:             \"Windsor\",\n   .....:         ],\n   .....:         \"Sales\": [13, 6, 16, 8, 4, 3, 1],\n   .....:     }\n   .....: )\n   .....: \n\nIn [152]: table = pd.pivot_table(\n   .....:     df,\n   .....:     values=[\"Sales\"],\n   .....:     index=[\"Province\"],\n   .....:     columns=[\"City\"],\n   .....:     aggfunc=np.sum,\n   .....:     margins=True,\n   .....: )\n   .....: \n\nIn [153]: table.stack(\"City\")\nOut[153]: \n                    Sales\nProvince City            \nAL       All         12.0\n         Calgary      8.0\n         Edmonton     4.0\nBC       All         16.0\n         Vancouver   16.0\n...                   ...\nAll      Montreal     6.0\n         Toronto     13.0\n         Vancouver   16.0\n         Windsor      1.0\n         Winnipeg     3.0\n\n[20 rows x 1 columns]\n  Frequency table like plyr in R \nIn [154]: grades = [48, 99, 75, 80, 42, 80, 72, 68, 36, 78]\n\nIn [155]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"ID\": [\"x%d\" % r for r in range(10)],\n   .....:         \"Gender\": [\"F\", \"M\", \"F\", \"M\", \"F\", \"M\", \"F\", \"M\", \"M\", \"M\"],\n   .....:         \"ExamYear\": [\n   .....:             \"2007\",\n   .....:             \"2007\",\n   .....:             \"2007\",\n   .....:             \"2008\",\n   .....:             \"2008\",\n   .....:             \"2008\",\n   .....:             \"2008\",\n   .....:             \"2009\",\n   .....:             \"2009\",\n   .....:             \"2009\",\n   .....:         ],\n   .....:         \"Class\": [\n   .....:             \"algebra\",\n   .....:             \"stats\",\n   .....:             \"bio\",\n   .....:             \"algebra\",\n   .....:             \"algebra\",\n   .....:             \"stats\",\n   .....:             \"stats\",\n   .....:             \"algebra\",\n   .....:             \"bio\",\n   .....:             \"bio\",\n   .....:         ],\n   .....:         \"Participated\": [\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"no\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:         ],\n   .....:         \"Passed\": [\"yes\" if x > 50 else \"no\" for x in grades],\n   .....:         \"Employed\": [\n   .....:             True,\n   .....:             True,\n   .....:             True,\n   .....:             False,\n   .....:             False,\n   .....:             False,\n   .....:             False,\n   .....:             True,\n   .....:             True,\n   .....:             False,\n   .....:         ],\n   .....:         \"Grade\": grades,\n   .....:     }\n   .....: )\n   .....: \n\nIn [156]: df.groupby(\"ExamYear\").agg(\n   .....:     {\n   .....:         \"Participated\": lambda x: x.value_counts()[\"yes\"],\n   .....:         \"Passed\": lambda x: sum(x == \"yes\"),\n   .....:         \"Employed\": lambda x: sum(x),\n   .....:         \"Grade\": lambda x: sum(x) \/ len(x),\n   .....:     }\n   .....: )\n   .....: \nOut[156]: \n          Participated  Passed  Employed      Grade\nExamYear                                           \n2007                 3       2         3  74.000000\n2008                 3       3         0  68.500000\n2009                 3       2         2  60.666667\n  Plot pandas DataFrame with year over year data To create year and month cross tabulation: \nIn [157]: df = pd.DataFrame(\n   .....:     {\"value\": np.random.randn(36)},\n   .....:     index=pd.date_range(\"2011-01-01\", freq=\"M\", periods=36),\n   .....: )\n   .....: \n\nIn [158]: pd.pivot_table(\n   .....:     df, index=df.index.month, columns=df.index.year, values=\"value\", aggfunc=\"sum\"\n   .....: )\n   .....: \nOut[158]: \n        2011      2012      2013\n1  -1.039268 -0.968914  2.565646\n2  -0.370647 -1.294524  1.431256\n3  -1.157892  0.413738  1.340309\n4  -1.344312  0.276662 -1.170299\n5   0.844885 -0.472035 -0.226169\n6   1.075770 -0.013960  0.410835\n7  -0.109050 -0.362543  0.813850\n8   1.643563 -0.006154  0.132003\n9  -1.469388 -0.923061 -0.827317\n10  0.357021  0.895717 -0.076467\n11 -0.674600  0.805244 -1.187678\n12 -1.776904 -1.206412  1.130127\n    Apply Rolling apply to organize - Turning embedded lists into a MultiIndex frame \nIn [159]: df = pd.DataFrame(\n   .....:     data={\n   .....:         \"A\": [[2, 4, 8, 16], [100, 200], [10, 20, 30]],\n   .....:         \"B\": [[\"a\", \"b\", \"c\"], [\"jj\", \"kk\"], [\"ccc\"]],\n   .....:     },\n   .....:     index=[\"I\", \"II\", \"III\"],\n   .....: )\n   .....: \n\nIn [160]: def SeriesFromSubList(aList):\n   .....:     return pd.Series(aList)\n   .....: \n\nIn [161]: df_orgz = pd.concat(\n   .....:     {ind: row.apply(SeriesFromSubList) for ind, row in df.iterrows()}\n   .....: )\n   .....: \n\nIn [162]: df_orgz\nOut[162]: \n         0     1     2     3\nI   A    2     4     8  16.0\n    B    a     b     c   NaN\nII  A  100   200   NaN   NaN\n    B   jj    kk   NaN   NaN\nIII A   10  20.0  30.0   NaN\n    B  ccc   NaN   NaN   NaN\n  Rolling apply with a DataFrame returning a Series Rolling Apply to multiple columns where function calculates a Series before a Scalar from the Series is returned \nIn [163]: df = pd.DataFrame(\n   .....:     data=np.random.randn(2000, 2) \/ 10000,\n   .....:     index=pd.date_range(\"2001-01-01\", periods=2000),\n   .....:     columns=[\"A\", \"B\"],\n   .....: )\n   .....: \n\nIn [164]: df\nOut[164]: \n                   A         B\n2001-01-01 -0.000144 -0.000141\n2001-01-02  0.000161  0.000102\n2001-01-03  0.000057  0.000088\n2001-01-04 -0.000221  0.000097\n2001-01-05 -0.000201 -0.000041\n...              ...       ...\n2006-06-19  0.000040 -0.000235\n2006-06-20 -0.000123 -0.000021\n2006-06-21 -0.000113  0.000114\n2006-06-22  0.000136  0.000109\n2006-06-23  0.000027  0.000030\n\n[2000 rows x 2 columns]\n\nIn [165]: def gm(df, const):\n   .....:     v = ((((df[\"A\"] + df[\"B\"]) + 1).cumprod()) - 1) * const\n   .....:     return v.iloc[-1]\n   .....: \n\nIn [166]: s = pd.Series(\n   .....:     {\n   .....:         df.index[i]: gm(df.iloc[i: min(i + 51, len(df) - 1)], 5)\n   .....:         for i in range(len(df) - 50)\n   .....:     }\n   .....: )\n   .....: \n\nIn [167]: s\nOut[167]: \n2001-01-01    0.000930\n2001-01-02    0.002615\n2001-01-03    0.001281\n2001-01-04    0.001117\n2001-01-05    0.002772\n                ...   \n2006-04-30    0.003296\n2006-05-01    0.002629\n2006-05-02    0.002081\n2006-05-03    0.004247\n2006-05-04    0.003928\nLength: 1950, dtype: float64\n  Rolling apply with a DataFrame returning a Scalar Rolling Apply to multiple columns where function returns a Scalar (Volume Weighted Average Price) \nIn [168]: rng = pd.date_range(start=\"2014-01-01\", periods=100)\n\nIn [169]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"Open\": np.random.randn(len(rng)),\n   .....:         \"Close\": np.random.randn(len(rng)),\n   .....:         \"Volume\": np.random.randint(100, 2000, len(rng)),\n   .....:     },\n   .....:     index=rng,\n   .....: )\n   .....: \n\nIn [170]: df\nOut[170]: \n                Open     Close  Volume\n2014-01-01 -1.611353 -0.492885    1219\n2014-01-02 -3.000951  0.445794    1054\n2014-01-03 -0.138359 -0.076081    1381\n2014-01-04  0.301568  1.198259    1253\n2014-01-05  0.276381 -0.669831    1728\n...              ...       ...     ...\n2014-04-06 -0.040338  0.937843    1188\n2014-04-07  0.359661 -0.285908    1864\n2014-04-08  0.060978  1.714814     941\n2014-04-09  1.759055 -0.455942    1065\n2014-04-10  0.138185 -1.147008    1453\n\n[100 rows x 3 columns]\n\nIn [171]: def vwap(bars):\n   .....:     return (bars.Close * bars.Volume).sum() \/ bars.Volume.sum()\n   .....: \n\nIn [172]: window = 5\n\nIn [173]: s = pd.concat(\n   .....:     [\n   .....:         (pd.Series(vwap(df.iloc[i: i + window]), index=[df.index[i + window]]))\n   .....:         for i in range(len(df) - window)\n   .....:     ]\n   .....: )\n   .....: \n\nIn [174]: s.round(2)\nOut[174]: \n2014-01-06    0.02\n2014-01-07    0.11\n2014-01-08    0.10\n2014-01-09    0.07\n2014-01-10   -0.29\n              ... \n2014-04-06   -0.63\n2014-04-07   -0.02\n2014-04-08   -0.03\n2014-04-09    0.34\n2014-04-10    0.29\nLength: 95, dtype: float64\n     Timeseries Between times Using indexer between time Constructing a datetime range that excludes weekends and includes only certain times Vectorized Lookup Aggregation and plotting time series Turn a matrix with hours in columns and days in rows into a continuous row sequence in the form of a time series. How to rearrange a Python pandas DataFrame? Dealing with duplicates when reindexing a timeseries to a specified frequency Calculate the first day of the month for each entry in a DatetimeIndex \nIn [175]: dates = pd.date_range(\"2000-01-01\", periods=5)\n\nIn [176]: dates.to_period(freq=\"M\").to_timestamp()\nOut[176]: \nDatetimeIndex(['2000-01-01', '2000-01-01', '2000-01-01', '2000-01-01',\n               '2000-01-01'],\n              dtype='datetime64[ns]', freq=None)\n   Resampling The Resample docs. Using Grouper instead of TimeGrouper for time grouping of values Time grouping with some missing values Valid frequency arguments to Grouper Timeseries Grouping using a MultiIndex Using TimeGrouper and another grouping to create subgroups, then apply a custom function GH3791 Resampling with custom periods Resample intraday frame without adding new days Resample minute data Resample with groupby    Merge The Join docs. Concatenate two dataframes with overlapping index (emulate R rbind) \nIn [177]: rng = pd.date_range(\"2000-01-01\", periods=6)\n\nIn [178]: df1 = pd.DataFrame(np.random.randn(6, 3), index=rng, columns=[\"A\", \"B\", \"C\"])\n\nIn [179]: df2 = df1.copy()\n  Depending on df construction, ignore_index may be needed \nIn [180]: df = pd.concat([df1, df2], ignore_index=True)\n\nIn [181]: df\nOut[181]: \n           A         B         C\n0  -0.870117 -0.479265 -0.790855\n1   0.144817  1.726395 -0.464535\n2  -0.821906  1.597605  0.187307\n3  -0.128342 -1.511638 -0.289858\n4   0.399194 -1.430030 -0.639760\n5   1.115116 -2.012600  1.810662\n6  -0.870117 -0.479265 -0.790855\n7   0.144817  1.726395 -0.464535\n8  -0.821906  1.597605  0.187307\n9  -0.128342 -1.511638 -0.289858\n10  0.399194 -1.430030 -0.639760\n11  1.115116 -2.012600  1.810662\n  Self Join of a DataFrame GH2996 \nIn [182]: df = pd.DataFrame(\n   .....:     data={\n   .....:         \"Area\": [\"A\"] * 5 + [\"C\"] * 2,\n   .....:         \"Bins\": [110] * 2 + [160] * 3 + [40] * 2,\n   .....:         \"Test_0\": [0, 1, 0, 1, 2, 0, 1],\n   .....:         \"Data\": np.random.randn(7),\n   .....:     }\n   .....: )\n   .....: \n\nIn [183]: df\nOut[183]: \n  Area  Bins  Test_0      Data\n0    A   110       0 -0.433937\n1    A   110       1 -0.160552\n2    A   160       0  0.744434\n3    A   160       1  1.754213\n4    A   160       2  0.000850\n5    C    40       0  0.342243\n6    C    40       1  1.070599\n\nIn [184]: df[\"Test_1\"] = df[\"Test_0\"] - 1\n\nIn [185]: pd.merge(\n   .....:     df,\n   .....:     df,\n   .....:     left_on=[\"Bins\", \"Area\", \"Test_0\"],\n   .....:     right_on=[\"Bins\", \"Area\", \"Test_1\"],\n   .....:     suffixes=(\"_L\", \"_R\"),\n   .....: )\n   .....: \nOut[185]: \n  Area  Bins  Test_0_L    Data_L  Test_1_L  Test_0_R    Data_R  Test_1_R\n0    A   110         0 -0.433937        -1         1 -0.160552         0\n1    A   160         0  0.744434        -1         1  1.754213         0\n2    A   160         1  1.754213         0         2  0.000850         1\n3    C    40         0  0.342243        -1         1  1.070599         0\n  How to set the index and join KDB like asof join Join with a criteria based on the values Using searchsorted to merge based on values inside a range   Plotting The Plotting docs. Make Matplotlib look like R Setting x-axis major and minor labels Plotting multiple charts in an IPython Jupyter notebook Creating a multi-line plot Plotting a heatmap Annotate a time-series plot Annotate a time-series plot #2 Generate Embedded plots in excel files using Pandas, Vincent and xlsxwriter Boxplot for each quartile of a stratifying variable \nIn [186]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"stratifying_var\": np.random.uniform(0, 100, 20),\n   .....:         \"price\": np.random.normal(100, 5, 20),\n   .....:     }\n   .....: )\n   .....: \n\nIn [187]: df[\"quartiles\"] = pd.qcut(\n   .....:     df[\"stratifying_var\"], 4, labels=[\"0-25%\", \"25-50%\", \"50-75%\", \"75-100%\"]\n   .....: )\n   .....: \n\nIn [188]: df.boxplot(column=\"price\", by=\"quartiles\")\nOut[188]: <AxesSubplot:title={'center':'price'}, xlabel='quartiles'>\n     Data in\/out Performance comparison of SQL vs HDF5  CSV The CSV docs read_csv in action appending to a csv Reading a csv chunk-by-chunk Reading only certain rows of a csv chunk-by-chunk Reading the first few lines of a frame Reading a file that is compressed but not by gzip\/bz2 (the native compressed formats which read_csv understands). This example shows a WinZipped file, but is a general application of opening the file within a context manager and using that handle to read. See here Inferring dtypes from a file Dealing with bad lines GH2886 Write a multi-row index CSV without writing duplicates  Reading multiple files to create a single DataFrame The best way to combine multiple files into a single DataFrame is to read the individual frames one by one, put all of the individual frames into a list, and then combine the frames in the list using pd.concat(): \nIn [189]: for i in range(3):\n   .....:     data = pd.DataFrame(np.random.randn(10, 4))\n   .....:     data.to_csv(\"file_{}.csv\".format(i))\n   .....: \n\nIn [190]: files = [\"file_0.csv\", \"file_1.csv\", \"file_2.csv\"]\n\nIn [191]: result = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n  You can use the same approach to read all files matching a pattern. Here is an example using glob: \nIn [192]: import glob\n\nIn [193]: import os\n\nIn [194]: files = glob.glob(\"file_*.csv\")\n\nIn [195]: result = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n  Finally, this strategy will work with the other pd.read_*(...) functions described in the io docs.   Parsing date components in multi-columns Parsing date components in multi-columns is faster with a format \nIn [196]: i = pd.date_range(\"20000101\", periods=10000)\n\nIn [197]: df = pd.DataFrame({\"year\": i.year, \"month\": i.month, \"day\": i.day})\n\nIn [198]: df.head()\nOut[198]: \n   year  month  day\n0  2000      1    1\n1  2000      1    2\n2  2000      1    3\n3  2000      1    4\n4  2000      1    5\n\nIn [199]: %timeit pd.to_datetime(df.year * 10000 + df.month * 100 + df.day, format='%Y%m%d')\n   .....: ds = df.apply(lambda x: \"%04d%02d%02d\" % (x[\"year\"], x[\"month\"], x[\"day\"]), axis=1)\n   .....: ds.head()\n   .....: %timeit pd.to_datetime(ds)\n   .....: \n8.7 ms +- 765 us per loop (mean +- std. dev. of 7 runs, 100 loops each)\n2.1 ms +- 419 us per loop (mean +- std. dev. of 7 runs, 100 loops each)\n    Skip row between header and data \nIn [200]: data = \"\"\";;;;\n   .....:  ;;;;\n   .....:  ;;;;\n   .....:  ;;;;\n   .....:  ;;;;\n   .....:  ;;;;\n   .....: ;;;;\n   .....:  ;;;;\n   .....:  ;;;;\n   .....: ;;;;\n   .....: date;Param1;Param2;Param4;Param5\n   .....:     ;m\u00b2;\u00b0C;m\u00b2;m\n   .....: ;;;;\n   .....: 01.01.1990 00:00;1;1;2;3\n   .....: 01.01.1990 01:00;5;3;4;5\n   .....: 01.01.1990 02:00;9;5;6;7\n   .....: 01.01.1990 03:00;13;7;8;9\n   .....: 01.01.1990 04:00;17;9;10;11\n   .....: 01.01.1990 05:00;21;11;12;13\n   .....: \"\"\"\n   .....: \n   Option 1: pass rows explicitly to skip rows \nIn [201]: from io import StringIO\n\nIn [202]: pd.read_csv(\n   .....:     StringIO(data),\n   .....:     sep=\";\",\n   .....:     skiprows=[11, 12],\n   .....:     index_col=0,\n   .....:     parse_dates=True,\n   .....:     header=10,\n   .....: )\n   .....: \nOut[202]: \n                     Param1  Param2  Param4  Param5\ndate                                               \n1990-01-01 00:00:00       1       1       2       3\n1990-01-01 01:00:00       5       3       4       5\n1990-01-01 02:00:00       9       5       6       7\n1990-01-01 03:00:00      13       7       8       9\n1990-01-01 04:00:00      17       9      10      11\n1990-01-01 05:00:00      21      11      12      13\n    Option 2: read column names and then data \nIn [203]: pd.read_csv(StringIO(data), sep=\";\", header=10, nrows=10).columns\nOut[203]: Index(['date', 'Param1', 'Param2', 'Param4', 'Param5'], dtype='object')\n\nIn [204]: columns = pd.read_csv(StringIO(data), sep=\";\", header=10, nrows=10).columns\n\nIn [205]: pd.read_csv(\n   .....:     StringIO(data), sep=\";\", index_col=0, header=12, parse_dates=True, names=columns\n   .....: )\n   .....: \nOut[205]: \n                     Param1  Param2  Param4  Param5\ndate                                               \n1990-01-01 00:00:00       1       1       2       3\n1990-01-01 01:00:00       5       3       4       5\n1990-01-01 02:00:00       9       5       6       7\n1990-01-01 03:00:00      13       7       8       9\n1990-01-01 04:00:00      17       9      10      11\n1990-01-01 05:00:00      21      11      12      13\n      SQL The SQL docs Reading from databases with SQL   Excel The Excel docs Reading from a filelike handle Modifying formatting in XlsxWriter output Loading only visible sheets GH19842#issuecomment-892150745   HTML Reading HTML tables from a server that cannot handle the default request header   HDFStore The HDFStores docs Simple queries with a Timestamp Index Managing heterogeneous data using a linked multiple table hierarchy GH3032 Merging on-disk tables with millions of rows Avoiding inconsistencies when writing to a store from multiple processes\/threads De-duplicating a large store by chunks, essentially a recursive reduction operation. Shows a function for taking in data from csv file and creating a store by chunks, with date parsing as well. See here Creating a store chunk-by-chunk from a csv file Appending to a store, while creating a unique index Large Data work flows Reading in a sequence of files, then providing a global unique index to a store while appending Groupby on a HDFStore with low group density Groupby on a HDFStore with high group density Hierarchical queries on a HDFStore Counting with a HDFStore Troubleshoot HDFStore exceptions Setting min_itemsize with strings Using ptrepack to create a completely-sorted-index on a store Storing Attributes to a group node \nIn [206]: df = pd.DataFrame(np.random.randn(8, 3))\n\nIn [207]: store = pd.HDFStore(\"test.h5\")\n\nIn [208]: store.put(\"df\", df)\n\n# you can store an arbitrary Python object via pickle\nIn [209]: store.get_storer(\"df\").attrs.my_attribute = {\"A\": 10}\n\nIn [210]: store.get_storer(\"df\").attrs.my_attribute\nOut[210]: {'A': 10}\n  You can create or load a HDFStore in-memory by passing the driver parameter to PyTables. Changes are only written to disk when the HDFStore is closed. \nIn [211]: store = pd.HDFStore(\"test.h5\", \"w\", driver=\"H5FD_CORE\")\n\nIn [212]: df = pd.DataFrame(np.random.randn(8, 3))\n\nIn [213]: store[\"test\"] = df\n\n# only after closing the store, data is written to disk:\nIn [214]: store.close()\n    Binary files pandas readily accepts NumPy record arrays, if you need to read in a binary file consisting of an array of C structs. For example, given this C program in a file called main.c compiled with gcc main.c -std=gnu99 on a 64-bit machine, \n#include <stdio.h>\n#include <stdint.h>\n\ntypedef struct _Data\n{\n    int32_t count;\n    double avg;\n    float scale;\n} Data;\n\nint main(int argc, const char *argv[])\n{\n    size_t n = 10;\n    Data d[n];\n\n    for (int i = 0; i < n; ++i)\n    {\n        d[i].count = i;\n        d[i].avg = i + 1.0;\n        d[i].scale = (float) i + 2.0f;\n    }\n\n    FILE *file = fopen(\"binary.dat\", \"wb\");\n    fwrite(&d, sizeof(Data), n, file);\n    fclose(file);\n\n    return 0;\n}\n  the following Python code will read the binary file 'binary.dat' into a pandas DataFrame, where each element of the struct corresponds to a column in the frame: \nnames = \"count\", \"avg\", \"scale\"\n\n# note that the offsets are larger than the size of the type because of\n# struct padding\noffsets = 0, 8, 16\nformats = \"i4\", \"f8\", \"f4\"\ndt = np.dtype({\"names\": names, \"offsets\": offsets, \"formats\": formats}, align=True)\ndf = pd.DataFrame(np.fromfile(\"binary.dat\", dt))\n   Note The offsets of the structure elements may be different depending on the architecture of the machine on which the file was created. Using a raw binary file format like this for general data storage is not recommended, as it is not cross platform. We recommended either HDF5 or parquet, both of which are supported by pandas\u2019 IO facilities.     Computation Numerical integration (sample-based) of a time series  Correlation Often it\u2019s useful to obtain the lower (or upper) triangular form of a correlation matrix calculated from DataFrame.corr(). This can be achieved by passing a boolean mask to where as follows: \nIn [215]: df = pd.DataFrame(np.random.random(size=(100, 5)))\n\nIn [216]: corr_mat = df.corr()\n\nIn [217]: mask = np.tril(np.ones_like(corr_mat, dtype=np.bool_), k=-1)\n\nIn [218]: corr_mat.where(mask)\nOut[218]: \n          0         1         2        3   4\n0       NaN       NaN       NaN      NaN NaN\n1 -0.079861       NaN       NaN      NaN NaN\n2 -0.236573  0.183801       NaN      NaN NaN\n3 -0.013795 -0.051975  0.037235      NaN NaN\n4 -0.031974  0.118342 -0.073499 -0.02063 NaN\n  The method argument within DataFrame.corr can accept a callable in addition to the named correlation types. Here we compute the distance correlation matrix for a DataFrame object. \nIn [219]: def distcorr(x, y):\n   .....:     n = len(x)\n   .....:     a = np.zeros(shape=(n, n))\n   .....:     b = np.zeros(shape=(n, n))\n   .....:     for i in range(n):\n   .....:         for j in range(i + 1, n):\n   .....:             a[i, j] = abs(x[i] - x[j])\n   .....:             b[i, j] = abs(y[i] - y[j])\n   .....:     a += a.T\n   .....:     b += b.T\n   .....:     a_bar = np.vstack([np.nanmean(a, axis=0)] * n)\n   .....:     b_bar = np.vstack([np.nanmean(b, axis=0)] * n)\n   .....:     A = a - a_bar - a_bar.T + np.full(shape=(n, n), fill_value=a_bar.mean())\n   .....:     B = b - b_bar - b_bar.T + np.full(shape=(n, n), fill_value=b_bar.mean())\n   .....:     cov_ab = np.sqrt(np.nansum(A * B)) \/ n\n   .....:     std_a = np.sqrt(np.sqrt(np.nansum(A ** 2)) \/ n)\n   .....:     std_b = np.sqrt(np.sqrt(np.nansum(B ** 2)) \/ n)\n   .....:     return cov_ab \/ std_a \/ std_b\n   .....: \n\nIn [220]: df = pd.DataFrame(np.random.normal(size=(100, 3)))\n\nIn [221]: df.corr(method=distcorr)\nOut[221]: \n          0         1         2\n0  1.000000  0.197613  0.216328\n1  0.197613  1.000000  0.208749\n2  0.216328  0.208749  1.000000\n     Timedeltas The Timedeltas docs. Using timedeltas \nIn [222]: import datetime\n\nIn [223]: s = pd.Series(pd.date_range(\"2012-1-1\", periods=3, freq=\"D\"))\n\nIn [224]: s - s.max()\nOut[224]: \n0   -2 days\n1   -1 days\n2    0 days\ndtype: timedelta64[ns]\n\nIn [225]: s.max() - s\nOut[225]: \n0   2 days\n1   1 days\n2   0 days\ndtype: timedelta64[ns]\n\nIn [226]: s - datetime.datetime(2011, 1, 1, 3, 5)\nOut[226]: \n0   364 days 20:55:00\n1   365 days 20:55:00\n2   366 days 20:55:00\ndtype: timedelta64[ns]\n\nIn [227]: s + datetime.timedelta(minutes=5)\nOut[227]: \n0   2012-01-01 00:05:00\n1   2012-01-02 00:05:00\n2   2012-01-03 00:05:00\ndtype: datetime64[ns]\n\nIn [228]: datetime.datetime(2011, 1, 1, 3, 5) - s\nOut[228]: \n0   -365 days +03:05:00\n1   -366 days +03:05:00\n2   -367 days +03:05:00\ndtype: timedelta64[ns]\n\nIn [229]: datetime.timedelta(minutes=5) + s\nOut[229]: \n0   2012-01-01 00:05:00\n1   2012-01-02 00:05:00\n2   2012-01-03 00:05:00\ndtype: datetime64[ns]\n  Adding and subtracting deltas and dates \nIn [230]: deltas = pd.Series([datetime.timedelta(days=i) for i in range(3)])\n\nIn [231]: df = pd.DataFrame({\"A\": s, \"B\": deltas})\n\nIn [232]: df\nOut[232]: \n           A      B\n0 2012-01-01 0 days\n1 2012-01-02 1 days\n2 2012-01-03 2 days\n\nIn [233]: df[\"New Dates\"] = df[\"A\"] + df[\"B\"]\n\nIn [234]: df[\"Delta\"] = df[\"A\"] - df[\"New Dates\"]\n\nIn [235]: df\nOut[235]: \n           A      B  New Dates   Delta\n0 2012-01-01 0 days 2012-01-01  0 days\n1 2012-01-02 1 days 2012-01-03 -1 days\n2 2012-01-03 2 days 2012-01-05 -2 days\n\nIn [236]: df.dtypes\nOut[236]: \nA             datetime64[ns]\nB            timedelta64[ns]\nNew Dates     datetime64[ns]\nDelta        timedelta64[ns]\ndtype: object\n  Another example Values can be set to NaT using np.nan, similar to datetime \nIn [237]: y = s - s.shift()\n\nIn [238]: y\nOut[238]: \n0      NaT\n1   1 days\n2   1 days\ndtype: timedelta64[ns]\n\nIn [239]: y[1] = np.nan\n\nIn [240]: y\nOut[240]: \n0      NaT\n1      NaT\n2   1 days\ndtype: timedelta64[ns]\n    Creating example data To create a dataframe from every combination of some given values, like R\u2019s expand.grid() function, we can create a dict where the keys are column names and the values are lists of the data values: \nIn [241]: def expand_grid(data_dict):\n   .....:     rows = itertools.product(*data_dict.values())\n   .....:     return pd.DataFrame.from_records(rows, columns=data_dict.keys())\n   .....: \n\nIn [242]: df = expand_grid(\n   .....:     {\"height\": [60, 70], \"weight\": [100, 140, 180], \"sex\": [\"Male\", \"Female\"]}\n   .....: )\n   .....: \n\nIn [243]: df\nOut[243]: \n    height  weight     sex\n0       60     100    Male\n1       60     100  Female\n2       60     140    Male\n3       60     140  Female\n4       60     180    Male\n5       60     180  Female\n6       70     100    Male\n7       70     100  Female\n8       70     140    Male\n9       70     140  Female\n10      70     180    Male\n11      70     180  Female","title":"pandas.user_guide.cookbook"},{"text":"pandas.BooleanDtype   classpandas.BooleanDtype[source]\n \nExtension dtype for boolean data.  New in version 1.0.0.   Warning BooleanDtype is considered experimental. The implementation and parts of the API may change without warning.  Examples \n>>> pd.BooleanDtype()\nBooleanDtype\n  Attributes       \nNone     Methods       \nNone","title":"pandas.reference.api.pandas.booleandtype"},{"text":"class BoolOr(expression, filter=None, default=None, **extra)  \nReturns True if at least one input value is true, default if all values are null or if there are no values, otherwise False. Usage example: class Comment(models.Model):\n    body = models.TextField()\n    published = models.BooleanField()\n    rank = models.IntegerField()\n\n>>> from django.db.models import Q\n>>> from django.contrib.postgres.aggregates import BoolOr\n>>> Comment.objects.aggregate(boolor=BoolOr('published'))\n{'boolor': True}\n>>> Comment.objects.aggregate(boolor=BoolOr(Q(rank__gt=2)))\n{'boolor': False}","title":"django.ref.contrib.postgres.aggregates#django.contrib.postgres.aggregates.BoolOr"},{"text":"pandas.Series.bool   Series.bool()[source]\n \nReturn the bool of a single element Series or DataFrame. This must be a boolean scalar value, either True or False. It will raise a ValueError if the Series or DataFrame does not have exactly 1 element, or that element is not boolean (integer values 0 and 1 will also raise an exception).  Returns \n bool\n\nThe value in the Series or DataFrame.      See also  Series.astype\n\nChange the data type of a Series, including to boolean.  DataFrame.astype\n\nChange the data type of a DataFrame, including to boolean.  numpy.bool_\n\nNumPy boolean data type, used by pandas for boolean values.    Examples The method will only work for single element objects with a boolean value: \n>>> pd.Series([True]).bool()\nTrue\n>>> pd.Series([False]).bool()\nFalse\n  \n>>> pd.DataFrame({'col': [True]}).bool()\nTrue\n>>> pd.DataFrame({'col': [False]}).bool()\nFalse","title":"pandas.reference.api.pandas.series.bool"},{"text":"class torch.distributions.studentT.StudentT(df, loc=0.0, scale=1.0, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a Student\u2019s t-distribution parameterized by degree of freedom df, mean loc and scale scale. Example: >>> m = StudentT(torch.tensor([2.0]))\n>>> m.sample()  # Student's t-distributed with degrees of freedom=2\ntensor([ 0.1046])\n  Parameters \n \ndf (float or Tensor) \u2013 degrees of freedom \nloc (float or Tensor) \u2013 mean of the distribution \nscale (float or Tensor) \u2013 scale of the distribution     \narg_constraints = {'df': GreaterThan(lower_bound=0.0), 'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} \n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = Real() \n  \nproperty variance","title":"torch.distributions#torch.distributions.studentT.StudentT"},{"text":"pandas.arrays.BooleanArray   classpandas.arrays.BooleanArray(values, mask, copy=False)[source]\n \nArray of boolean (True\/False) data with missing values. This is a pandas Extension array for boolean data, under the hood represented by 2 numpy arrays: a boolean array with the data and a boolean array with the mask (True indicating missing). BooleanArray implements Kleene logic (sometimes called three-value logic) for logical operations. See Kleene logical operations for more. To construct an BooleanArray from generic array-like input, use pandas.array() specifying dtype=\"boolean\" (see examples below).  New in version 1.0.0.   Warning BooleanArray is considered experimental. The implementation and parts of the API may change without warning.   Parameters \n \nvalues:numpy.ndarray\n\n\nA 1-d boolean-dtype array with the data.  \nmask:numpy.ndarray\n\n\nA 1-d boolean-dtype array indicating missing values (True indicates missing).  \ncopy:bool, default False\n\n\nWhether to copy the values and mask arrays.    Returns \n BooleanArray\n   Examples Create an BooleanArray with pandas.array(): \n>>> pd.array([True, False, None], dtype=\"boolean\")\n<BooleanArray>\n[True, False, <NA>]\nLength: 3, dtype: boolean\n  Attributes       \nNone     Methods       \nNone","title":"pandas.reference.api.pandas.arrays.booleanarray"},{"text":"numpy.ma.nonzero   ma.nonzero(self) = <numpy.ma.core._frommethod object>\n \nReturn the indices of unmasked elements that are not zero. Returns a tuple of arrays, one for each dimension, containing the indices of the non-zero elements in that dimension. The corresponding non-zero values can be obtained with: a[a.nonzero()]\n To group the indices by element, rather than dimension, use instead: np.transpose(a.nonzero())\n The result of this is always a 2d array, with a row for each non-zero element.  Parameters \n None\n  Returns \n \ntuple_of_arraystuple\n\n\nIndices of elements that are non-zero.      See also  numpy.nonzero\n\nFunction operating on ndarrays.  flatnonzero\n\nReturn indices that are non-zero in the flattened version of the input array.  numpy.ndarray.nonzero\n\nEquivalent ndarray method.  count_nonzero\n\nCounts the number of non-zero elements in the input array.    Examples >>> import numpy.ma as ma\n>>> x = ma.array(np.eye(3))\n>>> x\nmasked_array(\n  data=[[1., 0., 0.],\n        [0., 1., 0.],\n        [0., 0., 1.]],\n  mask=False,\n  fill_value=1e+20)\n>>> x.nonzero()\n(array([0, 1, 2]), array([0, 1, 2]))\n Masked elements are ignored. >>> x[1, 1] = ma.masked\n>>> x\nmasked_array(\n  data=[[1.0, 0.0, 0.0],\n        [0.0, --, 0.0],\n        [0.0, 0.0, 1.0]],\n  mask=[[False, False, False],\n        [False,  True, False],\n        [False, False, False]],\n  fill_value=1e+20)\n>>> x.nonzero()\n(array([0, 2]), array([0, 2]))\n Indices can also be grouped by element. >>> np.transpose(x.nonzero())\narray([[0, 0],\n       [2, 2]])\n A common use for nonzero is to find the indices of an array, where a condition is True. Given an array a, the condition a > 3 is a boolean array and since False is interpreted as 0, ma.nonzero(a > 3) yields the indices of the a where the condition is true. >>> a = ma.array([[1,2,3],[4,5,6],[7,8,9]])\n>>> a > 3\nmasked_array(\n  data=[[False, False, False],\n        [ True,  True,  True],\n        [ True,  True,  True]],\n  mask=False,\n  fill_value=True)\n>>> ma.nonzero(a > 3)\n(array([1, 1, 1, 2, 2, 2]), array([0, 1, 2, 0, 1, 2]))\n The nonzero method of the condition array can also be called. >>> (a > 3).nonzero()\n(array([1, 1, 1, 2, 2, 2]), array([0, 1, 2, 0, 1, 2]))","title":"numpy.reference.generated.numpy.ma.nonzero"}]}
{"task_id":299446,"prompt":"def f_299446(owd):\n\t","suffix":"\n\treturn ","canonical_solution":"os.chdir(owd)","test_start":"\nimport os\nfrom unittest.mock import Mock\n\ndef check(candidate):","test":["\n    os.chdir = Mock()\n    try:\n        candidate('\/')\n    except:\n        assert False\n"],"entry_point":"f_299446","intent":"change working directory to the directory `owd`","library":["os"],"docs":[{"text":"test.support.SAVEDCWD  \nSet to os.getcwd().","title":"python.library.test#test.support.SAVEDCWD"},{"text":"numpy.random.RandomState.wald method   random.RandomState.wald(mean, scale, size=None)\n \nDraw samples from a Wald, or inverse Gaussian, distribution. As the scale approaches infinity, the distribution becomes more like a Gaussian. Some references claim that the Wald is an inverse Gaussian with mean equal to 1, but this is by no means universal. The inverse Gaussian distribution was first studied in relationship to Brownian motion. In 1956 M.C.K. Tweedie used the name inverse Gaussian because there is an inverse relationship between the time to cover a unit distance and distance covered in unit time.  Note New code should use the wald method of a default_rng() instance instead; please see the Quick Start.   Parameters \n \nmeanfloat or array_like of floats\n\n\nDistribution mean, must be > 0.  \nscalefloat or array_like of floats\n\n\nScale parameter, must be > 0.  \nsizeint or tuple of ints, optional\n\n\nOutput shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if mean and scale are both scalars. Otherwise, np.broadcast(mean, scale).size samples are drawn.    Returns \n \noutndarray or scalar\n\n\nDrawn samples from the parameterized Wald distribution.      See also  Generator.wald\n\nwhich should be used for new code.    Notes The probability density function for the Wald distribution is  \\[P(x;mean,scale) = \\sqrt{\\frac{scale}{2\\pi x^3}}e^ \\frac{-scale(x-mean)^2}{2\\cdotp mean^2x}\\] As noted above the inverse Gaussian distribution first arise from attempts to model Brownian motion. It is also a competitor to the Weibull for use in reliability modeling and modeling stock returns and interest rate processes. References  1 \nBrighton Webs Ltd., Wald Distribution, https:\/\/web.archive.org\/web\/20090423014010\/http:\/\/www.brighton-webs.co.uk:80\/distributions\/wald.asp  2 \nChhikara, Raj S., and Folks, J. Leroy, \u201cThe Inverse Gaussian Distribution: Theory : Methodology, and Applications\u201d, CRC Press, 1988.  3 \nWikipedia, \u201cInverse Gaussian distribution\u201d https:\/\/en.wikipedia.org\/wiki\/Inverse_Gaussian_distribution   Examples Draw values from the distribution and plot the histogram: >>> import matplotlib.pyplot as plt\n>>> h = plt.hist(np.random.wald(3, 2, 100000), bins=200, density=True)\n>>> plt.show()","title":"numpy.reference.random.generated.numpy.random.randomstate.wald"},{"text":"numpy.random.wald   random.wald(mean, scale, size=None)\n \nDraw samples from a Wald, or inverse Gaussian, distribution. As the scale approaches infinity, the distribution becomes more like a Gaussian. Some references claim that the Wald is an inverse Gaussian with mean equal to 1, but this is by no means universal. The inverse Gaussian distribution was first studied in relationship to Brownian motion. In 1956 M.C.K. Tweedie used the name inverse Gaussian because there is an inverse relationship between the time to cover a unit distance and distance covered in unit time.  Note New code should use the wald method of a default_rng() instance instead; please see the Quick Start.   Parameters \n \nmeanfloat or array_like of floats\n\n\nDistribution mean, must be > 0.  \nscalefloat or array_like of floats\n\n\nScale parameter, must be > 0.  \nsizeint or tuple of ints, optional\n\n\nOutput shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if mean and scale are both scalars. Otherwise, np.broadcast(mean, scale).size samples are drawn.    Returns \n \noutndarray or scalar\n\n\nDrawn samples from the parameterized Wald distribution.      See also  Generator.wald\n\nwhich should be used for new code.    Notes The probability density function for the Wald distribution is  \\[P(x;mean,scale) = \\sqrt{\\frac{scale}{2\\pi x^3}}e^ \\frac{-scale(x-mean)^2}{2\\cdotp mean^2x}\\] As noted above the inverse Gaussian distribution first arise from attempts to model Brownian motion. It is also a competitor to the Weibull for use in reliability modeling and modeling stock returns and interest rate processes. References  1 \nBrighton Webs Ltd., Wald Distribution, https:\/\/web.archive.org\/web\/20090423014010\/http:\/\/www.brighton-webs.co.uk:80\/distributions\/wald.asp  2 \nChhikara, Raj S., and Folks, J. Leroy, \u201cThe Inverse Gaussian Distribution: Theory : Methodology, and Applications\u201d, CRC Press, 1988.  3 \nWikipedia, \u201cInverse Gaussian distribution\u201d https:\/\/en.wikipedia.org\/wiki\/Inverse_Gaussian_distribution   Examples Draw values from the distribution and plot the histogram: >>> import matplotlib.pyplot as plt\n>>> h = plt.hist(np.random.wald(3, 2, 100000), bins=200, density=True)\n>>> plt.show()","title":"numpy.reference.random.generated.numpy.random.wald"},{"text":"numpy.random.Generator.wald method   random.Generator.wald(mean, scale, size=None)\n \nDraw samples from a Wald, or inverse Gaussian, distribution. As the scale approaches infinity, the distribution becomes more like a Gaussian. Some references claim that the Wald is an inverse Gaussian with mean equal to 1, but this is by no means universal. The inverse Gaussian distribution was first studied in relationship to Brownian motion. In 1956 M.C.K. Tweedie used the name inverse Gaussian because there is an inverse relationship between the time to cover a unit distance and distance covered in unit time.  Parameters \n \nmeanfloat or array_like of floats\n\n\nDistribution mean, must be > 0.  \nscalefloat or array_like of floats\n\n\nScale parameter, must be > 0.  \nsizeint or tuple of ints, optional\n\n\nOutput shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if mean and scale are both scalars. Otherwise, np.broadcast(mean, scale).size samples are drawn.    Returns \n \noutndarray or scalar\n\n\nDrawn samples from the parameterized Wald distribution.     Notes The probability density function for the Wald distribution is  \\[P(x;mean,scale) = \\sqrt{\\frac{scale}{2\\pi x^3}}e^ \\frac{-scale(x-mean)^2}{2\\cdotp mean^2x}\\] As noted above the inverse Gaussian distribution first arise from attempts to model Brownian motion. It is also a competitor to the Weibull for use in reliability modeling and modeling stock returns and interest rate processes. References  1 \nBrighton Webs Ltd., Wald Distribution, https:\/\/web.archive.org\/web\/20090423014010\/http:\/\/www.brighton-webs.co.uk:80\/distributions\/wald.asp  2 \nChhikara, Raj S., and Folks, J. Leroy, \u201cThe Inverse Gaussian Distribution: Theory : Methodology, and Applications\u201d, CRC Press, 1988.  3 \nWikipedia, \u201cInverse Gaussian distribution\u201d https:\/\/en.wikipedia.org\/wiki\/Inverse_Gaussian_distribution   Examples Draw values from the distribution and plot the histogram: >>> import matplotlib.pyplot as plt\n>>> h = plt.hist(np.random.default_rng().wald(3, 2, 100000), bins=200, density=True)\n>>> plt.show()","title":"numpy.reference.random.generated.numpy.random.generator.wald"},{"text":"signal.SIGCLD  \nAlias to SIGCHLD.","title":"python.library.signal#signal.SIGCLD"},{"text":"os.fchdir(fd)  \nChange the current working directory to the directory represented by the file descriptor fd. The descriptor must refer to an opened directory, not an open file. As of Python 3.3, this is equivalent to os.chdir(fd). Raises an auditing event os.chdir with argument path. Availability: Unix.","title":"python.library.os#os.fchdir"},{"text":"signal.SIGCHLD  \nChild process stopped or terminated. Availability: Unix.","title":"python.library.signal#signal.SIGCHLD"},{"text":"os.chdir(path)  \nChange the current working directory to path. This function can support specifying a file descriptor. The descriptor must refer to an opened directory, not an open file. This function can raise OSError and subclasses such as FileNotFoundError, PermissionError, and NotADirectoryError. Raises an auditing event os.chdir with argument path.  New in version 3.3: Added support for specifying path as a file descriptor on some platforms.   Changed in version 3.6: Accepts a path-like object.","title":"python.library.os#os.chdir"},{"text":"FTP.cwd(pathname)  \nSet the current directory on the server.","title":"python.library.ftplib#ftplib.FTP.cwd"},{"text":"torch.hub.set_dir(d) [source]\n \nOptionally set the Torch Hub directory used to save downloaded models & weights.  Parameters \nd (string) \u2013 path to a local folder to save downloaded models & weights.","title":"torch.hub#torch.hub.set_dir"}]}
{"task_id":14695134,"prompt":"def f_14695134(c, testfield):\n\t","suffix":"\n\treturn ","canonical_solution":"c.execute(\"INSERT INTO test VALUES (?, 'bar')\", (testfield,))","test_start":"\nimport sqlite3\n\ndef check(candidate):","test":["\n    conn = sqlite3.connect('dev.db')\n    cur = conn.cursor()\n    cur.execute(\"CREATE TABLE test (x VARCHAR(10), y VARCHAR(10))\")\n    candidate(cur, 'kang')\n    cur.execute(\"SELECT * FROM test\")\n    rows = cur.fetchall()\n    assert len(rows) == 1\n"],"entry_point":"f_14695134","intent":"insert data from a string `testfield` to sqlite db `c`","library":["sqlite3"],"docs":[{"text":"stars.main() \n run a simple starfield example stars.main() -> None  A simple starfield example. You can change the center of perspective by leftclicking the mouse on the screen.","title":"pygame.ref.examples#pygame.examples.stars.main"},{"text":"sqlite3 \u2014 DB-API 2.0 interface for SQLite databases Source code: Lib\/sqlite3\/ SQLite is a C library that provides a lightweight disk-based database that doesn\u2019t require a separate server process and allows accessing the database using a nonstandard variant of the SQL query language. Some applications can use SQLite for internal data storage. It\u2019s also possible to prototype an application using SQLite and then port the code to a larger database such as PostgreSQL or Oracle. The sqlite3 module was written by Gerhard H\u00e4ring. It provides a SQL interface compliant with the DB-API 2.0 specification described by PEP 249. To use the module, you must first create a Connection object that represents the database. Here the data will be stored in the example.db file: import sqlite3\ncon = sqlite3.connect('example.db')\n You can also supply the special name :memory: to create a database in RAM. Once you have a Connection, you can create a Cursor object and call its execute() method to perform SQL commands: cur = con.cursor()\n\n# Create table\ncur.execute('''CREATE TABLE stocks\n               (date text, trans text, symbol text, qty real, price real)''')\n\n# Insert a row of data\ncur.execute(\"INSERT INTO stocks VALUES ('2006-01-05','BUY','RHAT',100,35.14)\")\n\n# Save (commit) the changes\ncon.commit()\n\n# We can also close the connection if we are done with it.\n# Just be sure any changes have been committed or they will be lost.\ncon.close()\n The data you\u2019ve saved is persistent and is available in subsequent sessions: import sqlite3\ncon = sqlite3.connect('example.db')\ncur = con.cursor()\n To retrieve data after executing a SELECT statement, you can either treat the cursor as an iterator, call the cursor\u2019s fetchone() method to retrieve a single matching row, or call fetchall() to get a list of the matching rows. This example uses the iterator form: >>> for row in cur.execute('SELECT * FROM stocks ORDER BY price'):\n        print(row)\n\n('2006-01-05', 'BUY', 'RHAT', 100, 35.14)\n('2006-03-28', 'BUY', 'IBM', 1000, 45.0)\n('2006-04-06', 'SELL', 'IBM', 500, 53.0)\n('2006-04-05', 'BUY', 'MSFT', 1000, 72.0)\n Usually your SQL operations will need to use values from Python variables. You shouldn\u2019t assemble your query using Python\u2019s string operations because doing so is insecure; it makes your program vulnerable to an SQL injection attack (see the xkcd webcomic for a humorous example of what can go wrong): # Never do this -- insecure!\nsymbol = 'RHAT'\ncur.execute(\"SELECT * FROM stocks WHERE symbol = '%s'\" % symbol)\n Instead, use the DB-API\u2019s parameter substitution. Put a placeholder wherever you want to use a value, and then provide a tuple of values as the second argument to the cursor\u2019s execute() method. An SQL statement may use one of two kinds of placeholders: question marks (qmark style) or named placeholders (named style). For the qmark style, parameters must be a sequence. For the named style, it can be either a sequence or dict instance. The length of the sequence must match the number of placeholders, or a ProgrammingError is raised. If a dict is given, it must contain keys for all named parameters. Any extra items are ignored. Here\u2019s an example of both styles: import sqlite3\n\ncon = sqlite3.connect(\":memory:\")\ncur = con.cursor()\ncur.execute(\"create table lang (lang_name, lang_age)\")\n\n# This is the qmark style:\ncur.execute(\"insert into lang values (?, ?)\", (\"C\", 49))\n\n# The qmark style used with executemany():\nlang_list = [\n    (\"Fortran\", 64),\n    (\"Python\", 30),\n    (\"Go\", 11),\n]\ncur.executemany(\"insert into lang values (?, ?)\", lang_list)\n\n# And this is the named style:\ncur.execute(\"select * from lang where lang_name=:name and lang_age=:age\",\n            {\"name\": \"C\", \"age\": 49})\nprint(cur.fetchall())\n\ncon.close()\n  See also  https:\/\/www.sqlite.org\n\nThe SQLite web page; the documentation describes the syntax and the available data types for the supported SQL dialect.  https:\/\/www.w3schools.com\/sql\/\n\nTutorial, reference and examples for learning SQL syntax.  \nPEP 249 - Database API Specification 2.0\n\nPEP written by Marc-Andr\u00e9 Lemburg.    Module functions and constants  \nsqlite3.version  \nThe version number of this module, as a string. This is not the version of the SQLite library. \n  \nsqlite3.version_info  \nThe version number of this module, as a tuple of integers. This is not the version of the SQLite library. \n  \nsqlite3.sqlite_version  \nThe version number of the run-time SQLite library, as a string. \n  \nsqlite3.sqlite_version_info  \nThe version number of the run-time SQLite library, as a tuple of integers. \n  \nsqlite3.PARSE_DECLTYPES  \nThis constant is meant to be used with the detect_types parameter of the connect() function. Setting it makes the sqlite3 module parse the declared type for each column it returns. It will parse out the first word of the declared type, i. e. for \u201cinteger primary key\u201d, it will parse out \u201cinteger\u201d, or for \u201cnumber(10)\u201d it will parse out \u201cnumber\u201d. Then for that column, it will look into the converters dictionary and use the converter function registered for that type there. \n  \nsqlite3.PARSE_COLNAMES  \nThis constant is meant to be used with the detect_types parameter of the connect() function. Setting this makes the SQLite interface parse the column name for each column it returns. It will look for a string formed [mytype] in there, and then decide that \u2018mytype\u2019 is the type of the column. It will try to find an entry of \u2018mytype\u2019 in the converters dictionary and then use the converter function found there to return the value. The column name found in Cursor.description does not include the type, i. e. if you use something like 'as \"Expiration date [datetime]\"' in your SQL, then we will parse out everything until the first '[' for the column name and strip the preceeding space: the column name would simply be \u201cExpiration date\u201d. \n  \nsqlite3.connect(database[, timeout, detect_types, isolation_level, check_same_thread, factory, cached_statements, uri])  \nOpens a connection to the SQLite database file database. By default returns a Connection object, unless a custom factory is given. database is a path-like object giving the pathname (absolute or relative to the current working directory) of the database file to be opened. You can use \":memory:\" to open a database connection to a database that resides in RAM instead of on disk. When a database is accessed by multiple connections, and one of the processes modifies the database, the SQLite database is locked until that transaction is committed. The timeout parameter specifies how long the connection should wait for the lock to go away until raising an exception. The default for the timeout parameter is 5.0 (five seconds). For the isolation_level parameter, please see the isolation_level property of Connection objects. SQLite natively supports only the types TEXT, INTEGER, REAL, BLOB and NULL. If you want to use other types you must add support for them yourself. The detect_types parameter and the using custom converters registered with the module-level register_converter() function allow you to easily do that. detect_types defaults to 0 (i. e. off, no type detection), you can set it to any combination of PARSE_DECLTYPES and PARSE_COLNAMES to turn type detection on. Due to SQLite behaviour, types can\u2019t be detected for generated fields (for example max(data)), even when detect_types parameter is set. In such case, the returned type is str. By default, check_same_thread is True and only the creating thread may use the connection. If set False, the returned connection may be shared across multiple threads. When using multiple threads with the same connection writing operations should be serialized by the user to avoid data corruption. By default, the sqlite3 module uses its Connection class for the connect call. You can, however, subclass the Connection class and make connect() use your class instead by providing your class for the factory parameter. Consult the section SQLite and Python types of this manual for details. The sqlite3 module internally uses a statement cache to avoid SQL parsing overhead. If you want to explicitly set the number of statements that are cached for the connection, you can set the cached_statements parameter. The currently implemented default is to cache 100 statements. If uri is true, database is interpreted as a URI. This allows you to specify options. For example, to open a database in read-only mode you can use: db = sqlite3.connect('file:path\/to\/database?mode=ro', uri=True)\n More information about this feature, including a list of recognized options, can be found in the SQLite URI documentation. Raises an auditing event sqlite3.connect with argument database.  Changed in version 3.4: Added the uri parameter.   Changed in version 3.7: database can now also be a path-like object, not only a string.  \n  \nsqlite3.register_converter(typename, callable)  \nRegisters a callable to convert a bytestring from the database into a custom Python type. The callable will be invoked for all database values that are of the type typename. Confer the parameter detect_types of the connect() function for how the type detection works. Note that typename and the name of the type in your query are matched in case-insensitive manner. \n  \nsqlite3.register_adapter(type, callable)  \nRegisters a callable to convert the custom Python type type into one of SQLite\u2019s supported types. The callable callable accepts as single parameter the Python value, and must return a value of the following types: int, float, str or bytes. \n  \nsqlite3.complete_statement(sql)  \nReturns True if the string sql contains one or more complete SQL statements terminated by semicolons. It does not verify that the SQL is syntactically correct, only that there are no unclosed string literals and the statement is terminated by a semicolon. This can be used to build a shell for SQLite, as in the following example: # A minimal SQLite shell for experiments\n\nimport sqlite3\n\ncon = sqlite3.connect(\":memory:\")\ncon.isolation_level = None\ncur = con.cursor()\n\nbuffer = \"\"\n\nprint(\"Enter your SQL commands to execute in sqlite3.\")\nprint(\"Enter a blank line to exit.\")\n\nwhile True:\n    line = input()\n    if line == \"\":\n        break\n    buffer += line\n    if sqlite3.complete_statement(buffer):\n        try:\n            buffer = buffer.strip()\n            cur.execute(buffer)\n\n            if buffer.lstrip().upper().startswith(\"SELECT\"):\n                print(cur.fetchall())\n        except sqlite3.Error as e:\n            print(\"An error occurred:\", e.args[0])\n        buffer = \"\"\n\ncon.close()\n \n  \nsqlite3.enable_callback_tracebacks(flag)  \nBy default you will not get any tracebacks in user-defined functions, aggregates, converters, authorizer callbacks etc. If you want to debug them, you can call this function with flag set to True. Afterwards, you will get tracebacks from callbacks on sys.stderr. Use False to disable the feature again. \n Connection Objects  \nclass sqlite3.Connection  \nA SQLite database connection has the following attributes and methods:  \nisolation_level  \nGet or set the current default isolation level. None for autocommit mode or one of \u201cDEFERRED\u201d, \u201cIMMEDIATE\u201d or \u201cEXCLUSIVE\u201d. See section Controlling Transactions for a more detailed explanation. \n  \nin_transaction  \nTrue if a transaction is active (there are uncommitted changes), False otherwise. Read-only attribute.  New in version 3.2.  \n  \ncursor(factory=Cursor)  \nThe cursor method accepts a single optional parameter factory. If supplied, this must be a callable returning an instance of Cursor or its subclasses. \n  \ncommit()  \nThis method commits the current transaction. If you don\u2019t call this method, anything you did since the last call to commit() is not visible from other database connections. If you wonder why you don\u2019t see the data you\u2019ve written to the database, please check you didn\u2019t forget to call this method. \n  \nrollback()  \nThis method rolls back any changes to the database since the last call to commit(). \n  \nclose()  \nThis closes the database connection. Note that this does not automatically call commit(). If you just close your database connection without calling commit() first, your changes will be lost! \n  \nexecute(sql[, parameters])  \nThis is a nonstandard shortcut that creates a cursor object by calling the cursor() method, calls the cursor\u2019s execute() method with the parameters given, and returns the cursor. \n  \nexecutemany(sql[, parameters])  \nThis is a nonstandard shortcut that creates a cursor object by calling the cursor() method, calls the cursor\u2019s executemany() method with the parameters given, and returns the cursor. \n  \nexecutescript(sql_script)  \nThis is a nonstandard shortcut that creates a cursor object by calling the cursor() method, calls the cursor\u2019s executescript() method with the given sql_script, and returns the cursor. \n  \ncreate_function(name, num_params, func, *, deterministic=False)  \nCreates a user-defined function that you can later use from within SQL statements under the function name name. num_params is the number of parameters the function accepts (if num_params is -1, the function may take any number of arguments), and func is a Python callable that is called as the SQL function. If deterministic is true, the created function is marked as deterministic, which allows SQLite to perform additional optimizations. This flag is supported by SQLite 3.8.3 or higher, NotSupportedError will be raised if used with older versions. The function can return any of the types supported by SQLite: bytes, str, int, float and None.  Changed in version 3.8: The deterministic parameter was added.  Example: import sqlite3\nimport hashlib\n\ndef md5sum(t):\n    return hashlib.md5(t).hexdigest()\n\ncon = sqlite3.connect(\":memory:\")\ncon.create_function(\"md5\", 1, md5sum)\ncur = con.cursor()\ncur.execute(\"select md5(?)\", (b\"foo\",))\nprint(cur.fetchone()[0])\n\ncon.close()\n \n  \ncreate_aggregate(name, num_params, aggregate_class)  \nCreates a user-defined aggregate function. The aggregate class must implement a step method, which accepts the number of parameters num_params (if num_params is -1, the function may take any number of arguments), and a finalize method which will return the final result of the aggregate. The finalize method can return any of the types supported by SQLite: bytes, str, int, float and None. Example: import sqlite3\n\nclass MySum:\n    def __init__(self):\n        self.count = 0\n\n    def step(self, value):\n        self.count += value\n\n    def finalize(self):\n        return self.count\n\ncon = sqlite3.connect(\":memory:\")\ncon.create_aggregate(\"mysum\", 1, MySum)\ncur = con.cursor()\ncur.execute(\"create table test(i)\")\ncur.execute(\"insert into test(i) values (1)\")\ncur.execute(\"insert into test(i) values (2)\")\ncur.execute(\"select mysum(i) from test\")\nprint(cur.fetchone()[0])\n\ncon.close()\n \n  \ncreate_collation(name, callable)  \nCreates a collation with the specified name and callable. The callable will be passed two string arguments. It should return -1 if the first is ordered lower than the second, 0 if they are ordered equal and 1 if the first is ordered higher than the second. Note that this controls sorting (ORDER BY in SQL) so your comparisons don\u2019t affect other SQL operations. Note that the callable will get its parameters as Python bytestrings, which will normally be encoded in UTF-8. The following example shows a custom collation that sorts \u201cthe wrong way\u201d: import sqlite3\n\ndef collate_reverse(string1, string2):\n    if string1 == string2:\n        return 0\n    elif string1 < string2:\n        return 1\n    else:\n        return -1\n\ncon = sqlite3.connect(\":memory:\")\ncon.create_collation(\"reverse\", collate_reverse)\n\ncur = con.cursor()\ncur.execute(\"create table test(x)\")\ncur.executemany(\"insert into test(x) values (?)\", [(\"a\",), (\"b\",)])\ncur.execute(\"select x from test order by x collate reverse\")\nfor row in cur:\n    print(row)\ncon.close()\n To remove a collation, call create_collation with None as callable: con.create_collation(\"reverse\", None)\n \n  \ninterrupt()  \nYou can call this method from a different thread to abort any queries that might be executing on the connection. The query will then abort and the caller will get an exception. \n  \nset_authorizer(authorizer_callback)  \nThis routine registers a callback. The callback is invoked for each attempt to access a column of a table in the database. The callback should return SQLITE_OK if access is allowed, SQLITE_DENY if the entire SQL statement should be aborted with an error and SQLITE_IGNORE if the column should be treated as a NULL value. These constants are available in the sqlite3 module. The first argument to the callback signifies what kind of operation is to be authorized. The second and third argument will be arguments or None depending on the first argument. The 4th argument is the name of the database (\u201cmain\u201d, \u201ctemp\u201d, etc.) if applicable. The 5th argument is the name of the inner-most trigger or view that is responsible for the access attempt or None if this access attempt is directly from input SQL code. Please consult the SQLite documentation about the possible values for the first argument and the meaning of the second and third argument depending on the first one. All necessary constants are available in the sqlite3 module. \n  \nset_progress_handler(handler, n)  \nThis routine registers a callback. The callback is invoked for every n instructions of the SQLite virtual machine. This is useful if you want to get called from SQLite during long-running operations, for example to update a GUI. If you want to clear any previously installed progress handler, call the method with None for handler. Returning a non-zero value from the handler function will terminate the currently executing query and cause it to raise an OperationalError exception. \n  \nset_trace_callback(trace_callback)  \nRegisters trace_callback to be called for each SQL statement that is actually executed by the SQLite backend. The only argument passed to the callback is the statement (as string) that is being executed. The return value of the callback is ignored. Note that the backend does not only run statements passed to the Cursor.execute() methods. Other sources include the transaction management of the Python module and the execution of triggers defined in the current database. Passing None as trace_callback will disable the trace callback.  New in version 3.3.  \n  \nenable_load_extension(enabled)  \nThis routine allows\/disallows the SQLite engine to load SQLite extensions from shared libraries. SQLite extensions can define new functions, aggregates or whole new virtual table implementations. One well-known extension is the fulltext-search extension distributed with SQLite. Loadable extensions are disabled by default. See 1.  New in version 3.2.  import sqlite3\n\ncon = sqlite3.connect(\":memory:\")\n\n# enable extension loading\ncon.enable_load_extension(True)\n\n# Load the fulltext search extension\ncon.execute(\"select load_extension('.\/fts3.so')\")\n\n# alternatively you can load the extension using an API call:\n# con.load_extension(\".\/fts3.so\")\n\n# disable extension loading again\ncon.enable_load_extension(False)\n\n# example from SQLite wiki\ncon.execute(\"create virtual table recipe using fts3(name, ingredients)\")\ncon.executescript(\"\"\"\n    insert into recipe (name, ingredients) values ('broccoli stew', 'broccoli peppers cheese tomatoes');\n    insert into recipe (name, ingredients) values ('pumpkin stew', 'pumpkin onions garlic celery');\n    insert into recipe (name, ingredients) values ('broccoli pie', 'broccoli cheese onions flour');\n    insert into recipe (name, ingredients) values ('pumpkin pie', 'pumpkin sugar flour butter');\n    \"\"\")\nfor row in con.execute(\"select rowid, name, ingredients from recipe where name match 'pie'\"):\n    print(row)\n\ncon.close()\n \n  \nload_extension(path)  \nThis routine loads a SQLite extension from a shared library. You have to enable extension loading with enable_load_extension() before you can use this routine. Loadable extensions are disabled by default. See 1.  New in version 3.2.  \n  \nrow_factory  \nYou can change this attribute to a callable that accepts the cursor and the original row as a tuple and will return the real result row. This way, you can implement more advanced ways of returning results, such as returning an object that can also access columns by name. Example: import sqlite3\n\ndef dict_factory(cursor, row):\n    d = {}\n    for idx, col in enumerate(cursor.description):\n        d[col[0]] = row[idx]\n    return d\n\ncon = sqlite3.connect(\":memory:\")\ncon.row_factory = dict_factory\ncur = con.cursor()\ncur.execute(\"select 1 as a\")\nprint(cur.fetchone()[\"a\"])\n\ncon.close()\n If returning a tuple doesn\u2019t suffice and you want name-based access to columns, you should consider setting row_factory to the highly-optimized sqlite3.Row type. Row provides both index-based and case-insensitive name-based access to columns with almost no memory overhead. It will probably be better than your own custom dictionary-based approach or even a db_row based solution. \n  \ntext_factory  \nUsing this attribute you can control what objects are returned for the TEXT data type. By default, this attribute is set to str and the sqlite3 module will return Unicode objects for TEXT. If you want to return bytestrings instead, you can set it to bytes. You can also set it to any other callable that accepts a single bytestring parameter and returns the resulting object. See the following example code for illustration: import sqlite3\n\ncon = sqlite3.connect(\":memory:\")\ncur = con.cursor()\n\nAUSTRIA = \"\\xd6sterreich\"\n\n# by default, rows are returned as Unicode\ncur.execute(\"select ?\", (AUSTRIA,))\nrow = cur.fetchone()\nassert row[0] == AUSTRIA\n\n# but we can make sqlite3 always return bytestrings ...\ncon.text_factory = bytes\ncur.execute(\"select ?\", (AUSTRIA,))\nrow = cur.fetchone()\nassert type(row[0]) is bytes\n# the bytestrings will be encoded in UTF-8, unless you stored garbage in the\n# database ...\nassert row[0] == AUSTRIA.encode(\"utf-8\")\n\n# we can also implement a custom text_factory ...\n# here we implement one that appends \"foo\" to all strings\ncon.text_factory = lambda x: x.decode(\"utf-8\") + \"foo\"\ncur.execute(\"select ?\", (\"bar\",))\nrow = cur.fetchone()\nassert row[0] == \"barfoo\"\n\ncon.close()\n \n  \ntotal_changes  \nReturns the total number of database rows that have been modified, inserted, or deleted since the database connection was opened. \n  \niterdump()  \nReturns an iterator to dump the database in an SQL text format. Useful when saving an in-memory database for later restoration. This function provides the same capabilities as the .dump command in the sqlite3 shell. Example: # Convert file existing_db.db to SQL dump file dump.sql\nimport sqlite3\n\ncon = sqlite3.connect('existing_db.db')\nwith open('dump.sql', 'w') as f:\n    for line in con.iterdump():\n        f.write('%s\\n' % line)\ncon.close()\n \n  \nbackup(target, *, pages=-1, progress=None, name=\"main\", sleep=0.250)  \nThis method makes a backup of a SQLite database even while it\u2019s being accessed by other clients, or concurrently by the same connection. The copy will be written into the mandatory argument target, that must be another Connection instance. By default, or when pages is either 0 or a negative integer, the entire database is copied in a single step; otherwise the method performs a loop copying up to pages pages at a time. If progress is specified, it must either be None or a callable object that will be executed at each iteration with three integer arguments, respectively the status of the last iteration, the remaining number of pages still to be copied and the total number of pages. The name argument specifies the database name that will be copied: it must be a string containing either \"main\", the default, to indicate the main database, \"temp\" to indicate the temporary database or the name specified after the AS keyword in an ATTACH DATABASE statement for an attached database. The sleep argument specifies the number of seconds to sleep by between successive attempts to backup remaining pages, can be specified either as an integer or a floating point value. Example 1, copy an existing database into another: import sqlite3\n\ndef progress(status, remaining, total):\n    print(f'Copied {total-remaining} of {total} pages...')\n\ncon = sqlite3.connect('existing_db.db')\nbck = sqlite3.connect('backup.db')\nwith bck:\n    con.backup(bck, pages=1, progress=progress)\nbck.close()\ncon.close()\n Example 2, copy an existing database into a transient copy: import sqlite3\n\nsource = sqlite3.connect('existing_db.db')\ndest = sqlite3.connect(':memory:')\nsource.backup(dest)\n Availability: SQLite 3.6.11 or higher  New in version 3.7.  \n \n Cursor Objects  \nclass sqlite3.Cursor  \nA Cursor instance has the following attributes and methods.  \nexecute(sql[, parameters])  \nExecutes an SQL statement. Values may be bound to the statement using placeholders. execute() will only execute a single SQL statement. If you try to execute more than one statement with it, it will raise a Warning. Use executescript() if you want to execute multiple SQL statements with one call. \n  \nexecutemany(sql, seq_of_parameters)  \nExecutes a parameterized SQL command against all parameter sequences or mappings found in the sequence seq_of_parameters. The sqlite3 module also allows using an iterator yielding parameters instead of a sequence. import sqlite3\n\nclass IterChars:\n    def __init__(self):\n        self.count = ord('a')\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if self.count > ord('z'):\n            raise StopIteration\n        self.count += 1\n        return (chr(self.count - 1),) # this is a 1-tuple\n\ncon = sqlite3.connect(\":memory:\")\ncur = con.cursor()\ncur.execute(\"create table characters(c)\")\n\ntheIter = IterChars()\ncur.executemany(\"insert into characters(c) values (?)\", theIter)\n\ncur.execute(\"select c from characters\")\nprint(cur.fetchall())\n\ncon.close()\n Here\u2019s a shorter example using a generator: import sqlite3\nimport string\n\ndef char_generator():\n    for c in string.ascii_lowercase:\n        yield (c,)\n\ncon = sqlite3.connect(\":memory:\")\ncur = con.cursor()\ncur.execute(\"create table characters(c)\")\n\ncur.executemany(\"insert into characters(c) values (?)\", char_generator())\n\ncur.execute(\"select c from characters\")\nprint(cur.fetchall())\n\ncon.close()\n \n  \nexecutescript(sql_script)  \nThis is a nonstandard convenience method for executing multiple SQL statements at once. It issues a COMMIT statement first, then executes the SQL script it gets as a parameter. sql_script can be an instance of str. Example: import sqlite3\n\ncon = sqlite3.connect(\":memory:\")\ncur = con.cursor()\ncur.executescript(\"\"\"\n    create table person(\n        firstname,\n        lastname,\n        age\n    );\n\n    create table book(\n        title,\n        author,\n        published\n    );\n\n    insert into book(title, author, published)\n    values (\n        'Dirk Gently''s Holistic Detective Agency',\n        'Douglas Adams',\n        1987\n    );\n    \"\"\")\ncon.close()\n \n  \nfetchone()  \nFetches the next row of a query result set, returning a single sequence, or None when no more data is available. \n  \nfetchmany(size=cursor.arraysize)  \nFetches the next set of rows of a query result, returning a list. An empty list is returned when no more rows are available. The number of rows to fetch per call is specified by the size parameter. If it is not given, the cursor\u2019s arraysize determines the number of rows to be fetched. The method should try to fetch as many rows as indicated by the size parameter. If this is not possible due to the specified number of rows not being available, fewer rows may be returned. Note there are performance considerations involved with the size parameter. For optimal performance, it is usually best to use the arraysize attribute. If the size parameter is used, then it is best for it to retain the same value from one fetchmany() call to the next. \n  \nfetchall()  \nFetches all (remaining) rows of a query result, returning a list. Note that the cursor\u2019s arraysize attribute can affect the performance of this operation. An empty list is returned when no rows are available. \n  \nclose()  \nClose the cursor now (rather than whenever __del__ is called). The cursor will be unusable from this point forward; a ProgrammingError exception will be raised if any operation is attempted with the cursor. \n  \nrowcount  \nAlthough the Cursor class of the sqlite3 module implements this attribute, the database engine\u2019s own support for the determination of \u201crows affected\u201d\/\u201drows selected\u201d is quirky. For executemany() statements, the number of modifications are summed up into rowcount. As required by the Python DB API Spec, the rowcount attribute \u201cis -1 in case no executeXX() has been performed on the cursor or the rowcount of the last operation is not determinable by the interface\u201d. This includes SELECT statements because we cannot determine the number of rows a query produced until all rows were fetched. With SQLite versions before 3.6.5, rowcount is set to 0 if you make a DELETE FROM table without any condition. \n  \nlastrowid  \nThis read-only attribute provides the rowid of the last modified row. It is only set if you issued an INSERT or a REPLACE statement using the execute() method. For operations other than INSERT or REPLACE or when executemany() is called, lastrowid is set to None. If the INSERT or REPLACE statement failed to insert the previous successful rowid is returned.  Changed in version 3.6: Added support for the REPLACE statement.  \n  \narraysize  \nRead\/write attribute that controls the number of rows returned by fetchmany(). The default value is 1 which means a single row would be fetched per call. \n  \ndescription  \nThis read-only attribute provides the column names of the last query. To remain compatible with the Python DB API, it returns a 7-tuple for each column where the last six items of each tuple are None. It is set for SELECT statements without any matching rows as well. \n  \nconnection  \nThis read-only attribute provides the SQLite database Connection used by the Cursor object. A Cursor object created by calling con.cursor() will have a connection attribute that refers to con: >>> con = sqlite3.connect(\":memory:\")\n>>> cur = con.cursor()\n>>> cur.connection == con\nTrue\n \n \n Row Objects  \nclass sqlite3.Row  \nA Row instance serves as a highly optimized row_factory for Connection objects. It tries to mimic a tuple in most of its features. It supports mapping access by column name and index, iteration, representation, equality testing and len(). If two Row objects have exactly the same columns and their members are equal, they compare equal.  \nkeys()  \nThis method returns a list of column names. Immediately after a query, it is the first member of each tuple in Cursor.description. \n  Changed in version 3.5: Added support of slicing.  \n Let\u2019s assume we initialize a table as in the example given above: con = sqlite3.connect(\":memory:\")\ncur = con.cursor()\ncur.execute('''create table stocks\n(date text, trans text, symbol text,\n qty real, price real)''')\ncur.execute(\"\"\"insert into stocks\n            values ('2006-01-05','BUY','RHAT',100,35.14)\"\"\")\ncon.commit()\ncur.close()\n Now we plug Row in: >>> con.row_factory = sqlite3.Row\n>>> cur = con.cursor()\n>>> cur.execute('select * from stocks')\n<sqlite3.Cursor object at 0x7f4e7dd8fa80>\n>>> r = cur.fetchone()\n>>> type(r)\n<class 'sqlite3.Row'>\n>>> tuple(r)\n('2006-01-05', 'BUY', 'RHAT', 100.0, 35.14)\n>>> len(r)\n5\n>>> r[2]\n'RHAT'\n>>> r.keys()\n['date', 'trans', 'symbol', 'qty', 'price']\n>>> r['qty']\n100.0\n>>> for member in r:\n...     print(member)\n...\n2006-01-05\nBUY\nRHAT\n100.0\n35.14\n Exceptions  \nexception sqlite3.Warning  \nA subclass of Exception. \n  \nexception sqlite3.Error  \nThe base class of the other exceptions in this module. It is a subclass of Exception. \n  \nexception sqlite3.DatabaseError  \nException raised for errors that are related to the database. \n  \nexception sqlite3.IntegrityError  \nException raised when the relational integrity of the database is affected, e.g. a foreign key check fails. It is a subclass of DatabaseError. \n  \nexception sqlite3.ProgrammingError  \nException raised for programming errors, e.g. table not found or already exists, syntax error in the SQL statement, wrong number of parameters specified, etc. It is a subclass of DatabaseError. \n  \nexception sqlite3.OperationalError  \nException raised for errors that are related to the database\u2019s operation and not necessarily under the control of the programmer, e.g. an unexpected disconnect occurs, the data source name is not found, a transaction could not be processed, etc. It is a subclass of DatabaseError. \n  \nexception sqlite3.NotSupportedError  \nException raised in case a method or database API was used which is not supported by the database, e.g. calling the rollback() method on a connection that does not support transaction or has transactions turned off. It is a subclass of DatabaseError. \n SQLite and Python types Introduction SQLite natively supports the following types: NULL, INTEGER, REAL, TEXT, BLOB. The following Python types can thus be sent to SQLite without any problem:   \nPython type SQLite type   \nNone NULL  \nint INTEGER  \nfloat REAL  \nstr TEXT  \nbytes BLOB   This is how SQLite types are converted to Python types by default:   \nSQLite type Python type   \nNULL None  \nINTEGER int  \nREAL float  \nTEXT depends on text_factory, str by default  \nBLOB bytes   The type system of the sqlite3 module is extensible in two ways: you can store additional Python types in a SQLite database via object adaptation, and you can let the sqlite3 module convert SQLite types to different Python types via converters. Using adapters to store additional Python types in SQLite databases As described before, SQLite supports only a limited set of types natively. To use other Python types with SQLite, you must adapt them to one of the sqlite3 module\u2019s supported types for SQLite: one of NoneType, int, float, str, bytes. There are two ways to enable the sqlite3 module to adapt a custom Python type to one of the supported ones. Letting your object adapt itself This is a good approach if you write the class yourself. Let\u2019s suppose you have a class like this: class Point:\n    def __init__(self, x, y):\n        self.x, self.y = x, y\n Now you want to store the point in a single SQLite column. First you\u2019ll have to choose one of the supported types to be used for representing the point. Let\u2019s just use str and separate the coordinates using a semicolon. Then you need to give your class a method __conform__(self, protocol) which must return the converted value. The parameter protocol will be PrepareProtocol. import sqlite3\n\nclass Point:\n    def __init__(self, x, y):\n        self.x, self.y = x, y\n\n    def __conform__(self, protocol):\n        if protocol is sqlite3.PrepareProtocol:\n            return \"%f;%f\" % (self.x, self.y)\n\ncon = sqlite3.connect(\":memory:\")\ncur = con.cursor()\n\np = Point(4.0, -3.2)\ncur.execute(\"select ?\", (p,))\nprint(cur.fetchone()[0])\n\ncon.close()\n Registering an adapter callable The other possibility is to create a function that converts the type to the string representation and register the function with register_adapter(). import sqlite3\n\nclass Point:\n    def __init__(self, x, y):\n        self.x, self.y = x, y\n\ndef adapt_point(point):\n    return \"%f;%f\" % (point.x, point.y)\n\nsqlite3.register_adapter(Point, adapt_point)\n\ncon = sqlite3.connect(\":memory:\")\ncur = con.cursor()\n\np = Point(4.0, -3.2)\ncur.execute(\"select ?\", (p,))\nprint(cur.fetchone()[0])\n\ncon.close()\n The sqlite3 module has two default adapters for Python\u2019s built-in datetime.date and datetime.datetime types. Now let\u2019s suppose we want to store datetime.datetime objects not in ISO representation, but as a Unix timestamp. import sqlite3\nimport datetime\nimport time\n\ndef adapt_datetime(ts):\n    return time.mktime(ts.timetuple())\n\nsqlite3.register_adapter(datetime.datetime, adapt_datetime)\n\ncon = sqlite3.connect(\":memory:\")\ncur = con.cursor()\n\nnow = datetime.datetime.now()\ncur.execute(\"select ?\", (now,))\nprint(cur.fetchone()[0])\n\ncon.close()\n Converting SQLite values to custom Python types Writing an adapter lets you send custom Python types to SQLite. But to make it really useful we need to make the Python to SQLite to Python roundtrip work. Enter converters. Let\u2019s go back to the Point class. We stored the x and y coordinates separated via semicolons as strings in SQLite. First, we\u2019ll define a converter function that accepts the string as a parameter and constructs a Point object from it.  Note Converter functions always get called with a bytes object, no matter under which data type you sent the value to SQLite.  def convert_point(s):\n    x, y = map(float, s.split(b\";\"))\n    return Point(x, y)\n Now you need to make the sqlite3 module know that what you select from the database is actually a point. There are two ways of doing this:  Implicitly via the declared type Explicitly via the column name  Both ways are described in section Module functions and constants, in the entries for the constants PARSE_DECLTYPES and PARSE_COLNAMES. The following example illustrates both approaches. import sqlite3\n\nclass Point:\n    def __init__(self, x, y):\n        self.x, self.y = x, y\n\n    def __repr__(self):\n        return \"(%f;%f)\" % (self.x, self.y)\n\ndef adapt_point(point):\n    return (\"%f;%f\" % (point.x, point.y)).encode('ascii')\n\ndef convert_point(s):\n    x, y = list(map(float, s.split(b\";\")))\n    return Point(x, y)\n\n# Register the adapter\nsqlite3.register_adapter(Point, adapt_point)\n\n# Register the converter\nsqlite3.register_converter(\"point\", convert_point)\n\np = Point(4.0, -3.2)\n\n#########################\n# 1) Using declared types\ncon = sqlite3.connect(\":memory:\", detect_types=sqlite3.PARSE_DECLTYPES)\ncur = con.cursor()\ncur.execute(\"create table test(p point)\")\n\ncur.execute(\"insert into test(p) values (?)\", (p,))\ncur.execute(\"select p from test\")\nprint(\"with declared types:\", cur.fetchone()[0])\ncur.close()\ncon.close()\n\n#######################\n# 1) Using column names\ncon = sqlite3.connect(\":memory:\", detect_types=sqlite3.PARSE_COLNAMES)\ncur = con.cursor()\ncur.execute(\"create table test(p)\")\n\ncur.execute(\"insert into test(p) values (?)\", (p,))\ncur.execute('select p as \"p [point]\" from test')\nprint(\"with column names:\", cur.fetchone()[0])\ncur.close()\ncon.close()\n Default adapters and converters There are default adapters for the date and datetime types in the datetime module. They will be sent as ISO dates\/ISO timestamps to SQLite. The default converters are registered under the name \u201cdate\u201d for datetime.date and under the name \u201ctimestamp\u201d for datetime.datetime. This way, you can use date\/timestamps from Python without any additional fiddling in most cases. The format of the adapters is also compatible with the experimental SQLite date\/time functions. The following example demonstrates this. import sqlite3\nimport datetime\n\ncon = sqlite3.connect(\":memory:\", detect_types=sqlite3.PARSE_DECLTYPES|sqlite3.PARSE_COLNAMES)\ncur = con.cursor()\ncur.execute(\"create table test(d date, ts timestamp)\")\n\ntoday = datetime.date.today()\nnow = datetime.datetime.now()\n\ncur.execute(\"insert into test(d, ts) values (?, ?)\", (today, now))\ncur.execute(\"select d, ts from test\")\nrow = cur.fetchone()\nprint(today, \"=>\", row[0], type(row[0]))\nprint(now, \"=>\", row[1], type(row[1]))\n\ncur.execute('select current_date as \"d [date]\", current_timestamp as \"ts [timestamp]\"')\nrow = cur.fetchone()\nprint(\"current_date\", row[0], type(row[0]))\nprint(\"current_timestamp\", row[1], type(row[1]))\n\ncon.close()\n If a timestamp stored in SQLite has a fractional part longer than 6 numbers, its value will be truncated to microsecond precision by the timestamp converter. Controlling Transactions The underlying sqlite3 library operates in autocommit mode by default, but the Python sqlite3 module by default does not. autocommit mode means that statements that modify the database take effect immediately. A BEGIN or SAVEPOINT statement disables autocommit mode, and a COMMIT, a ROLLBACK, or a RELEASE that ends the outermost transaction, turns autocommit mode back on. The Python sqlite3 module by default issues a BEGIN statement implicitly before a Data Modification Language (DML) statement (i.e. INSERT\/UPDATE\/DELETE\/REPLACE). You can control which kind of BEGIN statements sqlite3 implicitly executes via the isolation_level parameter to the connect() call, or via the isolation_level property of connections. If you specify no isolation_level, a plain BEGIN is used, which is equivalent to specifying DEFERRED. Other possible values are IMMEDIATE and EXCLUSIVE. You can disable the sqlite3 module\u2019s implicit transaction management by setting isolation_level to None. This will leave the underlying sqlite3 library operating in autocommit mode. You can then completely control the transaction state by explicitly issuing BEGIN, ROLLBACK, SAVEPOINT, and RELEASE statements in your code.  Changed in version 3.6: sqlite3 used to implicitly commit an open transaction before DDL statements. This is no longer the case.  Using sqlite3 efficiently Using shortcut methods Using the nonstandard execute(), executemany() and executescript() methods of the Connection object, your code can be written more concisely because you don\u2019t have to create the (often superfluous) Cursor objects explicitly. Instead, the Cursor objects are created implicitly and these shortcut methods return the cursor objects. This way, you can execute a SELECT statement and iterate over it directly using only a single call on the Connection object. import sqlite3\n\npersons = [\n    (\"Hugo\", \"Boss\"),\n    (\"Calvin\", \"Klein\")\n    ]\n\ncon = sqlite3.connect(\":memory:\")\n\n# Create the table\ncon.execute(\"create table person(firstname, lastname)\")\n\n# Fill the table\ncon.executemany(\"insert into person(firstname, lastname) values (?, ?)\", persons)\n\n# Print the table contents\nfor row in con.execute(\"select firstname, lastname from person\"):\n    print(row)\n\nprint(\"I just deleted\", con.execute(\"delete from person\").rowcount, \"rows\")\n\n# close is not a shortcut method and it's not called automatically,\n# so the connection object should be closed manually\ncon.close()\n Accessing columns by name instead of by index One useful feature of the sqlite3 module is the built-in sqlite3.Row class designed to be used as a row factory. Rows wrapped with this class can be accessed both by index (like tuples) and case-insensitively by name: import sqlite3\n\ncon = sqlite3.connect(\":memory:\")\ncon.row_factory = sqlite3.Row\n\ncur = con.cursor()\ncur.execute(\"select 'John' as name, 42 as age\")\nfor row in cur:\n    assert row[0] == row[\"name\"]\n    assert row[\"name\"] == row[\"nAmE\"]\n    assert row[1] == row[\"age\"]\n    assert row[1] == row[\"AgE\"]\n\ncon.close()\n Using the connection as a context manager Connection objects can be used as context managers that automatically commit or rollback transactions. In the event of an exception, the transaction is rolled back; otherwise, the transaction is committed: import sqlite3\n\ncon = sqlite3.connect(\":memory:\")\ncon.execute(\"create table person (id integer primary key, firstname varchar unique)\")\n\n# Successful, con.commit() is called automatically afterwards\nwith con:\n    con.execute(\"insert into person(firstname) values (?)\", (\"Joe\",))\n\n# con.rollback() is called after the with block finishes with an exception, the\n# exception is still raised and must be caught\ntry:\n    with con:\n        con.execute(\"insert into person(firstname) values (?)\", (\"Joe\",))\nexcept sqlite3.IntegrityError:\n    print(\"couldn't add Joe twice\")\n\n# Connection object used as context manager only commits or rollbacks transactions,\n# so the connection object should be closed manually\ncon.close()\n Footnotes  \n1(1,2)  \nThe sqlite3 module is not built with loadable extension support by default, because some platforms (notably Mac OS X) have SQLite libraries which are compiled without this feature. To get loadable extension support, you must pass --enable-loadable-sqlite-extensions to configure.","title":"python.library.sqlite3"},{"text":"msilib.add_data(database, table, records)  \nAdd all records to the table named table in database. The table argument must be one of the predefined tables in the MSI schema, e.g. 'Feature', 'File', 'Component', 'Dialog', 'Control', etc. records should be a list of tuples, each one containing all fields of a record according to the schema of the table. For optional fields, None can be passed. Field values can be ints, strings, or instances of the Binary class.","title":"python.library.msilib#msilib.add_data"},{"text":"test_cookie_worked()  \nReturns either True or False, depending on whether the user\u2019s browser accepted the test cookie. Due to the way cookies work, you\u2019ll have to call set_test_cookie() on a previous, separate page request. See Setting test cookies below for more information.","title":"django.topics.http.sessions#django.contrib.sessions.backends.base.SessionBase.test_cookie_worked"},{"text":"set_test_cookie()  \nSets a test cookie to determine whether the user\u2019s browser supports cookies. Due to the way cookies work, you won\u2019t be able to test this until the user\u2019s next page request. See Setting test cookies below for more information.","title":"django.topics.http.sessions#django.contrib.sessions.backends.base.SessionBase.set_test_cookie"},{"text":"class queue.Queue(maxsize=0)  \nConstructor for a FIFO queue. maxsize is an integer that sets the upperbound limit on the number of items that can be placed in the queue. Insertion will block once this size has been reached, until queue items are consumed. If maxsize is less than or equal to zero, the queue size is infinite.","title":"python.library.queue#queue.Queue"},{"text":"text_factory  \nUsing this attribute you can control what objects are returned for the TEXT data type. By default, this attribute is set to str and the sqlite3 module will return Unicode objects for TEXT. If you want to return bytestrings instead, you can set it to bytes. You can also set it to any other callable that accepts a single bytestring parameter and returns the resulting object. See the following example code for illustration: import sqlite3\n\ncon = sqlite3.connect(\":memory:\")\ncur = con.cursor()\n\nAUSTRIA = \"\\xd6sterreich\"\n\n# by default, rows are returned as Unicode\ncur.execute(\"select ?\", (AUSTRIA,))\nrow = cur.fetchone()\nassert row[0] == AUSTRIA\n\n# but we can make sqlite3 always return bytestrings ...\ncon.text_factory = bytes\ncur.execute(\"select ?\", (AUSTRIA,))\nrow = cur.fetchone()\nassert type(row[0]) is bytes\n# the bytestrings will be encoded in UTF-8, unless you stored garbage in the\n# database ...\nassert row[0] == AUSTRIA.encode(\"utf-8\")\n\n# we can also implement a custom text_factory ...\n# here we implement one that appends \"foo\" to all strings\ncon.text_factory = lambda x: x.decode(\"utf-8\") + \"foo\"\ncur.execute(\"select ?\", (\"bar\",))\nrow = cur.fetchone()\nassert row[0] == \"barfoo\"\n\ncon.close()","title":"python.library.sqlite3#sqlite3.Connection.text_factory"},{"text":"class queue.LifoQueue(maxsize=0)  \nConstructor for a LIFO queue. maxsize is an integer that sets the upperbound limit on the number of items that can be placed in the queue. Insertion will block once this size has been reached, until queue items are consumed. If maxsize is less than or equal to zero, the queue size is infinite.","title":"python.library.queue#queue.LifoQueue"},{"text":"Record.SetStream(field, value)  \nSet field to the contents of the file named value, through MsiRecordSetStream(). field must be an integer; value a string.","title":"python.library.msilib#msilib.Record.SetStream"},{"text":"rfile  \nAn io.BufferedIOBase input stream, ready to read from the start of the optional input data.","title":"python.library.http.server#http.server.BaseHTTPRequestHandler.rfile"}]}
{"task_id":24242433,"prompt":"def f_24242433():\n\treturn ","suffix":"","canonical_solution":"b'\\\\x89\\\\n'.decode('unicode_escape')","test_start":"\nimport sqlite3\n\ndef check(candidate):","test":["\n    assert candidate() == '\\x89\\n'\n"],"entry_point":"f_24242433","intent":"decode string \"\\\\x89\\\\n\" into a normal string","library":["sqlite3"],"docs":[]}
{"task_id":24242433,"prompt":"def f_24242433(raw_string):\n\treturn ","suffix":"","canonical_solution":"raw_string.decode('unicode_escape')","test_start":"\ndef check(candidate):","test":["\n    assert candidate(b\"Hello\") == \"Hello\"\n","\n    assert candidate(b\"hello world!\") == \"hello world!\"\n","\n    assert candidate(b\". ?? !!x\") == \". ?? !!x\"\n"],"entry_point":"f_24242433","intent":"convert a raw string `raw_string` into a normal string","library":[],"docs":[]}
{"task_id":24242433,"prompt":"def f_24242433(raw_byte_string):\n\treturn ","suffix":"","canonical_solution":"raw_byte_string.decode('unicode_escape')","test_start":"\ndef check(candidate):","test":["\n    assert candidate(b\"Hello\") == \"Hello\"\n","\n    assert candidate(b\"hello world!\") == \"hello world!\"\n","\n    assert candidate(b\". ?? !!x\") == \". ?? !!x\"\n"],"entry_point":"f_24242433","intent":"convert a raw string `raw_byte_string` into a normal string","library":[],"docs":[]}
{"task_id":22882922,"prompt":"def f_22882922(s):\n\treturn ","suffix":"","canonical_solution":"[m.group(0) for m in re.finditer('(\\\\d)\\\\1*', s)]","test_start":"\nimport re \n\ndef check(candidate):","test":["\n    assert candidate('111234') == ['111', '2', '3', '4']\n"],"entry_point":"f_22882922","intent":"split a string `s` with into all strings of repeated characters","library":["re"],"docs":[{"text":"str.splitlines([keepends])  \nReturn a list of the lines in the string, breaking at line boundaries. Line breaks are not included in the resulting list unless keepends is given and true. This method splits on the following line boundaries. In particular, the boundaries are a superset of universal newlines.   \nRepresentation Description   \n\\n Line Feed  \n\\r Carriage Return  \n\\r\\n Carriage Return + Line Feed  \n\\v or \\x0b Line Tabulation  \n\\f or \\x0c Form Feed  \n\\x1c File Separator  \n\\x1d Group Separator  \n\\x1e Record Separator  \n\\x85 Next Line (C1 Control Code)  \n\\u2028 Line Separator  \n\\u2029 Paragraph Separator    Changed in version 3.2: \\v and \\f added to list of line boundaries.  For example: >>> 'ab c\\n\\nde fg\\rkl\\r\\n'.splitlines()\n['ab c', '', 'de fg', 'kl']\n>>> 'ab c\\n\\nde fg\\rkl\\r\\n'.splitlines(keepends=True)\n['ab c\\n', '\\n', 'de fg\\r', 'kl\\r\\n']\n Unlike split() when a delimiter string sep is given, this method returns an empty list for the empty string, and a terminal line break does not result in an extra line: >>> \"\".splitlines()\n[]\n>>> \"One line\\n\".splitlines()\n['One line']\n For comparison, split('\\n') gives: >>> ''.split('\\n')\n['']\n>>> 'Two lines\\n'.split('\\n')\n['Two lines', '']","title":"python.library.stdtypes#str.splitlines"},{"text":"Pattern.split(string, maxsplit=0)  \nIdentical to the split() function, using the compiled pattern.","title":"python.library.re#re.Pattern.split"},{"text":"pandas.Series.str.repeat   Series.str.repeat(repeats)[source]\n \nDuplicate each string in the Series or Index.  Parameters \n \nrepeats:int or sequence of int\n\n\nSame value for all (int) or different value per (sequence).    Returns \n Series or Index of object\n\nSeries or Index of repeated string objects specified by input parameter repeats.     Examples \n>>> s = pd.Series(['a', 'b', 'c'])\n>>> s\n0    a\n1    b\n2    c\ndtype: object\n  Single int repeats string in Series \n>>> s.str.repeat(repeats=2)\n0    aa\n1    bb\n2    cc\ndtype: object\n  Sequence of int repeats corresponding string in Series \n>>> s.str.repeat(repeats=[1, 2, 3])\n0      a\n1     bb\n2    ccc\ndtype: object","title":"pandas.reference.api.pandas.series.str.repeat"},{"text":"bytes.splitlines(keepends=False)  \nbytearray.splitlines(keepends=False)  \nReturn a list of the lines in the binary sequence, breaking at ASCII line boundaries. This method uses the universal newlines approach to splitting lines. Line breaks are not included in the resulting list unless keepends is given and true. For example: >>> b'ab c\\n\\nde fg\\rkl\\r\\n'.splitlines()\n[b'ab c', b'', b'de fg', b'kl']\n>>> b'ab c\\n\\nde fg\\rkl\\r\\n'.splitlines(keepends=True)\n[b'ab c\\n', b'\\n', b'de fg\\r', b'kl\\r\\n']\n Unlike split() when a delimiter string sep is given, this method returns an empty list for the empty string, and a terminal line break does not result in an extra line: >>> b\"\".split(b'\\n'), b\"Two lines\\n\".split(b'\\n')\n([b''], [b'Two lines', b''])\n>>> b\"\".splitlines(), b\"One line\\n\".splitlines()\n([], [b'One line'])","title":"python.library.stdtypes#bytearray.splitlines"},{"text":"numpy.char.splitlines   char.splitlines(a, keepends=None)[source]\n \nFor each element in a, return a list of the lines in the element, breaking at line boundaries. Calls str.splitlines element-wise.  Parameters \n \naarray_like of str or unicode\n\n\nkeependsbool, optional\n\n\nLine breaks are not included in the resulting list unless keepends is given and true.    Returns \n \noutndarray\n\n\nArray of list objects      See also  str.splitlines","title":"numpy.reference.generated.numpy.char.splitlines"},{"text":"numpy.char.chararray.splitlines method   char.chararray.splitlines(keepends=None)[source]\n \nFor each element in self, return a list of the lines in the element, breaking at line boundaries.  See also  char.splitlines","title":"numpy.reference.generated.numpy.char.chararray.splitlines"},{"text":"bytes.splitlines(keepends=False)  \nbytearray.splitlines(keepends=False)  \nReturn a list of the lines in the binary sequence, breaking at ASCII line boundaries. This method uses the universal newlines approach to splitting lines. Line breaks are not included in the resulting list unless keepends is given and true. For example: >>> b'ab c\\n\\nde fg\\rkl\\r\\n'.splitlines()\n[b'ab c', b'', b'de fg', b'kl']\n>>> b'ab c\\n\\nde fg\\rkl\\r\\n'.splitlines(keepends=True)\n[b'ab c\\n', b'\\n', b'de fg\\r', b'kl\\r\\n']\n Unlike split() when a delimiter string sep is given, this method returns an empty list for the empty string, and a terminal line break does not result in an extra line: >>> b\"\".split(b'\\n'), b\"Two lines\\n\".split(b'\\n')\n([b''], [b'Two lines', b''])\n>>> b\"\".splitlines(), b\"One line\\n\".splitlines()\n([], [b'One line'])","title":"python.library.stdtypes#bytes.splitlines"},{"text":"shlex.split(s, comments=False, posix=True)  \nSplit the string s using shell-like syntax. If comments is False (the default), the parsing of comments in the given string will be disabled (setting the commenters attribute of the shlex instance to the empty string). This function operates in POSIX mode by default, but uses non-POSIX mode if the posix argument is false.  Note Since the split() function instantiates a shlex instance, passing None for s will read the string to split from standard input.   Deprecated since version 3.9: Passing None for s will raise an exception in future Python versions.","title":"python.library.shlex#shlex.split"},{"text":"numpy.chararray.splitlines method   chararray.splitlines(keepends=None)[source]\n \nFor each element in self, return a list of the lines in the element, breaking at line boundaries.  See also  char.splitlines","title":"numpy.reference.generated.numpy.chararray.splitlines"},{"text":"headers  \nThe headers received with the request.","title":"werkzeug.wrappers.index#werkzeug.wrappers.Request.headers"}]}
{"task_id":4143502,"prompt":"def f_4143502():\n\treturn ","suffix":"","canonical_solution":"plt.scatter(np.random.randn(100), np.random.randn(100), facecolors='none')","test_start":"\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef check(candidate):","test":["\n    assert 'matplotlib' in str(type(candidate()))\n"],"entry_point":"f_4143502","intent":"scatter a plot with x, y position of `np.random.randn(100)` and face color equal to none","library":["matplotlib","numpy"],"docs":[{"text":"matplotlib.pyplot.plot   matplotlib.pyplot.plot(*args, scalex=True, scaley=True, data=None, **kwargs)[source]\n \nPlot y versus x as lines and\/or markers. Call signatures: plot([x], y, [fmt], *, data=None, **kwargs)\nplot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n The coordinates of the points or line nodes are given by x, y. The optional parameter fmt is a convenient way for defining basic formatting like color, marker and linestyle. It's a shortcut string notation described in the Notes section below. >>> plot(x, y)        # plot x and y using default line style and color\n>>> plot(x, y, 'bo')  # plot x and y using blue circle markers\n>>> plot(y)           # plot y using x as index array 0..N-1\n>>> plot(y, 'r+')     # ditto, but with red plusses\n You can use Line2D properties as keyword arguments for more control on the appearance. Line properties and fmt can be mixed. The following two calls yield identical results: >>> plot(x, y, 'go--', linewidth=2, markersize=12)\n>>> plot(x, y, color='green', marker='o', linestyle='dashed',\n...      linewidth=2, markersize=12)\n When conflicting with fmt, keyword arguments take precedence. Plotting labelled data There's a convenient way for plotting objects with labelled data (i.e. data that can be accessed by index obj['y']). Instead of giving the data in x and y, you can provide the object in the data parameter and just give the labels for x and y: >>> plot('xlabel', 'ylabel', data=obj)\n All indexable objects are supported. This could e.g. be a dict, a pandas.DataFrame or a structured numpy array. Plotting multiple sets of data There are various ways to plot multiple sets of data.  \nThe most straight forward way is just to call plot multiple times. Example: >>> plot(x1, y1, 'bo')\n>>> plot(x2, y2, 'go')\n  \nIf x and\/or y are 2D arrays a separate data set will be drawn for every column. If both x and y are 2D, they must have the same shape. If only one of them is 2D with shape (N, m) the other must have length N and will be used for every data set m. Example: >>> x = [1, 2, 3]\n>>> y = np.array([[1, 2], [3, 4], [5, 6]])\n>>> plot(x, y)\n is equivalent to: >>> for col in range(y.shape[1]):\n...     plot(x, y[:, col])\n  \nThe third way is to specify multiple sets of [x], y, [fmt] groups: >>> plot(x1, y1, 'g^', x2, y2, 'g-')\n In this case, any additional keyword argument applies to all datasets. Also this syntax cannot be combined with the data parameter.   By default, each line is assigned a different style specified by a 'style cycle'. The fmt and line property parameters are only necessary if you want explicit deviations from these defaults. Alternatively, you can also change the style cycle using rcParams[\"axes.prop_cycle\"] (default: cycler('color', ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])).  Parameters \n \nx, yarray-like or scalar\n\n\nThe horizontal \/ vertical coordinates of the data points. x values are optional and default to range(len(y)). Commonly, these parameters are 1D arrays. They can also be scalars, or two-dimensional (in that case, the columns represent separate data sets). These arguments cannot be passed as keywords.  \nfmtstr, optional\n\n\nA format string, e.g. 'ro' for red circles. See the Notes section for a full description of the format strings. Format strings are just an abbreviation for quickly setting basic line properties. All of these and more can also be controlled by keyword arguments. This argument cannot be passed as keyword.  \ndataindexable object, optional\n\n\nAn object with labelled data. If given, provide the label names to plot in x and y.  Note Technically there's a slight ambiguity in calls where the second label is a valid fmt. plot('n', 'o', data=obj) could be plt(x, y) or plt(y, fmt). In such cases, the former interpretation is chosen, but a warning is issued. You may suppress the warning by adding an empty format string plot('n', 'o', '', data=obj).     Returns \n list of Line2D\n\n\nA list of lines representing the plotted data.    Other Parameters \n \nscalex, scaleybool, default: True\n\n\nThese parameters determine if the view limits are adapted to the data limits. The values are passed on to autoscale_view.  \n**kwargsLine2D properties, optional\n\n\nkwargs are used to specify properties like a line label (for auto legends), linewidth, antialiasing, marker face color. Example: >>> plot([1, 2, 3], [1, 2, 3], 'go-', label='line 1', linewidth=2)\n>>> plot([1, 2, 3], [1, 4, 9], 'rs', label='line 2')\n If you specify multiple lines with one plot call, the kwargs apply to all those lines. In case the label object is iterable, each element is used as labels for each set of data. Here is a list of available Line2D properties:   \nProperty Description   \nagg_filter a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array  \nalpha scalar or None  \nanimated bool  \nantialiased or aa bool  \nclip_box Bbox  \nclip_on bool  \nclip_path Patch or (Path, Transform) or None  \ncolor or c color  \ndash_capstyle CapStyle or {'butt', 'projecting', 'round'}  \ndash_joinstyle JoinStyle or {'miter', 'round', 'bevel'}  \ndashes sequence of floats (on\/off ink in points) or (None, None)  \ndata (2, N) array or two 1D arrays  \ndrawstyle or ds {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'  \nfigure Figure  \nfillstyle {'full', 'left', 'right', 'bottom', 'top', 'none'}  \ngid str  \nin_layout bool  \nlabel object  \nlinestyle or ls {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}  \nlinewidth or lw float  \nmarker marker style string, Path or MarkerStyle  \nmarkeredgecolor or mec color  \nmarkeredgewidth or mew float  \nmarkerfacecolor or mfc color  \nmarkerfacecoloralt or mfcalt color  \nmarkersize or ms float  \nmarkevery None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]  \npath_effects AbstractPathEffect  \npicker float or callable[[Artist, Event], tuple[bool, dict]]  \npickradius float  \nrasterized bool  \nsketch_params (scale: float, length: float, randomness: float)  \nsnap bool or None  \nsolid_capstyle CapStyle or {'butt', 'projecting', 'round'}  \nsolid_joinstyle JoinStyle or {'miter', 'round', 'bevel'}  \ntransform unknown  \nurl str  \nvisible bool  \nxdata 1D array  \nydata 1D array  \nzorder float        See also  scatter\n\nXY scatter plot with markers of varying size and\/or color ( sometimes also called bubble chart).    Notes Format Strings A format string consists of a part for color, marker and line: fmt = '[marker][line][color]'\n Each of them is optional. If not provided, the value from the style cycle is used. Exception: If line is given, but no marker, the data will be a line without markers. Other combinations such as [color][marker][line] are also supported, but note that their parsing may be ambiguous. Markers   \ncharacter description   \n'.' point marker  \n',' pixel marker  \n'o' circle marker  \n'v' triangle_down marker  \n'^' triangle_up marker  \n'<' triangle_left marker  \n'>' triangle_right marker  \n'1' tri_down marker  \n'2' tri_up marker  \n'3' tri_left marker  \n'4' tri_right marker  \n'8' octagon marker  \n's' square marker  \n'p' pentagon marker  \n'P' plus (filled) marker  \n'*' star marker  \n'h' hexagon1 marker  \n'H' hexagon2 marker  \n'+' plus marker  \n'x' x marker  \n'X' x (filled) marker  \n'D' diamond marker  \n'd' thin_diamond marker  \n'|' vline marker  \n'_' hline marker   Line Styles   \ncharacter description   \n'-' solid line style  \n'--' dashed line style  \n'-.' dash-dot line style  \n':' dotted line style   Example format strings: 'b'    # blue markers with default shape\n'or'   # red circles\n'-g'   # green solid line\n'--'   # dashed line with default color\n'^k:'  # black triangle_up markers connected by a dotted line\n Colors The supported color abbreviations are the single letter codes   \ncharacter color   \n'b' blue  \n'g' green  \n'r' red  \n'c' cyan  \n'm' magenta  \n'y' yellow  \n'k' black  \n'w' white   and the 'CN' colors that index into the default property cycle. If the color is the only part of the format string, you can additionally use any matplotlib.colors spec, e.g. full names ('green') or hex strings ('#008000'). \n  Examples using matplotlib.pyplot.plot\n \n   Plotting masked and NaN values   \n\n   Scatter Masked   \n\n   Stairs Demo   \n\n   Step Demo   \n\n   Custom Figure subclasses   \n\n   Managing multiple figures in pyplot   \n\n   Shared Axis   \n\n   Multiple subplots   \n\n   Controlling style of text and labels using a dictionary   \n\n   Title positioning   \n\n   Infinite lines   \n\n   plot() format string   \n\n   Pyplot Mathtext   \n\n   Pyplot Simple   \n\n   Pyplot Three   \n\n   Pyplot Two Subplots   \n\n   Dolphins   \n\n   Solarized Light stylesheet   \n\n   Frame grabbing   \n\n   Coords Report   \n\n   Customize Rc   \n\n   Findobj Demo   \n\n   Multipage PDF   \n\n   Print Stdout   \n\n   Set and get properties   \n\n   transforms.offset_copy   \n\n   Zorder Demo   \n\n   Custom scale   \n\n   Placing date ticks using recurrence rules   \n\n   Rotating custom tick labels   \n\n   Tool Manager   \n\n   Buttons   \n\n   Slider   \n\n   Snapping Sliders to Discrete Values   \n\n   Basic Usage   \n\n   Pyplot tutorial   \n\n   Customizing Matplotlib with style sheets and rcParams   \n\n   Path effects guide","title":"matplotlib._as_gen.matplotlib.pyplot.plot"},{"text":"matplotlib.pyplot.scatter   matplotlib.pyplot.scatter(x, y, s=None, c=None, marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=None, linewidths=None, *, edgecolors=None, plotnonfinite=False, data=None, **kwargs)[source]\n \nA scatter plot of y vs. x with varying marker size and\/or color.  Parameters \n \nx, yfloat or array-like, shape (n, )\n\n\nThe data positions.  \nsfloat or array-like, shape (n, ), optional\n\n\nThe marker size in points**2. Default is rcParams['lines.markersize'] ** 2.  \ncarray-like or list of colors or color, optional\n\n\nThe marker colors. Possible values:  A scalar or sequence of n numbers to be mapped to colors using cmap and norm. A 2D array in which the rows are RGB or RGBA. A sequence of colors of length n. A single color format string.  Note that c should not be a single numeric RGB or RGBA sequence because that is indistinguishable from an array of values to be colormapped. If you want to specify the same RGB or RGBA value for all points, use a 2D array with a single row. Otherwise, value- matching will have precedence in case of a size matching with x and y. If you wish to specify a single color for all points prefer the color keyword argument. Defaults to None. In that case the marker color is determined by the value of color, facecolor or facecolors. In case those are not specified or None, the marker color is determined by the next color of the Axes' current \"shape and fill\" color cycle. This cycle defaults to rcParams[\"axes.prop_cycle\"] (default: cycler('color', ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])).  \nmarkerMarkerStyle, default: rcParams[\"scatter.marker\"] (default: 'o')\n\n\nThe marker style. marker can be either an instance of the class or the text shorthand for a particular marker. See matplotlib.markers for more information about marker styles.  \ncmapstr or Colormap, default: rcParams[\"image.cmap\"] (default: 'viridis')\n\n\nA Colormap instance or registered colormap name. cmap is only used if c is an array of floats.  \nnormNormalize, default: None\n\n\nIf c is an array of floats, norm is used to scale the color data, c, in the range 0 to 1, in order to map into the colormap cmap. If None, use the default colors.Normalize.  \nvmin, vmaxfloat, default: None\n\n\nvmin and vmax are used in conjunction with the default norm to map the color array c to the colormap cmap. If None, the respective min and max of the color array is used. It is an error to use vmin\/vmax when norm is given.  \nalphafloat, default: None\n\n\nThe alpha blending value, between 0 (transparent) and 1 (opaque).  \nlinewidthsfloat or array-like, default: rcParams[\"lines.linewidth\"] (default: 1.5)\n\n\nThe linewidth of the marker edges. Note: The default edgecolors is 'face'. You may want to change this as well.  \nedgecolors{'face', 'none', None} or color or sequence of color, default: rcParams[\"scatter.edgecolors\"] (default: 'face')\n\n\nThe edge color of the marker. Possible values:  'face': The edge color will always be the same as the face color. 'none': No patch boundary will be drawn. A color or sequence of colors.  For non-filled markers, edgecolors is ignored. Instead, the color is determined like with 'face', i.e. from c, colors, or facecolors.  \nplotnonfinitebool, default: False\n\n\nWhether to plot points with nonfinite c (i.e. inf, -inf or nan). If True the points are drawn with the bad colormap color (see Colormap.set_bad).    Returns \n PathCollection\n  Other Parameters \n \ndataindexable object, optional\n\n\nIf given, the following parameters also accept a string s, which is interpreted as data[s] (unless this raises an exception): x, y, s, linewidths, edgecolors, c, facecolor, facecolors, color  \n**kwargsCollection properties\n\n    See also  plot\n\nTo plot scatter plots when markers are identical in size and color.    Notes  The plot function will be faster for scatterplots where markers don't vary in size or color. Any or all of x, y, s, and c may be masked arrays, in which case all masks will be combined and only unmasked points will be plotted. Fundamentally, scatter works with 1D arrays; x, y, s, and c may be input as N-D arrays, but within scatter they will be flattened. The exception is c, which will be flattened only if its size matches the size of x and y.  \n  Examples using matplotlib.pyplot.scatter\n \n   Scatter Masked   \n\n   Scatter Symbol   \n\n   Scatter plot   \n\n   Hyperlinks   \n\n   Pyplot tutorial","title":"matplotlib._as_gen.matplotlib.pyplot.scatter"},{"text":"matplotlib.axes.Axes.plot   Axes.plot(*args, scalex=True, scaley=True, data=None, **kwargs)[source]\n \nPlot y versus x as lines and\/or markers. Call signatures: plot([x], y, [fmt], *, data=None, **kwargs)\nplot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n The coordinates of the points or line nodes are given by x, y. The optional parameter fmt is a convenient way for defining basic formatting like color, marker and linestyle. It's a shortcut string notation described in the Notes section below. >>> plot(x, y)        # plot x and y using default line style and color\n>>> plot(x, y, 'bo')  # plot x and y using blue circle markers\n>>> plot(y)           # plot y using x as index array 0..N-1\n>>> plot(y, 'r+')     # ditto, but with red plusses\n You can use Line2D properties as keyword arguments for more control on the appearance. Line properties and fmt can be mixed. The following two calls yield identical results: >>> plot(x, y, 'go--', linewidth=2, markersize=12)\n>>> plot(x, y, color='green', marker='o', linestyle='dashed',\n...      linewidth=2, markersize=12)\n When conflicting with fmt, keyword arguments take precedence. Plotting labelled data There's a convenient way for plotting objects with labelled data (i.e. data that can be accessed by index obj['y']). Instead of giving the data in x and y, you can provide the object in the data parameter and just give the labels for x and y: >>> plot('xlabel', 'ylabel', data=obj)\n All indexable objects are supported. This could e.g. be a dict, a pandas.DataFrame or a structured numpy array. Plotting multiple sets of data There are various ways to plot multiple sets of data.  \nThe most straight forward way is just to call plot multiple times. Example: >>> plot(x1, y1, 'bo')\n>>> plot(x2, y2, 'go')\n  \nIf x and\/or y are 2D arrays a separate data set will be drawn for every column. If both x and y are 2D, they must have the same shape. If only one of them is 2D with shape (N, m) the other must have length N and will be used for every data set m. Example: >>> x = [1, 2, 3]\n>>> y = np.array([[1, 2], [3, 4], [5, 6]])\n>>> plot(x, y)\n is equivalent to: >>> for col in range(y.shape[1]):\n...     plot(x, y[:, col])\n  \nThe third way is to specify multiple sets of [x], y, [fmt] groups: >>> plot(x1, y1, 'g^', x2, y2, 'g-')\n In this case, any additional keyword argument applies to all datasets. Also this syntax cannot be combined with the data parameter.   By default, each line is assigned a different style specified by a 'style cycle'. The fmt and line property parameters are only necessary if you want explicit deviations from these defaults. Alternatively, you can also change the style cycle using rcParams[\"axes.prop_cycle\"] (default: cycler('color', ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])).  Parameters \n \nx, yarray-like or scalar\n\n\nThe horizontal \/ vertical coordinates of the data points. x values are optional and default to range(len(y)). Commonly, these parameters are 1D arrays. They can also be scalars, or two-dimensional (in that case, the columns represent separate data sets). These arguments cannot be passed as keywords.  \nfmtstr, optional\n\n\nA format string, e.g. 'ro' for red circles. See the Notes section for a full description of the format strings. Format strings are just an abbreviation for quickly setting basic line properties. All of these and more can also be controlled by keyword arguments. This argument cannot be passed as keyword.  \ndataindexable object, optional\n\n\nAn object with labelled data. If given, provide the label names to plot in x and y.  Note Technically there's a slight ambiguity in calls where the second label is a valid fmt. plot('n', 'o', data=obj) could be plt(x, y) or plt(y, fmt). In such cases, the former interpretation is chosen, but a warning is issued. You may suppress the warning by adding an empty format string plot('n', 'o', '', data=obj).     Returns \n list of Line2D\n\n\nA list of lines representing the plotted data.    Other Parameters \n \nscalex, scaleybool, default: True\n\n\nThese parameters determine if the view limits are adapted to the data limits. The values are passed on to autoscale_view.  \n**kwargsLine2D properties, optional\n\n\nkwargs are used to specify properties like a line label (for auto legends), linewidth, antialiasing, marker face color. Example: >>> plot([1, 2, 3], [1, 2, 3], 'go-', label='line 1', linewidth=2)\n>>> plot([1, 2, 3], [1, 4, 9], 'rs', label='line 2')\n If you specify multiple lines with one plot call, the kwargs apply to all those lines. In case the label object is iterable, each element is used as labels for each set of data. Here is a list of available Line2D properties:   \nProperty Description   \nagg_filter a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array  \nalpha scalar or None  \nanimated bool  \nantialiased or aa bool  \nclip_box Bbox  \nclip_on bool  \nclip_path Patch or (Path, Transform) or None  \ncolor or c color  \ndash_capstyle CapStyle or {'butt', 'projecting', 'round'}  \ndash_joinstyle JoinStyle or {'miter', 'round', 'bevel'}  \ndashes sequence of floats (on\/off ink in points) or (None, None)  \ndata (2, N) array or two 1D arrays  \ndrawstyle or ds {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'  \nfigure Figure  \nfillstyle {'full', 'left', 'right', 'bottom', 'top', 'none'}  \ngid str  \nin_layout bool  \nlabel object  \nlinestyle or ls {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}  \nlinewidth or lw float  \nmarker marker style string, Path or MarkerStyle  \nmarkeredgecolor or mec color  \nmarkeredgewidth or mew float  \nmarkerfacecolor or mfc color  \nmarkerfacecoloralt or mfcalt color  \nmarkersize or ms float  \nmarkevery None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]  \npath_effects AbstractPathEffect  \npicker float or callable[[Artist, Event], tuple[bool, dict]]  \npickradius float  \nrasterized bool  \nsketch_params (scale: float, length: float, randomness: float)  \nsnap bool or None  \nsolid_capstyle CapStyle or {'butt', 'projecting', 'round'}  \nsolid_joinstyle JoinStyle or {'miter', 'round', 'bevel'}  \ntransform unknown  \nurl str  \nvisible bool  \nxdata 1D array  \nydata 1D array  \nzorder float        See also  scatter\n\nXY scatter plot with markers of varying size and\/or color ( sometimes also called bubble chart).    Notes Format Strings A format string consists of a part for color, marker and line: fmt = '[marker][line][color]'\n Each of them is optional. If not provided, the value from the style cycle is used. Exception: If line is given, but no marker, the data will be a line without markers. Other combinations such as [color][marker][line] are also supported, but note that their parsing may be ambiguous. Markers   \ncharacter description   \n'.' point marker  \n',' pixel marker  \n'o' circle marker  \n'v' triangle_down marker  \n'^' triangle_up marker  \n'<' triangle_left marker  \n'>' triangle_right marker  \n'1' tri_down marker  \n'2' tri_up marker  \n'3' tri_left marker  \n'4' tri_right marker  \n'8' octagon marker  \n's' square marker  \n'p' pentagon marker  \n'P' plus (filled) marker  \n'*' star marker  \n'h' hexagon1 marker  \n'H' hexagon2 marker  \n'+' plus marker  \n'x' x marker  \n'X' x (filled) marker  \n'D' diamond marker  \n'd' thin_diamond marker  \n'|' vline marker  \n'_' hline marker   Line Styles   \ncharacter description   \n'-' solid line style  \n'--' dashed line style  \n'-.' dash-dot line style  \n':' dotted line style   Example format strings: 'b'    # blue markers with default shape\n'or'   # red circles\n'-g'   # green solid line\n'--'   # dashed line with default color\n'^k:'  # black triangle_up markers connected by a dotted line\n Colors The supported color abbreviations are the single letter codes   \ncharacter color   \n'b' blue  \n'g' green  \n'r' red  \n'c' cyan  \n'm' magenta  \n'y' yellow  \n'k' black  \n'w' white   and the 'CN' colors that index into the default property cycle. If the color is the only part of the format string, you can additionally use any matplotlib.colors spec, e.g. full names ('green') or hex strings ('#008000'). \n  Examples using matplotlib.axes.Axes.plot\n \n   Plotting categorical variables   \n\n   CSD Demo   \n\n   Curve with error band   \n\n   EventCollection Demo   \n\n   Fill Between and Alpha   \n\n   Filling the area between lines   \n\n   Fill Betweenx Demo   \n\n   Customizing dashed line styles   \n\n   Lines with a ticked patheffect   \n\n   Marker reference   \n\n   Markevery Demo   \n\n   prop_cycle property markevery in rcParams   \n\n   Psd Demo   \n\n   Simple Plot   \n\n   Using span_where   \n\n   Creating a timeline with lines, dates, and text   \n\n   hlines and vlines   \n\n   Contour Corner Mask   \n\n   Contour plot of irregularly spaced data   \n\n   pcolormesh grids and shading   \n\n   Streamplot   \n\n   Spectrogram Demo   \n\n   Watermark image   \n\n   Aligning Labels   \n\n   Axes box aspect   \n\n   Axes Demo   \n\n   Controlling view limits using margins and sticky_edges   \n\n   Axes Props   \n\n   axhspan Demo   \n\n   Broken Axis   \n\n   Resizing axes with constrained layout   \n\n   Resizing axes with tight layout   \n\n   Figure labels: suptitle, supxlabel, supylabel   \n\n   Invert Axes   \n\n   Secondary Axis   \n\n   Sharing axis limits and views   \n\n   Figure subfigures   \n\n   Multiple subplots   \n\n   Creating multiple subplots using plt.subplots   \n\n   Plots with different scales   \n\n   Boxplots   \n\n   Using histograms to plot a cumulative distribution   \n\n   Some features of the histogram (hist) function   \n\n   Polar plot   \n\n   Polar Legend   \n\n   Using accented text in matplotlib   \n\n   Scale invariant angle label   \n\n   Annotating Plots   \n\n   Composing Custom Legends   \n\n   Date tick labels   \n\n   Custom tick formatter for time series   \n\n   AnnotationBbox demo   \n\n   Labeling ticks using engineering notation   \n\n   Annotation arrow style reference   \n\n   Legend using pre-defined labels   \n\n   Legend Demo   \n\n   Mathtext   \n\n   Math fontfamily   \n\n   Multiline   \n\n   Rendering math equations using TeX   \n\n   Text Rotation Relative To Line   \n\n   Title positioning   \n\n   Text watermark   \n\n   Annotate Transform   \n\n   Annotating a plot   \n\n   Annotation Polar   \n\n   Programmatically controlling subplot adjustment   \n\n   Dollar Ticks   \n\n   Simple axes labels   \n\n   Text Commands   \n\n   Color Demo   \n\n   Color by y-value   \n\n   PathPatch object   \n\n   Bezier Curve   \n\n   Dark background style sheet   \n\n   FiveThirtyEight style sheet   \n\n   ggplot style sheet   \n\n   Axes with a fixed physical size   \n\n   Parasite Simple   \n\n   Simple Axisline4   \n\n   Axis line styles   \n\n   Parasite Axes demo   \n\n   Parasite axis demo   \n\n   Custom spines with axisartist   \n\n   Simple Axisline   \n\n   Anatomy of a figure   \n\n   Bachelor's degrees by gender   \n\n   Integral as the area under a curve   \n\n   XKCD   \n\n   Decay   \n\n   The Bayes update   \n\n   The double pendulum problem   \n\n   Animated 3D random walk   \n\n   Animated line plot   \n\n   MATPLOTLIB UNCHAINED   \n\n   Mouse move and click events   \n\n   Data Browser   \n\n   Keypress event   \n\n   Legend Picking   \n\n   Looking Glass   \n\n   Path Editor   \n\n   Pick Event Demo2   \n\n   Resampling Data   \n\n   Timers   \n\n   Frontpage histogram example   \n\n   Frontpage plot example   \n\n   Changing colors of lines intersecting a box   \n\n   Cross hair cursor   \n\n   Custom projection   \n\n   Patheffect Demo   \n\n   Pythonic Matplotlib   \n\n   SVG Filter Line   \n\n   TickedStroke patheffect   \n\n   Zorder Demo   \n\n   Plot 2D data on 3D plot   \n\n   3D box surface plot   \n\n   Parametric Curve   \n\n   Lorenz Attractor   \n\n   2D and 3D Axes in same Figure   \n\n   Loglog Aspect   \n\n   Scales   \n\n   Symlog Demo   \n\n   Anscombe's quartet   \n\n   Radar chart (aka spider or star chart)   \n\n   Centered spines with arrows   \n\n   Multiple Yaxis With Spines   \n\n   Spine Placement   \n\n   Spines   \n\n   Custom spine bounds   \n\n   Centering labels between ticks   \n\n   Formatting date ticks using ConciseDateFormatter   \n\n   Date Demo Convert   \n\n   Date Index Formatter   \n\n   Date Precision and Epochs   \n\n   Major and minor ticks   \n\n   The default tick formatter   \n\n   Set default y-axis tick labels on the right   \n\n   Setting tick labels from a list of values   \n\n   Set default x-axis tick labels on the top   \n\n   Evans test   \n\n   CanvasAgg demo   \n\n   Annotate Explain   \n\n   Connect Simple01   \n\n   Connection styles for annotations   \n\n   Nested GridSpecs   \n\n   Pgf Fonts   \n\n   Pgf Texsystem   \n\n   Simple Annotate01   \n\n   Simple Legend01   \n\n   Simple Legend02   \n\n   Annotated Cursor   \n\n   Check Buttons   \n\n   Cursor   \n\n   Multicursor   \n\n   Radio Buttons   \n\n   Rectangle and ellipse selectors   \n\n   Span Selector   \n\n   Textbox   \n\n   Basic Usage   \n\n   Artist tutorial   \n\n   Legend guide   \n\n   Styling with cycler   \n\n   Constrained Layout Guide   \n\n   Tight Layout guide   \n\n   Arranging multiple Axes in a Figure   \n\n   Autoscaling   \n\n   Faster rendering by using blitting   \n\n   Path Tutorial   \n\n   Transformations Tutorial   \n\n   Specifying Colors   \n\n   Text in Matplotlib Plots   \n\n   plot(x, y)   \n\n   fill_between(x, y1, y2)   \n\n   tricontour(x, y, z)   \n\n   tricontourf(x, y, z)   \n\n   tripcolor(x, y, z)","title":"matplotlib._as_gen.matplotlib.axes.axes.plot"},{"text":"matplotlib.axes.Axes.scatter   Axes.scatter(x, y, s=None, c=None, marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=None, linewidths=None, *, edgecolors=None, plotnonfinite=False, data=None, **kwargs)[source]\n \nA scatter plot of y vs. x with varying marker size and\/or color.  Parameters \n \nx, yfloat or array-like, shape (n, )\n\n\nThe data positions.  \nsfloat or array-like, shape (n, ), optional\n\n\nThe marker size in points**2. Default is rcParams['lines.markersize'] ** 2.  \ncarray-like or list of colors or color, optional\n\n\nThe marker colors. Possible values:  A scalar or sequence of n numbers to be mapped to colors using cmap and norm. A 2D array in which the rows are RGB or RGBA. A sequence of colors of length n. A single color format string.  Note that c should not be a single numeric RGB or RGBA sequence because that is indistinguishable from an array of values to be colormapped. If you want to specify the same RGB or RGBA value for all points, use a 2D array with a single row. Otherwise, value- matching will have precedence in case of a size matching with x and y. If you wish to specify a single color for all points prefer the color keyword argument. Defaults to None. In that case the marker color is determined by the value of color, facecolor or facecolors. In case those are not specified or None, the marker color is determined by the next color of the Axes' current \"shape and fill\" color cycle. This cycle defaults to rcParams[\"axes.prop_cycle\"] (default: cycler('color', ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])).  \nmarkerMarkerStyle, default: rcParams[\"scatter.marker\"] (default: 'o')\n\n\nThe marker style. marker can be either an instance of the class or the text shorthand for a particular marker. See matplotlib.markers for more information about marker styles.  \ncmapstr or Colormap, default: rcParams[\"image.cmap\"] (default: 'viridis')\n\n\nA Colormap instance or registered colormap name. cmap is only used if c is an array of floats.  \nnormNormalize, default: None\n\n\nIf c is an array of floats, norm is used to scale the color data, c, in the range 0 to 1, in order to map into the colormap cmap. If None, use the default colors.Normalize.  \nvmin, vmaxfloat, default: None\n\n\nvmin and vmax are used in conjunction with the default norm to map the color array c to the colormap cmap. If None, the respective min and max of the color array is used. It is an error to use vmin\/vmax when norm is given.  \nalphafloat, default: None\n\n\nThe alpha blending value, between 0 (transparent) and 1 (opaque).  \nlinewidthsfloat or array-like, default: rcParams[\"lines.linewidth\"] (default: 1.5)\n\n\nThe linewidth of the marker edges. Note: The default edgecolors is 'face'. You may want to change this as well.  \nedgecolors{'face', 'none', None} or color or sequence of color, default: rcParams[\"scatter.edgecolors\"] (default: 'face')\n\n\nThe edge color of the marker. Possible values:  'face': The edge color will always be the same as the face color. 'none': No patch boundary will be drawn. A color or sequence of colors.  For non-filled markers, edgecolors is ignored. Instead, the color is determined like with 'face', i.e. from c, colors, or facecolors.  \nplotnonfinitebool, default: False\n\n\nWhether to plot points with nonfinite c (i.e. inf, -inf or nan). If True the points are drawn with the bad colormap color (see Colormap.set_bad).    Returns \n PathCollection\n  Other Parameters \n \ndataindexable object, optional\n\n\nIf given, the following parameters also accept a string s, which is interpreted as data[s] (unless this raises an exception): x, y, s, linewidths, edgecolors, c, facecolor, facecolors, color  \n**kwargsCollection properties\n\n    See also  plot\n\nTo plot scatter plots when markers are identical in size and color.    Notes  The plot function will be faster for scatterplots where markers don't vary in size or color. Any or all of x, y, s, and c may be masked arrays, in which case all masks will be combined and only unmasked points will be plotted. Fundamentally, scatter works with 1D arrays; x, y, s, and c may be input as N-D arrays, but within scatter they will be flattened. The exception is c, which will be flattened only if its size matches the size of x and y.  \n  Examples using matplotlib.axes.Axes.scatter\n \n   Scatter Custom Symbol   \n\n   Scatter Demo2   \n\n   Scatter plot with histograms   \n\n   Scatter plot with pie chart markers   \n\n   Scatter plots with a legend   \n\n   Advanced quiver and quiverkey functions   \n\n   Axes box aspect   \n\n   Axis Label Position   \n\n   Plot a confidence ellipse of a two-dimensional dataset   \n\n   Violin plot customization   \n\n   Scatter plot on polar axis   \n\n   Legend Demo   \n\n   Scatter Histogram (Locatable Axes)   \n\n   mpl_toolkits.axisartist.floating_axes features   \n\n   Rain simulation   \n\n   Zoom Window   \n\n   Plotting with keywords   \n\n   Zorder Demo   \n\n   Plot 2D data on 3D plot   \n\n   3D scatterplot   \n\n   Automatically setting tick positions   \n\n   Unit handling   \n\n   Annotate Text Arrow   \n\n   Polygon Selector   \n\n   Basic Usage   \n\n   Choosing Colormaps in Matplotlib   \n\n   scatter(x, y)","title":"matplotlib._as_gen.matplotlib.axes.axes.scatter"},{"text":"pandas.DataFrame.plot.scatter   DataFrame.plot.scatter(x, y, s=None, c=None, **kwargs)[source]\n \nCreate a scatter plot with varying marker point size and color. The coordinates of each point are defined by two dataframe columns and filled circles are used to represent each point. This kind of plot is useful to see complex correlations between two variables. Points could be for instance natural 2D coordinates like longitude and latitude in a map or, in general, any pair of metrics that can be plotted against each other.  Parameters \n \nx:int or str\n\n\nThe column name or column position to be used as horizontal coordinates for each point.  \ny:int or str\n\n\nThe column name or column position to be used as vertical coordinates for each point.  \ns:str, scalar or array-like, optional\n\n\nThe size of each point. Possible values are:  A string with the name of the column to be used for marker\u2019s size. A single scalar so all points have the same size. \nA sequence of scalars, which will be used for each point\u2019s size recursively. For instance, when passing [2,14] all points size will be either 2 or 14, alternatively.  Changed in version 1.1.0.     \nc:str, int or array-like, optional\n\n\nThe color of each point. Possible values are:  A single color string referred to by name, RGB or RGBA code, for instance \u2018red\u2019 or \u2018#a98d19\u2019. A sequence of color strings referred to by name, RGB or RGBA code, which will be used for each point\u2019s color recursively. For instance [\u2018green\u2019,\u2019yellow\u2019] all points will be filled in green or yellow, alternatively. A column name or position whose values will be used to color the marker points according to a colormap.   **kwargs\n\nKeyword arguments to pass on to DataFrame.plot().    Returns \n \nmatplotlib.axes.Axes or numpy.ndarray of them\n    See also  matplotlib.pyplot.scatter\n\nScatter plot using multiple input data formats.    Examples Let\u2019s see how to draw a scatter plot using coordinates from the values in a DataFrame\u2019s columns. \n>>> df = pd.DataFrame([[5.1, 3.5, 0], [4.9, 3.0, 0], [7.0, 3.2, 1],\n...                    [6.4, 3.2, 1], [5.9, 3.0, 2]],\n...                   columns=['length', 'width', 'species'])\n>>> ax1 = df.plot.scatter(x='length',\n...                       y='width',\n...                       c='DarkBlue')\n     And now with the color determined by a column as well. \n>>> ax2 = df.plot.scatter(x='length',\n...                       y='width',\n...                       c='species',\n...                       colormap='viridis')","title":"pandas.reference.api.pandas.dataframe.plot.scatter"},{"text":"set_fc(c)[source]\n \nAlias for set_facecolor.","title":"matplotlib.collections_api#matplotlib.collections.StarPolygonCollection.set_fc"},{"text":"set_fc(c)[source]\n \nAlias for set_facecolor.","title":"matplotlib.collections_api#matplotlib.collections.AsteriskPolygonCollection.set_fc"},{"text":"scatter(xs, ys, zs=0, zdir='z', s=20, c=None, depthshade=True, *args, data=None, **kwargs)[source]\n \nCreate a scatter plot.  Parameters \n \nxs, ysarray-like\n\n\nThe data positions.  \nzsfloat or array-like, default: 0\n\n\nThe z-positions. Either an array of the same length as xs and ys or a single value to place all points in the same plane.  \nzdir{'x', 'y', 'z', '-x', '-y', '-z'}, default: 'z'\n\n\nThe axis direction for the zs. This is useful when plotting 2D data on a 3D Axes. The data must be passed as xs, ys. Setting zdir to 'y' then plots the data to the x-z-plane. See also Plot 2D data on 3D plot.  \nsfloat or array-like, default: 20\n\n\nThe marker size in points**2. Either an array of the same length as xs and ys or a single value to make all markers the same size.  \nccolor, sequence, or sequence of colors, optional\n\n\nThe marker color. Possible values:  A single color format string. A sequence of colors of length n. A sequence of n numbers to be mapped to colors using cmap and norm. A 2D array in which the rows are RGB or RGBA.  For more details see the c argument of scatter.  \ndepthshadebool, default: True\n\n\nWhether to shade the scatter markers to give the appearance of depth. Each call to scatter() will perform its depthshading independently.  \ndataindexable object, optional\n\n\nIf given, the following parameters also accept a string s, which is interpreted as data[s] (unless this raises an exception): xs, ys, zs, s, edgecolors, c, facecolor, facecolors, color  **kwargs\n\nAll other arguments are passed on to scatter.    Returns \n \npathsPathCollection","title":"matplotlib._as_gen.mpl_toolkits.mplot3d.axes3d.axes3d#mpl_toolkits.mplot3d.axes3d.Axes3D.scatter"},{"text":"set_fc(c)[source]\n \nAlias for set_facecolor.","title":"matplotlib.collections_api#matplotlib.collections.PolyCollection.set_fc"},{"text":"set_fc(c)[source]\n \nAlias for set_facecolor.","title":"matplotlib.collections_api#matplotlib.collections.CircleCollection.set_fc"}]}
{"task_id":4143502,"prompt":"def f_4143502():\n\treturn ","suffix":"","canonical_solution":"plt.plot(np.random.randn(100), np.random.randn(100), 'o', mfc='none')","test_start":"\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef check(candidate):","test":["\n    assert 'matplotlib' in str(type(candidate()[0]))\n"],"entry_point":"f_4143502","intent":"do a scatter plot with empty circles","library":["matplotlib","numpy"],"docs":[{"text":"get_datalim(transData)[source]","title":"matplotlib.collections_api#matplotlib.collections.CircleCollection.get_datalim"},{"text":"classmatplotlib.legend_handler.HandlerCircleCollection(yoffsets=None, sizes=None, **kwargs)[source]\n \nHandler for CircleCollections.  Parameters \n \nnumpointsint\n\n\nNumber of points to show in legend entry.  \nyoffsetsarray of floats\n\n\nLength numpoints list of y offsets for each point in legend entry.  **kwargs\n\nKeyword arguments forwarded to HandlerNpoints.       create_collection(orig_handle, sizes, offsets, transOffset)[source]","title":"matplotlib.legend_handler_api#matplotlib.legend_handler.HandlerCircleCollection"},{"text":"autoscale_None()[source]\n \nAutoscale the scalar limits on the norm instance using the current array, changing only limits that are None","title":"matplotlib.collections_api#matplotlib.collections.CircleCollection.autoscale_None"},{"text":"classmatplotlib.collections.CircleCollection(sizes, **kwargs)[source]\n \nBases: matplotlib.collections._CollectionWithSizes A collection of circles, drawn using splines.  Parameters \n \nsizesfloat or array-like\n\n\nThe area of each circle in points^2.  **kwargs\n\nForwarded to Collection.       add_callback(func)[source]\n \nAdd a callback function that will be called whenever one of the Artist's properties changes.  Parameters \n \nfunccallable\n\n\nThe callback function. It must have the signature: def func(artist: Artist) -> Any\n where artist is the calling Artist. Return values may exist but are ignored.    Returns \n int\n\nThe observer id associated with the callback. This id can be used for removing the callback with remove_callback later.      See also  remove_callback\n  \n   autoscale()[source]\n \nAutoscale the scalar limits on the norm instance using the current array \n   autoscale_None()[source]\n \nAutoscale the scalar limits on the norm instance using the current array, changing only limits that are None \n   propertyaxes\n \nThe Axes instance the artist resides in, or None. \n   propertycallbacksSM[source]\n\n   changed()[source]\n \nCall this whenever the mappable is changed to notify all the callbackSM listeners to the 'changed' signal. \n   colorbar\n \nThe last colorbar associated with this ScalarMappable. May be None. \n   contains(mouseevent)[source]\n \nTest whether the mouse event occurred in the collection. Returns bool, dict(ind=itemlist), where every item in itemlist contains the event. \n   convert_xunits(x)[source]\n \nConvert x using the unit type of the xaxis. If the artist is not in contained in an Axes or if the xaxis does not have units, x itself is returned. \n   convert_yunits(y)[source]\n \nConvert y using the unit type of the yaxis. If the artist is not in contained in an Axes or if the yaxis does not have units, y itself is returned. \n   draw(renderer)[source]\n \nDraw the Artist (and its children) using the given renderer. This has no effect if the artist is not visible (Artist.get_visible returns False).  Parameters \n \nrendererRendererBase subclass.\n\n   Notes This method is overridden in the Artist subclasses. \n   findobj(match=None, include_self=True)[source]\n \nFind artist objects. Recursively find all Artist instances contained in the artist.  Parameters \n match\n\nA filter criterion for the matches. This can be  \nNone: Return all objects contained in artist. A function with signature def match(artist: Artist) -> bool. The result will only contain artists for which the function returns True. A class instance: e.g., Line2D. The result will only contain artists of this class or its subclasses (isinstance check).   \ninclude_selfbool\n\n\nInclude self in the list to be checked for a match.    Returns \n list of Artist\n\n   \n   format_cursor_data(data)[source]\n \nReturn a string representation of data.  Note This method is intended to be overridden by artist subclasses. As an end-user of Matplotlib you will most likely not call this method yourself.  The default implementation converts ints and floats and arrays of ints and floats into a comma-separated string enclosed in square brackets, unless the artist has an associated colorbar, in which case scalar values are formatted using the colorbar's formatter.  See also  get_cursor_data\n  \n   get_agg_filter()[source]\n \nReturn filter function to be used for agg filter. \n   get_alpha()[source]\n \nReturn the alpha value used for blending - not supported on all backends. \n   get_animated()[source]\n \nReturn whether the artist is animated. \n   get_array()[source]\n \nReturn the array of values, that are mapped to colors. The base class ScalarMappable does not make any assumptions on the dimensionality and shape of the array. \n   get_capstyle()[source]\n\n   get_children()[source]\n \nReturn a list of the child Artists of this Artist. \n   get_clim()[source]\n \nReturn the values (min, max) that are mapped to the colormap limits. \n   get_clip_box()[source]\n \nReturn the clipbox. \n   get_clip_on()[source]\n \nReturn whether the artist uses clipping. \n   get_clip_path()[source]\n \nReturn the clip path. \n   get_cmap()[source]\n \nReturn the Colormap instance. \n   get_cursor_data(event)[source]\n \nReturn the cursor data for a given event.  Note This method is intended to be overridden by artist subclasses. As an end-user of Matplotlib you will most likely not call this method yourself.  Cursor data can be used by Artists to provide additional context information for a given event. The default implementation just returns None. Subclasses can override the method and return arbitrary data. However, when doing so, they must ensure that format_cursor_data can convert the data to a string representation. The only current use case is displaying the z-value of an AxesImage in the status bar of a plot window, while moving the mouse.  Parameters \n \neventmatplotlib.backend_bases.MouseEvent\n\n    See also  format_cursor_data\n  \n   get_dashes()[source]\n \nAlias for get_linestyle. \n   get_datalim(transData)[source]\n\n   get_ec()[source]\n \nAlias for get_edgecolor. \n   get_edgecolor()[source]\n\n   get_edgecolors()[source]\n \nAlias for get_edgecolor. \n   get_facecolor()[source]\n\n   get_facecolors()[source]\n \nAlias for get_facecolor. \n   get_fc()[source]\n \nAlias for get_facecolor. \n   get_figure()[source]\n \nReturn the Figure instance the artist belongs to. \n   get_fill()[source]\n \nReturn whether face is colored. \n   get_gid()[source]\n \nReturn the group id. \n   get_hatch()[source]\n \nReturn the current hatching pattern. \n   get_in_layout()[source]\n \nReturn boolean flag, True if artist is included in layout calculations. E.g. Constrained Layout Guide, Figure.tight_layout(), and fig.savefig(fname, bbox_inches='tight'). \n   get_joinstyle()[source]\n\n   get_label()[source]\n \nReturn the label used for this artist in the legend. \n   get_linestyle()[source]\n\n   get_linestyles()[source]\n \nAlias for get_linestyle. \n   get_linewidth()[source]\n\n   get_linewidths()[source]\n \nAlias for get_linewidth. \n   get_ls()[source]\n \nAlias for get_linestyle. \n   get_lw()[source]\n \nAlias for get_linewidth. \n   get_offset_transform()[source]\n \nReturn the Transform instance used by this artist offset. \n   get_offsets()[source]\n \nReturn the offsets for the collection. \n   get_path_effects()[source]\n\n   get_paths()[source]\n\n   get_picker()[source]\n \nReturn the picking behavior of the artist. The possible values are described in set_picker.  See also  \nset_picker, pickable, pick\n\n  \n   get_pickradius()[source]\n\n   get_rasterized()[source]\n \nReturn whether the artist is to be rasterized. \n   get_sizes()[source]\n \nReturn the sizes ('areas') of the elements in the collection.  Returns \n array\n\nThe 'area' of each element.     \n   get_sketch_params()[source]\n \nReturn the sketch parameters for the artist.  Returns \n tuple or None\n\nA 3-tuple with the following elements:  \nscale: The amplitude of the wiggle perpendicular to the source line. \nlength: The length of the wiggle along the line. \nrandomness: The scale factor by which the length is shrunken or expanded.  Returns None if no sketch parameters were set.     \n   get_snap()[source]\n \nReturn the snap setting. See set_snap for details. \n   get_tightbbox(renderer)[source]\n \nLike Artist.get_window_extent, but includes any clipping.  Parameters \n \nrendererRendererBase subclass\n\n\nrenderer that will be used to draw the figures (i.e. fig.canvas.get_renderer())    Returns \n Bbox\n\nThe enclosing bounding box (in figure pixel coordinates).     \n   get_transform()[source]\n \nReturn the Transform instance used by this artist. \n   get_transformed_clip_path_and_affine()[source]\n \nReturn the clip path with the non-affine part of its transformation applied, and the remaining affine part of its transformation. \n   get_transforms()[source]\n\n   get_url()[source]\n \nReturn the url. \n   get_urls()[source]\n \nReturn a list of URLs, one for each element of the collection. The list contains None for elements without a URL. See Hyperlinks for an example. \n   get_visible()[source]\n \nReturn the visibility. \n   get_window_extent(renderer)[source]\n \nGet the artist's bounding box in display space. The bounding box' width and height are nonnegative. Subclasses should override for inclusion in the bounding box \"tight\" calculation. Default is to return an empty bounding box at 0, 0. Be careful when using this function, the results will not update if the artist window extent of the artist changes. The extent can change due to any changes in the transform stack, such as changing the axes limits, the figure size, or the canvas used (as is done when saving a figure). This can lead to unexpected behavior where interactive figures will look fine on the screen, but will save incorrectly. \n   get_zorder()[source]\n \nReturn the artist's zorder. \n   have_units()[source]\n \nReturn whether units are set on any axis. \n   is_transform_set()[source]\n \nReturn whether the Artist has an explicitly set transform. This is True after set_transform has been called. \n   propertymouseover\n \nIf this property is set to True, the artist will be queried for custom context information when the mouse cursor moves over it. See also get_cursor_data(), ToolCursorPosition and NavigationToolbar2. \n   propertynorm\n\n   pchanged()[source]\n \nCall all of the registered callbacks. This function is triggered internally when a property is changed.  See also  add_callback\nremove_callback\n  \n   pick(mouseevent)[source]\n \nProcess a pick event. Each child artist will fire a pick event if mouseevent is over the artist and the artist has picker set.  See also  \nset_picker, get_picker, pickable\n\n  \n   pickable()[source]\n \nReturn whether the artist is pickable.  See also  \nset_picker, get_picker, pick\n\n  \n   properties()[source]\n \nReturn a dictionary of all the properties of the artist. \n   remove()[source]\n \nRemove the artist from the figure if possible. The effect will not be visible until the figure is redrawn, e.g., with FigureCanvasBase.draw_idle. Call relim to update the axes limits if desired. Note: relim will not see collections even if the collection was added to the axes with autolim = True. Note: there is no support for removing the artist's legend entry. \n   remove_callback(oid)[source]\n \nRemove a callback based on its observer id.  See also  add_callback\n  \n   set(*, agg_filter=<UNSET>, alpha=<UNSET>, animated=<UNSET>, antialiased=<UNSET>, array=<UNSET>, capstyle=<UNSET>, clim=<UNSET>, clip_box=<UNSET>, clip_on=<UNSET>, clip_path=<UNSET>, cmap=<UNSET>, color=<UNSET>, edgecolor=<UNSET>, facecolor=<UNSET>, gid=<UNSET>, hatch=<UNSET>, in_layout=<UNSET>, joinstyle=<UNSET>, label=<UNSET>, linestyle=<UNSET>, linewidth=<UNSET>, norm=<UNSET>, offset_transform=<UNSET>, offsets=<UNSET>, path_effects=<UNSET>, picker=<UNSET>, pickradius=<UNSET>, rasterized=<UNSET>, sizes=<UNSET>, sketch_params=<UNSET>, snap=<UNSET>, transform=<UNSET>, url=<UNSET>, urls=<UNSET>, visible=<UNSET>, zorder=<UNSET>)[source]\n \nSet multiple properties at once. Supported properties are   \nProperty Description   \nagg_filter a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array  \nalpha array-like or scalar or None  \nanimated bool  \nantialiased or aa or antialiaseds bool or list of bools  \narray array-like or None  \ncapstyle CapStyle or {'butt', 'projecting', 'round'}  \nclim (vmin: float, vmax: float)  \nclip_box Bbox  \nclip_on bool  \nclip_path Patch or (Path, Transform) or None  \ncmap Colormap or str or None  \ncolor color or list of rgba tuples  \nedgecolor or ec or edgecolors color or list of colors or 'face'  \nfacecolor or facecolors or fc color or list of colors  \nfigure Figure  \ngid str  \nhatch {'\/', '\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}  \nin_layout bool  \njoinstyle JoinStyle or {'miter', 'round', 'bevel'}  \nlabel object  \nlinestyle or dashes or linestyles or ls str or tuple or list thereof  \nlinewidth or linewidths or lw float or list of floats  \nnorm Normalize or None  \noffset_transform Transform  \noffsets (N, 2) or (2,) array-like  \npath_effects AbstractPathEffect  \npicker None or bool or float or callable  \npickradius float  \nrasterized bool  \nsizes ndarray or None  \nsketch_params (scale: float, length: float, randomness: float)  \nsnap bool or None  \ntransform Transform  \nurl str  \nurls list of str or None  \nvisible bool  \nzorder float   \n   set_aa(aa)[source]\n \nAlias for set_antialiased. \n   set_agg_filter(filter_func)[source]\n \nSet the agg filter.  Parameters \n \nfilter_funccallable\n\n\nA filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array.     \n   set_alpha(alpha)[source]\n \nSet the alpha value used for blending - not supported on all backends.  Parameters \n \nalphaarray-like or scalar or None\n\n\nAll values must be within the 0-1 range, inclusive. Masked values and nans are not supported.     \n   set_animated(b)[source]\n \nSet whether the artist is intended to be used in an animation. If True, the artist is excluded from regular drawing of the figure. You have to call Figure.draw_artist \/ Axes.draw_artist explicitly on the artist. This appoach is used to speed up animations using blitting. See also matplotlib.animation and Faster rendering by using blitting.  Parameters \n \nbbool\n\n   \n   set_antialiased(aa)[source]\n \nSet the antialiasing state for rendering.  Parameters \n \naabool or list of bools\n\n   \n   set_antialiaseds(aa)[source]\n \nAlias for set_antialiased. \n   set_array(A)[source]\n \nSet the value array from array-like A.  Parameters \n \nAarray-like or None\n\n\nThe values that are mapped to colors. The base class ScalarMappable does not make any assumptions on the dimensionality and shape of the value array A.     \n   set_capstyle(cs)[source]\n \nSet the CapStyle for the collection (for all its elements).  Parameters \n \ncsCapStyle or {'butt', 'projecting', 'round'}\n\n   \n   set_clim(vmin=None, vmax=None)[source]\n \nSet the norm limits for image scaling.  Parameters \n \nvmin, vmaxfloat\n\n\nThe limits. The limits may also be passed as a tuple (vmin, vmax) as a single positional argument.     \n   set_clip_box(clipbox)[source]\n \nSet the artist's clip Bbox.  Parameters \n \nclipboxBbox\n\n   \n   set_clip_on(b)[source]\n \nSet whether the artist uses clipping. When False artists will be visible outside of the axes which can lead to unexpected results.  Parameters \n \nbbool\n\n   \n   set_clip_path(path, transform=None)[source]\n \nSet the artist's clip path.  Parameters \n \npathPatch or Path or TransformedPath or None\n\n\nThe clip path. If given a Path, transform must be provided as well. If None, a previously set clip path is removed.  \ntransformTransform, optional\n\n\nOnly used if path is a Path, in which case the given Path is converted to a TransformedPath using transform.     Notes For efficiency, if path is a Rectangle this method will set the clipping box to the corresponding rectangle and set the clipping path to None. For technical reasons (support of set), a tuple (path, transform) is also accepted as a single positional parameter. \n   set_cmap(cmap)[source]\n \nSet the colormap for luminance data.  Parameters \n \ncmapColormap or str or None\n\n   \n   set_color(c)[source]\n \nSet both the edgecolor and the facecolor.  Parameters \n \nccolor or list of rgba tuples\n\n    See also  \nCollection.set_facecolor, Collection.set_edgecolor\n\n\nFor setting the edge or face color individually.    \n   set_dashes(ls)[source]\n \nAlias for set_linestyle. \n   set_ec(c)[source]\n \nAlias for set_edgecolor. \n   set_edgecolor(c)[source]\n \nSet the edgecolor(s) of the collection.  Parameters \n \nccolor or list of colors or 'face'\n\n\nThe collection edgecolor(s). If a sequence, the patches cycle through it. If 'face', match the facecolor.     \n   set_edgecolors(c)[source]\n \nAlias for set_edgecolor. \n   set_facecolor(c)[source]\n \nSet the facecolor(s) of the collection. c can be a color (all patches have same color), or a sequence of colors; if it is a sequence the patches will cycle through the sequence. If c is 'none', the patch will not be filled.  Parameters \n \nccolor or list of colors\n\n   \n   set_facecolors(c)[source]\n \nAlias for set_facecolor. \n   set_fc(c)[source]\n \nAlias for set_facecolor. \n   set_figure(fig)[source]\n \nSet the Figure instance the artist belongs to.  Parameters \n \nfigFigure\n\n   \n   set_gid(gid)[source]\n \nSet the (group) id for the artist.  Parameters \n \ngidstr\n\n   \n   set_hatch(hatch)[source]\n \nSet the hatching pattern hatch can be one of: \/   - diagonal hatching\n\\   - back diagonal\n|   - vertical\n-   - horizontal\n+   - crossed\nx   - crossed diagonal\no   - small circle\nO   - large circle\n.   - dots\n*   - stars\n Letters can be combined, in which case all the specified hatchings are done. If same letter repeats, it increases the density of hatching of that pattern. Hatching is supported in the PostScript, PDF, SVG and Agg backends only. Unlike other properties such as linewidth and colors, hatching can only be specified for the collection as a whole, not separately for each member.  Parameters \n \nhatch{'\/', '\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n\n   \n   set_in_layout(in_layout)[source]\n \nSet if artist is to be included in layout calculations, E.g. Constrained Layout Guide, Figure.tight_layout(), and fig.savefig(fname, bbox_inches='tight').  Parameters \n \nin_layoutbool\n\n   \n   set_joinstyle(js)[source]\n \nSet the JoinStyle for the collection (for all its elements).  Parameters \n \njsJoinStyle or {'miter', 'round', 'bevel'}\n\n   \n   set_label(s)[source]\n \nSet a label that will be displayed in the legend.  Parameters \n \nsobject\n\n\ns will be converted to a string by calling str.     \n   set_linestyle(ls)[source]\n \nSet the linestyle(s) for the collection.   \nlinestyle description   \n'-' or 'solid' solid line  \n'--' or 'dashed' dashed line  \n'-.' or 'dashdot' dash-dotted line  \n':' or 'dotted' dotted line   Alternatively a dash tuple of the following form can be provided: (offset, onoffseq),\n where onoffseq is an even length tuple of on and off ink in points.  Parameters \n \nlsstr or tuple or list thereof\n\n\nValid values for individual linestyles include {'-', '--', '-.', ':', '', (offset, on-off-seq)}. See Line2D.set_linestyle for a complete description.     \n   set_linestyles(ls)[source]\n \nAlias for set_linestyle. \n   set_linewidth(lw)[source]\n \nSet the linewidth(s) for the collection. lw can be a scalar or a sequence; if it is a sequence the patches will cycle through the sequence  Parameters \n \nlwfloat or list of floats\n\n   \n   set_linewidths(lw)[source]\n \nAlias for set_linewidth. \n   set_ls(ls)[source]\n \nAlias for set_linestyle. \n   set_lw(lw)[source]\n \nAlias for set_linewidth. \n   set_norm(norm)[source]\n \nSet the normalization instance.  Parameters \n \nnormNormalize or None\n\n   Notes If there are any colorbars using the mappable for this norm, setting the norm of the mappable will reset the norm, locator, and formatters on the colorbar to default. \n   set_offset_transform(transOffset)[source]\n \nSet the artist offset transform.  Parameters \n \ntransOffsetTransform\n\n   \n   set_offsets(offsets)[source]\n \nSet the offsets for the collection.  Parameters \n \noffsets(N, 2) or (2,) array-like\n\n   \n   set_path_effects(path_effects)[source]\n \nSet the path effects.  Parameters \n \npath_effectsAbstractPathEffect\n\n   \n   set_paths()[source]\n\n   set_picker(picker)[source]\n \nDefine the picking behavior of the artist.  Parameters \n \npickerNone or bool or float or callable\n\n\nThis can be one of the following:  \nNone: Picking is disabled for this artist (default). A boolean: If True then picking will be enabled and the artist will fire a pick event if the mouse event is over the artist. A float: If picker is a number it is interpreted as an epsilon tolerance in points and the artist will fire off an event if its data is within epsilon of the mouse event. For some artists like lines and patch collections, the artist may provide additional data to the pick event that is generated, e.g., the indices of the data within epsilon of the pick event \nA function: If picker is callable, it is a user supplied function which determines whether the artist is hit by the mouse event: hit, props = picker(artist, mouseevent)\n to determine the hit test. if the mouse event is over the artist, return hit=True and props is a dictionary of properties you want added to the PickEvent attributes.       \n   set_pickradius(pr)[source]\n \nSet the pick radius used for containment tests.  Parameters \n \nprfloat\n\n\nPick radius, in points.     \n   set_rasterized(rasterized)[source]\n \nForce rasterized (bitmap) drawing for vector graphics output. Rasterized drawing is not supported by all artists. If you try to enable this on an artist that does not support it, the command has no effect and a warning will be issued. This setting is ignored for pixel-based output. See also Rasterization for vector graphics.  Parameters \n \nrasterizedbool\n\n   \n   set_sizes(sizes, dpi=72.0)[source]\n \nSet the sizes of each member of the collection.  Parameters \n \nsizesndarray or None\n\n\nThe size to set for each element of the collection. The value is the 'area' of the element.  \ndpifloat, default: 72\n\n\nThe dpi of the canvas.     \n   set_sketch_params(scale=None, length=None, randomness=None)[source]\n \nSet the sketch parameters.  Parameters \n \nscalefloat, optional\n\n\nThe amplitude of the wiggle perpendicular to the source line, in pixels. If scale is None, or not provided, no sketch filter will be provided.  \nlengthfloat, optional\n\n\nThe length of the wiggle along the line, in pixels (default 128.0)  \nrandomnessfloat, optional\n\n\nThe scale factor by which the length is shrunken or expanded (default 16.0) The PGF backend uses this argument as an RNG seed and not as described above. Using the same seed yields the same random shape.     \n   set_snap(snap)[source]\n \nSet the snapping behavior. Snapping aligns positions with the pixel grid, which results in clearer images. For example, if a black line of 1px width was defined at a position in between two pixels, the resulting image would contain the interpolated value of that line in the pixel grid, which would be a grey value on both adjacent pixel positions. In contrast, snapping will move the line to the nearest integer pixel value, so that the resulting image will really contain a 1px wide black line. Snapping is currently only supported by the Agg and MacOSX backends.  Parameters \n \nsnapbool or None\n\n\nPossible values:  \nTrue: Snap vertices to the nearest pixel center. \nFalse: Do not modify vertex positions. \nNone: (auto) If the path contains only rectilinear line segments, round to the nearest pixel center.      \n   set_transform(t)[source]\n \nSet the artist transform.  Parameters \n \ntTransform\n\n   \n   set_url(url)[source]\n \nSet the url for the artist.  Parameters \n \nurlstr\n\n   \n   set_urls(urls)[source]\n \n Parameters \n \nurlslist of str or None\n\n   Notes URLs are currently only implemented by the SVG backend. They are ignored by all other backends. \n   set_visible(b)[source]\n \nSet the artist's visibility.  Parameters \n \nbbool\n\n   \n   set_zorder(level)[source]\n \nSet the zorder for the artist. Artists with lower zorder values are drawn first.  Parameters \n \nlevelfloat\n\n   \n   propertystale\n \nWhether the artist is 'stale' and needs to be re-drawn for the output to match the internal state of the artist. \n   propertysticky_edges\n \nx and y sticky edge lists for autoscaling. When performing autoscaling, if a data limit coincides with a value in the corresponding sticky_edges list, then no margin will be added--the view limit \"sticks\" to the edge. A typical use case is histograms, where one usually expects no margin on the bottom edge (0) of the histogram. Moreover, margin expansion \"bumps\" against sticky edges and cannot cross them. For example, if the upper data limit is 1.0, the upper view limit computed by simple margin application is 1.2, but there is a sticky edge at 1.1, then the actual upper view limit will be 1.1. This attribute cannot be assigned to; however, the x and y lists can be modified in place as needed. Examples >>> artist.sticky_edges.x[:] = (xmin, xmax)\n>>> artist.sticky_edges.y[:] = (ymin, ymax)\n \n   to_rgba(x, alpha=None, bytes=False, norm=True)[source]\n \nReturn a normalized rgba array corresponding to x. In the normal case, x is a 1D or 2D sequence of scalars, and the corresponding ndarray of rgba values will be returned, based on the norm and colormap set for this ScalarMappable. There is one special case, for handling images that are already rgb or rgba, such as might have been read from an image file. If x is an ndarray with 3 dimensions, and the last dimension is either 3 or 4, then it will be treated as an rgb or rgba array, and no mapping will be done. The array can be uint8, or it can be floating point with values in the 0-1 range; otherwise a ValueError will be raised. If it is a masked array, the mask will be ignored. If the last dimension is 3, the alpha kwarg (defaulting to 1) will be used to fill in the transparency. If the last dimension is 4, the alpha kwarg is ignored; it does not replace the pre-existing alpha. A ValueError will be raised if the third dimension is other than 3 or 4. In either case, if bytes is False (default), the rgba array will be floats in the 0-1 range; if it is True, the returned rgba array will be uint8 in the 0 to 255 range. If norm is False, no normalization of the input data is performed, and it is assumed to be in the range (0-1). \n   update(props)[source]\n \nUpdate this artist's properties from the dict props.  Parameters \n \npropsdict\n\n   \n   update_from(other)[source]\n \nCopy properties from other to self. \n   update_scalarmappable()[source]\n \nUpdate colors from the scalar mappable array, if any. Assign colors to edges and faces based on the array and\/or colors that were directly set, as appropriate. \n   zorder=0","title":"matplotlib.collections_api#matplotlib.collections.CircleCollection"},{"text":"matplotlib.axes.Axes.spy   Axes.spy(Z, precision=0, marker=None, markersize=None, aspect='equal', origin='upper', **kwargs)[source]\n \nPlot the sparsity pattern of a 2D array. This visualizes the non-zero values of the array. Two plotting styles are available: image and marker. Both are available for full arrays, but only the marker style works for scipy.sparse.spmatrix instances. Image style If marker and markersize are None, imshow is used. Any extra remaining keyword arguments are passed to this method. Marker style If Z is a scipy.sparse.spmatrix or marker or markersize are None, a Line2D object will be returned with the value of marker determining the marker type, and any remaining keyword arguments passed to plot.  Parameters \n \nZ(M, N) array-like\n\n\nThe array to be plotted.  \nprecisionfloat or 'present', default: 0\n\n\nIf precision is 0, any non-zero value will be plotted. Otherwise, values of \\(|Z| > precision\\) will be plotted. For scipy.sparse.spmatrix instances, you can also pass 'present'. In this case any value present in the array will be plotted, even if it is identically zero.  \naspect{'equal', 'auto', None} or float, default: 'equal'\n\n\nThe aspect ratio of the Axes. This parameter is particularly relevant for images since it determines whether data pixels are square. This parameter is a shortcut for explicitly calling Axes.set_aspect. See there for further details.  'equal': Ensures an aspect ratio of 1. Pixels will be square. 'auto': The Axes is kept fixed and the aspect is adjusted so that the data fit in the Axes. In general, this will result in non-square pixels. \nNone: Use rcParams[\"image.aspect\"] (default: 'equal').   \norigin{'upper', 'lower'}, default: rcParams[\"image.origin\"] (default: 'upper')\n\n\nPlace the [0, 0] index of the array in the upper left or lower left corner of the Axes. The convention 'upper' is typically used for matrices and images.    Returns \n \nAxesImage or Line2D\n\n\nThe return type depends on the plotting style (see above).    Other Parameters \n **kwargs\n\nThe supported additional parameters depend on the plotting style. For the image style, you can pass the following additional parameters of imshow:  cmap alpha url any Artist properties (passed on to the AxesImage)  For the marker style, you can pass any Line2D property except for linestyle:   \nProperty Description   \nagg_filter a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array  \nalpha scalar or None  \nanimated bool  \nantialiased or aa bool  \nclip_box Bbox  \nclip_on bool  \nclip_path Patch or (Path, Transform) or None  \ncolor or c color  \ndash_capstyle CapStyle or {'butt', 'projecting', 'round'}  \ndash_joinstyle JoinStyle or {'miter', 'round', 'bevel'}  \ndashes sequence of floats (on\/off ink in points) or (None, None)  \ndata (2, N) array or two 1D arrays  \ndrawstyle or ds {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'  \nfigure Figure  \nfillstyle {'full', 'left', 'right', 'bottom', 'top', 'none'}  \ngid str  \nin_layout bool  \nlabel object  \nlinestyle or ls {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}  \nlinewidth or lw float  \nmarker marker style string, Path or MarkerStyle  \nmarkeredgecolor or mec color  \nmarkeredgewidth or mew float  \nmarkerfacecolor or mfc color  \nmarkerfacecoloralt or mfcalt color  \nmarkersize or ms float  \nmarkevery None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]  \npath_effects AbstractPathEffect  \npicker float or callable[[Artist, Event], tuple[bool, dict]]  \npickradius float  \nrasterized bool  \nsketch_params (scale: float, length: float, randomness: float)  \nsnap bool or None  \nsolid_capstyle CapStyle or {'butt', 'projecting', 'round'}  \nsolid_joinstyle JoinStyle or {'miter', 'round', 'bevel'}  \ntransform unknown  \nurl str  \nvisible bool  \nxdata 1D array  \nydata 1D array  \nzorder float       \n  Examples using matplotlib.axes.Axes.spy\n \n   Spy Demos","title":"matplotlib._as_gen.matplotlib.axes.axes.spy"},{"text":"emit(record)  \nThis method does nothing.","title":"python.library.logging.handlers#logging.NullHandler.emit"},{"text":"matplotlib.pyplot.scatter   matplotlib.pyplot.scatter(x, y, s=None, c=None, marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=None, linewidths=None, *, edgecolors=None, plotnonfinite=False, data=None, **kwargs)[source]\n \nA scatter plot of y vs. x with varying marker size and\/or color.  Parameters \n \nx, yfloat or array-like, shape (n, )\n\n\nThe data positions.  \nsfloat or array-like, shape (n, ), optional\n\n\nThe marker size in points**2. Default is rcParams['lines.markersize'] ** 2.  \ncarray-like or list of colors or color, optional\n\n\nThe marker colors. Possible values:  A scalar or sequence of n numbers to be mapped to colors using cmap and norm. A 2D array in which the rows are RGB or RGBA. A sequence of colors of length n. A single color format string.  Note that c should not be a single numeric RGB or RGBA sequence because that is indistinguishable from an array of values to be colormapped. If you want to specify the same RGB or RGBA value for all points, use a 2D array with a single row. Otherwise, value- matching will have precedence in case of a size matching with x and y. If you wish to specify a single color for all points prefer the color keyword argument. Defaults to None. In that case the marker color is determined by the value of color, facecolor or facecolors. In case those are not specified or None, the marker color is determined by the next color of the Axes' current \"shape and fill\" color cycle. This cycle defaults to rcParams[\"axes.prop_cycle\"] (default: cycler('color', ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])).  \nmarkerMarkerStyle, default: rcParams[\"scatter.marker\"] (default: 'o')\n\n\nThe marker style. marker can be either an instance of the class or the text shorthand for a particular marker. See matplotlib.markers for more information about marker styles.  \ncmapstr or Colormap, default: rcParams[\"image.cmap\"] (default: 'viridis')\n\n\nA Colormap instance or registered colormap name. cmap is only used if c is an array of floats.  \nnormNormalize, default: None\n\n\nIf c is an array of floats, norm is used to scale the color data, c, in the range 0 to 1, in order to map into the colormap cmap. If None, use the default colors.Normalize.  \nvmin, vmaxfloat, default: None\n\n\nvmin and vmax are used in conjunction with the default norm to map the color array c to the colormap cmap. If None, the respective min and max of the color array is used. It is an error to use vmin\/vmax when norm is given.  \nalphafloat, default: None\n\n\nThe alpha blending value, between 0 (transparent) and 1 (opaque).  \nlinewidthsfloat or array-like, default: rcParams[\"lines.linewidth\"] (default: 1.5)\n\n\nThe linewidth of the marker edges. Note: The default edgecolors is 'face'. You may want to change this as well.  \nedgecolors{'face', 'none', None} or color or sequence of color, default: rcParams[\"scatter.edgecolors\"] (default: 'face')\n\n\nThe edge color of the marker. Possible values:  'face': The edge color will always be the same as the face color. 'none': No patch boundary will be drawn. A color or sequence of colors.  For non-filled markers, edgecolors is ignored. Instead, the color is determined like with 'face', i.e. from c, colors, or facecolors.  \nplotnonfinitebool, default: False\n\n\nWhether to plot points with nonfinite c (i.e. inf, -inf or nan). If True the points are drawn with the bad colormap color (see Colormap.set_bad).    Returns \n PathCollection\n  Other Parameters \n \ndataindexable object, optional\n\n\nIf given, the following parameters also accept a string s, which is interpreted as data[s] (unless this raises an exception): x, y, s, linewidths, edgecolors, c, facecolor, facecolors, color  \n**kwargsCollection properties\n\n    See also  plot\n\nTo plot scatter plots when markers are identical in size and color.    Notes  The plot function will be faster for scatterplots where markers don't vary in size or color. Any or all of x, y, s, and c may be masked arrays, in which case all masks will be combined and only unmasked points will be plotted. Fundamentally, scatter works with 1D arrays; x, y, s, and c may be input as N-D arrays, but within scatter they will be flattened. The exception is c, which will be flattened only if its size matches the size of x and y.  \n  Examples using matplotlib.pyplot.scatter\n \n   Scatter Masked   \n\n   Scatter Symbol   \n\n   Scatter plot   \n\n   Hyperlinks   \n\n   Pyplot tutorial","title":"matplotlib._as_gen.matplotlib.pyplot.scatter"},{"text":"create_collection(orig_handle, sizes, offsets, transOffset)[source]","title":"matplotlib.legend_handler_api#matplotlib.legend_handler.HandlerCircleCollection.create_collection"},{"text":"GEOSGeometry.empty  \nReturns whether or not the set of points in the geometry is empty.","title":"django.ref.contrib.gis.geos#django.contrib.gis.geos.GEOSGeometry.empty"},{"text":"autoscale_None()[source]\n \nAutoscale the scalar limits on the norm instance using the current array, changing only limits that are None","title":"matplotlib.collections_api#matplotlib.collections.EllipseCollection.autoscale_None"}]}
{"task_id":32063985,"prompt":"def f_32063985(soup):\n\treturn ","suffix":"","canonical_solution":"soup.find('div', id='main-content').decompose()","test_start":"\nfrom bs4 import BeautifulSoup\n\ndef check(candidate):","test":["\n    markup = \"<a>This is not div <div>This is div 1<\/div><div id='main-content'>This is div 2<\/div><\/a>\"\n    soup = BeautifulSoup(markup,\"html.parser\")\n    candidate(soup)\n    assert str(soup) == '<a>This is not div <div>This is div 1<\/div><\/a>'\n"],"entry_point":"f_32063985","intent":"remove a div from `soup` with a id `main-content` using beautifulsoup","library":["bs4"],"docs":[]}
{"task_id":27975069,"prompt":"def f_27975069(df):\n\treturn ","suffix":"","canonical_solution":"df[df['ids'].str.contains('ball')]","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    f = pd.DataFrame([[\"ball1\", 1, 2], [\"hall\", 5, 4]], columns = ['ids', 'x', 'y'])\n    f1 = candidate(f)\n    assert f1['x'][0] == 1\n    assert f1['y'][0] == 2\n"],"entry_point":"f_27975069","intent":"filter rows of datafram `df` containing key word `ball` in column `ids`","library":["pandas"],"docs":[{"text":"class torch.distributions.chi2.Chi2(df, validate_args=None) [source]\n \nBases: torch.distributions.gamma.Gamma Creates a Chi2 distribution parameterized by shape parameter df. This is exactly equivalent to Gamma(alpha=0.5*df, beta=0.5) Example: >>> m = Chi2(torch.tensor([1.0]))\n>>> m.sample()  # Chi2 distributed with shape df=1\ntensor([ 0.1046])\n  Parameters \ndf (float or Tensor) \u2013 shape parameter of the distribution    \narg_constraints = {'df': GreaterThan(lower_bound=0.0)} \n  \nproperty df \n  \nexpand(batch_shape, _instance=None) [source]","title":"torch.distributions#torch.distributions.chi2.Chi2"},{"text":"numpy.random.RandomState.chisquare method   random.RandomState.chisquare(df, size=None)\n \nDraw samples from a chi-square distribution. When df independent random variables, each with standard normal distributions (mean 0, variance 1), are squared and summed, the resulting distribution is chi-square (see Notes). This distribution is often used in hypothesis testing.  Note New code should use the chisquare method of a default_rng() instance instead; please see the Quick Start.   Parameters \n \ndffloat or array_like of floats\n\n\nNumber of degrees of freedom, must be > 0.  \nsizeint or tuple of ints, optional\n\n\nOutput shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if df is a scalar. Otherwise, np.array(df).size samples are drawn.    Returns \n \noutndarray or scalar\n\n\nDrawn samples from the parameterized chi-square distribution.    Raises \n ValueError\n\nWhen df <= 0 or when an inappropriate size (e.g. size=-1) is given.      See also  Generator.chisquare\n\nwhich should be used for new code.    Notes The variable obtained by summing the squares of df independent, standard normally distributed random variables:  \\[Q = \\sum_{i=0}^{\\mathtt{df}} X^2_i\\] is chi-square distributed, denoted  \\[Q \\sim \\chi^2_k.\\] The probability density function of the chi-squared distribution is  \\[p(x) = \\frac{(1\/2)^{k\/2}}{\\Gamma(k\/2)} x^{k\/2 - 1} e^{-x\/2},\\] where \\(\\Gamma\\) is the gamma function,  \\[\\Gamma(x) = \\int_0^{-\\infty} t^{x - 1} e^{-t} dt.\\] References  1 \nNIST \u201cEngineering Statistics Handbook\u201d https:\/\/www.itl.nist.gov\/div898\/handbook\/eda\/section3\/eda3666.htm   Examples >>> np.random.chisquare(2,4)\narray([ 1.89920014,  9.00867716,  3.13710533,  5.62318272]) # random","title":"numpy.reference.random.generated.numpy.random.randomstate.chisquare"},{"text":"numpy.random.chisquare   random.chisquare(df, size=None)\n \nDraw samples from a chi-square distribution. When df independent random variables, each with standard normal distributions (mean 0, variance 1), are squared and summed, the resulting distribution is chi-square (see Notes). This distribution is often used in hypothesis testing.  Note New code should use the chisquare method of a default_rng() instance instead; please see the Quick Start.   Parameters \n \ndffloat or array_like of floats\n\n\nNumber of degrees of freedom, must be > 0.  \nsizeint or tuple of ints, optional\n\n\nOutput shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if df is a scalar. Otherwise, np.array(df).size samples are drawn.    Returns \n \noutndarray or scalar\n\n\nDrawn samples from the parameterized chi-square distribution.    Raises \n ValueError\n\nWhen df <= 0 or when an inappropriate size (e.g. size=-1) is given.      See also  Generator.chisquare\n\nwhich should be used for new code.    Notes The variable obtained by summing the squares of df independent, standard normally distributed random variables:  \\[Q = \\sum_{i=0}^{\\mathtt{df}} X^2_i\\] is chi-square distributed, denoted  \\[Q \\sim \\chi^2_k.\\] The probability density function of the chi-squared distribution is  \\[p(x) = \\frac{(1\/2)^{k\/2}}{\\Gamma(k\/2)} x^{k\/2 - 1} e^{-x\/2},\\] where \\(\\Gamma\\) is the gamma function,  \\[\\Gamma(x) = \\int_0^{-\\infty} t^{x - 1} e^{-t} dt.\\] References  1 \nNIST \u201cEngineering Statistics Handbook\u201d https:\/\/www.itl.nist.gov\/div898\/handbook\/eda\/section3\/eda3666.htm   Examples >>> np.random.chisquare(2,4)\narray([ 1.89920014,  9.00867716,  3.13710533,  5.62318272]) # random","title":"numpy.reference.random.generated.numpy.random.chisquare"},{"text":"pandas.DataFrame.filter   DataFrame.filter(items=None, like=None, regex=None, axis=None)[source]\n \nSubset the dataframe rows or columns according to the specified index labels. Note that this routine does not filter a dataframe on its contents. The filter is applied to the labels of the index.  Parameters \n \nitems:list-like\n\n\nKeep labels from axis which are in items.  \nlike:str\n\n\nKeep labels from axis for which \u201clike in label == True\u201d.  \nregex:str (regular expression)\n\n\nKeep labels from axis for which re.search(regex, label) == True.  \naxis:{0 or \u2018index\u2019, 1 or \u2018columns\u2019, None}, default None\n\n\nThe axis to filter on, expressed either as an index (int) or axis name (str). By default this is the info axis, \u2018index\u2019 for Series, \u2018columns\u2019 for DataFrame.    Returns \n same type as input object\n    See also  DataFrame.loc\n\nAccess a group of rows and columns by label(s) or a boolean array.    Notes The items, like, and regex parameters are enforced to be mutually exclusive. axis defaults to the info axis that is used when indexing with []. Examples \n>>> df = pd.DataFrame(np.array(([1, 2, 3], [4, 5, 6])),\n...                   index=['mouse', 'rabbit'],\n...                   columns=['one', 'two', 'three'])\n>>> df\n        one  two  three\nmouse     1    2      3\nrabbit    4    5      6\n  \n>>> # select columns by name\n>>> df.filter(items=['one', 'three'])\n         one  three\nmouse     1      3\nrabbit    4      6\n  \n>>> # select columns by regular expression\n>>> df.filter(regex='e$', axis=1)\n         one  three\nmouse     1      3\nrabbit    4      6\n  \n>>> # select rows containing 'bbi'\n>>> df.filter(like='bbi', axis=0)\n         one  two  three\nrabbit    4    5      6","title":"pandas.reference.api.pandas.dataframe.filter"},{"text":"numpy.random.Generator.chisquare method   random.Generator.chisquare(df, size=None)\n \nDraw samples from a chi-square distribution. When df independent random variables, each with standard normal distributions (mean 0, variance 1), are squared and summed, the resulting distribution is chi-square (see Notes). This distribution is often used in hypothesis testing.  Parameters \n \ndffloat or array_like of floats\n\n\nNumber of degrees of freedom, must be > 0.  \nsizeint or tuple of ints, optional\n\n\nOutput shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if df is a scalar. Otherwise, np.array(df).size samples are drawn.    Returns \n \noutndarray or scalar\n\n\nDrawn samples from the parameterized chi-square distribution.    Raises \n ValueError\n\nWhen df <= 0 or when an inappropriate size (e.g. size=-1) is given.     Notes The variable obtained by summing the squares of df independent, standard normally distributed random variables:  \\[Q = \\sum_{i=0}^{\\mathtt{df}} X^2_i\\] is chi-square distributed, denoted  \\[Q \\sim \\chi^2_k.\\] The probability density function of the chi-squared distribution is  \\[p(x) = \\frac{(1\/2)^{k\/2}}{\\Gamma(k\/2)} x^{k\/2 - 1} e^{-x\/2},\\] where \\(\\Gamma\\) is the gamma function,  \\[\\Gamma(x) = \\int_0^{-\\infty} t^{x - 1} e^{-t} dt.\\] References  1 \nNIST \u201cEngineering Statistics Handbook\u201d https:\/\/www.itl.nist.gov\/div898\/handbook\/eda\/section3\/eda3666.htm   Examples >>> np.random.default_rng().chisquare(2,4)\narray([ 1.89920014,  9.00867716,  3.13710533,  5.62318272]) # random","title":"numpy.reference.random.generated.numpy.random.generator.chisquare"},{"text":"kevent.data  \nFilter specific data.","title":"python.library.select#select.kevent.data"},{"text":"property df","title":"torch.distributions#torch.distributions.chi2.Chi2.df"},{"text":"skimage.filters.farid_v(image, *, mask=None) [source]\n \nFind the vertical edges of an image using the Farid transform.  Parameters \n \nimage2-D array \n\nImage to process.  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Farid edge map.     Notes The kernel was constructed using the 5-tap weights from [1]. References  \n1  \nFarid, H. and Simoncelli, E. P., \u201cDifferentiation of discrete multidimensional signals\u201d, IEEE Transactions on Image Processing 13(4): 496-508, 2004. DOI:10.1109\/TIP.2004.823819","title":"skimage.api.skimage.filters#skimage.filters.farid_v"},{"text":"get_numballs() \n get the number of trackballs on a Joystick get_numballs() -> int  Returns the number of trackball devices on a Joystick. These devices work similar to a mouse but they have no absolute position; they only have relative amounts of movement. The pygame.JOYBALLMOTION event will be sent when the trackball is rolled. It will report the amount of movement on the trackball.","title":"pygame.ref.joystick#pygame.joystick.Joystick.get_numballs"},{"text":"pandas.DataFrame.style   propertyDataFrame.style\n \nReturns a Styler object. Contains methods for building a styled HTML representation of the DataFrame.  See also  io.formats.style.Styler\n\nHelps style a DataFrame or Series according to the data with HTML and CSS.","title":"pandas.reference.api.pandas.dataframe.style"}]}
{"task_id":20461165,"prompt":"def f_20461165(df):\n\treturn ","suffix":"","canonical_solution":"df.reset_index(level=0, inplace=True)","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    df = pd.DataFrame([[384, 593], [781, 123]], columns = ['gi', 'ptt_loc'])\n    candidate(df)\n    assert df['index'][0] == 0\n    assert df['index'][1] == 1\n"],"entry_point":"f_20461165","intent":"convert index at level 0 into a column in dataframe `df`","library":["pandas"],"docs":[{"text":"pandas.Series.keys   Series.keys()[source]\n \nReturn alias for index.  Returns \n Index\n\nIndex of the Series.","title":"pandas.reference.api.pandas.series.keys"},{"text":"pandas.TimedeltaIndex.to_frame   TimedeltaIndex.to_frame(index=True, name=NoDefault.no_default)[source]\n \nCreate a DataFrame with a column containing the Index.  Parameters \n \nindex:bool, default True\n\n\nSet the index of the returned DataFrame as the original Index.  \nname:object, default None\n\n\nThe passed name should substitute for the index name (if it has one).    Returns \n DataFrame\n\nDataFrame containing the original Index data.      See also  Index.to_series\n\nConvert an Index to a Series.  Series.to_frame\n\nConvert Series to DataFrame.    Examples \n>>> idx = pd.Index(['Ant', 'Bear', 'Cow'], name='animal')\n>>> idx.to_frame()\n       animal\nanimal\nAnt       Ant\nBear     Bear\nCow       Cow\n  By default, the original Index is reused. To enforce a new Index: \n>>> idx.to_frame(index=False)\n    animal\n0   Ant\n1  Bear\n2   Cow\n  To override the name of the resulting column, specify name: \n>>> idx.to_frame(index=False, name='zoo')\n    zoo\n0   Ant\n1  Bear\n2   Cow","title":"pandas.reference.api.pandas.timedeltaindex.to_frame"},{"text":"pandas.Index.to_frame   Index.to_frame(index=True, name=NoDefault.no_default)[source]\n \nCreate a DataFrame with a column containing the Index.  Parameters \n \nindex:bool, default True\n\n\nSet the index of the returned DataFrame as the original Index.  \nname:object, default None\n\n\nThe passed name should substitute for the index name (if it has one).    Returns \n DataFrame\n\nDataFrame containing the original Index data.      See also  Index.to_series\n\nConvert an Index to a Series.  Series.to_frame\n\nConvert Series to DataFrame.    Examples \n>>> idx = pd.Index(['Ant', 'Bear', 'Cow'], name='animal')\n>>> idx.to_frame()\n       animal\nanimal\nAnt       Ant\nBear     Bear\nCow       Cow\n  By default, the original Index is reused. To enforce a new Index: \n>>> idx.to_frame(index=False)\n    animal\n0   Ant\n1  Bear\n2   Cow\n  To override the name of the resulting column, specify name: \n>>> idx.to_frame(index=False, name='zoo')\n    zoo\n0   Ant\n1  Bear\n2   Cow","title":"pandas.reference.api.pandas.index.to_frame"},{"text":"pandas.DatetimeIndex.to_frame   DatetimeIndex.to_frame(index=True, name=NoDefault.no_default)[source]\n \nCreate a DataFrame with a column containing the Index.  Parameters \n \nindex:bool, default True\n\n\nSet the index of the returned DataFrame as the original Index.  \nname:object, default None\n\n\nThe passed name should substitute for the index name (if it has one).    Returns \n DataFrame\n\nDataFrame containing the original Index data.      See also  Index.to_series\n\nConvert an Index to a Series.  Series.to_frame\n\nConvert Series to DataFrame.    Examples \n>>> idx = pd.Index(['Ant', 'Bear', 'Cow'], name='animal')\n>>> idx.to_frame()\n       animal\nanimal\nAnt       Ant\nBear     Bear\nCow       Cow\n  By default, the original Index is reused. To enforce a new Index: \n>>> idx.to_frame(index=False)\n    animal\n0   Ant\n1  Bear\n2   Cow\n  To override the name of the resulting column, specify name: \n>>> idx.to_frame(index=False, name='zoo')\n    zoo\n0   Ant\n1  Bear\n2   Cow","title":"pandas.reference.api.pandas.datetimeindex.to_frame"},{"text":"pandas.DataFrame.index   DataFrame.index\n \nThe index (row labels) of the DataFrame.","title":"pandas.reference.api.pandas.dataframe.index"},{"text":"pandas.Index.droplevel   finalIndex.droplevel(level=0)[source]\n \nReturn index with requested level(s) removed. If resulting index has only 1 level left, the result will be of Index type, not MultiIndex.  Parameters \n \nlevel:int, str, or list-like, default 0\n\n\nIf a string is given, must be the name of a level If list-like, elements must be names or indexes of levels.    Returns \n Index or MultiIndex\n   Examples \n>>> mi = pd.MultiIndex.from_arrays(\n... [[1, 2], [3, 4], [5, 6]], names=['x', 'y', 'z'])\n>>> mi\nMultiIndex([(1, 3, 5),\n            (2, 4, 6)],\n           names=['x', 'y', 'z'])\n  \n>>> mi.droplevel()\nMultiIndex([(3, 5),\n            (4, 6)],\n           names=['y', 'z'])\n  \n>>> mi.droplevel(2)\nMultiIndex([(1, 3),\n            (2, 4)],\n           names=['x', 'y'])\n  \n>>> mi.droplevel('z')\nMultiIndex([(1, 3),\n            (2, 4)],\n           names=['x', 'y'])\n  \n>>> mi.droplevel(['x', 'y'])\nInt64Index([5, 6], dtype='int64', name='z')","title":"pandas.reference.api.pandas.index.droplevel"},{"text":"pandas.DataFrame.reorder_levels   DataFrame.reorder_levels(order, axis=0)[source]\n \nRearrange index levels using input order. May not drop or duplicate levels.  Parameters \n \norder:list of int or list of str\n\n\nList representing new level order. Reference level by number (position) or by key (label).  \naxis:{0 or \u2018index\u2019, 1 or \u2018columns\u2019}, default 0\n\n\nWhere to reorder levels.    Returns \n DataFrame\n   Examples \n>>> data = {\n...     \"class\": [\"Mammals\", \"Mammals\", \"Reptiles\"],\n...     \"diet\": [\"Omnivore\", \"Carnivore\", \"Carnivore\"],\n...     \"species\": [\"Humans\", \"Dogs\", \"Snakes\"],\n... }\n>>> df = pd.DataFrame(data, columns=[\"class\", \"diet\", \"species\"])\n>>> df = df.set_index([\"class\", \"diet\"])\n>>> df\n                                  species\nclass      diet\nMammals    Omnivore                Humans\n           Carnivore                 Dogs\nReptiles   Carnivore               Snakes\n  Let\u2019s reorder the levels of the index: \n>>> df.reorder_levels([\"diet\", \"class\"])\n                                  species\ndiet      class\nOmnivore  Mammals                  Humans\nCarnivore Mammals                    Dogs\n          Reptiles                 Snakes","title":"pandas.reference.api.pandas.dataframe.reorder_levels"},{"text":"pandas.Index.to_series   Index.to_series(index=None, name=None)[source]\n \nCreate a Series with both index and values equal to the index keys. Useful with map for returning an indexer based on an index.  Parameters \n \nindex:Index, optional\n\n\nIndex of resulting Series. If None, defaults to original index.  \nname:str, optional\n\n\nName of resulting Series. If None, defaults to name of original index.    Returns \n Series\n\nThe dtype will be based on the type of the Index values.      See also  Index.to_frame\n\nConvert an Index to a DataFrame.  Series.to_frame\n\nConvert Series to DataFrame.    Examples \n>>> idx = pd.Index(['Ant', 'Bear', 'Cow'], name='animal')\n  By default, the original Index and original name is reused. \n>>> idx.to_series()\nanimal\nAnt      Ant\nBear    Bear\nCow      Cow\nName: animal, dtype: object\n  To enforce a new Index, specify new labels to index: \n>>> idx.to_series(index=[0, 1, 2])\n0     Ant\n1    Bear\n2     Cow\nName: animal, dtype: object\n  To override the name of the resulting column, specify name: \n>>> idx.to_series(name='zoo')\nanimal\nAnt      Ant\nBear    Bear\nCow      Cow\nName: zoo, dtype: object","title":"pandas.reference.api.pandas.index.to_series"},{"text":"pandas.DataFrame.reset_index   DataFrame.reset_index(level=None, drop=False, inplace=False, col_level=0, col_fill='')[source]\n \nReset the index, or a level of it. Reset the index of the DataFrame, and use the default one instead. If the DataFrame has a MultiIndex, this method can remove one or more levels.  Parameters \n \nlevel:int, str, tuple, or list, default None\n\n\nOnly remove the given levels from the index. Removes all levels by default.  \ndrop:bool, default False\n\n\nDo not try to insert index into dataframe columns. This resets the index to the default integer index.  \ninplace:bool, default False\n\n\nModify the DataFrame in place (do not create a new object).  \ncol_level:int or str, default 0\n\n\nIf the columns have multiple levels, determines which level the labels are inserted into. By default it is inserted into the first level.  \ncol_fill:object, default \u2018\u2019\n\n\nIf the columns have multiple levels, determines how the other levels are named. If None then the index name is repeated.    Returns \n DataFrame or None\n\nDataFrame with the new index or None if inplace=True.      See also  DataFrame.set_index\n\nOpposite of reset_index.  DataFrame.reindex\n\nChange to new indices or expand indices.  DataFrame.reindex_like\n\nChange to same indices as other DataFrame.    Examples \n>>> df = pd.DataFrame([('bird', 389.0),\n...                    ('bird', 24.0),\n...                    ('mammal', 80.5),\n...                    ('mammal', np.nan)],\n...                   index=['falcon', 'parrot', 'lion', 'monkey'],\n...                   columns=('class', 'max_speed'))\n>>> df\n         class  max_speed\nfalcon    bird      389.0\nparrot    bird       24.0\nlion    mammal       80.5\nmonkey  mammal        NaN\n  When we reset the index, the old index is added as a column, and a new sequential index is used: \n>>> df.reset_index()\n    index   class  max_speed\n0  falcon    bird      389.0\n1  parrot    bird       24.0\n2    lion  mammal       80.5\n3  monkey  mammal        NaN\n  We can use the drop parameter to avoid the old index being added as a column: \n>>> df.reset_index(drop=True)\n    class  max_speed\n0    bird      389.0\n1    bird       24.0\n2  mammal       80.5\n3  mammal        NaN\n  You can also use reset_index with MultiIndex. \n>>> index = pd.MultiIndex.from_tuples([('bird', 'falcon'),\n...                                    ('bird', 'parrot'),\n...                                    ('mammal', 'lion'),\n...                                    ('mammal', 'monkey')],\n...                                   names=['class', 'name'])\n>>> columns = pd.MultiIndex.from_tuples([('speed', 'max'),\n...                                      ('species', 'type')])\n>>> df = pd.DataFrame([(389.0, 'fly'),\n...                    ( 24.0, 'fly'),\n...                    ( 80.5, 'run'),\n...                    (np.nan, 'jump')],\n...                   index=index,\n...                   columns=columns)\n>>> df\n               speed species\n                 max    type\nclass  name\nbird   falcon  389.0     fly\n       parrot   24.0     fly\nmammal lion     80.5     run\n       monkey    NaN    jump\n  If the index has multiple levels, we can reset a subset of them: \n>>> df.reset_index(level='class')\n         class  speed species\n                  max    type\nname\nfalcon    bird  389.0     fly\nparrot    bird   24.0     fly\nlion    mammal   80.5     run\nmonkey  mammal    NaN    jump\n  If we are not dropping the index, by default, it is placed in the top level. We can place it in another level: \n>>> df.reset_index(level='class', col_level=1)\n                speed species\n         class    max    type\nname\nfalcon    bird  389.0     fly\nparrot    bird   24.0     fly\nlion    mammal   80.5     run\nmonkey  mammal    NaN    jump\n  When the index is inserted under another level, we can specify under which one with the parameter col_fill: \n>>> df.reset_index(level='class', col_level=1, col_fill='species')\n              species  speed species\n                class    max    type\nname\nfalcon           bird  389.0     fly\nparrot           bird   24.0     fly\nlion           mammal   80.5     run\nmonkey         mammal    NaN    jump\n  If we specify a nonexistent level for col_fill, it is created: \n>>> df.reset_index(level='class', col_level=1, col_fill='genus')\n                genus  speed species\n                class    max    type\nname\nfalcon           bird  389.0     fly\nparrot           bird   24.0     fly\nlion           mammal   80.5     run\nmonkey         mammal    NaN    jump","title":"pandas.reference.api.pandas.dataframe.reset_index"},{"text":"pandas.TimedeltaIndex.to_series   TimedeltaIndex.to_series(index=None, name=None)[source]\n \nCreate a Series with both index and values equal to the index keys. Useful with map for returning an indexer based on an index.  Parameters \n \nindex:Index, optional\n\n\nIndex of resulting Series. If None, defaults to original index.  \nname:str, optional\n\n\nName of resulting Series. If None, defaults to name of original index.    Returns \n Series\n\nThe dtype will be based on the type of the Index values.      See also  Index.to_frame\n\nConvert an Index to a DataFrame.  Series.to_frame\n\nConvert Series to DataFrame.    Examples \n>>> idx = pd.Index(['Ant', 'Bear', 'Cow'], name='animal')\n  By default, the original Index and original name is reused. \n>>> idx.to_series()\nanimal\nAnt      Ant\nBear    Bear\nCow      Cow\nName: animal, dtype: object\n  To enforce a new Index, specify new labels to index: \n>>> idx.to_series(index=[0, 1, 2])\n0     Ant\n1    Bear\n2     Cow\nName: animal, dtype: object\n  To override the name of the resulting column, specify name: \n>>> idx.to_series(name='zoo')\nanimal\nAnt      Ant\nBear    Bear\nCow      Cow\nName: zoo, dtype: object","title":"pandas.reference.api.pandas.timedeltaindex.to_series"}]}
{"task_id":20461165,"prompt":"def f_20461165(df):\n\t","suffix":"\n\treturn ","canonical_solution":"df['index1'] = df.index","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    df = pd.DataFrame([[384, 593], [781, 123]], columns = ['gi', 'ptt_loc'])\n    candidate(df)\n    assert df['index1'][0] == 0\n    assert df['index1'][1] == 1\n"],"entry_point":"f_20461165","intent":"Add indexes in a data frame `df` to a column `index1`","library":["pandas"],"docs":[{"text":"pandas.MultiIndex.from_frame   classmethodMultiIndex.from_frame(df, sortorder=None, names=None)[source]\n \nMake a MultiIndex from a DataFrame.  Parameters \n \ndf:DataFrame\n\n\nDataFrame to be converted to MultiIndex.  \nsortorder:int, optional\n\n\nLevel of sortedness (must be lexicographically sorted by that level).  \nnames:list-like, optional\n\n\nIf no names are provided, use the column names, or tuple of column names if the columns is a MultiIndex. If a sequence, overwrite names with the given sequence.    Returns \n MultiIndex\n\nThe MultiIndex representation of the given DataFrame.      See also  MultiIndex.from_arrays\n\nConvert list of arrays to MultiIndex.  MultiIndex.from_tuples\n\nConvert list of tuples to MultiIndex.  MultiIndex.from_product\n\nMake a MultiIndex from cartesian product of iterables.    Examples \n>>> df = pd.DataFrame([['HI', 'Temp'], ['HI', 'Precip'],\n...                    ['NJ', 'Temp'], ['NJ', 'Precip']],\n...                   columns=['a', 'b'])\n>>> df\n      a       b\n0    HI    Temp\n1    HI  Precip\n2    NJ    Temp\n3    NJ  Precip\n  \n>>> pd.MultiIndex.from_frame(df)\nMultiIndex([('HI',   'Temp'),\n            ('HI', 'Precip'),\n            ('NJ',   'Temp'),\n            ('NJ', 'Precip')],\n           names=['a', 'b'])\n  Using explicit names, instead of the column names \n>>> pd.MultiIndex.from_frame(df, names=['state', 'observation'])\nMultiIndex([('HI',   'Temp'),\n            ('HI', 'Precip'),\n            ('NJ',   'Temp'),\n            ('NJ', 'Precip')],\n           names=['state', 'observation'])","title":"pandas.reference.api.pandas.multiindex.from_frame"},{"text":"pandas.DataFrame.reindex_like   DataFrame.reindex_like(other, method=None, copy=True, limit=None, tolerance=None)[source]\n \nReturn an object with matching indices as other object. Conform the object to the same index on all axes. Optional filling logic, placing NaN in locations having no value in the previous index. A new object is produced unless the new index is equivalent to the current one and copy=False.  Parameters \n \nother:Object of the same data type\n\n\nIts row and column indices are used to define the new indices of this object.  \nmethod:{None, \u2018backfill\u2019\/\u2019bfill\u2019, \u2018pad\u2019\/\u2019ffill\u2019, \u2018nearest\u2019}\n\n\nMethod to use for filling holes in reindexed DataFrame. Please note: this is only applicable to DataFrames\/Series with a monotonically increasing\/decreasing index.  None (default): don\u2019t fill gaps pad \/ ffill: propagate last valid observation forward to next valid backfill \/ bfill: use next valid observation to fill gap nearest: use nearest valid observations to fill gap.   \ncopy:bool, default True\n\n\nReturn a new object, even if the passed indexes are the same.  \nlimit:int, default None\n\n\nMaximum number of consecutive labels to fill for inexact matches.  \ntolerance:optional\n\n\nMaximum distance between original and new labels for inexact matches. The values of the index at the matching locations must satisfy the equation abs(index[indexer] - target) <= tolerance. Tolerance may be a scalar value, which applies the same tolerance to all values, or list-like, which applies variable tolerance per element. List-like includes list, tuple, array, Series, and must be the same size as the index and its dtype must exactly match the index\u2019s type.    Returns \n Series or DataFrame\n\nSame type as caller, but with changed indices on each axis.      See also  DataFrame.set_index\n\nSet row labels.  DataFrame.reset_index\n\nRemove row labels or move them to new columns.  DataFrame.reindex\n\nChange to new indices or expand indices.    Notes Same as calling .reindex(index=other.index, columns=other.columns,...). Examples \n>>> df1 = pd.DataFrame([[24.3, 75.7, 'high'],\n...                     [31, 87.8, 'high'],\n...                     [22, 71.6, 'medium'],\n...                     [35, 95, 'medium']],\n...                    columns=['temp_celsius', 'temp_fahrenheit',\n...                             'windspeed'],\n...                    index=pd.date_range(start='2014-02-12',\n...                                        end='2014-02-15', freq='D'))\n  \n>>> df1\n            temp_celsius  temp_fahrenheit windspeed\n2014-02-12          24.3             75.7      high\n2014-02-13          31.0             87.8      high\n2014-02-14          22.0             71.6    medium\n2014-02-15          35.0             95.0    medium\n  \n>>> df2 = pd.DataFrame([[28, 'low'],\n...                     [30, 'low'],\n...                     [35.1, 'medium']],\n...                    columns=['temp_celsius', 'windspeed'],\n...                    index=pd.DatetimeIndex(['2014-02-12', '2014-02-13',\n...                                            '2014-02-15']))\n  \n>>> df2\n            temp_celsius windspeed\n2014-02-12          28.0       low\n2014-02-13          30.0       low\n2014-02-15          35.1    medium\n  \n>>> df2.reindex_like(df1)\n            temp_celsius  temp_fahrenheit windspeed\n2014-02-12          28.0              NaN       low\n2014-02-13          30.0              NaN       low\n2014-02-14           NaN              NaN       NaN\n2014-02-15          35.1              NaN    medium","title":"pandas.reference.api.pandas.dataframe.reindex_like"},{"text":"pandas.DataFrame.reindex   DataFrame.reindex(labels=None, index=None, columns=None, axis=None, method=None, copy=True, level=None, fill_value=nan, limit=None, tolerance=None)[source]\n \nConform Series\/DataFrame to new index with optional filling logic. Places NA\/NaN in locations having no value in the previous index. A new object is produced unless the new index is equivalent to the current one and copy=False.  Parameters \n \nkeywords for axes:array-like, optional\n\n\nNew labels \/ index to conform to, should be specified using keywords. Preferably an Index object to avoid duplicating data.  \nmethod:{None, \u2018backfill\u2019\/\u2019bfill\u2019, \u2018pad\u2019\/\u2019ffill\u2019, \u2018nearest\u2019}\n\n\nMethod to use for filling holes in reindexed DataFrame. Please note: this is only applicable to DataFrames\/Series with a monotonically increasing\/decreasing index.  None (default): don\u2019t fill gaps pad \/ ffill: Propagate last valid observation forward to next valid. backfill \/ bfill: Use next valid observation to fill gap. nearest: Use nearest valid observations to fill gap.   \ncopy:bool, default True\n\n\nReturn a new object, even if the passed indexes are the same.  \nlevel:int or name\n\n\nBroadcast across a level, matching Index values on the passed MultiIndex level.  \nfill_value:scalar, default np.NaN\n\n\nValue to use for missing values. Defaults to NaN, but can be any \u201ccompatible\u201d value.  \nlimit:int, default None\n\n\nMaximum number of consecutive elements to forward or backward fill.  \ntolerance:optional\n\n\nMaximum distance between original and new labels for inexact matches. The values of the index at the matching locations most satisfy the equation abs(index[indexer] - target) <= tolerance. Tolerance may be a scalar value, which applies the same tolerance to all values, or list-like, which applies variable tolerance per element. List-like includes list, tuple, array, Series, and must be the same size as the index and its dtype must exactly match the index\u2019s type.    Returns \n Series\/DataFrame with changed index.\n    See also  DataFrame.set_index\n\nSet row labels.  DataFrame.reset_index\n\nRemove row labels or move them to new columns.  DataFrame.reindex_like\n\nChange to same indices as other DataFrame.    Examples DataFrame.reindex supports two calling conventions  (index=index_labels, columns=column_labels, ...) (labels, axis={'index', 'columns'}, ...)  We highly recommend using keyword arguments to clarify your intent. Create a dataframe with some fictional data. \n>>> index = ['Firefox', 'Chrome', 'Safari', 'IE10', 'Konqueror']\n>>> df = pd.DataFrame({'http_status': [200, 200, 404, 404, 301],\n...                   'response_time': [0.04, 0.02, 0.07, 0.08, 1.0]},\n...                   index=index)\n>>> df\n           http_status  response_time\nFirefox            200           0.04\nChrome             200           0.02\nSafari             404           0.07\nIE10               404           0.08\nKonqueror          301           1.00\n  Create a new index and reindex the dataframe. By default values in the new index that do not have corresponding records in the dataframe are assigned NaN. \n>>> new_index = ['Safari', 'Iceweasel', 'Comodo Dragon', 'IE10',\n...              'Chrome']\n>>> df.reindex(new_index)\n               http_status  response_time\nSafari               404.0           0.07\nIceweasel              NaN            NaN\nComodo Dragon          NaN            NaN\nIE10                 404.0           0.08\nChrome               200.0           0.02\n  We can fill in the missing values by passing a value to the keyword fill_value. Because the index is not monotonically increasing or decreasing, we cannot use arguments to the keyword method to fill the NaN values. \n>>> df.reindex(new_index, fill_value=0)\n               http_status  response_time\nSafari                 404           0.07\nIceweasel                0           0.00\nComodo Dragon            0           0.00\nIE10                   404           0.08\nChrome                 200           0.02\n  \n>>> df.reindex(new_index, fill_value='missing')\n              http_status response_time\nSafari                404          0.07\nIceweasel         missing       missing\nComodo Dragon     missing       missing\nIE10                  404          0.08\nChrome                200          0.02\n  We can also reindex the columns. \n>>> df.reindex(columns=['http_status', 'user_agent'])\n           http_status  user_agent\nFirefox            200         NaN\nChrome             200         NaN\nSafari             404         NaN\nIE10               404         NaN\nKonqueror          301         NaN\n  Or we can use \u201caxis-style\u201d keyword arguments \n>>> df.reindex(['http_status', 'user_agent'], axis=\"columns\")\n           http_status  user_agent\nFirefox            200         NaN\nChrome             200         NaN\nSafari             404         NaN\nIE10               404         NaN\nKonqueror          301         NaN\n  To further illustrate the filling functionality in reindex, we will create a dataframe with a monotonically increasing index (for example, a sequence of dates). \n>>> date_index = pd.date_range('1\/1\/2010', periods=6, freq='D')\n>>> df2 = pd.DataFrame({\"prices\": [100, 101, np.nan, 100, 89, 88]},\n...                    index=date_index)\n>>> df2\n            prices\n2010-01-01   100.0\n2010-01-02   101.0\n2010-01-03     NaN\n2010-01-04   100.0\n2010-01-05    89.0\n2010-01-06    88.0\n  Suppose we decide to expand the dataframe to cover a wider date range. \n>>> date_index2 = pd.date_range('12\/29\/2009', periods=10, freq='D')\n>>> df2.reindex(date_index2)\n            prices\n2009-12-29     NaN\n2009-12-30     NaN\n2009-12-31     NaN\n2010-01-01   100.0\n2010-01-02   101.0\n2010-01-03     NaN\n2010-01-04   100.0\n2010-01-05    89.0\n2010-01-06    88.0\n2010-01-07     NaN\n  The index entries that did not have a value in the original data frame (for example, \u20182009-12-29\u2019) are by default filled with NaN. If desired, we can fill in the missing values using one of several options. For example, to back-propagate the last valid value to fill the NaN values, pass bfill as an argument to the method keyword. \n>>> df2.reindex(date_index2, method='bfill')\n            prices\n2009-12-29   100.0\n2009-12-30   100.0\n2009-12-31   100.0\n2010-01-01   100.0\n2010-01-02   101.0\n2010-01-03     NaN\n2010-01-04   100.0\n2010-01-05    89.0\n2010-01-06    88.0\n2010-01-07     NaN\n  Please note that the NaN value present in the original dataframe (at index value 2010-01-03) will not be filled by any of the value propagation schemes. This is because filling while reindexing does not look at dataframe values, but only compares the original and desired indexes. If you do want to fill in the NaN values present in the original dataframe, use the fillna() method. See the user guide for more.","title":"pandas.reference.api.pandas.dataframe.reindex"},{"text":"pandas.Series.reindex_like   Series.reindex_like(other, method=None, copy=True, limit=None, tolerance=None)[source]\n \nReturn an object with matching indices as other object. Conform the object to the same index on all axes. Optional filling logic, placing NaN in locations having no value in the previous index. A new object is produced unless the new index is equivalent to the current one and copy=False.  Parameters \n \nother:Object of the same data type\n\n\nIts row and column indices are used to define the new indices of this object.  \nmethod:{None, \u2018backfill\u2019\/\u2019bfill\u2019, \u2018pad\u2019\/\u2019ffill\u2019, \u2018nearest\u2019}\n\n\nMethod to use for filling holes in reindexed DataFrame. Please note: this is only applicable to DataFrames\/Series with a monotonically increasing\/decreasing index.  None (default): don\u2019t fill gaps pad \/ ffill: propagate last valid observation forward to next valid backfill \/ bfill: use next valid observation to fill gap nearest: use nearest valid observations to fill gap.   \ncopy:bool, default True\n\n\nReturn a new object, even if the passed indexes are the same.  \nlimit:int, default None\n\n\nMaximum number of consecutive labels to fill for inexact matches.  \ntolerance:optional\n\n\nMaximum distance between original and new labels for inexact matches. The values of the index at the matching locations must satisfy the equation abs(index[indexer] - target) <= tolerance. Tolerance may be a scalar value, which applies the same tolerance to all values, or list-like, which applies variable tolerance per element. List-like includes list, tuple, array, Series, and must be the same size as the index and its dtype must exactly match the index\u2019s type.    Returns \n Series or DataFrame\n\nSame type as caller, but with changed indices on each axis.      See also  DataFrame.set_index\n\nSet row labels.  DataFrame.reset_index\n\nRemove row labels or move them to new columns.  DataFrame.reindex\n\nChange to new indices or expand indices.    Notes Same as calling .reindex(index=other.index, columns=other.columns,...). Examples \n>>> df1 = pd.DataFrame([[24.3, 75.7, 'high'],\n...                     [31, 87.8, 'high'],\n...                     [22, 71.6, 'medium'],\n...                     [35, 95, 'medium']],\n...                    columns=['temp_celsius', 'temp_fahrenheit',\n...                             'windspeed'],\n...                    index=pd.date_range(start='2014-02-12',\n...                                        end='2014-02-15', freq='D'))\n  \n>>> df1\n            temp_celsius  temp_fahrenheit windspeed\n2014-02-12          24.3             75.7      high\n2014-02-13          31.0             87.8      high\n2014-02-14          22.0             71.6    medium\n2014-02-15          35.0             95.0    medium\n  \n>>> df2 = pd.DataFrame([[28, 'low'],\n...                     [30, 'low'],\n...                     [35.1, 'medium']],\n...                    columns=['temp_celsius', 'windspeed'],\n...                    index=pd.DatetimeIndex(['2014-02-12', '2014-02-13',\n...                                            '2014-02-15']))\n  \n>>> df2\n            temp_celsius windspeed\n2014-02-12          28.0       low\n2014-02-13          30.0       low\n2014-02-15          35.1    medium\n  \n>>> df2.reindex_like(df1)\n            temp_celsius  temp_fahrenheit windspeed\n2014-02-12          28.0              NaN       low\n2014-02-13          30.0              NaN       low\n2014-02-14           NaN              NaN       NaN\n2014-02-15          35.1              NaN    medium","title":"pandas.reference.api.pandas.series.reindex_like"},{"text":"pandas.Series.reindex   Series.reindex(*args, **kwargs)[source]\n \nConform Series to new index with optional filling logic. Places NA\/NaN in locations having no value in the previous index. A new object is produced unless the new index is equivalent to the current one and copy=False.  Parameters \n \nindex:array-like, optional\n\n\nNew labels \/ index to conform to, should be specified using keywords. Preferably an Index object to avoid duplicating data.  \nmethod:{None, \u2018backfill\u2019\/\u2019bfill\u2019, \u2018pad\u2019\/\u2019ffill\u2019, \u2018nearest\u2019}\n\n\nMethod to use for filling holes in reindexed DataFrame. Please note: this is only applicable to DataFrames\/Series with a monotonically increasing\/decreasing index.  None (default): don\u2019t fill gaps pad \/ ffill: Propagate last valid observation forward to next valid. backfill \/ bfill: Use next valid observation to fill gap. nearest: Use nearest valid observations to fill gap.   \ncopy:bool, default True\n\n\nReturn a new object, even if the passed indexes are the same.  \nlevel:int or name\n\n\nBroadcast across a level, matching Index values on the passed MultiIndex level.  \nfill_value:scalar, default np.NaN\n\n\nValue to use for missing values. Defaults to NaN, but can be any \u201ccompatible\u201d value.  \nlimit:int, default None\n\n\nMaximum number of consecutive elements to forward or backward fill.  \ntolerance:optional\n\n\nMaximum distance between original and new labels for inexact matches. The values of the index at the matching locations most satisfy the equation abs(index[indexer] - target) <= tolerance. Tolerance may be a scalar value, which applies the same tolerance to all values, or list-like, which applies variable tolerance per element. List-like includes list, tuple, array, Series, and must be the same size as the index and its dtype must exactly match the index\u2019s type.    Returns \n Series with changed index.\n    See also  DataFrame.set_index\n\nSet row labels.  DataFrame.reset_index\n\nRemove row labels or move them to new columns.  DataFrame.reindex_like\n\nChange to same indices as other DataFrame.    Examples DataFrame.reindex supports two calling conventions  (index=index_labels, columns=column_labels, ...) (labels, axis={'index', 'columns'}, ...)  We highly recommend using keyword arguments to clarify your intent. Create a dataframe with some fictional data. \n>>> index = ['Firefox', 'Chrome', 'Safari', 'IE10', 'Konqueror']\n>>> df = pd.DataFrame({'http_status': [200, 200, 404, 404, 301],\n...                   'response_time': [0.04, 0.02, 0.07, 0.08, 1.0]},\n...                   index=index)\n>>> df\n           http_status  response_time\nFirefox            200           0.04\nChrome             200           0.02\nSafari             404           0.07\nIE10               404           0.08\nKonqueror          301           1.00\n  Create a new index and reindex the dataframe. By default values in the new index that do not have corresponding records in the dataframe are assigned NaN. \n>>> new_index = ['Safari', 'Iceweasel', 'Comodo Dragon', 'IE10',\n...              'Chrome']\n>>> df.reindex(new_index)\n               http_status  response_time\nSafari               404.0           0.07\nIceweasel              NaN            NaN\nComodo Dragon          NaN            NaN\nIE10                 404.0           0.08\nChrome               200.0           0.02\n  We can fill in the missing values by passing a value to the keyword fill_value. Because the index is not monotonically increasing or decreasing, we cannot use arguments to the keyword method to fill the NaN values. \n>>> df.reindex(new_index, fill_value=0)\n               http_status  response_time\nSafari                 404           0.07\nIceweasel                0           0.00\nComodo Dragon            0           0.00\nIE10                   404           0.08\nChrome                 200           0.02\n  \n>>> df.reindex(new_index, fill_value='missing')\n              http_status response_time\nSafari                404          0.07\nIceweasel         missing       missing\nComodo Dragon     missing       missing\nIE10                  404          0.08\nChrome                200          0.02\n  We can also reindex the columns. \n>>> df.reindex(columns=['http_status', 'user_agent'])\n           http_status  user_agent\nFirefox            200         NaN\nChrome             200         NaN\nSafari             404         NaN\nIE10               404         NaN\nKonqueror          301         NaN\n  Or we can use \u201caxis-style\u201d keyword arguments \n>>> df.reindex(['http_status', 'user_agent'], axis=\"columns\")\n           http_status  user_agent\nFirefox            200         NaN\nChrome             200         NaN\nSafari             404         NaN\nIE10               404         NaN\nKonqueror          301         NaN\n  To further illustrate the filling functionality in reindex, we will create a dataframe with a monotonically increasing index (for example, a sequence of dates). \n>>> date_index = pd.date_range('1\/1\/2010', periods=6, freq='D')\n>>> df2 = pd.DataFrame({\"prices\": [100, 101, np.nan, 100, 89, 88]},\n...                    index=date_index)\n>>> df2\n            prices\n2010-01-01   100.0\n2010-01-02   101.0\n2010-01-03     NaN\n2010-01-04   100.0\n2010-01-05    89.0\n2010-01-06    88.0\n  Suppose we decide to expand the dataframe to cover a wider date range. \n>>> date_index2 = pd.date_range('12\/29\/2009', periods=10, freq='D')\n>>> df2.reindex(date_index2)\n            prices\n2009-12-29     NaN\n2009-12-30     NaN\n2009-12-31     NaN\n2010-01-01   100.0\n2010-01-02   101.0\n2010-01-03     NaN\n2010-01-04   100.0\n2010-01-05    89.0\n2010-01-06    88.0\n2010-01-07     NaN\n  The index entries that did not have a value in the original data frame (for example, \u20182009-12-29\u2019) are by default filled with NaN. If desired, we can fill in the missing values using one of several options. For example, to back-propagate the last valid value to fill the NaN values, pass bfill as an argument to the method keyword. \n>>> df2.reindex(date_index2, method='bfill')\n            prices\n2009-12-29   100.0\n2009-12-30   100.0\n2009-12-31   100.0\n2010-01-01   100.0\n2010-01-02   101.0\n2010-01-03     NaN\n2010-01-04   100.0\n2010-01-05    89.0\n2010-01-06    88.0\n2010-01-07     NaN\n  Please note that the NaN value present in the original dataframe (at index value 2010-01-03) will not be filled by any of the value propagation schemes. This is because filling while reindexing does not look at dataframe values, but only compares the original and desired indexes. If you do want to fill in the NaN values present in the original dataframe, use the fillna() method. See the user guide for more.","title":"pandas.reference.api.pandas.series.reindex"},{"text":"pandas.DataFrame.sort_index   DataFrame.sort_index(axis=0, level=None, ascending=True, inplace=False, kind='quicksort', na_position='last', sort_remaining=True, ignore_index=False, key=None)[source]\n \nSort object by labels (along an axis). Returns a new DataFrame sorted by label if inplace argument is False, otherwise updates the original DataFrame and returns None.  Parameters \n \naxis:{0 or \u2018index\u2019, 1 or \u2018columns\u2019}, default 0\n\n\nThe axis along which to sort. The value 0 identifies the rows, and 1 identifies the columns.  \nlevel:int or level name or list of ints or list of level names\n\n\nIf not None, sort on values in specified index level(s).  \nascending:bool or list-like of bools, default True\n\n\nSort ascending vs. descending. When the index is a MultiIndex the sort direction can be controlled for each level individually.  \ninplace:bool, default False\n\n\nIf True, perform operation in-place.  \nkind:{\u2018quicksort\u2019, \u2018mergesort\u2019, \u2018heapsort\u2019, \u2018stable\u2019}, default \u2018quicksort\u2019\n\n\nChoice of sorting algorithm. See also numpy.sort() for more information. mergesort and stable are the only stable algorithms. For DataFrames, this option is only applied when sorting on a single column or label.  \nna_position:{\u2018first\u2019, \u2018last\u2019}, default \u2018last\u2019\n\n\nPuts NaNs at the beginning if first; last puts NaNs at the end. Not implemented for MultiIndex.  \nsort_remaining:bool, default True\n\n\nIf True and sorting by level and index is multilevel, sort by other levels too (in order) after sorting by specified level.  \nignore_index:bool, default False\n\n\nIf True, the resulting axis will be labeled 0, 1, \u2026, n - 1.  New in version 1.0.0.   \nkey:callable, optional\n\n\nIf not None, apply the key function to the index values before sorting. This is similar to the key argument in the builtin sorted() function, with the notable difference that this key function should be vectorized. It should expect an Index and return an Index of the same shape. For MultiIndex inputs, the key is applied per level.  New in version 1.1.0.     Returns \n DataFrame or None\n\nThe original DataFrame sorted by the labels or None if inplace=True.      See also  Series.sort_index\n\nSort Series by the index.  DataFrame.sort_values\n\nSort DataFrame by the value.  Series.sort_values\n\nSort Series by the value.    Examples \n>>> df = pd.DataFrame([1, 2, 3, 4, 5], index=[100, 29, 234, 1, 150],\n...                   columns=['A'])\n>>> df.sort_index()\n     A\n1    4\n29   2\n100  1\n150  5\n234  3\n  By default, it sorts in ascending order, to sort in descending order, use ascending=False \n>>> df.sort_index(ascending=False)\n     A\n234  3\n150  5\n100  1\n29   2\n1    4\n  A key function can be specified which is applied to the index before sorting. For a MultiIndex this is applied to each level separately. \n>>> df = pd.DataFrame({\"a\": [1, 2, 3, 4]}, index=['A', 'b', 'C', 'd'])\n>>> df.sort_index(key=lambda x: x.str.lower())\n   a\nA  1\nb  2\nC  3\nd  4","title":"pandas.reference.api.pandas.dataframe.sort_index"},{"text":"BaseDatabaseSchemaEditor.add_index(model, index)","title":"django.ref.schema-editor#django.db.backends.base.schema.BaseDatabaseSchemaEditor.add_index"},{"text":"BaseDatabaseSchemaEditor.alter_index_together(model, old_index_together, new_index_together)","title":"django.ref.schema-editor#django.db.backends.base.schema.BaseDatabaseSchemaEditor.alter_index_together"},{"text":"index_put(tensor1, indices, values, accumulate=False) \u2192 Tensor  \nOut-place version of index_put_(). tensor1 corresponds to self in torch.Tensor.index_put_().","title":"torch.tensors#torch.Tensor.index_put"},{"text":"pandas.MultiIndex.from_product   classmethodMultiIndex.from_product(iterables, sortorder=None, names=NoDefault.no_default)[source]\n \nMake a MultiIndex from the cartesian product of multiple iterables.  Parameters \n \niterables:list \/ sequence of iterables\n\n\nEach iterable has unique labels for each level of the index.  \nsortorder:int or None\n\n\nLevel of sortedness (must be lexicographically sorted by that level).  \nnames:list \/ sequence of str, optional\n\n\nNames for the levels in the index.  Changed in version 1.0.0: If not explicitly provided, names will be inferred from the elements of iterables if an element has a name attribute     Returns \n MultiIndex\n    See also  MultiIndex.from_arrays\n\nConvert list of arrays to MultiIndex.  MultiIndex.from_tuples\n\nConvert list of tuples to MultiIndex.  MultiIndex.from_frame\n\nMake a MultiIndex from a DataFrame.    Examples \n>>> numbers = [0, 1, 2]\n>>> colors = ['green', 'purple']\n>>> pd.MultiIndex.from_product([numbers, colors],\n...                            names=['number', 'color'])\nMultiIndex([(0,  'green'),\n            (0, 'purple'),\n            (1,  'green'),\n            (1, 'purple'),\n            (2,  'green'),\n            (2, 'purple')],\n           names=['number', 'color'])","title":"pandas.reference.api.pandas.multiindex.from_product"}]}
{"task_id":20461165,"prompt":"def f_20461165(df):\n\treturn ","suffix":"","canonical_solution":"df.reset_index(level=['tick', 'obs'])","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    df = pd.DataFrame([['2016-09-13', 'C', 2, 0.0139], ['2016-07-17', 'A', 2, 0.5577]], columns = ['tick', 'tag', 'obs', 'val'])\n    df = df.set_index(['tick', 'tag', 'obs'])\n    df = candidate(df)\n    assert df['tick']['C'] == '2016-09-13'\n"],"entry_point":"f_20461165","intent":"convert pandas index in a dataframe `df` to columns","library":["pandas"],"docs":[{"text":"pandas.DataFrame.index   DataFrame.index\n \nThe index (row labels) of the DataFrame.","title":"pandas.reference.api.pandas.dataframe.index"},{"text":"pandas.MultiIndex.from_frame   classmethodMultiIndex.from_frame(df, sortorder=None, names=None)[source]\n \nMake a MultiIndex from a DataFrame.  Parameters \n \ndf:DataFrame\n\n\nDataFrame to be converted to MultiIndex.  \nsortorder:int, optional\n\n\nLevel of sortedness (must be lexicographically sorted by that level).  \nnames:list-like, optional\n\n\nIf no names are provided, use the column names, or tuple of column names if the columns is a MultiIndex. If a sequence, overwrite names with the given sequence.    Returns \n MultiIndex\n\nThe MultiIndex representation of the given DataFrame.      See also  MultiIndex.from_arrays\n\nConvert list of arrays to MultiIndex.  MultiIndex.from_tuples\n\nConvert list of tuples to MultiIndex.  MultiIndex.from_product\n\nMake a MultiIndex from cartesian product of iterables.    Examples \n>>> df = pd.DataFrame([['HI', 'Temp'], ['HI', 'Precip'],\n...                    ['NJ', 'Temp'], ['NJ', 'Precip']],\n...                   columns=['a', 'b'])\n>>> df\n      a       b\n0    HI    Temp\n1    HI  Precip\n2    NJ    Temp\n3    NJ  Precip\n  \n>>> pd.MultiIndex.from_frame(df)\nMultiIndex([('HI',   'Temp'),\n            ('HI', 'Precip'),\n            ('NJ',   'Temp'),\n            ('NJ', 'Precip')],\n           names=['a', 'b'])\n  Using explicit names, instead of the column names \n>>> pd.MultiIndex.from_frame(df, names=['state', 'observation'])\nMultiIndex([('HI',   'Temp'),\n            ('HI', 'Precip'),\n            ('NJ',   'Temp'),\n            ('NJ', 'Precip')],\n           names=['state', 'observation'])","title":"pandas.reference.api.pandas.multiindex.from_frame"},{"text":"pandas.Index.to_frame   Index.to_frame(index=True, name=NoDefault.no_default)[source]\n \nCreate a DataFrame with a column containing the Index.  Parameters \n \nindex:bool, default True\n\n\nSet the index of the returned DataFrame as the original Index.  \nname:object, default None\n\n\nThe passed name should substitute for the index name (if it has one).    Returns \n DataFrame\n\nDataFrame containing the original Index data.      See also  Index.to_series\n\nConvert an Index to a Series.  Series.to_frame\n\nConvert Series to DataFrame.    Examples \n>>> idx = pd.Index(['Ant', 'Bear', 'Cow'], name='animal')\n>>> idx.to_frame()\n       animal\nanimal\nAnt       Ant\nBear     Bear\nCow       Cow\n  By default, the original Index is reused. To enforce a new Index: \n>>> idx.to_frame(index=False)\n    animal\n0   Ant\n1  Bear\n2   Cow\n  To override the name of the resulting column, specify name: \n>>> idx.to_frame(index=False, name='zoo')\n    zoo\n0   Ant\n1  Bear\n2   Cow","title":"pandas.reference.api.pandas.index.to_frame"},{"text":"pandas.DatetimeIndex.to_frame   DatetimeIndex.to_frame(index=True, name=NoDefault.no_default)[source]\n \nCreate a DataFrame with a column containing the Index.  Parameters \n \nindex:bool, default True\n\n\nSet the index of the returned DataFrame as the original Index.  \nname:object, default None\n\n\nThe passed name should substitute for the index name (if it has one).    Returns \n DataFrame\n\nDataFrame containing the original Index data.      See also  Index.to_series\n\nConvert an Index to a Series.  Series.to_frame\n\nConvert Series to DataFrame.    Examples \n>>> idx = pd.Index(['Ant', 'Bear', 'Cow'], name='animal')\n>>> idx.to_frame()\n       animal\nanimal\nAnt       Ant\nBear     Bear\nCow       Cow\n  By default, the original Index is reused. To enforce a new Index: \n>>> idx.to_frame(index=False)\n    animal\n0   Ant\n1  Bear\n2   Cow\n  To override the name of the resulting column, specify name: \n>>> idx.to_frame(index=False, name='zoo')\n    zoo\n0   Ant\n1  Bear\n2   Cow","title":"pandas.reference.api.pandas.datetimeindex.to_frame"},{"text":"pandas.Series.keys   Series.keys()[source]\n \nReturn alias for index.  Returns \n Index\n\nIndex of the Series.","title":"pandas.reference.api.pandas.series.keys"},{"text":"pandas.TimedeltaIndex.to_frame   TimedeltaIndex.to_frame(index=True, name=NoDefault.no_default)[source]\n \nCreate a DataFrame with a column containing the Index.  Parameters \n \nindex:bool, default True\n\n\nSet the index of the returned DataFrame as the original Index.  \nname:object, default None\n\n\nThe passed name should substitute for the index name (if it has one).    Returns \n DataFrame\n\nDataFrame containing the original Index data.      See also  Index.to_series\n\nConvert an Index to a Series.  Series.to_frame\n\nConvert Series to DataFrame.    Examples \n>>> idx = pd.Index(['Ant', 'Bear', 'Cow'], name='animal')\n>>> idx.to_frame()\n       animal\nanimal\nAnt       Ant\nBear     Bear\nCow       Cow\n  By default, the original Index is reused. To enforce a new Index: \n>>> idx.to_frame(index=False)\n    animal\n0   Ant\n1  Bear\n2   Cow\n  To override the name of the resulting column, specify name: \n>>> idx.to_frame(index=False, name='zoo')\n    zoo\n0   Ant\n1  Bear\n2   Cow","title":"pandas.reference.api.pandas.timedeltaindex.to_frame"},{"text":"pandas.DataFrame.keys   DataFrame.keys()[source]\n \nGet the \u2018info axis\u2019 (see Indexing for more). This is index for Series, columns for DataFrame.  Returns \n Index\n\nInfo axis.","title":"pandas.reference.api.pandas.dataframe.keys"},{"text":"pandas.Index.to_series   Index.to_series(index=None, name=None)[source]\n \nCreate a Series with both index and values equal to the index keys. Useful with map for returning an indexer based on an index.  Parameters \n \nindex:Index, optional\n\n\nIndex of resulting Series. If None, defaults to original index.  \nname:str, optional\n\n\nName of resulting Series. If None, defaults to name of original index.    Returns \n Series\n\nThe dtype will be based on the type of the Index values.      See also  Index.to_frame\n\nConvert an Index to a DataFrame.  Series.to_frame\n\nConvert Series to DataFrame.    Examples \n>>> idx = pd.Index(['Ant', 'Bear', 'Cow'], name='animal')\n  By default, the original Index and original name is reused. \n>>> idx.to_series()\nanimal\nAnt      Ant\nBear    Bear\nCow      Cow\nName: animal, dtype: object\n  To enforce a new Index, specify new labels to index: \n>>> idx.to_series(index=[0, 1, 2])\n0     Ant\n1    Bear\n2     Cow\nName: animal, dtype: object\n  To override the name of the resulting column, specify name: \n>>> idx.to_series(name='zoo')\nanimal\nAnt      Ant\nBear    Bear\nCow      Cow\nName: zoo, dtype: object","title":"pandas.reference.api.pandas.index.to_series"},{"text":"pandas.TimedeltaIndex.to_series   TimedeltaIndex.to_series(index=None, name=None)[source]\n \nCreate a Series with both index and values equal to the index keys. Useful with map for returning an indexer based on an index.  Parameters \n \nindex:Index, optional\n\n\nIndex of resulting Series. If None, defaults to original index.  \nname:str, optional\n\n\nName of resulting Series. If None, defaults to name of original index.    Returns \n Series\n\nThe dtype will be based on the type of the Index values.      See also  Index.to_frame\n\nConvert an Index to a DataFrame.  Series.to_frame\n\nConvert Series to DataFrame.    Examples \n>>> idx = pd.Index(['Ant', 'Bear', 'Cow'], name='animal')\n  By default, the original Index and original name is reused. \n>>> idx.to_series()\nanimal\nAnt      Ant\nBear    Bear\nCow      Cow\nName: animal, dtype: object\n  To enforce a new Index, specify new labels to index: \n>>> idx.to_series(index=[0, 1, 2])\n0     Ant\n1    Bear\n2     Cow\nName: animal, dtype: object\n  To override the name of the resulting column, specify name: \n>>> idx.to_series(name='zoo')\nanimal\nAnt      Ant\nBear    Bear\nCow      Cow\nName: zoo, dtype: object","title":"pandas.reference.api.pandas.timedeltaindex.to_series"},{"text":"pandas.Series.index   Series.index\n \nThe index (axis labels) of the Series.","title":"pandas.reference.api.pandas.series.index"}]}
{"task_id":4685571,"prompt":"def f_4685571(b):\n\treturn ","suffix":"","canonical_solution":"[x[::-1] for x in b]","test_start":"\ndef check(candidate):","test":["\n    b = [('spam',0), ('eggs',1)]\n    b1 = candidate(b)\n    assert b1 == [(0, 'spam'), (1, 'eggs')]\n"],"entry_point":"f_4685571","intent":"Get reverse of list items from list 'b' using extended slicing","library":[],"docs":[]}
{"task_id":17960441,"prompt":"def f_17960441(a, b):\n\treturn ","suffix":"","canonical_solution":"np.array([zip(x, y) for x, y in zip(a, b)])","test_start":"\nimport numpy as np\n\ndef check(candidate):","test":["\n    a = np.array([[9, 8], [7, 6]])\n    b = np.array([[7, 1], [5, 2]])\n    c = candidate(a, b)\n    expected = [(9, 7), (8, 1)]\n    ctr = 0\n    for i in c[0]:\n        assert i == expected[ctr]\n        ctr += 1\n"],"entry_point":"f_17960441","intent":"join each element in array `a` with element at the same index in array `b` as a tuple","library":["numpy"],"docs":[{"text":"numpy.lib.recfunctions.rec_join(key, r1, r2, jointype='inner', r1postfix='1', r2postfix='2', defaults=None)[source]\n \nJoin arrays r1 and r2 on keys. Alternative to join_by, that always returns a np.recarray.  See also  join_by\n\nequivalent function","title":"numpy.user.basics.rec#numpy.lib.recfunctions.rec_join"},{"text":"zip(*iterables)  \nMake an iterator that aggregates elements from each of the iterables. Returns an iterator of tuples, where the i-th tuple contains the i-th element from each of the argument sequences or iterables. The iterator stops when the shortest input iterable is exhausted. With a single iterable argument, it returns an iterator of 1-tuples. With no arguments, it returns an empty iterator. Equivalent to: def zip(*iterables):\n    # zip('ABCD', 'xy') --> Ax By\n    sentinel = object()\n    iterators = [iter(it) for it in iterables]\n    while iterators:\n        result = []\n        for it in iterators:\n            elem = next(it, sentinel)\n            if elem is sentinel:\n                return\n            result.append(elem)\n        yield tuple(result)\n The left-to-right evaluation order of the iterables is guaranteed. This makes possible an idiom for clustering a data series into n-length groups using zip(*[iter(s)]*n). This repeats the same iterator n times so that each output tuple has the result of n calls to the iterator. This has the effect of dividing the input into n-length chunks. zip() should only be used with unequal length inputs when you don\u2019t care about trailing, unmatched values from the longer iterables. If those values are important, use itertools.zip_longest() instead. zip() in conjunction with the * operator can be used to unzip a list: >>> x = [1, 2, 3]\n>>> y = [4, 5, 6]\n>>> zipped = zip(x, y)\n>>> list(zipped)\n[(1, 4), (2, 5), (3, 6)]\n>>> x2, y2 = zip(*zip(x, y))\n>>> x == list(x2) and y == list(y2)\nTrue","title":"python.library.functions#zip"},{"text":"itertools.zip_longest(*iterables, fillvalue=None)  \nMake an iterator that aggregates elements from each of the iterables. If the iterables are of uneven length, missing values are filled-in with fillvalue. Iteration continues until the longest iterable is exhausted. Roughly equivalent to: def zip_longest(*args, fillvalue=None):\n    # zip_longest('ABCD', 'xy', fillvalue='-') --> Ax By C- D-\n    iterators = [iter(it) for it in args]\n    num_active = len(iterators)\n    if not num_active:\n        return\n    while True:\n        values = []\n        for i, it in enumerate(iterators):\n            try:\n                value = next(it)\n            except StopIteration:\n                num_active -= 1\n                if not num_active:\n                    return\n                iterators[i] = repeat(fillvalue)\n                value = fillvalue\n            values.append(value)\n        yield tuple(values)\n If one of the iterables is potentially infinite, then the zip_longest() function should be wrapped with something that limits the number of calls (for example islice() or takewhile()). If not specified, fillvalue defaults to None.","title":"python.library.itertools#itertools.zip_longest"},{"text":"numpy.column_stack   numpy.column_stack(tup)[source]\n \nStack 1-D arrays as columns into a 2-D array. Take a sequence of 1-D arrays and stack them as columns to make a single 2-D array. 2-D arrays are stacked as-is, just like with hstack. 1-D arrays are turned into 2-D columns first.  Parameters \n \ntupsequence of 1-D or 2-D arrays.\n\n\nArrays to stack. All of them must have the same first dimension.    Returns \n \nstacked2-D array\n\n\nThe array formed by stacking the given arrays.      See also  \nstack, hstack, vstack, concatenate\n\n  Examples >>> a = np.array((1,2,3))\n>>> b = np.array((2,3,4))\n>>> np.column_stack((a,b))\narray([[1, 2],\n       [2, 3],\n       [3, 4]])","title":"numpy.reference.generated.numpy.column_stack"},{"text":"join(a, *args)[source]\n \nJoin given arguments into the same set. Accepts one or more arguments.","title":"matplotlib.cbook_api#matplotlib.cbook.Grouper.join"},{"text":"numpy.distutils.misc_util.dot_join(*args)[source]","title":"numpy.reference.distutils.misc_util#numpy.distutils.misc_util.dot_join"},{"text":"numpy.lib.recfunctions.join_by(key, r1, r2, jointype='inner', r1postfix='1', r2postfix='2', defaults=None, usemask=True, asrecarray=False)[source]\n \nJoin arrays r1 and r2 on key key. The key should be either a string or a sequence of string corresponding to the fields used to join the array. An exception is raised if the key field cannot be found in the two input arrays. Neither r1 nor r2 should have any duplicates along key: the presence of duplicates will make the output quite unreliable. Note that duplicates are not looked for by the algorithm.  Parameters \n \nkey{string, sequence}\n\n\nA string or a sequence of strings corresponding to the fields used for comparison.  \nr1, r2arrays\n\n\nStructured arrays.  \njointype{\u2018inner\u2019, \u2018outer\u2019, \u2018leftouter\u2019}, optional\n\n\nIf \u2018inner\u2019, returns the elements common to both r1 and r2. If \u2018outer\u2019, returns the common elements as well as the elements of r1 not in r2 and the elements of not in r2. If \u2018leftouter\u2019, returns the common elements and the elements of r1 not in r2.  \nr1postfixstring, optional\n\n\nString appended to the names of the fields of r1 that are present in r2 but absent of the key.  \nr2postfixstring, optional\n\n\nString appended to the names of the fields of r2 that are present in r1 but absent of the key.  \ndefaults{dictionary}, optional\n\n\nDictionary mapping field names to the corresponding default values.  \nusemask{True, False}, optional\n\n\nWhether to return a MaskedArray (or MaskedRecords is asrecarray==True) or a ndarray.  \nasrecarray{False, True}, optional\n\n\nWhether to return a recarray (or MaskedRecords if usemask==True) or just a flexible-type ndarray.     Notes  The output is sorted along the key. A temporary array is formed by dropping the fields not in the key for the two arrays and concatenating the result. This array is then sorted, and the common entries selected. The output is constructed by filling the fields with the selected entries. Matching is not preserved if there are some duplicates\u2026","title":"numpy.user.basics.rec#numpy.lib.recfunctions.join_by"},{"text":"classmatplotlib.cbook.Grouper(init=())[source]\n \nBases: object A disjoint-set data structure. Objects can be joined using join(), tested for connectedness using joined(), and all disjoint sets can be retrieved by using the object as an iterator. The objects being joined must be hashable and weak-referenceable. Examples >>> from matplotlib.cbook import Grouper\n>>> class Foo:\n...     def __init__(self, s):\n...         self.s = s\n...     def __repr__(self):\n...         return self.s\n...\n>>> a, b, c, d, e, f = [Foo(x) for x in 'abcdef']\n>>> grp = Grouper()\n>>> grp.join(a, b)\n>>> grp.join(b, c)\n>>> grp.join(d, e)\n>>> list(grp)\n[[a, b, c], [d, e]]\n>>> grp.joined(a, b)\nTrue\n>>> grp.joined(a, c)\nTrue\n>>> grp.joined(a, d)\nFalse\n   clean()[source]\n \nClean dead weak references from the dictionary. \n   get_siblings(a)[source]\n \nReturn all of the items joined with a, including itself. \n   join(a, *args)[source]\n \nJoin given arguments into the same set. Accepts one or more arguments. \n   joined(a, b)[source]\n \nReturn whether a and b are members of the same set. \n   remove(a)[source]","title":"matplotlib.cbook_api#matplotlib.cbook.Grouper"},{"text":"numpy.hstack   numpy.hstack(tup)[source]\n \nStack arrays in sequence horizontally (column wise). This is equivalent to concatenation along the second axis, except for 1-D arrays where it concatenates along the first axis. Rebuilds arrays divided by hsplit. This function makes most sense for arrays with up to 3 dimensions. For instance, for pixel-data with a height (first axis), width (second axis), and r\/g\/b channels (third axis). The functions concatenate, stack and block provide more general stacking and concatenation operations.  Parameters \n \ntupsequence of ndarrays\n\n\nThe arrays must have the same shape along all but the second axis, except 1-D arrays which can be any length.    Returns \n \nstackedndarray\n\n\nThe array formed by stacking the given arrays.      See also  concatenate\n\nJoin a sequence of arrays along an existing axis.  stack\n\nJoin a sequence of arrays along a new axis.  block\n\nAssemble an nd-array from nested lists of blocks.  vstack\n\nStack arrays in sequence vertically (row wise).  dstack\n\nStack arrays in sequence depth wise (along third axis).  column_stack\n\nStack 1-D arrays as columns into a 2-D array.  hsplit\n\nSplit an array into multiple sub-arrays horizontally (column-wise).    Examples >>> a = np.array((1,2,3))\n>>> b = np.array((4,5,6))\n>>> np.hstack((a,b))\narray([1, 2, 3, 4, 5, 6])\n>>> a = np.array([[1],[2],[3]])\n>>> b = np.array([[4],[5],[6]])\n>>> np.hstack((a,b))\narray([[1, 4],\n       [2, 5],\n       [3, 6]])","title":"numpy.reference.generated.numpy.hstack"},{"text":"numpy.row_stack   numpy.row_stack(tup)[source]\n \nStack arrays in sequence vertically (row wise). This is equivalent to concatenation along the first axis after 1-D arrays of shape (N,) have been reshaped to (1,N). Rebuilds arrays divided by vsplit. This function makes most sense for arrays with up to 3 dimensions. For instance, for pixel-data with a height (first axis), width (second axis), and r\/g\/b channels (third axis). The functions concatenate, stack and block provide more general stacking and concatenation operations.  Parameters \n \ntupsequence of ndarrays\n\n\nThe arrays must have the same shape along all but the first axis. 1-D arrays must have the same length.    Returns \n \nstackedndarray\n\n\nThe array formed by stacking the given arrays, will be at least 2-D.      See also  concatenate\n\nJoin a sequence of arrays along an existing axis.  stack\n\nJoin a sequence of arrays along a new axis.  block\n\nAssemble an nd-array from nested lists of blocks.  hstack\n\nStack arrays in sequence horizontally (column wise).  dstack\n\nStack arrays in sequence depth wise (along third axis).  column_stack\n\nStack 1-D arrays as columns into a 2-D array.  vsplit\n\nSplit an array into multiple sub-arrays vertically (row-wise).    Examples >>> a = np.array([1, 2, 3])\n>>> b = np.array([4, 5, 6])\n>>> np.vstack((a,b))\narray([[1, 2, 3],\n       [4, 5, 6]])\n >>> a = np.array([[1], [2], [3]])\n>>> b = np.array([[4], [5], [6]])\n>>> np.vstack((a,b))\narray([[1],\n       [2],\n       [3],\n       [4],\n       [5],\n       [6]])","title":"numpy.reference.generated.numpy.row_stack"}]}
{"task_id":17960441,"prompt":"def f_17960441(a, b):\n\treturn ","suffix":"","canonical_solution":"np.array(list(zip(a.ravel(),b.ravel())), dtype=('i4,i4')).reshape(a.shape)","test_start":"\nimport numpy as np\n\ndef check(candidate):","test":["\n    a = np.array([[9, 8], [7, 6]])\n    b = np.array([[7, 1], [5, 2]])\n    c = candidate(a, b)\n    e = np.array([[(9, 7), (8, 1)], [(7, 5), (6, 2)]], dtype=[('f0', '<i4'), ('f1', '<i4')])\n    assert np.array_equal(c, e)\n"],"entry_point":"f_17960441","intent":"zip two 2-d arrays `a` and `b`","library":["numpy"],"docs":[{"text":"numpy.column_stack   numpy.column_stack(tup)[source]\n \nStack 1-D arrays as columns into a 2-D array. Take a sequence of 1-D arrays and stack them as columns to make a single 2-D array. 2-D arrays are stacked as-is, just like with hstack. 1-D arrays are turned into 2-D columns first.  Parameters \n \ntupsequence of 1-D or 2-D arrays.\n\n\nArrays to stack. All of them must have the same first dimension.    Returns \n \nstacked2-D array\n\n\nThe array formed by stacking the given arrays.      See also  \nstack, hstack, vstack, concatenate\n\n  Examples >>> a = np.array((1,2,3))\n>>> b = np.array((2,3,4))\n>>> np.column_stack((a,b))\narray([[1, 2],\n       [2, 3],\n       [3, 4]])","title":"numpy.reference.generated.numpy.column_stack"},{"text":"numpy.ma.column_stack   ma.column_stack(*args, **kwargs) = <numpy.ma.extras._fromnxfunction_seq object>\n \nStack 1-D arrays as columns into a 2-D array. Take a sequence of 1-D arrays and stack them as columns to make a single 2-D array. 2-D arrays are stacked as-is, just like with hstack. 1-D arrays are turned into 2-D columns first.  Parameters \n \ntupsequence of 1-D or 2-D arrays.\n\n\nArrays to stack. All of them must have the same first dimension.    Returns \n \nstacked2-D array\n\n\nThe array formed by stacking the given arrays.      See also  \nstack, hstack, vstack, concatenate\n\n  Notes The function is applied to both the _data and the _mask, if any. Examples >>> a = np.array((1,2,3))\n>>> b = np.array((2,3,4))\n>>> np.column_stack((a,b))\narray([[1, 2],\n       [2, 3],\n       [3, 4]])","title":"numpy.reference.generated.numpy.ma.column_stack"},{"text":"numpy.atleast_2d   numpy.atleast_2d(*arys)[source]\n \nView inputs as arrays with at least two dimensions.  Parameters \n \narys1, arys2, \u2026array_like\n\n\nOne or more array-like sequences. Non-array inputs are converted to arrays. Arrays that already have two or more dimensions are preserved.    Returns \n \nres, res2, \u2026ndarray\n\n\nAn array, or list of arrays, each with a.ndim >= 2. Copies are avoided where possible, and views with two or more dimensions are returned.      See also  \natleast_1d, atleast_3d\n\n  Examples >>> np.atleast_2d(3.0)\narray([[3.]])\n >>> x = np.arange(3.0)\n>>> np.atleast_2d(x)\narray([[0., 1., 2.]])\n>>> np.atleast_2d(x).base is x\nTrue\n >>> np.atleast_2d(1, [1, 2], [[1, 2]])\n[array([[1]]), array([[1, 2]]), array([[1, 2]])]","title":"numpy.reference.generated.numpy.atleast_2d"},{"text":"numpy.c_   numpy.c_ = <numpy.lib.index_tricks.CClass object>\n \nTranslates slice objects to concatenation along the second axis. This is short-hand for np.r_['-1,2,0', index expression], which is useful because of its common occurrence. In particular, arrays will be stacked along their last axis after being upgraded to at least 2-D with 1\u2019s post-pended to the shape (column vectors made out of 1-D arrays).  See also  column_stack\n\nStack 1-D arrays as columns into a 2-D array.  r_\n\nFor more detailed documentation.    Examples >>> np.c_[np.array([1,2,3]), np.array([4,5,6])]\narray([[1, 4],\n       [2, 5],\n       [3, 6]])\n>>> np.c_[np.array([[1,2,3]]), 0, 0, np.array([[4,5,6]])]\narray([[1, 2, 3, ..., 4, 5, 6]])","title":"numpy.reference.generated.numpy.c_"},{"text":"numpy.hstack   numpy.hstack(tup)[source]\n \nStack arrays in sequence horizontally (column wise). This is equivalent to concatenation along the second axis, except for 1-D arrays where it concatenates along the first axis. Rebuilds arrays divided by hsplit. This function makes most sense for arrays with up to 3 dimensions. For instance, for pixel-data with a height (first axis), width (second axis), and r\/g\/b channels (third axis). The functions concatenate, stack and block provide more general stacking and concatenation operations.  Parameters \n \ntupsequence of ndarrays\n\n\nThe arrays must have the same shape along all but the second axis, except 1-D arrays which can be any length.    Returns \n \nstackedndarray\n\n\nThe array formed by stacking the given arrays.      See also  concatenate\n\nJoin a sequence of arrays along an existing axis.  stack\n\nJoin a sequence of arrays along a new axis.  block\n\nAssemble an nd-array from nested lists of blocks.  vstack\n\nStack arrays in sequence vertically (row wise).  dstack\n\nStack arrays in sequence depth wise (along third axis).  column_stack\n\nStack 1-D arrays as columns into a 2-D array.  hsplit\n\nSplit an array into multiple sub-arrays horizontally (column-wise).    Examples >>> a = np.array((1,2,3))\n>>> b = np.array((4,5,6))\n>>> np.hstack((a,b))\narray([1, 2, 3, 4, 5, 6])\n>>> a = np.array([[1],[2],[3]])\n>>> b = np.array([[4],[5],[6]])\n>>> np.hstack((a,b))\narray([[1, 4],\n       [2, 5],\n       [3, 6]])","title":"numpy.reference.generated.numpy.hstack"},{"text":"numpy.ma.append   ma.append(a, b, axis=None)[source]\n \nAppend values to the end of an array.  New in version 1.9.0.   Parameters \n \naarray_like\n\n\nValues are appended to a copy of this array.  \nbarray_like\n\n\nThese values are appended to a copy of a. It must be of the correct shape (the same shape as a, excluding axis). If axis is not specified, b can be any shape and will be flattened before use.  \naxisint, optional\n\n\nThe axis along which v are appended. If axis is not given, both a and b are flattened before use.    Returns \n \nappendMaskedArray\n\n\nA copy of a with b appended to axis. Note that append does not occur in-place: a new array is allocated and filled. If axis is None, the result is a flattened array.      See also  numpy.append\n\nEquivalent function in the top-level NumPy module.    Examples >>> import numpy.ma as ma\n>>> a = ma.masked_values([1, 2, 3], 2)\n>>> b = ma.masked_values([[4, 5, 6], [7, 8, 9]], 7)\n>>> ma.append(a, b)\nmasked_array(data=[1, --, 3, 4, 5, 6, --, 8, 9],\n             mask=[False,  True, False, False, False, False,  True, False,\n                   False],\n       fill_value=999999)","title":"numpy.reference.generated.numpy.ma.append"},{"text":"numpy.dstack   numpy.dstack(tup)[source]\n \nStack arrays in sequence depth wise (along third axis). This is equivalent to concatenation along the third axis after 2-D arrays of shape (M,N) have been reshaped to (M,N,1) and 1-D arrays of shape (N,) have been reshaped to (1,N,1). Rebuilds arrays divided by dsplit. This function makes most sense for arrays with up to 3 dimensions. For instance, for pixel-data with a height (first axis), width (second axis), and r\/g\/b channels (third axis). The functions concatenate, stack and block provide more general stacking and concatenation operations.  Parameters \n \ntupsequence of arrays\n\n\nThe arrays must have the same shape along all but the third axis. 1-D or 2-D arrays must have the same shape.    Returns \n \nstackedndarray\n\n\nThe array formed by stacking the given arrays, will be at least 3-D.      See also  concatenate\n\nJoin a sequence of arrays along an existing axis.  stack\n\nJoin a sequence of arrays along a new axis.  block\n\nAssemble an nd-array from nested lists of blocks.  vstack\n\nStack arrays in sequence vertically (row wise).  hstack\n\nStack arrays in sequence horizontally (column wise).  column_stack\n\nStack 1-D arrays as columns into a 2-D array.  dsplit\n\nSplit array along third axis.    Examples >>> a = np.array((1,2,3))\n>>> b = np.array((2,3,4))\n>>> np.dstack((a,b))\narray([[[1, 2],\n        [2, 3],\n        [3, 4]]])\n >>> a = np.array([[1],[2],[3]])\n>>> b = np.array([[2],[3],[4]])\n>>> np.dstack((a,b))\narray([[[1, 2]],\n       [[2, 3]],\n       [[3, 4]]])","title":"numpy.reference.generated.numpy.dstack"},{"text":"numpy.kron   numpy.kron(a, b)[source]\n \nKronecker product of two arrays. Computes the Kronecker product, a composite array made of blocks of the second array scaled by the first.  Parameters \n \na, barray_like\n\n  Returns \n \noutndarray\n\n    See also  outer\n\nThe outer product    Notes The function assumes that the number of dimensions of a and b are the same, if necessary prepending the smallest with ones. If a.shape = (r0,r1,..,rN) and b.shape = (s0,s1,...,sN), the Kronecker product has shape (r0*s0, r1*s1, ..., rN*SN). The elements are products of elements from a and b, organized explicitly by: kron(a,b)[k0,k1,...,kN] = a[i0,i1,...,iN] * b[j0,j1,...,jN]\n where: kt = it * st + jt,  t = 0,...,N\n In the common 2-D case (N=1), the block structure can be visualized: [[ a[0,0]*b,   a[0,1]*b,  ... , a[0,-1]*b  ],\n [  ...                              ...   ],\n [ a[-1,0]*b,  a[-1,1]*b, ... , a[-1,-1]*b ]]\n Examples >>> np.kron([1,10,100], [5,6,7])\narray([  5,   6,   7, ..., 500, 600, 700])\n>>> np.kron([5,6,7], [1,10,100])\narray([  5,  50, 500, ...,   7,  70, 700])\n >>> np.kron(np.eye(2), np.ones((2,2)))\narray([[1.,  1.,  0.,  0.],\n       [1.,  1.,  0.,  0.],\n       [0.,  0.,  1.,  1.],\n       [0.,  0.,  1.,  1.]])\n >>> a = np.arange(100).reshape((2,5,2,5))\n>>> b = np.arange(24).reshape((2,3,4))\n>>> c = np.kron(a,b)\n>>> c.shape\n(2, 10, 6, 20)\n>>> I = (1,3,0,2)\n>>> J = (0,2,1)\n>>> J1 = (0,) + J             # extend to ndim=4\n>>> S1 = (1,) + b.shape\n>>> K = tuple(np.array(I) * np.array(S1) + np.array(J1))\n>>> c[K] == a[I]*b[J]\nTrue","title":"numpy.reference.generated.numpy.kron"},{"text":"numpy.row_stack   numpy.row_stack(tup)[source]\n \nStack arrays in sequence vertically (row wise). This is equivalent to concatenation along the first axis after 1-D arrays of shape (N,) have been reshaped to (1,N). Rebuilds arrays divided by vsplit. This function makes most sense for arrays with up to 3 dimensions. For instance, for pixel-data with a height (first axis), width (second axis), and r\/g\/b channels (third axis). The functions concatenate, stack and block provide more general stacking and concatenation operations.  Parameters \n \ntupsequence of ndarrays\n\n\nThe arrays must have the same shape along all but the first axis. 1-D arrays must have the same length.    Returns \n \nstackedndarray\n\n\nThe array formed by stacking the given arrays, will be at least 2-D.      See also  concatenate\n\nJoin a sequence of arrays along an existing axis.  stack\n\nJoin a sequence of arrays along a new axis.  block\n\nAssemble an nd-array from nested lists of blocks.  hstack\n\nStack arrays in sequence horizontally (column wise).  dstack\n\nStack arrays in sequence depth wise (along third axis).  column_stack\n\nStack 1-D arrays as columns into a 2-D array.  vsplit\n\nSplit an array into multiple sub-arrays vertically (row-wise).    Examples >>> a = np.array([1, 2, 3])\n>>> b = np.array([4, 5, 6])\n>>> np.vstack((a,b))\narray([[1, 2, 3],\n       [4, 5, 6]])\n >>> a = np.array([[1], [2], [3]])\n>>> b = np.array([[4], [5], [6]])\n>>> np.vstack((a,b))\narray([[1],\n       [2],\n       [3],\n       [4],\n       [5],\n       [6]])","title":"numpy.reference.generated.numpy.row_stack"},{"text":"numpy.vstack   numpy.vstack(tup)[source]\n \nStack arrays in sequence vertically (row wise). This is equivalent to concatenation along the first axis after 1-D arrays of shape (N,) have been reshaped to (1,N). Rebuilds arrays divided by vsplit. This function makes most sense for arrays with up to 3 dimensions. For instance, for pixel-data with a height (first axis), width (second axis), and r\/g\/b channels (third axis). The functions concatenate, stack and block provide more general stacking and concatenation operations.  Parameters \n \ntupsequence of ndarrays\n\n\nThe arrays must have the same shape along all but the first axis. 1-D arrays must have the same length.    Returns \n \nstackedndarray\n\n\nThe array formed by stacking the given arrays, will be at least 2-D.      See also  concatenate\n\nJoin a sequence of arrays along an existing axis.  stack\n\nJoin a sequence of arrays along a new axis.  block\n\nAssemble an nd-array from nested lists of blocks.  hstack\n\nStack arrays in sequence horizontally (column wise).  dstack\n\nStack arrays in sequence depth wise (along third axis).  column_stack\n\nStack 1-D arrays as columns into a 2-D array.  vsplit\n\nSplit an array into multiple sub-arrays vertically (row-wise).    Examples >>> a = np.array([1, 2, 3])\n>>> b = np.array([4, 5, 6])\n>>> np.vstack((a,b))\narray([[1, 2, 3],\n       [4, 5, 6]])\n >>> a = np.array([[1], [2], [3]])\n>>> b = np.array([[4], [5], [6]])\n>>> np.vstack((a,b))\narray([[1],\n       [2],\n       [3],\n       [4],\n       [5],\n       [6]])","title":"numpy.reference.generated.numpy.vstack"}]}
{"task_id":438684,"prompt":"def f_438684(list_of_ints):\n\treturn ","suffix":"","canonical_solution":"\"\"\",\"\"\".join([str(i) for i in list_of_ints])","test_start":"\ndef check(candidate):","test":["\n    list_of_ints = [8, 7, 6]\n    assert candidate(list_of_ints) == '8,7,6'\n","\n    list_of_ints = [0, 1, 6]\n    assert candidate(list_of_ints) == '0,1,6'\n"],"entry_point":"f_438684","intent":"convert list `list_of_ints` into a comma separated string","library":[],"docs":[]}
{"task_id":8519922,"prompt":"def f_8519922(url, DATA, HEADERS_DICT, username, password):\n\treturn ","suffix":"","canonical_solution":"requests.post(url, data=DATA, headers=HEADERS_DICT, auth=(username, password))","test_start":"\nimport requests\nfrom unittest.mock import Mock\n\ndef check(candidate):","test":["\n    url='https:\/\/www.google.com'\n    HEADERS_DICT = {'Accept':'text\/json'}\n    requests.post = Mock()\n    try:\n        candidate(url, \"{'name': 'abc'}\", HEADERS_DICT, 'admin', 'admin123')\n    except:\n        assert False\n"],"entry_point":"f_8519922","intent":"Send a post request with raw data `DATA` and basic authentication with `username` and `password`","library":["requests"],"docs":[]}
{"task_id":26443308,"prompt":"def f_26443308():\n\treturn ","suffix":"","canonical_solution":"'abcd}def}'.rfind('}')","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == 8\n"],"entry_point":"f_26443308","intent":"Find last occurrence of character '}' in string \"abcd}def}\"","library":[],"docs":[]}
{"task_id":22365172,"prompt":"def f_22365172():\n\treturn ","suffix":"","canonical_solution":"[item for item in [1, 2, 3]]","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == [1,2,3]\n"],"entry_point":"f_22365172","intent":"Iterate ove list `[1, 2, 3]` using list comprehension","library":[],"docs":[]}
{"task_id":12300912,"prompt":"def f_12300912(d):\n\treturn ","suffix":"","canonical_solution":"[(x['x'], x['y']) for x in d]","test_start":"\ndef check(candidate):","test":["\n    data = [{'x': 1, 'y': 10}, {'x': 3, 'y': 15}, {'x': 2, 'y': 1}]\n    res = candidate(data)\n    assert res == [(1, 10), (3, 15), (2, 1)]\n"],"entry_point":"f_12300912","intent":"extract all the values with keys 'x' and 'y' from a list of dictionaries `d` to list of tuples","library":[],"docs":[]}
{"task_id":678236,"prompt":"def f_678236():\n\treturn ","suffix":"","canonical_solution":"os.path.splitext(os.path.basename('hemanth.txt'))[0]","test_start":"\nimport os \n\ndef check(candidate):","test":["\n    assert candidate() == \"hemanth\"\n"],"entry_point":"f_678236","intent":"get the filename without the extension from file 'hemanth.txt'","library":["os"],"docs":[{"text":"numpy.polynomial.hermite_e.hermeline   polynomial.hermite_e.hermeline(off, scl)[source]\n \nHermite series whose graph is a straight line.  Parameters \n \noff, sclscalars\n\n\nThe specified line is given by off + scl*x.    Returns \n \nyndarray\n\n\nThis module\u2019s representation of the Hermite series for off + scl*x.      See also  numpy.polynomial.polynomial.polyline\nnumpy.polynomial.chebyshev.chebline\nnumpy.polynomial.legendre.legline\nnumpy.polynomial.laguerre.lagline\nnumpy.polynomial.hermite.hermline\n  Examples >>> from numpy.polynomial.hermite_e import hermeline\n>>> from numpy.polynomial.hermite_e import hermeline, hermeval\n>>> hermeval(0,hermeline(3, 2))\n3.0\n>>> hermeval(1,hermeline(3, 2))\n5.0","title":"numpy.reference.generated.numpy.polynomial.hermite_e.hermeline"},{"text":"tf.keras.initializers.HeNormal He normal initializer. Inherits From: VarianceScaling, Initializer  View aliases  Main aliases \ntf.initializers.HeNormal, tf.initializers.he_normal, tf.keras.initializers.he_normal  \ntf.keras.initializers.HeNormal(\n    seed=None\n)\n Also available via the shortcut function tf.keras.initializers.he_normal. It draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 \/ fan_in) where fan_in is the number of input units in the weight tensor. Examples: \n# Standalone usage:\ninitializer = tf.keras.initializers.HeNormal()\nvalues = initializer(shape=(2, 2))\n \n# Usage in a Keras layer:\ninitializer = tf.keras.initializers.HeNormal()\nlayer = tf.keras.layers.Dense(3, kernel_initializer=initializer)\n\n \n\n\n Arguments\n  seed   A Python integer. An initializer created with a given seed will always produce the same random tensor for a given shape and dtype.    References: He et al., 2015 (pdf) Methods from_config View source \n@classmethod\nfrom_config(\n    config\n)\n Instantiates an initializer from a configuration dictionary. Example: initializer = RandomUniform(-1, 1)\nconfig = initializer.get_config()\ninitializer = RandomUniform.from_config(config)\n\n \n\n\n Args\n  config   A Python dictionary. It will typically be the output of get_config.   \n \n\n\n Returns   An Initializer instance.  \n get_config View source \nget_config()\n Returns the configuration of the initializer as a JSON-serializable dict.\n \n\n\n Returns   A JSON-serializable Python dict.  \n __call__ View source \n__call__(\n    shape, dtype=None, **kwargs\n)\n Returns a tensor object initialized as specified by the initializer.\n \n\n\n Args\n  shape   Shape of the tensor.  \n  dtype   Optional dtype of the tensor. Only floating point types are supported. If not specified, tf.keras.backend.floatx() is used, which default to float32 unless you configured it otherwise (via tf.keras.backend.set_floatx(float_dtype))  \n  **kwargs   Additional keyword arguments.","title":"tensorflow.keras.initializers.henormal"},{"text":"tf.keras.initializers.HeUniform He uniform variance scaling initializer. Inherits From: VarianceScaling, Initializer  View aliases  Main aliases \ntf.initializers.HeUniform, tf.initializers.he_uniform, tf.keras.initializers.he_uniform  \ntf.keras.initializers.HeUniform(\n    seed=None\n)\n Also available via the shortcut function tf.keras.initializers.he_uniform. Draws samples from a uniform distribution within [-limit, limit], where limit = sqrt(6 \/ fan_in) (fan_in is the number of input units in the weight tensor). Examples: \n# Standalone usage:\ninitializer = tf.keras.initializers.HeUniform()\nvalues = initializer(shape=(2, 2))\n \n# Usage in a Keras layer:\ninitializer = tf.keras.initializers.HeUniform()\nlayer = tf.keras.layers.Dense(3, kernel_initializer=initializer)\n\n \n\n\n Arguments\n  seed   A Python integer. An initializer created with a given seed will always produce the same random tensor for a given shape and dtype.    References: He et al., 2015 (pdf) Methods from_config View source \n@classmethod\nfrom_config(\n    config\n)\n Instantiates an initializer from a configuration dictionary. Example: initializer = RandomUniform(-1, 1)\nconfig = initializer.get_config()\ninitializer = RandomUniform.from_config(config)\n\n \n\n\n Args\n  config   A Python dictionary. It will typically be the output of get_config.   \n \n\n\n Returns   An Initializer instance.  \n get_config View source \nget_config()\n Returns the configuration of the initializer as a JSON-serializable dict.\n \n\n\n Returns   A JSON-serializable Python dict.  \n __call__ View source \n__call__(\n    shape, dtype=None, **kwargs\n)\n Returns a tensor object initialized as specified by the initializer.\n \n\n\n Args\n  shape   Shape of the tensor.  \n  dtype   Optional dtype of the tensor. Only floating point types are supported. If not specified, tf.keras.backend.floatx() is used, which default to float32 unless you configured it otherwise (via tf.keras.backend.set_floatx(float_dtype))  \n  **kwargs   Additional keyword arguments.","title":"tensorflow.keras.initializers.heuniform"},{"text":"numpy.polynomial.hermite.hermline   polynomial.hermite.hermline(off, scl)[source]\n \nHermite series whose graph is a straight line.  Parameters \n \noff, sclscalars\n\n\nThe specified line is given by off + scl*x.    Returns \n \nyndarray\n\n\nThis module\u2019s representation of the Hermite series for off + scl*x.      See also  numpy.polynomial.polynomial.polyline\nnumpy.polynomial.chebyshev.chebline\nnumpy.polynomial.legendre.legline\nnumpy.polynomial.laguerre.lagline\nnumpy.polynomial.hermite_e.hermeline\n  Examples >>> from numpy.polynomial.hermite import hermline, hermval\n>>> hermval(0,hermline(3, 2))\n3.0\n>>> hermval(1,hermline(3, 2))\n5.0","title":"numpy.reference.generated.numpy.polynomial.hermite.hermline"},{"text":"exception smtplib.SMTPHeloError  \nThe server refused our HELO message.","title":"python.library.smtplib#smtplib.SMTPHeloError"},{"text":"numpy.polynomial.hermite_e.hermeweight   polynomial.hermite_e.hermeweight(x)[source]\n \nWeight function of the Hermite_e polynomials. The weight function is \\(\\exp(-x^2\/2)\\) and the interval of integration is \\([-\\inf, \\inf]\\). the HermiteE polynomials are orthogonal, but not normalized, with respect to this weight function.  Parameters \n \nxarray_like\n\n\nValues at which the weight function will be computed.    Returns \n \nwndarray\n\n\nThe weight function at x.     Notes  New in version 1.7.0.","title":"numpy.reference.generated.numpy.polynomial.hermite_e.hermeweight"},{"text":"numpy.polynomial.hermite.Hermite.__call__ method   polynomial.hermite.Hermite.__call__(arg)[source]\n \nCall self as a function.","title":"numpy.reference.generated.numpy.polynomial.hermite.hermite.__call__"},{"text":"tf.compat.v1.initializers.he_normal He normal initializer. \ntf.compat.v1.initializers.he_normal(\n    seed=None\n)\n It draws samples from a truncated normal distribution centered on 0 with standard deviation (after truncation) given by stddev = sqrt(2 \/ fan_in) where fan_in is the number of input units in the weight tensor.\n \n\n\n Arguments\n  seed   A Python integer. Used to seed the random generator.   \n \n\n\n Returns   An initializer.  \n References: He et al., 2015 (pdf)","title":"tensorflow.compat.v1.initializers.he_normal"},{"text":"tf.compat.v1.initializers.he_uniform He uniform variance scaling initializer. \ntf.compat.v1.initializers.he_uniform(\n    seed=None\n)\n It draws samples from a uniform distribution within [-limit, limit] where limit is sqrt(6 \/ fan_in) where fan_in is the number of input units in the weight tensor.\n \n\n\n Arguments\n  seed   A Python integer. Used to seed the random generator.   \n \n\n\n Returns   An initializer.  \n References: He et al., 2015 (pdf)","title":"tensorflow.compat.v1.initializers.he_uniform"},{"text":"numpy.polynomial.hermite_e.HermiteE.__call__ method   polynomial.hermite_e.HermiteE.__call__(arg)[source]\n \nCall self as a function.","title":"numpy.reference.generated.numpy.polynomial.hermite_e.hermitee.__call__"}]}
{"task_id":7895449,"prompt":"def f_7895449():\n\treturn ","suffix":"","canonical_solution":"sum([['A', 'B', 'C'], ['D', 'E', 'F'], ['G', 'H', 'I']], [])","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I']\n"],"entry_point":"f_7895449","intent":"create a list containing flattened list `[['A', 'B', 'C'], ['D', 'E', 'F'], ['G', 'H', 'I']]`","library":[],"docs":[]}
{"task_id":31617845,"prompt":"def f_31617845(df):\n\t","suffix":"\n\treturn df","canonical_solution":"df = df[(df['closing_price'] >= 99) & (df['closing_price'] <= 101)]","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    df = pd.DataFrame([67, 68, 69, 70, 99, 100, 101, 102], columns = ['closing_price'])\n    assert candidate(df).shape[0] == 3\n"],"entry_point":"f_31617845","intent":"select rows in a dataframe `df` column 'closing_price' between two values 99 and 101","library":["pandas"],"docs":[]}
{"task_id":25698710,"prompt":"def f_25698710(df):\n\treturn ","suffix":"","canonical_solution":"df.replace({'\\n': '<br>'}, regex=True)","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    df = pd.DataFrame(['klm\\npqr', 'wxy\\njkl'], columns = ['val'])\n    expected = pd.DataFrame(['klm<br>pqr', 'wxy<br>jkl'], columns = ['val'])\n    assert pd.DataFrame.equals(candidate(df), expected)\n"],"entry_point":"f_25698710","intent":"replace all occurences of newlines `\\n` with `<br>` in dataframe `df`","library":["pandas"],"docs":[{"text":"curses.nl()  \nEnter newline mode. This mode translates the return key into newline on input, and translates newline into return and line-feed on output. Newline mode is initially on.","title":"python.library.curses#curses.nl"},{"text":"pandas.DataFrame.to_html   DataFrame.to_html(buf=None, columns=None, col_space=None, header=True, index=True, na_rep='NaN', formatters=None, float_format=None, sparsify=None, index_names=True, justify=None, max_rows=None, max_cols=None, show_dimensions=False, decimal='.', bold_rows=True, classes=None, escape=True, notebook=False, border=None, table_id=None, render_links=False, encoding=None)[source]\n \nRender a DataFrame as an HTML table.  Parameters \n \nbuf:str, Path or StringIO-like, optional, default None\n\n\nBuffer to write to. If None, the output is returned as a string.  \ncolumns:sequence, optional, default None\n\n\nThe subset of columns to write. Writes all columns by default.  \ncol_space:str or int, list or dict of int or str, optional\n\n\nThe minimum width of each column in CSS length units. An int is assumed to be px units.  New in version 0.25.0: Ability to use str.   \nheader:bool, optional\n\n\nWhether to print column labels, default True.  \nindex:bool, optional, default True\n\n\nWhether to print index (row) labels.  \nna_rep:str, optional, default \u2018NaN\u2019\n\n\nString representation of NaN to use.  \nformatters:list, tuple or dict of one-param. functions, optional\n\n\nFormatter functions to apply to columns\u2019 elements by position or name. The result of each function must be a unicode string. List\/tuple must be of length equal to the number of columns.  \nfloat_format:one-parameter function, optional, default None\n\n\nFormatter function to apply to columns\u2019 elements if they are floats. This function must return a unicode string and will be applied only to the non-NaN elements, with NaN being handled by na_rep.  Changed in version 1.2.0.   \nsparsify:bool, optional, default True\n\n\nSet to False for a DataFrame with a hierarchical index to print every multiindex key at each row.  \nindex_names:bool, optional, default True\n\n\nPrints the names of the indexes.  \njustify:str, default None\n\n\nHow to justify the column labels. If None uses the option from the print configuration (controlled by set_option), \u2018right\u2019 out of the box. Valid values are  left right center justify justify-all start end inherit match-parent initial unset.   \nmax_rows:int, optional\n\n\nMaximum number of rows to display in the console.  \nmax_cols:int, optional\n\n\nMaximum number of columns to display in the console.  \nshow_dimensions:bool, default False\n\n\nDisplay DataFrame dimensions (number of rows by number of columns).  \ndecimal:str, default \u2018.\u2019\n\n\nCharacter recognized as decimal separator, e.g. \u2018,\u2019 in Europe.  \nbold_rows:bool, default True\n\n\nMake the row labels bold in the output.  \nclasses:str or list or tuple, default None\n\n\nCSS class(es) to apply to the resulting html table.  \nescape:bool, default True\n\n\nConvert the characters <, >, and & to HTML-safe sequences.  \nnotebook:{True, False}, default False\n\n\nWhether the generated HTML is for IPython Notebook.  \nborder:int\n\n\nA border=border attribute is included in the opening <table> tag. Default pd.options.display.html.border.  \ntable_id:str, optional\n\n\nA css id is included in the opening <table> tag if specified.  \nrender_links:bool, default False\n\n\nConvert URLs to HTML links.  \nencoding:str, default \u201cutf-8\u201d\n\n\nSet character encoding.  New in version 1.0.     Returns \n str or None\n\nIf buf is None, returns the result as a string. Otherwise returns None.      See also  to_string\n\nConvert DataFrame to a string.","title":"pandas.reference.api.pandas.dataframe.to_html"},{"text":"pandas.io.formats.style.Styler   classpandas.io.formats.style.Styler(data, precision=None, table_styles=None, uuid=None, caption=None, table_attributes=None, cell_ids=True, na_rep=None, uuid_len=5, decimal=None, thousands=None, escape=None, formatter=None)[source]\n \nHelps style a DataFrame or Series according to the data with HTML and CSS.  Parameters \n \ndata:Series or DataFrame\n\n\nData to be styled - either a Series or DataFrame.  \nprecision:int, optional\n\n\nPrecision to round floats to. If not given defaults to pandas.options.styler.format.precision.  Changed in version 1.4.0.   \ntable_styles:list-like, default None\n\n\nList of {selector: (attr, value)} dicts; see Notes.  \nuuid:str, default None\n\n\nA unique identifier to avoid CSS collisions; generated automatically.  \ncaption:str, tuple, default None\n\n\nString caption to attach to the table. Tuple only used for LaTeX dual captions.  \ntable_attributes:str, default None\n\n\nItems that show up in the opening <table> tag in addition to automatic (by default) id.  \ncell_ids:bool, default True\n\n\nIf True, each cell will have an id attribute in their HTML tag. The id takes the form T_<uuid>_row<num_row>_col<num_col> where <uuid> is the unique identifier, <num_row> is the row number and <num_col> is the column number.  \nna_rep:str, optional\n\n\nRepresentation for missing values. If na_rep is None, no special formatting is applied, and falls back to pandas.options.styler.format.na_rep.  New in version 1.0.0.   \nuuid_len:int, default 5\n\n\nIf uuid is not specified, the length of the uuid to randomly generate expressed in hex characters, in range [0, 32].  New in version 1.2.0.   \ndecimal:str, optional\n\n\nCharacter used as decimal separator for floats, complex and integers. If not given uses pandas.options.styler.format.decimal.  New in version 1.3.0.   \nthousands:str, optional, default None\n\n\nCharacter used as thousands separator for floats, complex and integers. If not given uses pandas.options.styler.format.thousands.  New in version 1.3.0.   \nescape:str, optional\n\n\nUse \u2018html\u2019 to replace the characters &, <, >, ', and \" in cell display string with HTML-safe sequences. Use \u2018latex\u2019 to replace the characters &, %, $, #, _, {, }, ~, ^, and \\ in the cell display string with LaTeX-safe sequences. If not given uses pandas.options.styler.format.escape.  New in version 1.3.0.   \nformatter:str, callable, dict, optional\n\n\nObject to define how values are displayed. See Styler.format. If not given uses pandas.options.styler.format.formatter.  New in version 1.4.0.       See also  DataFrame.style\n\nReturn a Styler object containing methods for building a styled HTML representation for the DataFrame.    Notes Most styling will be done by passing style functions into Styler.apply or Styler.applymap. Style functions should return values with strings containing CSS 'attr: value' that will be applied to the indicated cells. If using in the Jupyter notebook, Styler has defined a _repr_html_ to automatically render itself. Otherwise call Styler.to_html to get the generated HTML. CSS classes are attached to the generated HTML  Index and Column names include index_name and level<k> where k is its level in a MultiIndex \nIndex label cells include  row_heading row<n> where n is the numeric position of the row level<k> where k is the level in a MultiIndex   Column label cells include * col_heading * col<n> where n is the numeric position of the column * level<k> where k is the level in a MultiIndex Blank cells include blank Data cells include data Trimmed cells include col_trim or row_trim.  Any, or all, or these classes can be renamed by using the css_class_names argument in Styler.set_table_classes, giving a value such as {\u201crow\u201d: \u201cMY_ROW_CLASS\u201d, \u201ccol_trim\u201d: \u201c\u201d, \u201crow_trim\u201d: \u201c\u201d}. Attributes       \nenv (Jinja2 jinja2.Environment)  \ntemplate_html (Jinja2 Template)  \ntemplate_html_table (Jinja2 Template)  \ntemplate_html_style (Jinja2 Template)  \ntemplate_latex (Jinja2 Template)  \nloader (Jinja2 Loader)    Methods       \napply(func[, axis, subset]) Apply a CSS-styling function column-wise, row-wise, or table-wise.  \napply_index(func[, axis, level]) Apply a CSS-styling function to the index or column headers, level-wise.  \napplymap(func[, subset]) Apply a CSS-styling function elementwise.  \napplymap_index(func[, axis, level]) Apply a CSS-styling function to the index or column headers, elementwise.  \nbackground_gradient([cmap, low, high, axis, ...]) Color the background in a gradient style.  \nbar([subset, axis, color, cmap, width, ...]) Draw bar chart in the cell backgrounds.  \nclear() Reset the Styler, removing any previously applied styles.  \nexport() Export the styles applied to the current Styler.  \nformat([formatter, subset, na_rep, ...]) Format the text display value of cells.  \nformat_index([formatter, axis, level, ...]) Format the text display value of index labels or column headers.  \nfrom_custom_template(searchpath[, ...]) Factory function for creating a subclass of Styler.  \nhide([subset, axis, level, names]) Hide the entire index \/ column headers, or specific rows \/ columns from display.  \nhide_columns([subset, level, names]) Hide the column headers or specific keys in the columns from rendering.  \nhide_index([subset, level, names]) (DEPRECATED) Hide the entire index, or specific keys in the index from rendering.  \nhighlight_between([subset, color, axis, ...]) Highlight a defined range with a style.  \nhighlight_max([subset, color, axis, props]) Highlight the maximum with a style.  \nhighlight_min([subset, color, axis, props]) Highlight the minimum with a style.  \nhighlight_null([null_color, subset, props]) Highlight missing values with a style.  \nhighlight_quantile([subset, color, axis, ...]) Highlight values defined by a quantile with a style.  \npipe(func, *args, **kwargs) Apply func(self, *args, **kwargs), and return the result.  \nrender([sparse_index, sparse_columns]) (DEPRECATED) Render the Styler including all applied styles to HTML.  \nset_caption(caption) Set the text added to a <caption> HTML element.  \nset_na_rep(na_rep) (DEPRECATED) Set the missing data representation on a Styler.  \nset_precision(precision) (DEPRECATED) Set the precision used to display values.  \nset_properties([subset]) Set defined CSS-properties to each <td> HTML element within the given subset.  \nset_sticky([axis, pixel_size, levels]) Add CSS to permanently display the index or column headers in a scrolling frame.  \nset_table_attributes(attributes) Set the table attributes added to the <table> HTML element.  \nset_table_styles([table_styles, axis, ...]) Set the table styles included within the <style> HTML element.  \nset_td_classes(classes) Set the DataFrame of strings added to the class attribute of <td> HTML elements.  \nset_tooltips(ttips[, props, css_class]) Set the DataFrame of strings on Styler generating :hover tooltips.  \nset_uuid(uuid) Set the uuid applied to id attributes of HTML elements.  \ntext_gradient([cmap, low, high, axis, ...]) Color the text in a gradient style.  \nto_excel(excel_writer[, sheet_name, na_rep, ...]) Write Styler to an Excel sheet.  \nto_html([buf, table_uuid, table_attributes, ...]) Write Styler to a file, buffer or string in HTML-CSS format.  \nto_latex([buf, column_format, position, ...]) Write Styler to a file, buffer or string in LaTeX format.  \nuse(styles) Set the styles on the current Styler.  \nwhere(cond, value[, other, subset]) (DEPRECATED) Apply CSS-styles based on a conditional function elementwise.","title":"pandas.reference.api.pandas.io.formats.style.styler"},{"text":"token.NL  \nToken value used to indicate a non-terminating newline. The NEWLINE token indicates the end of a logical line of Python code; NL tokens are generated when a logical line of code is continued over multiple physical lines.","title":"python.library.token#token.NL"},{"text":"window.insertln()  \nInsert a blank line under the cursor. All following lines are moved down by one line.","title":"python.library.curses#curses.window.insertln"},{"text":"Dialect.lineterminator  \nThe string used to terminate lines produced by the writer. It defaults to '\\r\\n'.  Note The reader is hard-coded to recognise either '\\r' or '\\n' as end-of-line, and ignores lineterminator. This behavior may change in the future.","title":"python.library.csv#csv.Dialect.lineterminator"},{"text":"signal.SIGBREAK  \nInterrupt from keyboard (CTRL + BREAK). Availability: Windows.","title":"python.library.signal#signal.SIGBREAK"},{"text":"pandas.DataFrame.to_markdown   DataFrame.to_markdown(buf=None, mode='wt', index=True, storage_options=None, **kwargs)[source]\n \nPrint DataFrame in Markdown-friendly format.  New in version 1.0.0.   Parameters \n \nbuf:str, Path or StringIO-like, optional, default None\n\n\nBuffer to write to. If None, the output is returned as a string.  \nmode:str, optional\n\n\nMode in which file is opened, \u201cwt\u201d by default.  \nindex:bool, optional, default True\n\n\nAdd index (row) labels.  New in version 1.1.0.   \nstorage_options:dict, optional\n\n\nExtra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib as header options. For other URLs (e.g. starting with \u201cs3:\/\/\u201d, and \u201cgcs:\/\/\u201d) the key-value pairs are forwarded to fsspec. Please see fsspec and urllib for more details.  New in version 1.2.0.   **kwargs\n\nThese parameters will be passed to tabulate.    Returns \n str\n\nDataFrame in Markdown-friendly format.     Notes Requires the tabulate package. Examples \n>>> df = pd.DataFrame(\n...     data={\"animal_1\": [\"elk\", \"pig\"], \"animal_2\": [\"dog\", \"quetzal\"]}\n... )\n>>> print(df.to_markdown())\n|    | animal_1   | animal_2   |\n|---:|:-----------|:-----------|\n|  0 | elk        | dog        |\n|  1 | pig        | quetzal    |\n  Output markdown with a tabulate option. \n>>> print(df.to_markdown(tablefmt=\"grid\"))\n+----+------------+------------+\n|    | animal_1   | animal_2   |\n+====+============+============+\n|  0 | elk        | dog        |\n+----+------------+------------+\n|  1 | pig        | quetzal    |\n+----+------------+------------+","title":"pandas.reference.api.pandas.dataframe.to_markdown"},{"text":"curses.cbreak()  \nEnter cbreak mode. In cbreak mode (sometimes called \u201crare\u201d mode) normal tty line buffering is turned off and characters are available to be read one by one. However, unlike raw mode, special characters (interrupt, quit, suspend, and flow control) retain their effects on the tty driver and calling program. Calling first raw() then cbreak() leaves the terminal in cbreak mode.","title":"python.library.curses#curses.cbreak"},{"text":"codecs.BOM  \ncodecs.BOM_BE  \ncodecs.BOM_LE  \ncodecs.BOM_UTF8  \ncodecs.BOM_UTF16  \ncodecs.BOM_UTF16_BE  \ncodecs.BOM_UTF16_LE  \ncodecs.BOM_UTF32  \ncodecs.BOM_UTF32_BE  \ncodecs.BOM_UTF32_LE  \nThese constants define various byte sequences, being Unicode byte order marks (BOMs) for several encodings. They are used in UTF-16 and UTF-32 data streams to indicate the byte order used, and in UTF-8 as a Unicode signature. BOM_UTF16 is either BOM_UTF16_BE or BOM_UTF16_LE depending on the platform\u2019s native byte order, BOM is an alias for BOM_UTF16, BOM_LE for BOM_UTF16_LE and BOM_BE for BOM_UTF16_BE. The others represent the BOM in UTF-8 and UTF-32 encodings.","title":"python.library.codecs#codecs.BOM_UTF32_LE"}]}
{"task_id":25698710,"prompt":"def f_25698710(df):\n\treturn ","suffix":"","canonical_solution":"df.replace({'\\n': '<br>'}, regex=True)","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    df = pd.DataFrame(['klm\\npqr', 'wxy\\njkl'], columns = ['val'])\n    expected = pd.DataFrame(['klm<br>pqr', 'wxy<br>jkl'], columns = ['val'])\n    assert pd.DataFrame.equals(candidate(df), expected)\n"],"entry_point":"f_25698710","intent":"replace all occurrences of a string `\\n` by string `<br>` in a pandas data frame `df`","library":["pandas"],"docs":[{"text":"curses.nl()  \nEnter newline mode. This mode translates the return key into newline on input, and translates newline into return and line-feed on output. Newline mode is initially on.","title":"python.library.curses#curses.nl"},{"text":"pandas.DataFrame.to_html   DataFrame.to_html(buf=None, columns=None, col_space=None, header=True, index=True, na_rep='NaN', formatters=None, float_format=None, sparsify=None, index_names=True, justify=None, max_rows=None, max_cols=None, show_dimensions=False, decimal='.', bold_rows=True, classes=None, escape=True, notebook=False, border=None, table_id=None, render_links=False, encoding=None)[source]\n \nRender a DataFrame as an HTML table.  Parameters \n \nbuf:str, Path or StringIO-like, optional, default None\n\n\nBuffer to write to. If None, the output is returned as a string.  \ncolumns:sequence, optional, default None\n\n\nThe subset of columns to write. Writes all columns by default.  \ncol_space:str or int, list or dict of int or str, optional\n\n\nThe minimum width of each column in CSS length units. An int is assumed to be px units.  New in version 0.25.0: Ability to use str.   \nheader:bool, optional\n\n\nWhether to print column labels, default True.  \nindex:bool, optional, default True\n\n\nWhether to print index (row) labels.  \nna_rep:str, optional, default \u2018NaN\u2019\n\n\nString representation of NaN to use.  \nformatters:list, tuple or dict of one-param. functions, optional\n\n\nFormatter functions to apply to columns\u2019 elements by position or name. The result of each function must be a unicode string. List\/tuple must be of length equal to the number of columns.  \nfloat_format:one-parameter function, optional, default None\n\n\nFormatter function to apply to columns\u2019 elements if they are floats. This function must return a unicode string and will be applied only to the non-NaN elements, with NaN being handled by na_rep.  Changed in version 1.2.0.   \nsparsify:bool, optional, default True\n\n\nSet to False for a DataFrame with a hierarchical index to print every multiindex key at each row.  \nindex_names:bool, optional, default True\n\n\nPrints the names of the indexes.  \njustify:str, default None\n\n\nHow to justify the column labels. If None uses the option from the print configuration (controlled by set_option), \u2018right\u2019 out of the box. Valid values are  left right center justify justify-all start end inherit match-parent initial unset.   \nmax_rows:int, optional\n\n\nMaximum number of rows to display in the console.  \nmax_cols:int, optional\n\n\nMaximum number of columns to display in the console.  \nshow_dimensions:bool, default False\n\n\nDisplay DataFrame dimensions (number of rows by number of columns).  \ndecimal:str, default \u2018.\u2019\n\n\nCharacter recognized as decimal separator, e.g. \u2018,\u2019 in Europe.  \nbold_rows:bool, default True\n\n\nMake the row labels bold in the output.  \nclasses:str or list or tuple, default None\n\n\nCSS class(es) to apply to the resulting html table.  \nescape:bool, default True\n\n\nConvert the characters <, >, and & to HTML-safe sequences.  \nnotebook:{True, False}, default False\n\n\nWhether the generated HTML is for IPython Notebook.  \nborder:int\n\n\nA border=border attribute is included in the opening <table> tag. Default pd.options.display.html.border.  \ntable_id:str, optional\n\n\nA css id is included in the opening <table> tag if specified.  \nrender_links:bool, default False\n\n\nConvert URLs to HTML links.  \nencoding:str, default \u201cutf-8\u201d\n\n\nSet character encoding.  New in version 1.0.     Returns \n str or None\n\nIf buf is None, returns the result as a string. Otherwise returns None.      See also  to_string\n\nConvert DataFrame to a string.","title":"pandas.reference.api.pandas.dataframe.to_html"},{"text":"Dialect.lineterminator  \nThe string used to terminate lines produced by the writer. It defaults to '\\r\\n'.  Note The reader is hard-coded to recognise either '\\r' or '\\n' as end-of-line, and ignores lineterminator. This behavior may change in the future.","title":"python.library.csv#csv.Dialect.lineterminator"},{"text":"token.NL  \nToken value used to indicate a non-terminating newline. The NEWLINE token indicates the end of a logical line of Python code; NL tokens are generated when a logical line of code is continued over multiple physical lines.","title":"python.library.token#token.NL"},{"text":"pandas.DataFrame.replace   DataFrame.replace(to_replace=None, value=NoDefault.no_default, inplace=False, limit=None, regex=False, method=NoDefault.no_default)[source]\n \nReplace values given in to_replace with value. Values of the DataFrame are replaced with other values dynamically. This differs from updating with .loc or .iloc, which require you to specify a location to update with some value.  Parameters \n \nto_replace:str, regex, list, dict, Series, int, float, or None\n\n\nHow to find the values that will be replaced.  \nnumeric, str or regex:  \n numeric: numeric values equal to to_replace will be replaced with value str: string exactly matching to_replace will be replaced with value regex: regexs matching to_replace will be replaced with value  \n  \nlist of str, regex, or numeric:  \n First, if to_replace and value are both lists, they must be the same length. Second, if regex=True then all of the strings in both lists will be interpreted as regexs otherwise they will match directly. This doesn\u2019t matter much for value since there are only a few possible substitution regexes you can use. str, regex and numeric rules apply as above.  \n  \ndict:  \n Dicts can be used to specify different replacement values for different existing values. For example, {'a': 'b', 'y': 'z'} replaces the value \u2018a\u2019 with \u2018b\u2019 and \u2018y\u2019 with \u2018z\u2019. To use a dict in this way the value parameter should be None. For a DataFrame a dict can specify that different values should be replaced in different columns. For example, {'a': 1, 'b': 'z'} looks for the value 1 in column \u2018a\u2019 and the value \u2018z\u2019 in column \u2018b\u2019 and replaces these values with whatever is specified in value. The value parameter should not be None in this case. You can treat this as a special case of passing two lists except that you are specifying the column to search in. For a DataFrame nested dictionaries, e.g., {'a': {'b': np.nan}}, are read as follows: look in column \u2018a\u2019 for the value \u2018b\u2019 and replace it with NaN. The value parameter should be None to use a nested dict in this way. You can nest regular expressions as well. Note that column names (the top-level dictionary keys in a nested dictionary) cannot be regular expressions.  \n  \nNone:  \n This means that the regex argument must be a string, compiled regular expression, or list, dict, ndarray or Series of such elements. If value is also None then this must be a nested dictionary or Series.  \n   See the examples section for examples of each of these.  \nvalue:scalar, dict, list, str, regex, default None\n\n\nValue to replace any values matching to_replace with. For a DataFrame a dict of values can be used to specify which value to use for each column (columns not in the dict will not be filled). Regular expressions, strings and lists or dicts of such objects are also allowed.  \ninplace:bool, default False\n\n\nIf True, performs operation inplace and returns None.  \nlimit:int, default None\n\n\nMaximum size gap to forward or backward fill.  \nregex:bool or same types as to_replace, default False\n\n\nWhether to interpret to_replace and\/or value as regular expressions. If this is True then to_replace must be a string. Alternatively, this could be a regular expression or a list, dict, or array of regular expressions in which case to_replace must be None.  \nmethod:{\u2018pad\u2019, \u2018ffill\u2019, \u2018bfill\u2019, None}\n\n\nThe method to use when for replacement, when to_replace is a scalar, list or tuple and value is None.  Changed in version 0.23.0: Added to DataFrame.     Returns \n DataFrame\n\nObject after replacement.    Raises \n AssertionError\n\n If regex is not a bool and to_replace is not None.   TypeError\n\n If to_replace is not a scalar, array-like, dict, or None If to_replace is a dict and value is not a list, dict, ndarray, or Series If to_replace is None and regex is not compilable into a regular expression or is a list, dict, ndarray, or Series. When replacing multiple bool or datetime64 objects and the arguments to to_replace does not match the type of the value being replaced   ValueError\n\n If a list or an ndarray is passed to to_replace and value but they are not the same length.       See also  DataFrame.fillna\n\nFill NA values.  DataFrame.where\n\nReplace values based on boolean condition.  Series.str.replace\n\nSimple string replacement.    Notes  Regex substitution is performed under the hood with re.sub. The rules for substitution for re.sub are the same. Regular expressions will only substitute on strings, meaning you cannot provide, for example, a regular expression matching floating point numbers and expect the columns in your frame that have a numeric dtype to be matched. However, if those floating point numbers are strings, then you can do this. This method has a lot of options. You are encouraged to experiment and play with this method to gain intuition about how it works. When dict is used as the to_replace value, it is like key(s) in the dict are the to_replace part and value(s) in the dict are the value parameter.  Examples Scalar `to_replace` and `value` \n>>> s = pd.Series([1, 2, 3, 4, 5])\n>>> s.replace(1, 5)\n0    5\n1    2\n2    3\n3    4\n4    5\ndtype: int64\n  \n>>> df = pd.DataFrame({'A': [0, 1, 2, 3, 4],\n...                    'B': [5, 6, 7, 8, 9],\n...                    'C': ['a', 'b', 'c', 'd', 'e']})\n>>> df.replace(0, 5)\n    A  B  C\n0  5  5  a\n1  1  6  b\n2  2  7  c\n3  3  8  d\n4  4  9  e\n  List-like `to_replace` \n>>> df.replace([0, 1, 2, 3], 4)\n    A  B  C\n0  4  5  a\n1  4  6  b\n2  4  7  c\n3  4  8  d\n4  4  9  e\n  \n>>> df.replace([0, 1, 2, 3], [4, 3, 2, 1])\n    A  B  C\n0  4  5  a\n1  3  6  b\n2  2  7  c\n3  1  8  d\n4  4  9  e\n  \n>>> s.replace([1, 2], method='bfill')\n0    3\n1    3\n2    3\n3    4\n4    5\ndtype: int64\n  dict-like `to_replace` \n>>> df.replace({0: 10, 1: 100})\n        A  B  C\n0   10  5  a\n1  100  6  b\n2    2  7  c\n3    3  8  d\n4    4  9  e\n  \n>>> df.replace({'A': 0, 'B': 5}, 100)\n        A    B  C\n0  100  100  a\n1    1    6  b\n2    2    7  c\n3    3    8  d\n4    4    9  e\n  \n>>> df.replace({'A': {0: 100, 4: 400}})\n        A  B  C\n0  100  5  a\n1    1  6  b\n2    2  7  c\n3    3  8  d\n4  400  9  e\n  Regular expression `to_replace` \n>>> df = pd.DataFrame({'A': ['bat', 'foo', 'bait'],\n...                    'B': ['abc', 'bar', 'xyz']})\n>>> df.replace(to_replace=r'^ba.$', value='new', regex=True)\n        A    B\n0   new  abc\n1   foo  new\n2  bait  xyz\n  \n>>> df.replace({'A': r'^ba.$'}, {'A': 'new'}, regex=True)\n        A    B\n0   new  abc\n1   foo  bar\n2  bait  xyz\n  \n>>> df.replace(regex=r'^ba.$', value='new')\n        A    B\n0   new  abc\n1   foo  new\n2  bait  xyz\n  \n>>> df.replace(regex={r'^ba.$': 'new', 'foo': 'xyz'})\n        A    B\n0   new  abc\n1   xyz  new\n2  bait  xyz\n  \n>>> df.replace(regex=[r'^ba.$', 'foo'], value='new')\n        A    B\n0   new  abc\n1   new  new\n2  bait  xyz\n  Compare the behavior of s.replace({'a': None}) and s.replace('a', None) to understand the peculiarities of the to_replace parameter: \n>>> s = pd.Series([10, 'a', 'a', 'b', 'a'])\n  When one uses a dict as the to_replace value, it is like the value(s) in the dict are equal to the value parameter. s.replace({'a': None}) is equivalent to s.replace(to_replace={'a': None}, value=None, method=None): \n>>> s.replace({'a': None})\n0      10\n1    None\n2    None\n3       b\n4    None\ndtype: object\n  When value is not explicitly passed and to_replace is a scalar, list or tuple, replace uses the method parameter (default \u2018pad\u2019) to do the replacement. So this is why the \u2018a\u2019 values are being replaced by 10 in rows 1 and 2 and \u2018b\u2019 in row 4 in this case. \n>>> s.replace('a')\n0    10\n1    10\n2    10\n3     b\n4     b\ndtype: object\n  On the other hand, if None is explicitly passed for value, it will be respected: \n>>> s.replace('a', None)\n0      10\n1    None\n2    None\n3       b\n4    None\ndtype: object\n   \n Changed in version 1.4.0: Previously the explicit None was silently ignored.","title":"pandas.reference.api.pandas.dataframe.replace"},{"text":"signal.SIGBREAK  \nInterrupt from keyboard (CTRL + BREAK). Availability: Windows.","title":"python.library.signal#signal.SIGBREAK"},{"text":"numpy.record.newbyteorder method   record.newbyteorder(new_order='S', \/)\n \nReturn a new dtype with a different byte order. Changes are also made in all fields and sub-arrays of the data type. The new_order code can be any from the following:  \u2018S\u2019 - swap dtype from current to opposite endian {\u2018<\u2019, \u2018little\u2019} - little endian {\u2018>\u2019, \u2018big\u2019} - big endian {\u2018=\u2019, \u2018native\u2019} - native order {\u2018|\u2019, \u2018I\u2019} - ignore (no change to byte order)   Parameters \n \nnew_orderstr, optional\n\n\nByte order to force; a value from the byte order specifications above. The default value (\u2018S\u2019) results in swapping the current byte order.    Returns \n \nnew_dtypedtype\n\n\nNew dtype object with the given change to the byte order.","title":"numpy.reference.generated.numpy.record.newbyteorder"},{"text":"window.insertln()  \nInsert a blank line under the cursor. All following lines are moved down by one line.","title":"python.library.curses#curses.window.insertln"},{"text":"tf.image.adjust_brightness     View source on GitHub    Adjust the brightness of RGB or Grayscale images.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.image.adjust_brightness  \ntf.image.adjust_brightness(\n    image, delta\n)\n This is a convenience method that converts RGB images to float representation, adjusts their brightness, and then converts them back to the original data type. If several adjustments are chained, it is advisable to minimize the number of redundant conversions. The value delta is added to all components of the tensor image. image is converted to float and scaled appropriately if it is in fixed-point representation, and delta is converted to the same data type. For regular images, delta should be in the range (-1,1), as it is added to the image in floating point representation, where pixel values are in the [0,1) range. Usage Example: \nx = [[[1.0, 2.0, 3.0],\n      [4.0, 5.0, 6.0]],\n    [[7.0, 8.0, 9.0],\n      [10.0, 11.0, 12.0]]]\ntf.image.adjust_brightness(x, delta=0.1)\n<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=\narray([[[ 1.1,  2.1,  3.1],\n        [ 4.1,  5.1,  6.1]],\n       [[ 7.1,  8.1,  9.1],\n        [10.1, 11.1, 12.1]]], dtype=float32)>\n\n \n\n\n Args\n  image   RGB image or images to adjust.  \n  delta   A scalar. Amount to add to the pixel values.   \n \n\n\n Returns   A brightness-adjusted tensor of the same shape and type as image.","title":"tensorflow.image.adjust_brightness"},{"text":"get_file_breaks(filename)  \nReturn all breakpoints in filename, or an empty list if none are set.","title":"python.library.bdb#bdb.Bdb.get_file_breaks"}]}
{"task_id":41923858,"prompt":"def f_41923858(word):\n\treturn ","suffix":"","canonical_solution":"[(x + y) for x, y in zip(word, word[1:])]","test_start":"\ndef check(candidate):","test":["\n    assert candidate('abcdef') == ['ab', 'bc', 'cd', 'de', 'ef']\n","\n    assert candidate([\"hello\", \"world\", \"!\"]) == [\"helloworld\", \"world!\"]\n"],"entry_point":"f_41923858","intent":"create a list containing each two adjacent letters in string `word` as its elements","library":[],"docs":[]}
{"task_id":41923858,"prompt":"def f_41923858(word):\n\treturn ","suffix":"","canonical_solution":"list(map(lambda x, y: x + y, word[:-1], word[1:]))","test_start":"\ndef check(candidate):","test":["\n    assert candidate('abcdef') == ['ab', 'bc', 'cd', 'de', 'ef']\n"],"entry_point":"f_41923858","intent":"Get a list of pairs from a string `word` using lambda function","library":[],"docs":[]}
{"task_id":9760588,"prompt":"def f_9760588(myString):\n\treturn ","suffix":"","canonical_solution":"re.findall('(https?:\/\/[^\\\\s]+)', myString)","test_start":"\nimport re\n\ndef check(candidate):","test":["\n    assert candidate(\"This is a link http:\/\/www.google.com\") == [\"http:\/\/www.google.com\"]\n","\n    assert candidate(\"Please refer to the website: http:\/\/www.google.com\") == [\"http:\/\/www.google.com\"]\n"],"entry_point":"f_9760588","intent":"extract a url from a string `myString`","library":["re"],"docs":[{"text":"urllib.parse.unwrap(url)  \nExtract the url from a wrapped URL (that is, a string formatted as <URL:scheme:\/\/host\/path>, <scheme:\/\/host\/path>, URL:scheme:\/\/host\/path or scheme:\/\/host\/path). If url is not a wrapped URL, it is returned without changes.","title":"python.library.urllib.parse#urllib.parse.unwrap"},{"text":"urllib.parse.unquote_plus(string, encoding='utf-8', errors='replace')  \nLike unquote(), but also replace plus signs with spaces, as required for unquoting HTML form values. string must be a str. Example: unquote_plus('\/El+Ni%C3%B1o\/') yields '\/El Ni\u00f1o\/'.","title":"python.library.urllib.parse#urllib.parse.unquote_plus"},{"text":"werkzeug.urls.url_fix(s, charset='utf-8')  \nSometimes you get an URL by a user that just isn\u2019t a real URL because it contains unsafe characters like \u2018 \u2018 and so on. This function can fix some of the problems in a similar way browsers handle data entered by the user: >>> url_fix('http:\/\/de.wikipedia.org\/wiki\/Elf (Begriffskl\\xe4rung)')\n'http:\/\/de.wikipedia.org\/wiki\/Elf%20(Begriffskl%C3%A4rung)'\n  Parameters \n \ns (str) \u2013 the string with the URL to fix. \ncharset (str) \u2013 The target charset for the URL if the url was given as a string.   Return type \nstr","title":"werkzeug.urls.index#werkzeug.urls.url_fix"},{"text":"urllib.parse.urlsplit(urlstring, scheme='', allow_fragments=True)  \nThis is similar to urlparse(), but does not split the params from the URL. This should generally be used instead of urlparse() if the more recent URL syntax allowing parameters to be applied to each segment of the path portion of the URL (see RFC 2396) is wanted. A separate function is needed to separate the path segments and parameters. This function returns a 5-item named tuple: (addressing scheme, network location, path, query, fragment identifier).\n The return value is a named tuple, its items can be accessed by index or as named attributes:   \nAttribute Index Value Value if not present   \nscheme 0 URL scheme specifier scheme parameter  \nnetloc 1 Network location part empty string  \npath 2 Hierarchical path empty string  \nquery 3 Query component empty string  \nfragment 4 Fragment identifier empty string  \nusername  User name None  \npassword  Password None  \nhostname  Host name (lower case) None  \nport  Port number as integer, if present None   Reading the port attribute will raise a ValueError if an invalid port is specified in the URL. See section Structured Parse Results for more information on the result object. Unmatched square brackets in the netloc attribute will raise a ValueError. Characters in the netloc attribute that decompose under NFKC normalization (as used by the IDNA encoding) into any of \/, ?, #, @, or : will raise a ValueError. If the URL is decomposed before parsing, no error will be raised.  Changed in version 3.6: Out-of-range port numbers now raise ValueError, instead of returning None.   Changed in version 3.8: Characters that affect netloc parsing under NFKC normalization will now raise ValueError.","title":"python.library.urllib.parse#urllib.parse.urlsplit"},{"text":"urllib.parse.SplitResult.geturl()  \nReturn the re-combined version of the original URL as a string. This may differ from the original URL in that the scheme may be normalized to lower case and empty components may be dropped. Specifically, empty parameters, queries, and fragment identifiers will be removed. For urldefrag() results, only empty fragment identifiers will be removed. For urlsplit() and urlparse() results, all noted changes will be made to the URL returned by this method. The result of this method remains unchanged if passed back through the original parsing function: >>> from urllib.parse import urlsplit\n>>> url = 'HTTP:\/\/www.Python.org\/doc\/#'\n>>> r1 = urlsplit(url)\n>>> r1.geturl()\n'http:\/\/www.Python.org\/doc\/'\n>>> r2 = urlsplit(r1.geturl())\n>>> r2.geturl()\n'http:\/\/www.Python.org\/doc\/'","title":"python.library.urllib.parse#urllib.parse.urllib.parse.SplitResult.geturl"},{"text":"urllib.parse \u2014 Parse URLs into components Source code: Lib\/urllib\/parse.py This module defines a standard interface to break Uniform Resource Locator (URL) strings up in components (addressing scheme, network location, path etc.), to combine the components back into a URL string, and to convert a \u201crelative URL\u201d to an absolute URL given a \u201cbase URL.\u201d The module has been designed to match the Internet RFC on Relative Uniform Resource Locators. It supports the following URL schemes: file, ftp, gopher, hdl, http, https, imap, mailto, mms, news, nntp, prospero, rsync, rtsp, rtspu, sftp, shttp, sip, sips, snews, svn, svn+ssh, telnet, wais, ws, wss. The urllib.parse module defines functions that fall into two broad categories: URL parsing and URL quoting. These are covered in detail in the following sections. URL Parsing The URL parsing functions focus on splitting a URL string into its components, or on combining URL components into a URL string.  \nurllib.parse.urlparse(urlstring, scheme='', allow_fragments=True)  \nParse a URL into six components, returning a 6-item named tuple. This corresponds to the general structure of a URL: scheme:\/\/netloc\/path;parameters?query#fragment. Each tuple item is a string, possibly empty. The components are not broken up into smaller parts (for example, the network location is a single string), and % escapes are not expanded. The delimiters as shown above are not part of the result, except for a leading slash in the path component, which is retained if present. For example: >>> from urllib.parse import urlparse\n>>> o = urlparse('http:\/\/www.cwi.nl:80\/%7Eguido\/Python.html')\n>>> o   \nParseResult(scheme='http', netloc='www.cwi.nl:80', path='\/%7Eguido\/Python.html',\n            params='', query='', fragment='')\n>>> o.scheme\n'http'\n>>> o.port\n80\n>>> o.geturl()\n'http:\/\/www.cwi.nl:80\/%7Eguido\/Python.html'\n Following the syntax specifications in RFC 1808, urlparse recognizes a netloc only if it is properly introduced by \u2018\/\/\u2019. Otherwise the input is presumed to be a relative URL and thus to start with a path component. >>> from urllib.parse import urlparse\n>>> urlparse('\/\/www.cwi.nl:80\/%7Eguido\/Python.html')\nParseResult(scheme='', netloc='www.cwi.nl:80', path='\/%7Eguido\/Python.html',\n            params='', query='', fragment='')\n>>> urlparse('www.cwi.nl\/%7Eguido\/Python.html')\nParseResult(scheme='', netloc='', path='www.cwi.nl\/%7Eguido\/Python.html',\n            params='', query='', fragment='')\n>>> urlparse('help\/Python.html')\nParseResult(scheme='', netloc='', path='help\/Python.html', params='',\n            query='', fragment='')\n The scheme argument gives the default addressing scheme, to be used only if the URL does not specify one. It should be the same type (text or bytes) as urlstring, except that the default value '' is always allowed, and is automatically converted to b'' if appropriate. If the allow_fragments argument is false, fragment identifiers are not recognized. Instead, they are parsed as part of the path, parameters or query component, and fragment is set to the empty string in the return value. The return value is a named tuple, which means that its items can be accessed by index or as named attributes, which are:   \nAttribute Index Value Value if not present   \nscheme 0 URL scheme specifier scheme parameter  \nnetloc 1 Network location part empty string  \npath 2 Hierarchical path empty string  \nparams 3 Parameters for last path element empty string  \nquery 4 Query component empty string  \nfragment 5 Fragment identifier empty string  \nusername  User name None  \npassword  Password None  \nhostname  Host name (lower case) None  \nport  Port number as integer, if present None   Reading the port attribute will raise a ValueError if an invalid port is specified in the URL. See section Structured Parse Results for more information on the result object. Unmatched square brackets in the netloc attribute will raise a ValueError. Characters in the netloc attribute that decompose under NFKC normalization (as used by the IDNA encoding) into any of \/, ?, #, @, or : will raise a ValueError. If the URL is decomposed before parsing, no error will be raised. As is the case with all named tuples, the subclass has a few additional methods and attributes that are particularly useful. One such method is _replace(). The _replace() method will return a new ParseResult object replacing specified fields with new values. >>> from urllib.parse import urlparse\n>>> u = urlparse('\/\/www.cwi.nl:80\/%7Eguido\/Python.html')\n>>> u\nParseResult(scheme='', netloc='www.cwi.nl:80', path='\/%7Eguido\/Python.html',\n            params='', query='', fragment='')\n>>> u._replace(scheme='http')\nParseResult(scheme='http', netloc='www.cwi.nl:80', path='\/%7Eguido\/Python.html',\n            params='', query='', fragment='')\n  Changed in version 3.2: Added IPv6 URL parsing capabilities.   Changed in version 3.3: The fragment is now parsed for all URL schemes (unless allow_fragment is false), in accordance with RFC 3986. Previously, a whitelist of schemes that support fragments existed.   Changed in version 3.6: Out-of-range port numbers now raise ValueError, instead of returning None.   Changed in version 3.8: Characters that affect netloc parsing under NFKC normalization will now raise ValueError.  \n  \nurllib.parse.parse_qs(qs, keep_blank_values=False, strict_parsing=False, encoding='utf-8', errors='replace', max_num_fields=None, separator='&')  \nParse a query string given as a string argument (data of type application\/x-www-form-urlencoded). Data are returned as a dictionary. The dictionary keys are the unique query variable names and the values are lists of values for each name. The optional argument keep_blank_values is a flag indicating whether blank values in percent-encoded queries should be treated as blank strings. A true value indicates that blanks should be retained as blank strings. The default false value indicates that blank values are to be ignored and treated as if they were not included. The optional argument strict_parsing is a flag indicating what to do with parsing errors. If false (the default), errors are silently ignored. If true, errors raise a ValueError exception. The optional encoding and errors parameters specify how to decode percent-encoded sequences into Unicode characters, as accepted by the bytes.decode() method. The optional argument max_num_fields is the maximum number of fields to read. If set, then throws a ValueError if there are more than max_num_fields fields read. The optional argument separator is the symbol to use for separating the query arguments. It defaults to &. Use the urllib.parse.urlencode() function (with the doseq parameter set to True) to convert such dictionaries into query strings.  Changed in version 3.2: Add encoding and errors parameters.   Changed in version 3.8: Added max_num_fields parameter.   Changed in version 3.9.2: Added separator parameter with the default value of &. Python versions earlier than Python 3.9.2 allowed using both ; and & as query parameter separator. This has been changed to allow only a single separator key, with & as the default separator.  \n  \nurllib.parse.parse_qsl(qs, keep_blank_values=False, strict_parsing=False, encoding='utf-8', errors='replace', max_num_fields=None, separator='&')  \nParse a query string given as a string argument (data of type application\/x-www-form-urlencoded). Data are returned as a list of name, value pairs. The optional argument keep_blank_values is a flag indicating whether blank values in percent-encoded queries should be treated as blank strings. A true value indicates that blanks should be retained as blank strings. The default false value indicates that blank values are to be ignored and treated as if they were not included. The optional argument strict_parsing is a flag indicating what to do with parsing errors. If false (the default), errors are silently ignored. If true, errors raise a ValueError exception. The optional encoding and errors parameters specify how to decode percent-encoded sequences into Unicode characters, as accepted by the bytes.decode() method. The optional argument max_num_fields is the maximum number of fields to read. If set, then throws a ValueError if there are more than max_num_fields fields read. The optional argument separator is the symbol to use for separating the query arguments. It defaults to &. Use the urllib.parse.urlencode() function to convert such lists of pairs into query strings.  Changed in version 3.2: Add encoding and errors parameters.   Changed in version 3.8: Added max_num_fields parameter.   Changed in version 3.9.2: Added separator parameter with the default value of &. Python versions earlier than Python 3.9.2 allowed using both ; and & as query parameter separator. This has been changed to allow only a single separator key, with & as the default separator.  \n  \nurllib.parse.urlunparse(parts)  \nConstruct a URL from a tuple as returned by urlparse(). The parts argument can be any six-item iterable. This may result in a slightly different, but equivalent URL, if the URL that was parsed originally had unnecessary delimiters (for example, a ? with an empty query; the RFC states that these are equivalent). \n  \nurllib.parse.urlsplit(urlstring, scheme='', allow_fragments=True)  \nThis is similar to urlparse(), but does not split the params from the URL. This should generally be used instead of urlparse() if the more recent URL syntax allowing parameters to be applied to each segment of the path portion of the URL (see RFC 2396) is wanted. A separate function is needed to separate the path segments and parameters. This function returns a 5-item named tuple: (addressing scheme, network location, path, query, fragment identifier).\n The return value is a named tuple, its items can be accessed by index or as named attributes:   \nAttribute Index Value Value if not present   \nscheme 0 URL scheme specifier scheme parameter  \nnetloc 1 Network location part empty string  \npath 2 Hierarchical path empty string  \nquery 3 Query component empty string  \nfragment 4 Fragment identifier empty string  \nusername  User name None  \npassword  Password None  \nhostname  Host name (lower case) None  \nport  Port number as integer, if present None   Reading the port attribute will raise a ValueError if an invalid port is specified in the URL. See section Structured Parse Results for more information on the result object. Unmatched square brackets in the netloc attribute will raise a ValueError. Characters in the netloc attribute that decompose under NFKC normalization (as used by the IDNA encoding) into any of \/, ?, #, @, or : will raise a ValueError. If the URL is decomposed before parsing, no error will be raised.  Changed in version 3.6: Out-of-range port numbers now raise ValueError, instead of returning None.   Changed in version 3.8: Characters that affect netloc parsing under NFKC normalization will now raise ValueError.  \n  \nurllib.parse.urlunsplit(parts)  \nCombine the elements of a tuple as returned by urlsplit() into a complete URL as a string. The parts argument can be any five-item iterable. This may result in a slightly different, but equivalent URL, if the URL that was parsed originally had unnecessary delimiters (for example, a ? with an empty query; the RFC states that these are equivalent). \n  \nurllib.parse.urljoin(base, url, allow_fragments=True)  \nConstruct a full (\u201cabsolute\u201d) URL by combining a \u201cbase URL\u201d (base) with another URL (url). Informally, this uses components of the base URL, in particular the addressing scheme, the network location and (part of) the path, to provide missing components in the relative URL. For example: >>> from urllib.parse import urljoin\n>>> urljoin('http:\/\/www.cwi.nl\/%7Eguido\/Python.html', 'FAQ.html')\n'http:\/\/www.cwi.nl\/%7Eguido\/FAQ.html'\n The allow_fragments argument has the same meaning and default as for urlparse().  Note If url is an absolute URL (that is, it starts with \/\/ or scheme:\/\/), the url\u2019s hostname and\/or scheme will be present in the result. For example: >>> urljoin('http:\/\/www.cwi.nl\/%7Eguido\/Python.html',\n...         '\/\/www.python.org\/%7Eguido')\n'http:\/\/www.python.org\/%7Eguido'\n If you do not want that behavior, preprocess the url with urlsplit() and urlunsplit(), removing possible scheme and netloc parts.   Changed in version 3.5: Behavior updated to match the semantics defined in RFC 3986.  \n  \nurllib.parse.urldefrag(url)  \nIf url contains a fragment identifier, return a modified version of url with no fragment identifier, and the fragment identifier as a separate string. If there is no fragment identifier in url, return url unmodified and an empty string. The return value is a named tuple, its items can be accessed by index or as named attributes:   \nAttribute Index Value Value if not present   \nurl 0 URL with no fragment empty string  \nfragment 1 Fragment identifier empty string   See section Structured Parse Results for more information on the result object.  Changed in version 3.2: Result is a structured object rather than a simple 2-tuple.  \n  \nurllib.parse.unwrap(url)  \nExtract the url from a wrapped URL (that is, a string formatted as <URL:scheme:\/\/host\/path>, <scheme:\/\/host\/path>, URL:scheme:\/\/host\/path or scheme:\/\/host\/path). If url is not a wrapped URL, it is returned without changes. \n Parsing ASCII Encoded Bytes The URL parsing functions were originally designed to operate on character strings only. In practice, it is useful to be able to manipulate properly quoted and encoded URLs as sequences of ASCII bytes. Accordingly, the URL parsing functions in this module all operate on bytes and bytearray objects in addition to str objects. If str data is passed in, the result will also contain only str data. If bytes or bytearray data is passed in, the result will contain only bytes data. Attempting to mix str data with bytes or bytearray in a single function call will result in a TypeError being raised, while attempting to pass in non-ASCII byte values will trigger UnicodeDecodeError. To support easier conversion of result objects between str and bytes, all return values from URL parsing functions provide either an encode() method (when the result contains str data) or a decode() method (when the result contains bytes data). The signatures of these methods match those of the corresponding str and bytes methods (except that the default encoding is 'ascii' rather than 'utf-8'). Each produces a value of a corresponding type that contains either bytes data (for encode() methods) or str data (for decode() methods). Applications that need to operate on potentially improperly quoted URLs that may contain non-ASCII data will need to do their own decoding from bytes to characters before invoking the URL parsing methods. The behaviour described in this section applies only to the URL parsing functions. The URL quoting functions use their own rules when producing or consuming byte sequences as detailed in the documentation of the individual URL quoting functions.  Changed in version 3.2: URL parsing functions now accept ASCII encoded byte sequences  Structured Parse Results The result objects from the urlparse(), urlsplit() and urldefrag() functions are subclasses of the tuple type. These subclasses add the attributes listed in the documentation for those functions, the encoding and decoding support described in the previous section, as well as an additional method:  \nurllib.parse.SplitResult.geturl()  \nReturn the re-combined version of the original URL as a string. This may differ from the original URL in that the scheme may be normalized to lower case and empty components may be dropped. Specifically, empty parameters, queries, and fragment identifiers will be removed. For urldefrag() results, only empty fragment identifiers will be removed. For urlsplit() and urlparse() results, all noted changes will be made to the URL returned by this method. The result of this method remains unchanged if passed back through the original parsing function: >>> from urllib.parse import urlsplit\n>>> url = 'HTTP:\/\/www.Python.org\/doc\/#'\n>>> r1 = urlsplit(url)\n>>> r1.geturl()\n'http:\/\/www.Python.org\/doc\/'\n>>> r2 = urlsplit(r1.geturl())\n>>> r2.geturl()\n'http:\/\/www.Python.org\/doc\/'\n \n The following classes provide the implementations of the structured parse results when operating on str objects:  \nclass urllib.parse.DefragResult(url, fragment)  \nConcrete class for urldefrag() results containing str data. The encode() method returns a DefragResultBytes instance.  New in version 3.2.  \n  \nclass urllib.parse.ParseResult(scheme, netloc, path, params, query, fragment)  \nConcrete class for urlparse() results containing str data. The encode() method returns a ParseResultBytes instance. \n  \nclass urllib.parse.SplitResult(scheme, netloc, path, query, fragment)  \nConcrete class for urlsplit() results containing str data. The encode() method returns a SplitResultBytes instance. \n The following classes provide the implementations of the parse results when operating on bytes or bytearray objects:  \nclass urllib.parse.DefragResultBytes(url, fragment)  \nConcrete class for urldefrag() results containing bytes data. The decode() method returns a DefragResult instance.  New in version 3.2.  \n  \nclass urllib.parse.ParseResultBytes(scheme, netloc, path, params, query, fragment)  \nConcrete class for urlparse() results containing bytes data. The decode() method returns a ParseResult instance.  New in version 3.2.  \n  \nclass urllib.parse.SplitResultBytes(scheme, netloc, path, query, fragment)  \nConcrete class for urlsplit() results containing bytes data. The decode() method returns a SplitResult instance.  New in version 3.2.  \n URL Quoting The URL quoting functions focus on taking program data and making it safe for use as URL components by quoting special characters and appropriately encoding non-ASCII text. They also support reversing these operations to recreate the original data from the contents of a URL component if that task isn\u2019t already covered by the URL parsing functions above.  \nurllib.parse.quote(string, safe='\/', encoding=None, errors=None)  \nReplace special characters in string using the %xx escape. Letters, digits, and the characters '_.-~' are never quoted. By default, this function is intended for quoting the path section of a URL. The optional safe parameter specifies additional ASCII characters that should not be quoted \u2014 its default value is '\/'. string may be either a str or a bytes object.  Changed in version 3.7: Moved from RFC 2396 to RFC 3986 for quoting URL strings. \u201c~\u201d is now included in the set of unreserved characters.  The optional encoding and errors parameters specify how to deal with non-ASCII characters, as accepted by the str.encode() method. encoding defaults to 'utf-8'. errors defaults to 'strict', meaning unsupported characters raise a UnicodeEncodeError. encoding and errors must not be supplied if string is a bytes, or a TypeError is raised. Note that quote(string, safe, encoding, errors) is equivalent to quote_from_bytes(string.encode(encoding, errors), safe). Example: quote('\/El Ni\u00f1o\/') yields '\/El%20Ni%C3%B1o\/'. \n  \nurllib.parse.quote_plus(string, safe='', encoding=None, errors=None)  \nLike quote(), but also replace spaces with plus signs, as required for quoting HTML form values when building up a query string to go into a URL. Plus signs in the original string are escaped unless they are included in safe. It also does not have safe default to '\/'. Example: quote_plus('\/El Ni\u00f1o\/') yields '%2FEl+Ni%C3%B1o%2F'. \n  \nurllib.parse.quote_from_bytes(bytes, safe='\/')  \nLike quote(), but accepts a bytes object rather than a str, and does not perform string-to-bytes encoding. Example: quote_from_bytes(b'a&\\xef') yields 'a%26%EF'. \n  \nurllib.parse.unquote(string, encoding='utf-8', errors='replace')  \nReplace %xx escapes with their single-character equivalent. The optional encoding and errors parameters specify how to decode percent-encoded sequences into Unicode characters, as accepted by the bytes.decode() method. string may be either a str or a bytes object. encoding defaults to 'utf-8'. errors defaults to 'replace', meaning invalid sequences are replaced by a placeholder character. Example: unquote('\/El%20Ni%C3%B1o\/') yields '\/El Ni\u00f1o\/'.  Changed in version 3.9: string parameter supports bytes and str objects (previously only str).  \n  \nurllib.parse.unquote_plus(string, encoding='utf-8', errors='replace')  \nLike unquote(), but also replace plus signs with spaces, as required for unquoting HTML form values. string must be a str. Example: unquote_plus('\/El+Ni%C3%B1o\/') yields '\/El Ni\u00f1o\/'. \n  \nurllib.parse.unquote_to_bytes(string)  \nReplace %xx escapes with their single-octet equivalent, and return a bytes object. string may be either a str or a bytes object. If it is a str, unescaped non-ASCII characters in string are encoded into UTF-8 bytes. Example: unquote_to_bytes('a%26%EF') yields b'a&\\xef'. \n  \nurllib.parse.urlencode(query, doseq=False, safe='', encoding=None, errors=None, quote_via=quote_plus)  \nConvert a mapping object or a sequence of two-element tuples, which may contain str or bytes objects, to a percent-encoded ASCII text string. If the resultant string is to be used as a data for POST operation with the urlopen() function, then it should be encoded to bytes, otherwise it would result in a TypeError. The resulting string is a series of key=value pairs separated by '&' characters, where both key and value are quoted using the quote_via function. By default, quote_plus() is used to quote the values, which means spaces are quoted as a '+' character and \u2018\/\u2019 characters are encoded as %2F, which follows the standard for GET requests (application\/x-www-form-urlencoded). An alternate function that can be passed as quote_via is quote(), which will encode spaces as %20 and not encode \u2018\/\u2019 characters. For maximum control of what is quoted, use quote and specify a value for safe. When a sequence of two-element tuples is used as the query argument, the first element of each tuple is a key and the second is a value. The value element in itself can be a sequence and in that case, if the optional parameter doseq evaluates to True, individual key=value pairs separated by '&' are generated for each element of the value sequence for the key. The order of parameters in the encoded string will match the order of parameter tuples in the sequence. The safe, encoding, and errors parameters are passed down to quote_via (the encoding and errors parameters are only passed when a query element is a str). To reverse this encoding process, parse_qs() and parse_qsl() are provided in this module to parse query strings into Python data structures. Refer to urllib examples to find out how the urllib.parse.urlencode() method can be used for generating the query string of a URL or data for a POST request.  Changed in version 3.2: query supports bytes and string objects.   New in version 3.5: quote_via parameter.  \n  See also  \nRFC 3986 - Uniform Resource Identifiers\n\nThis is the current standard (STD66). Any changes to urllib.parse module should conform to this. Certain deviations could be observed, which are mostly for backward compatibility purposes and for certain de-facto parsing requirements as commonly observed in major browsers.  \nRFC 2732 - Format for Literal IPv6 Addresses in URL\u2019s.\n\nThis specifies the parsing requirements of IPv6 URLs.  \nRFC 2396 - Uniform Resource Identifiers (URI): Generic Syntax\n\nDocument describing the generic syntactic requirements for both Uniform Resource Names (URNs) and Uniform Resource Locators (URLs).  \nRFC 2368 - The mailto URL scheme.\n\nParsing requirements for mailto URL schemes.  \nRFC 1808 - Relative Uniform Resource Locators\n\nThis Request For Comments includes the rules for joining an absolute and a relative URL, including a fair number of \u201cAbnormal Examples\u201d which govern the treatment of border cases.  \nRFC 1738 - Uniform Resource Locators (URL)\n\nThis specifies the formal syntax and semantics of absolute URLs.","title":"python.library.urllib.parse"},{"text":"url_name  \nThe name of the URL pattern that matches the URL.","title":"django.ref.urlresolvers#django.urls.ResolverMatch.url_name"},{"text":"urllib.parse.urlparse(urlstring, scheme='', allow_fragments=True)  \nParse a URL into six components, returning a 6-item named tuple. This corresponds to the general structure of a URL: scheme:\/\/netloc\/path;parameters?query#fragment. Each tuple item is a string, possibly empty. The components are not broken up into smaller parts (for example, the network location is a single string), and % escapes are not expanded. The delimiters as shown above are not part of the result, except for a leading slash in the path component, which is retained if present. For example: >>> from urllib.parse import urlparse\n>>> o = urlparse('http:\/\/www.cwi.nl:80\/%7Eguido\/Python.html')\n>>> o   \nParseResult(scheme='http', netloc='www.cwi.nl:80', path='\/%7Eguido\/Python.html',\n            params='', query='', fragment='')\n>>> o.scheme\n'http'\n>>> o.port\n80\n>>> o.geturl()\n'http:\/\/www.cwi.nl:80\/%7Eguido\/Python.html'\n Following the syntax specifications in RFC 1808, urlparse recognizes a netloc only if it is properly introduced by \u2018\/\/\u2019. Otherwise the input is presumed to be a relative URL and thus to start with a path component. >>> from urllib.parse import urlparse\n>>> urlparse('\/\/www.cwi.nl:80\/%7Eguido\/Python.html')\nParseResult(scheme='', netloc='www.cwi.nl:80', path='\/%7Eguido\/Python.html',\n            params='', query='', fragment='')\n>>> urlparse('www.cwi.nl\/%7Eguido\/Python.html')\nParseResult(scheme='', netloc='', path='www.cwi.nl\/%7Eguido\/Python.html',\n            params='', query='', fragment='')\n>>> urlparse('help\/Python.html')\nParseResult(scheme='', netloc='', path='help\/Python.html', params='',\n            query='', fragment='')\n The scheme argument gives the default addressing scheme, to be used only if the URL does not specify one. It should be the same type (text or bytes) as urlstring, except that the default value '' is always allowed, and is automatically converted to b'' if appropriate. If the allow_fragments argument is false, fragment identifiers are not recognized. Instead, they are parsed as part of the path, parameters or query component, and fragment is set to the empty string in the return value. The return value is a named tuple, which means that its items can be accessed by index or as named attributes, which are:   \nAttribute Index Value Value if not present   \nscheme 0 URL scheme specifier scheme parameter  \nnetloc 1 Network location part empty string  \npath 2 Hierarchical path empty string  \nparams 3 Parameters for last path element empty string  \nquery 4 Query component empty string  \nfragment 5 Fragment identifier empty string  \nusername  User name None  \npassword  Password None  \nhostname  Host name (lower case) None  \nport  Port number as integer, if present None   Reading the port attribute will raise a ValueError if an invalid port is specified in the URL. See section Structured Parse Results for more information on the result object. Unmatched square brackets in the netloc attribute will raise a ValueError. Characters in the netloc attribute that decompose under NFKC normalization (as used by the IDNA encoding) into any of \/, ?, #, @, or : will raise a ValueError. If the URL is decomposed before parsing, no error will be raised. As is the case with all named tuples, the subclass has a few additional methods and attributes that are particularly useful. One such method is _replace(). The _replace() method will return a new ParseResult object replacing specified fields with new values. >>> from urllib.parse import urlparse\n>>> u = urlparse('\/\/www.cwi.nl:80\/%7Eguido\/Python.html')\n>>> u\nParseResult(scheme='', netloc='www.cwi.nl:80', path='\/%7Eguido\/Python.html',\n            params='', query='', fragment='')\n>>> u._replace(scheme='http')\nParseResult(scheme='http', netloc='www.cwi.nl:80', path='\/%7Eguido\/Python.html',\n            params='', query='', fragment='')\n  Changed in version 3.2: Added IPv6 URL parsing capabilities.   Changed in version 3.3: The fragment is now parsed for all URL schemes (unless allow_fragment is false), in accordance with RFC 3986. Previously, a whitelist of schemes that support fragments existed.   Changed in version 3.6: Out-of-range port numbers now raise ValueError, instead of returning None.   Changed in version 3.8: Characters that affect netloc parsing under NFKC normalization will now raise ValueError.","title":"python.library.urllib.parse#urllib.parse.urlparse"},{"text":"urllib.parse.unquote(string, encoding='utf-8', errors='replace')  \nReplace %xx escapes with their single-character equivalent. The optional encoding and errors parameters specify how to decode percent-encoded sequences into Unicode characters, as accepted by the bytes.decode() method. string may be either a str or a bytes object. encoding defaults to 'utf-8'. errors defaults to 'replace', meaning invalid sequences are replaced by a placeholder character. Example: unquote('\/El%20Ni%C3%B1o\/') yields '\/El Ni\u00f1o\/'.  Changed in version 3.9: string parameter supports bytes and str objects (previously only str).","title":"python.library.urllib.parse#urllib.parse.unquote"},{"text":"to_url()  \nReturns a URL string or bytes depending on the type of the information stored. This is just a convenience function for calling url_unparse() for this URL.  Return type \nstr","title":"werkzeug.urls.index#werkzeug.urls.BaseURL.to_url"}]}
{"task_id":9760588,"prompt":"def f_9760588(myString):\n\treturn ","suffix":"","canonical_solution":"re.search('(?P<url>https?:\/\/[^\\\\s]+)', myString).group('url')","test_start":"\nimport re\n\ndef check(candidate):","test":["\n    assert candidate(\"This is a link http:\/\/www.google.com\") == \"http:\/\/www.google.com\"\n","\n    assert candidate(\"Please refer to the website: http:\/\/www.google.com\") == \"http:\/\/www.google.com\"\n"],"entry_point":"f_9760588","intent":"extract a url from a string `myString`","library":["re"],"docs":[{"text":"urllib.parse.unwrap(url)  \nExtract the url from a wrapped URL (that is, a string formatted as <URL:scheme:\/\/host\/path>, <scheme:\/\/host\/path>, URL:scheme:\/\/host\/path or scheme:\/\/host\/path). If url is not a wrapped URL, it is returned without changes.","title":"python.library.urllib.parse#urllib.parse.unwrap"},{"text":"urllib.parse.unquote_plus(string, encoding='utf-8', errors='replace')  \nLike unquote(), but also replace plus signs with spaces, as required for unquoting HTML form values. string must be a str. Example: unquote_plus('\/El+Ni%C3%B1o\/') yields '\/El Ni\u00f1o\/'.","title":"python.library.urllib.parse#urllib.parse.unquote_plus"},{"text":"werkzeug.urls.url_fix(s, charset='utf-8')  \nSometimes you get an URL by a user that just isn\u2019t a real URL because it contains unsafe characters like \u2018 \u2018 and so on. This function can fix some of the problems in a similar way browsers handle data entered by the user: >>> url_fix('http:\/\/de.wikipedia.org\/wiki\/Elf (Begriffskl\\xe4rung)')\n'http:\/\/de.wikipedia.org\/wiki\/Elf%20(Begriffskl%C3%A4rung)'\n  Parameters \n \ns (str) \u2013 the string with the URL to fix. \ncharset (str) \u2013 The target charset for the URL if the url was given as a string.   Return type \nstr","title":"werkzeug.urls.index#werkzeug.urls.url_fix"},{"text":"urllib.parse.urlsplit(urlstring, scheme='', allow_fragments=True)  \nThis is similar to urlparse(), but does not split the params from the URL. This should generally be used instead of urlparse() if the more recent URL syntax allowing parameters to be applied to each segment of the path portion of the URL (see RFC 2396) is wanted. A separate function is needed to separate the path segments and parameters. This function returns a 5-item named tuple: (addressing scheme, network location, path, query, fragment identifier).\n The return value is a named tuple, its items can be accessed by index or as named attributes:   \nAttribute Index Value Value if not present   \nscheme 0 URL scheme specifier scheme parameter  \nnetloc 1 Network location part empty string  \npath 2 Hierarchical path empty string  \nquery 3 Query component empty string  \nfragment 4 Fragment identifier empty string  \nusername  User name None  \npassword  Password None  \nhostname  Host name (lower case) None  \nport  Port number as integer, if present None   Reading the port attribute will raise a ValueError if an invalid port is specified in the URL. See section Structured Parse Results for more information on the result object. Unmatched square brackets in the netloc attribute will raise a ValueError. Characters in the netloc attribute that decompose under NFKC normalization (as used by the IDNA encoding) into any of \/, ?, #, @, or : will raise a ValueError. If the URL is decomposed before parsing, no error will be raised.  Changed in version 3.6: Out-of-range port numbers now raise ValueError, instead of returning None.   Changed in version 3.8: Characters that affect netloc parsing under NFKC normalization will now raise ValueError.","title":"python.library.urllib.parse#urllib.parse.urlsplit"},{"text":"urllib.parse.SplitResult.geturl()  \nReturn the re-combined version of the original URL as a string. This may differ from the original URL in that the scheme may be normalized to lower case and empty components may be dropped. Specifically, empty parameters, queries, and fragment identifiers will be removed. For urldefrag() results, only empty fragment identifiers will be removed. For urlsplit() and urlparse() results, all noted changes will be made to the URL returned by this method. The result of this method remains unchanged if passed back through the original parsing function: >>> from urllib.parse import urlsplit\n>>> url = 'HTTP:\/\/www.Python.org\/doc\/#'\n>>> r1 = urlsplit(url)\n>>> r1.geturl()\n'http:\/\/www.Python.org\/doc\/'\n>>> r2 = urlsplit(r1.geturl())\n>>> r2.geturl()\n'http:\/\/www.Python.org\/doc\/'","title":"python.library.urllib.parse#urllib.parse.urllib.parse.SplitResult.geturl"},{"text":"urllib.parse \u2014 Parse URLs into components Source code: Lib\/urllib\/parse.py This module defines a standard interface to break Uniform Resource Locator (URL) strings up in components (addressing scheme, network location, path etc.), to combine the components back into a URL string, and to convert a \u201crelative URL\u201d to an absolute URL given a \u201cbase URL.\u201d The module has been designed to match the Internet RFC on Relative Uniform Resource Locators. It supports the following URL schemes: file, ftp, gopher, hdl, http, https, imap, mailto, mms, news, nntp, prospero, rsync, rtsp, rtspu, sftp, shttp, sip, sips, snews, svn, svn+ssh, telnet, wais, ws, wss. The urllib.parse module defines functions that fall into two broad categories: URL parsing and URL quoting. These are covered in detail in the following sections. URL Parsing The URL parsing functions focus on splitting a URL string into its components, or on combining URL components into a URL string.  \nurllib.parse.urlparse(urlstring, scheme='', allow_fragments=True)  \nParse a URL into six components, returning a 6-item named tuple. This corresponds to the general structure of a URL: scheme:\/\/netloc\/path;parameters?query#fragment. Each tuple item is a string, possibly empty. The components are not broken up into smaller parts (for example, the network location is a single string), and % escapes are not expanded. The delimiters as shown above are not part of the result, except for a leading slash in the path component, which is retained if present. For example: >>> from urllib.parse import urlparse\n>>> o = urlparse('http:\/\/www.cwi.nl:80\/%7Eguido\/Python.html')\n>>> o   \nParseResult(scheme='http', netloc='www.cwi.nl:80', path='\/%7Eguido\/Python.html',\n            params='', query='', fragment='')\n>>> o.scheme\n'http'\n>>> o.port\n80\n>>> o.geturl()\n'http:\/\/www.cwi.nl:80\/%7Eguido\/Python.html'\n Following the syntax specifications in RFC 1808, urlparse recognizes a netloc only if it is properly introduced by \u2018\/\/\u2019. Otherwise the input is presumed to be a relative URL and thus to start with a path component. >>> from urllib.parse import urlparse\n>>> urlparse('\/\/www.cwi.nl:80\/%7Eguido\/Python.html')\nParseResult(scheme='', netloc='www.cwi.nl:80', path='\/%7Eguido\/Python.html',\n            params='', query='', fragment='')\n>>> urlparse('www.cwi.nl\/%7Eguido\/Python.html')\nParseResult(scheme='', netloc='', path='www.cwi.nl\/%7Eguido\/Python.html',\n            params='', query='', fragment='')\n>>> urlparse('help\/Python.html')\nParseResult(scheme='', netloc='', path='help\/Python.html', params='',\n            query='', fragment='')\n The scheme argument gives the default addressing scheme, to be used only if the URL does not specify one. It should be the same type (text or bytes) as urlstring, except that the default value '' is always allowed, and is automatically converted to b'' if appropriate. If the allow_fragments argument is false, fragment identifiers are not recognized. Instead, they are parsed as part of the path, parameters or query component, and fragment is set to the empty string in the return value. The return value is a named tuple, which means that its items can be accessed by index or as named attributes, which are:   \nAttribute Index Value Value if not present   \nscheme 0 URL scheme specifier scheme parameter  \nnetloc 1 Network location part empty string  \npath 2 Hierarchical path empty string  \nparams 3 Parameters for last path element empty string  \nquery 4 Query component empty string  \nfragment 5 Fragment identifier empty string  \nusername  User name None  \npassword  Password None  \nhostname  Host name (lower case) None  \nport  Port number as integer, if present None   Reading the port attribute will raise a ValueError if an invalid port is specified in the URL. See section Structured Parse Results for more information on the result object. Unmatched square brackets in the netloc attribute will raise a ValueError. Characters in the netloc attribute that decompose under NFKC normalization (as used by the IDNA encoding) into any of \/, ?, #, @, or : will raise a ValueError. If the URL is decomposed before parsing, no error will be raised. As is the case with all named tuples, the subclass has a few additional methods and attributes that are particularly useful. One such method is _replace(). The _replace() method will return a new ParseResult object replacing specified fields with new values. >>> from urllib.parse import urlparse\n>>> u = urlparse('\/\/www.cwi.nl:80\/%7Eguido\/Python.html')\n>>> u\nParseResult(scheme='', netloc='www.cwi.nl:80', path='\/%7Eguido\/Python.html',\n            params='', query='', fragment='')\n>>> u._replace(scheme='http')\nParseResult(scheme='http', netloc='www.cwi.nl:80', path='\/%7Eguido\/Python.html',\n            params='', query='', fragment='')\n  Changed in version 3.2: Added IPv6 URL parsing capabilities.   Changed in version 3.3: The fragment is now parsed for all URL schemes (unless allow_fragment is false), in accordance with RFC 3986. Previously, a whitelist of schemes that support fragments existed.   Changed in version 3.6: Out-of-range port numbers now raise ValueError, instead of returning None.   Changed in version 3.8: Characters that affect netloc parsing under NFKC normalization will now raise ValueError.  \n  \nurllib.parse.parse_qs(qs, keep_blank_values=False, strict_parsing=False, encoding='utf-8', errors='replace', max_num_fields=None, separator='&')  \nParse a query string given as a string argument (data of type application\/x-www-form-urlencoded). Data are returned as a dictionary. The dictionary keys are the unique query variable names and the values are lists of values for each name. The optional argument keep_blank_values is a flag indicating whether blank values in percent-encoded queries should be treated as blank strings. A true value indicates that blanks should be retained as blank strings. The default false value indicates that blank values are to be ignored and treated as if they were not included. The optional argument strict_parsing is a flag indicating what to do with parsing errors. If false (the default), errors are silently ignored. If true, errors raise a ValueError exception. The optional encoding and errors parameters specify how to decode percent-encoded sequences into Unicode characters, as accepted by the bytes.decode() method. The optional argument max_num_fields is the maximum number of fields to read. If set, then throws a ValueError if there are more than max_num_fields fields read. The optional argument separator is the symbol to use for separating the query arguments. It defaults to &. Use the urllib.parse.urlencode() function (with the doseq parameter set to True) to convert such dictionaries into query strings.  Changed in version 3.2: Add encoding and errors parameters.   Changed in version 3.8: Added max_num_fields parameter.   Changed in version 3.9.2: Added separator parameter with the default value of &. Python versions earlier than Python 3.9.2 allowed using both ; and & as query parameter separator. This has been changed to allow only a single separator key, with & as the default separator.  \n  \nurllib.parse.parse_qsl(qs, keep_blank_values=False, strict_parsing=False, encoding='utf-8', errors='replace', max_num_fields=None, separator='&')  \nParse a query string given as a string argument (data of type application\/x-www-form-urlencoded). Data are returned as a list of name, value pairs. The optional argument keep_blank_values is a flag indicating whether blank values in percent-encoded queries should be treated as blank strings. A true value indicates that blanks should be retained as blank strings. The default false value indicates that blank values are to be ignored and treated as if they were not included. The optional argument strict_parsing is a flag indicating what to do with parsing errors. If false (the default), errors are silently ignored. If true, errors raise a ValueError exception. The optional encoding and errors parameters specify how to decode percent-encoded sequences into Unicode characters, as accepted by the bytes.decode() method. The optional argument max_num_fields is the maximum number of fields to read. If set, then throws a ValueError if there are more than max_num_fields fields read. The optional argument separator is the symbol to use for separating the query arguments. It defaults to &. Use the urllib.parse.urlencode() function to convert such lists of pairs into query strings.  Changed in version 3.2: Add encoding and errors parameters.   Changed in version 3.8: Added max_num_fields parameter.   Changed in version 3.9.2: Added separator parameter with the default value of &. Python versions earlier than Python 3.9.2 allowed using both ; and & as query parameter separator. This has been changed to allow only a single separator key, with & as the default separator.  \n  \nurllib.parse.urlunparse(parts)  \nConstruct a URL from a tuple as returned by urlparse(). The parts argument can be any six-item iterable. This may result in a slightly different, but equivalent URL, if the URL that was parsed originally had unnecessary delimiters (for example, a ? with an empty query; the RFC states that these are equivalent). \n  \nurllib.parse.urlsplit(urlstring, scheme='', allow_fragments=True)  \nThis is similar to urlparse(), but does not split the params from the URL. This should generally be used instead of urlparse() if the more recent URL syntax allowing parameters to be applied to each segment of the path portion of the URL (see RFC 2396) is wanted. A separate function is needed to separate the path segments and parameters. This function returns a 5-item named tuple: (addressing scheme, network location, path, query, fragment identifier).\n The return value is a named tuple, its items can be accessed by index or as named attributes:   \nAttribute Index Value Value if not present   \nscheme 0 URL scheme specifier scheme parameter  \nnetloc 1 Network location part empty string  \npath 2 Hierarchical path empty string  \nquery 3 Query component empty string  \nfragment 4 Fragment identifier empty string  \nusername  User name None  \npassword  Password None  \nhostname  Host name (lower case) None  \nport  Port number as integer, if present None   Reading the port attribute will raise a ValueError if an invalid port is specified in the URL. See section Structured Parse Results for more information on the result object. Unmatched square brackets in the netloc attribute will raise a ValueError. Characters in the netloc attribute that decompose under NFKC normalization (as used by the IDNA encoding) into any of \/, ?, #, @, or : will raise a ValueError. If the URL is decomposed before parsing, no error will be raised.  Changed in version 3.6: Out-of-range port numbers now raise ValueError, instead of returning None.   Changed in version 3.8: Characters that affect netloc parsing under NFKC normalization will now raise ValueError.  \n  \nurllib.parse.urlunsplit(parts)  \nCombine the elements of a tuple as returned by urlsplit() into a complete URL as a string. The parts argument can be any five-item iterable. This may result in a slightly different, but equivalent URL, if the URL that was parsed originally had unnecessary delimiters (for example, a ? with an empty query; the RFC states that these are equivalent). \n  \nurllib.parse.urljoin(base, url, allow_fragments=True)  \nConstruct a full (\u201cabsolute\u201d) URL by combining a \u201cbase URL\u201d (base) with another URL (url). Informally, this uses components of the base URL, in particular the addressing scheme, the network location and (part of) the path, to provide missing components in the relative URL. For example: >>> from urllib.parse import urljoin\n>>> urljoin('http:\/\/www.cwi.nl\/%7Eguido\/Python.html', 'FAQ.html')\n'http:\/\/www.cwi.nl\/%7Eguido\/FAQ.html'\n The allow_fragments argument has the same meaning and default as for urlparse().  Note If url is an absolute URL (that is, it starts with \/\/ or scheme:\/\/), the url\u2019s hostname and\/or scheme will be present in the result. For example: >>> urljoin('http:\/\/www.cwi.nl\/%7Eguido\/Python.html',\n...         '\/\/www.python.org\/%7Eguido')\n'http:\/\/www.python.org\/%7Eguido'\n If you do not want that behavior, preprocess the url with urlsplit() and urlunsplit(), removing possible scheme and netloc parts.   Changed in version 3.5: Behavior updated to match the semantics defined in RFC 3986.  \n  \nurllib.parse.urldefrag(url)  \nIf url contains a fragment identifier, return a modified version of url with no fragment identifier, and the fragment identifier as a separate string. If there is no fragment identifier in url, return url unmodified and an empty string. The return value is a named tuple, its items can be accessed by index or as named attributes:   \nAttribute Index Value Value if not present   \nurl 0 URL with no fragment empty string  \nfragment 1 Fragment identifier empty string   See section Structured Parse Results for more information on the result object.  Changed in version 3.2: Result is a structured object rather than a simple 2-tuple.  \n  \nurllib.parse.unwrap(url)  \nExtract the url from a wrapped URL (that is, a string formatted as <URL:scheme:\/\/host\/path>, <scheme:\/\/host\/path>, URL:scheme:\/\/host\/path or scheme:\/\/host\/path). If url is not a wrapped URL, it is returned without changes. \n Parsing ASCII Encoded Bytes The URL parsing functions were originally designed to operate on character strings only. In practice, it is useful to be able to manipulate properly quoted and encoded URLs as sequences of ASCII bytes. Accordingly, the URL parsing functions in this module all operate on bytes and bytearray objects in addition to str objects. If str data is passed in, the result will also contain only str data. If bytes or bytearray data is passed in, the result will contain only bytes data. Attempting to mix str data with bytes or bytearray in a single function call will result in a TypeError being raised, while attempting to pass in non-ASCII byte values will trigger UnicodeDecodeError. To support easier conversion of result objects between str and bytes, all return values from URL parsing functions provide either an encode() method (when the result contains str data) or a decode() method (when the result contains bytes data). The signatures of these methods match those of the corresponding str and bytes methods (except that the default encoding is 'ascii' rather than 'utf-8'). Each produces a value of a corresponding type that contains either bytes data (for encode() methods) or str data (for decode() methods). Applications that need to operate on potentially improperly quoted URLs that may contain non-ASCII data will need to do their own decoding from bytes to characters before invoking the URL parsing methods. The behaviour described in this section applies only to the URL parsing functions. The URL quoting functions use their own rules when producing or consuming byte sequences as detailed in the documentation of the individual URL quoting functions.  Changed in version 3.2: URL parsing functions now accept ASCII encoded byte sequences  Structured Parse Results The result objects from the urlparse(), urlsplit() and urldefrag() functions are subclasses of the tuple type. These subclasses add the attributes listed in the documentation for those functions, the encoding and decoding support described in the previous section, as well as an additional method:  \nurllib.parse.SplitResult.geturl()  \nReturn the re-combined version of the original URL as a string. This may differ from the original URL in that the scheme may be normalized to lower case and empty components may be dropped. Specifically, empty parameters, queries, and fragment identifiers will be removed. For urldefrag() results, only empty fragment identifiers will be removed. For urlsplit() and urlparse() results, all noted changes will be made to the URL returned by this method. The result of this method remains unchanged if passed back through the original parsing function: >>> from urllib.parse import urlsplit\n>>> url = 'HTTP:\/\/www.Python.org\/doc\/#'\n>>> r1 = urlsplit(url)\n>>> r1.geturl()\n'http:\/\/www.Python.org\/doc\/'\n>>> r2 = urlsplit(r1.geturl())\n>>> r2.geturl()\n'http:\/\/www.Python.org\/doc\/'\n \n The following classes provide the implementations of the structured parse results when operating on str objects:  \nclass urllib.parse.DefragResult(url, fragment)  \nConcrete class for urldefrag() results containing str data. The encode() method returns a DefragResultBytes instance.  New in version 3.2.  \n  \nclass urllib.parse.ParseResult(scheme, netloc, path, params, query, fragment)  \nConcrete class for urlparse() results containing str data. The encode() method returns a ParseResultBytes instance. \n  \nclass urllib.parse.SplitResult(scheme, netloc, path, query, fragment)  \nConcrete class for urlsplit() results containing str data. The encode() method returns a SplitResultBytes instance. \n The following classes provide the implementations of the parse results when operating on bytes or bytearray objects:  \nclass urllib.parse.DefragResultBytes(url, fragment)  \nConcrete class for urldefrag() results containing bytes data. The decode() method returns a DefragResult instance.  New in version 3.2.  \n  \nclass urllib.parse.ParseResultBytes(scheme, netloc, path, params, query, fragment)  \nConcrete class for urlparse() results containing bytes data. The decode() method returns a ParseResult instance.  New in version 3.2.  \n  \nclass urllib.parse.SplitResultBytes(scheme, netloc, path, query, fragment)  \nConcrete class for urlsplit() results containing bytes data. The decode() method returns a SplitResult instance.  New in version 3.2.  \n URL Quoting The URL quoting functions focus on taking program data and making it safe for use as URL components by quoting special characters and appropriately encoding non-ASCII text. They also support reversing these operations to recreate the original data from the contents of a URL component if that task isn\u2019t already covered by the URL parsing functions above.  \nurllib.parse.quote(string, safe='\/', encoding=None, errors=None)  \nReplace special characters in string using the %xx escape. Letters, digits, and the characters '_.-~' are never quoted. By default, this function is intended for quoting the path section of a URL. The optional safe parameter specifies additional ASCII characters that should not be quoted \u2014 its default value is '\/'. string may be either a str or a bytes object.  Changed in version 3.7: Moved from RFC 2396 to RFC 3986 for quoting URL strings. \u201c~\u201d is now included in the set of unreserved characters.  The optional encoding and errors parameters specify how to deal with non-ASCII characters, as accepted by the str.encode() method. encoding defaults to 'utf-8'. errors defaults to 'strict', meaning unsupported characters raise a UnicodeEncodeError. encoding and errors must not be supplied if string is a bytes, or a TypeError is raised. Note that quote(string, safe, encoding, errors) is equivalent to quote_from_bytes(string.encode(encoding, errors), safe). Example: quote('\/El Ni\u00f1o\/') yields '\/El%20Ni%C3%B1o\/'. \n  \nurllib.parse.quote_plus(string, safe='', encoding=None, errors=None)  \nLike quote(), but also replace spaces with plus signs, as required for quoting HTML form values when building up a query string to go into a URL. Plus signs in the original string are escaped unless they are included in safe. It also does not have safe default to '\/'. Example: quote_plus('\/El Ni\u00f1o\/') yields '%2FEl+Ni%C3%B1o%2F'. \n  \nurllib.parse.quote_from_bytes(bytes, safe='\/')  \nLike quote(), but accepts a bytes object rather than a str, and does not perform string-to-bytes encoding. Example: quote_from_bytes(b'a&\\xef') yields 'a%26%EF'. \n  \nurllib.parse.unquote(string, encoding='utf-8', errors='replace')  \nReplace %xx escapes with their single-character equivalent. The optional encoding and errors parameters specify how to decode percent-encoded sequences into Unicode characters, as accepted by the bytes.decode() method. string may be either a str or a bytes object. encoding defaults to 'utf-8'. errors defaults to 'replace', meaning invalid sequences are replaced by a placeholder character. Example: unquote('\/El%20Ni%C3%B1o\/') yields '\/El Ni\u00f1o\/'.  Changed in version 3.9: string parameter supports bytes and str objects (previously only str).  \n  \nurllib.parse.unquote_plus(string, encoding='utf-8', errors='replace')  \nLike unquote(), but also replace plus signs with spaces, as required for unquoting HTML form values. string must be a str. Example: unquote_plus('\/El+Ni%C3%B1o\/') yields '\/El Ni\u00f1o\/'. \n  \nurllib.parse.unquote_to_bytes(string)  \nReplace %xx escapes with their single-octet equivalent, and return a bytes object. string may be either a str or a bytes object. If it is a str, unescaped non-ASCII characters in string are encoded into UTF-8 bytes. Example: unquote_to_bytes('a%26%EF') yields b'a&\\xef'. \n  \nurllib.parse.urlencode(query, doseq=False, safe='', encoding=None, errors=None, quote_via=quote_plus)  \nConvert a mapping object or a sequence of two-element tuples, which may contain str or bytes objects, to a percent-encoded ASCII text string. If the resultant string is to be used as a data for POST operation with the urlopen() function, then it should be encoded to bytes, otherwise it would result in a TypeError. The resulting string is a series of key=value pairs separated by '&' characters, where both key and value are quoted using the quote_via function. By default, quote_plus() is used to quote the values, which means spaces are quoted as a '+' character and \u2018\/\u2019 characters are encoded as %2F, which follows the standard for GET requests (application\/x-www-form-urlencoded). An alternate function that can be passed as quote_via is quote(), which will encode spaces as %20 and not encode \u2018\/\u2019 characters. For maximum control of what is quoted, use quote and specify a value for safe. When a sequence of two-element tuples is used as the query argument, the first element of each tuple is a key and the second is a value. The value element in itself can be a sequence and in that case, if the optional parameter doseq evaluates to True, individual key=value pairs separated by '&' are generated for each element of the value sequence for the key. The order of parameters in the encoded string will match the order of parameter tuples in the sequence. The safe, encoding, and errors parameters are passed down to quote_via (the encoding and errors parameters are only passed when a query element is a str). To reverse this encoding process, parse_qs() and parse_qsl() are provided in this module to parse query strings into Python data structures. Refer to urllib examples to find out how the urllib.parse.urlencode() method can be used for generating the query string of a URL or data for a POST request.  Changed in version 3.2: query supports bytes and string objects.   New in version 3.5: quote_via parameter.  \n  See also  \nRFC 3986 - Uniform Resource Identifiers\n\nThis is the current standard (STD66). Any changes to urllib.parse module should conform to this. Certain deviations could be observed, which are mostly for backward compatibility purposes and for certain de-facto parsing requirements as commonly observed in major browsers.  \nRFC 2732 - Format for Literal IPv6 Addresses in URL\u2019s.\n\nThis specifies the parsing requirements of IPv6 URLs.  \nRFC 2396 - Uniform Resource Identifiers (URI): Generic Syntax\n\nDocument describing the generic syntactic requirements for both Uniform Resource Names (URNs) and Uniform Resource Locators (URLs).  \nRFC 2368 - The mailto URL scheme.\n\nParsing requirements for mailto URL schemes.  \nRFC 1808 - Relative Uniform Resource Locators\n\nThis Request For Comments includes the rules for joining an absolute and a relative URL, including a fair number of \u201cAbnormal Examples\u201d which govern the treatment of border cases.  \nRFC 1738 - Uniform Resource Locators (URL)\n\nThis specifies the formal syntax and semantics of absolute URLs.","title":"python.library.urllib.parse"},{"text":"url_name  \nThe name of the URL pattern that matches the URL.","title":"django.ref.urlresolvers#django.urls.ResolverMatch.url_name"},{"text":"urllib.parse.urlparse(urlstring, scheme='', allow_fragments=True)  \nParse a URL into six components, returning a 6-item named tuple. This corresponds to the general structure of a URL: scheme:\/\/netloc\/path;parameters?query#fragment. Each tuple item is a string, possibly empty. The components are not broken up into smaller parts (for example, the network location is a single string), and % escapes are not expanded. The delimiters as shown above are not part of the result, except for a leading slash in the path component, which is retained if present. For example: >>> from urllib.parse import urlparse\n>>> o = urlparse('http:\/\/www.cwi.nl:80\/%7Eguido\/Python.html')\n>>> o   \nParseResult(scheme='http', netloc='www.cwi.nl:80', path='\/%7Eguido\/Python.html',\n            params='', query='', fragment='')\n>>> o.scheme\n'http'\n>>> o.port\n80\n>>> o.geturl()\n'http:\/\/www.cwi.nl:80\/%7Eguido\/Python.html'\n Following the syntax specifications in RFC 1808, urlparse recognizes a netloc only if it is properly introduced by \u2018\/\/\u2019. Otherwise the input is presumed to be a relative URL and thus to start with a path component. >>> from urllib.parse import urlparse\n>>> urlparse('\/\/www.cwi.nl:80\/%7Eguido\/Python.html')\nParseResult(scheme='', netloc='www.cwi.nl:80', path='\/%7Eguido\/Python.html',\n            params='', query='', fragment='')\n>>> urlparse('www.cwi.nl\/%7Eguido\/Python.html')\nParseResult(scheme='', netloc='', path='www.cwi.nl\/%7Eguido\/Python.html',\n            params='', query='', fragment='')\n>>> urlparse('help\/Python.html')\nParseResult(scheme='', netloc='', path='help\/Python.html', params='',\n            query='', fragment='')\n The scheme argument gives the default addressing scheme, to be used only if the URL does not specify one. It should be the same type (text or bytes) as urlstring, except that the default value '' is always allowed, and is automatically converted to b'' if appropriate. If the allow_fragments argument is false, fragment identifiers are not recognized. Instead, they are parsed as part of the path, parameters or query component, and fragment is set to the empty string in the return value. The return value is a named tuple, which means that its items can be accessed by index or as named attributes, which are:   \nAttribute Index Value Value if not present   \nscheme 0 URL scheme specifier scheme parameter  \nnetloc 1 Network location part empty string  \npath 2 Hierarchical path empty string  \nparams 3 Parameters for last path element empty string  \nquery 4 Query component empty string  \nfragment 5 Fragment identifier empty string  \nusername  User name None  \npassword  Password None  \nhostname  Host name (lower case) None  \nport  Port number as integer, if present None   Reading the port attribute will raise a ValueError if an invalid port is specified in the URL. See section Structured Parse Results for more information on the result object. Unmatched square brackets in the netloc attribute will raise a ValueError. Characters in the netloc attribute that decompose under NFKC normalization (as used by the IDNA encoding) into any of \/, ?, #, @, or : will raise a ValueError. If the URL is decomposed before parsing, no error will be raised. As is the case with all named tuples, the subclass has a few additional methods and attributes that are particularly useful. One such method is _replace(). The _replace() method will return a new ParseResult object replacing specified fields with new values. >>> from urllib.parse import urlparse\n>>> u = urlparse('\/\/www.cwi.nl:80\/%7Eguido\/Python.html')\n>>> u\nParseResult(scheme='', netloc='www.cwi.nl:80', path='\/%7Eguido\/Python.html',\n            params='', query='', fragment='')\n>>> u._replace(scheme='http')\nParseResult(scheme='http', netloc='www.cwi.nl:80', path='\/%7Eguido\/Python.html',\n            params='', query='', fragment='')\n  Changed in version 3.2: Added IPv6 URL parsing capabilities.   Changed in version 3.3: The fragment is now parsed for all URL schemes (unless allow_fragment is false), in accordance with RFC 3986. Previously, a whitelist of schemes that support fragments existed.   Changed in version 3.6: Out-of-range port numbers now raise ValueError, instead of returning None.   Changed in version 3.8: Characters that affect netloc parsing under NFKC normalization will now raise ValueError.","title":"python.library.urllib.parse#urllib.parse.urlparse"},{"text":"urllib.parse.unquote(string, encoding='utf-8', errors='replace')  \nReplace %xx escapes with their single-character equivalent. The optional encoding and errors parameters specify how to decode percent-encoded sequences into Unicode characters, as accepted by the bytes.decode() method. string may be either a str or a bytes object. encoding defaults to 'utf-8'. errors defaults to 'replace', meaning invalid sequences are replaced by a placeholder character. Example: unquote('\/El%20Ni%C3%B1o\/') yields '\/El Ni\u00f1o\/'.  Changed in version 3.9: string parameter supports bytes and str objects (previously only str).","title":"python.library.urllib.parse#urllib.parse.unquote"},{"text":"to_url()  \nReturns a URL string or bytes depending on the type of the information stored. This is just a convenience function for calling url_unparse() for this URL.  Return type \nstr","title":"werkzeug.urls.index#werkzeug.urls.BaseURL.to_url"}]}
{"task_id":5843518,"prompt":"def f_5843518(mystring):\n\treturn ","suffix":"","canonical_solution":"re.sub('[^A-Za-z0-9]+', '', mystring)","test_start":"\nimport re\n\ndef check(candidate):","test":["\n    assert candidate('Special $#! characters   spaces 888323') == 'Specialcharactersspaces888323'\n"],"entry_point":"f_5843518","intent":"remove all special characters, punctuation and spaces from a string `mystring` using regex","library":["re"],"docs":[{"text":"IMAP4.myrights(mailbox)  \nShow my ACLs for a mailbox (i.e. the rights that I have on mailbox).","title":"python.library.imaplib#imaplib.IMAP4.myrights"},{"text":"locale.YESEXPR  \nGet a regular expression that can be used with the regex function to recognize a positive response to a yes\/no question.  Note The expression is in the syntax suitable for the regex() function from the C library, which might differ from the syntax used in re.","title":"python.library.locale#locale.YESEXPR"},{"text":"Pattern.split(string, maxsplit=0)  \nIdentical to the split() function, using the compiled pattern.","title":"python.library.re#re.Pattern.split"},{"text":"transform(X) [source]\n \nReduce X to the selected features.  Parameters \n \nXarray of shape [n_samples, n_features] \n\nThe input samples.    Returns \n \nX_rarray of shape [n_samples, n_selected_features] \n\nThe input samples with only the selected features.","title":"sklearn.modules.generated.sklearn.feature_selection.selectfrommodel#sklearn.feature_selection.SelectFromModel.transform"},{"text":"re.purge()  \nClear the regular expression cache.","title":"python.library.re#re.purge"},{"text":"transform(X) [source]\n \nReduce X to the selected features.  Parameters \n \nXarray of shape [n_samples, n_features] \n\nThe input samples.    Returns \n \nX_rarray of shape [n_samples, n_selected_features] \n\nThe input samples with only the selected features.","title":"sklearn.modules.generated.sklearn.feature_selection.selectormixin#sklearn.feature_selection.SelectorMixin.transform"},{"text":"transform(X) [source]\n \nReduce X to the selected features.  Parameters \n \nXarray of shape [n_samples, n_features] \n\nThe input samples.    Returns \n \nX_rarray of shape [n_samples, n_selected_features] \n\nThe input samples with only the selected features.","title":"sklearn.modules.generated.sklearn.feature_selection.genericunivariateselect#sklearn.feature_selection.GenericUnivariateSelect.transform"},{"text":"class dict(**kwarg)  \nclass dict(mapping, **kwarg)  \nclass dict(iterable, **kwarg)  \nReturn a new dictionary initialized from an optional positional argument and a possibly empty set of keyword arguments. Dictionaries can be created by several means:  Use a comma-separated list of key: value pairs within braces: {'jack': 4098, 'sjoerd': 4127} or {4098: 'jack', 4127: 'sjoerd'}\n Use a dict comprehension: {}, {x: x ** 2 for x in range(10)}\n Use the type constructor: dict(), dict([('foo', 100), ('bar', 200)]), dict(foo=100, bar=200)\n  If no positional argument is given, an empty dictionary is created. If a positional argument is given and it is a mapping object, a dictionary is created with the same key-value pairs as the mapping object. Otherwise, the positional argument must be an iterable object. Each item in the iterable must itself be an iterable with exactly two objects. The first object of each item becomes a key in the new dictionary, and the second object the corresponding value. If a key occurs more than once, the last value for that key becomes the corresponding value in the new dictionary. If keyword arguments are given, the keyword arguments and their values are added to the dictionary created from the positional argument. If a key being added is already present, the value from the keyword argument replaces the value from the positional argument. To illustrate, the following examples all return a dictionary equal to {\"one\": 1, \"two\": 2, \"three\": 3}: >>> a = dict(one=1, two=2, three=3)\n>>> b = {'one': 1, 'two': 2, 'three': 3}\n>>> c = dict(zip(['one', 'two', 'three'], [1, 2, 3]))\n>>> d = dict([('two', 2), ('one', 1), ('three', 3)])\n>>> e = dict({'three': 3, 'one': 1, 'two': 2})\n>>> f = dict({'one': 1, 'three': 3}, two=2)\n>>> a == b == c == d == e == f\nTrue\n Providing keyword arguments as in the first example only works for keys that are valid Python identifiers. Otherwise, any valid keys can be used. These are the operations that dictionaries support (and therefore, custom mapping types should support too):  \nlist(d)  \nReturn a list of all the keys used in the dictionary d. \n  \nlen(d)  \nReturn the number of items in the dictionary d. \n  \nd[key]  \nReturn the item of d with key key. Raises a KeyError if key is not in the map. If a subclass of dict defines a method __missing__() and key is not present, the d[key] operation calls that method with the key key as argument. The d[key] operation then returns or raises whatever is returned or raised by the __missing__(key) call. No other operations or methods invoke __missing__(). If __missing__() is not defined, KeyError is raised. __missing__() must be a method; it cannot be an instance variable: >>> class Counter(dict):\n...     def __missing__(self, key):\n...         return 0\n>>> c = Counter()\n>>> c['red']\n0\n>>> c['red'] += 1\n>>> c['red']\n1\n The example above shows part of the implementation of collections.Counter. A different __missing__ method is used by collections.defaultdict. \n  \nd[key] = value  \nSet d[key] to value. \n  \ndel d[key]  \nRemove d[key] from d. Raises a KeyError if key is not in the map. \n  \nkey in d  \nReturn True if d has a key key, else False. \n  \nkey not in d  \nEquivalent to not key in d. \n  \niter(d)  \nReturn an iterator over the keys of the dictionary. This is a shortcut for iter(d.keys()). \n  \nclear()  \nRemove all items from the dictionary. \n  \ncopy()  \nReturn a shallow copy of the dictionary. \n  \nclassmethod fromkeys(iterable[, value])  \nCreate a new dictionary with keys from iterable and values set to value. fromkeys() is a class method that returns a new dictionary. value defaults to None. All of the values refer to just a single instance, so it generally doesn\u2019t make sense for value to be a mutable object such as an empty list. To get distinct values, use a dict comprehension instead. \n  \nget(key[, default])  \nReturn the value for key if key is in the dictionary, else default. If default is not given, it defaults to None, so that this method never raises a KeyError. \n  \nitems()  \nReturn a new view of the dictionary\u2019s items ((key, value) pairs). See the documentation of view objects. \n  \nkeys()  \nReturn a new view of the dictionary\u2019s keys. See the documentation of view objects. \n  \npop(key[, default])  \nIf key is in the dictionary, remove it and return its value, else return default. If default is not given and key is not in the dictionary, a KeyError is raised. \n  \npopitem()  \nRemove and return a (key, value) pair from the dictionary. Pairs are returned in LIFO order. popitem() is useful to destructively iterate over a dictionary, as often used in set algorithms. If the dictionary is empty, calling popitem() raises a KeyError.  Changed in version 3.7: LIFO order is now guaranteed. In prior versions, popitem() would return an arbitrary key\/value pair.  \n  \nreversed(d)  \nReturn a reverse iterator over the keys of the dictionary. This is a shortcut for reversed(d.keys()).  New in version 3.8.  \n  \nsetdefault(key[, default])  \nIf key is in the dictionary, return its value. If not, insert key with a value of default and return default. default defaults to None. \n  \nupdate([other])  \nUpdate the dictionary with the key\/value pairs from other, overwriting existing keys. Return None. update() accepts either another dictionary object or an iterable of key\/value pairs (as tuples or other iterables of length two). If keyword arguments are specified, the dictionary is then updated with those key\/value pairs: d.update(red=1, blue=2). \n  \nvalues()  \nReturn a new view of the dictionary\u2019s values. See the documentation of view objects. An equality comparison between one dict.values() view and another will always return False. This also applies when comparing dict.values() to itself: >>> d = {'a': 1}\n>>> d.values() == d.values()\nFalse\n \n  \nd | other  \nCreate a new dictionary with the merged keys and values of d and other, which must both be dictionaries. The values of other take priority when d and other share keys.  New in version 3.9.  \n  \nd |= other  \nUpdate the dictionary d with keys and values from other, which may be either a mapping or an iterable of key\/value pairs. The values of other take priority when d and other share keys.  New in version 3.9.  \n Dictionaries compare equal if and only if they have the same (key,\nvalue) pairs (regardless of ordering). Order comparisons (\u2018<\u2019, \u2018<=\u2019, \u2018>=\u2019, \u2018>\u2019) raise TypeError. Dictionaries preserve insertion order. Note that updating a key does not affect the order. Keys added after deletion are inserted at the end. >>> d = {\"one\": 1, \"two\": 2, \"three\": 3, \"four\": 4}\n>>> d\n{'one': 1, 'two': 2, 'three': 3, 'four': 4}\n>>> list(d)\n['one', 'two', 'three', 'four']\n>>> list(d.values())\n[1, 2, 3, 4]\n>>> d[\"one\"] = 42\n>>> d\n{'one': 42, 'two': 2, 'three': 3, 'four': 4}\n>>> del d[\"two\"]\n>>> d[\"two\"] = None\n>>> d\n{'one': 42, 'three': 3, 'four': 4, 'two': None}\n  Changed in version 3.7: Dictionary order is guaranteed to be insertion order. This behavior was an implementation detail of CPython from 3.6.  Dictionaries and dictionary views are reversible. >>> d = {\"one\": 1, \"two\": 2, \"three\": 3, \"four\": 4}\n>>> d\n{'one': 1, 'two': 2, 'three': 3, 'four': 4}\n>>> list(reversed(d))\n['four', 'three', 'two', 'one']\n>>> list(reversed(d.values()))\n[4, 3, 2, 1]\n>>> list(reversed(d.items()))\n[('four', 4), ('three', 3), ('two', 2), ('one', 1)]\n  Changed in version 3.8: Dictionaries are now reversible.","title":"python.library.stdtypes#dict"},{"text":"datetime.day  \nBetween 1 and the number of days in the given month of the given year.","title":"python.library.datetime#datetime.datetime.day"},{"text":"pygame.surfarray.make_surface() \n Copy an array to a new surface make_surface(array) -> Surface  Create a new Surface that best resembles the data and format on the array. The array can be 2D or 3D with any sized integer values. Function make_surface uses the array struct interface to acquire array properties, so is not limited to just NumPy arrays. See pygame.pixelcopy. New in pygame 1.9.2: array struct interface support.","title":"pygame.ref.surfarray#pygame.surfarray.make_surface"}]}
{"task_id":36674519,"prompt":"def f_36674519():\n\treturn ","suffix":"","canonical_solution":"pd.date_range('2016-01-01', freq='WOM-2FRI', periods=13)","test_start":"\nimport pandas as pd\nimport datetime\n\ndef check(candidate):","test":["\n    actual = candidate() \n    expected = [[2016, 1, 8], [2016, 2, 12],\n                [2016, 3, 11], [2016, 4, 8],\n                [2016, 5, 13], [2016, 6, 10],\n                [2016, 7, 8], [2016, 8, 12],\n                [2016, 9, 9], [2016, 10, 14],\n                [2016, 11, 11], [2016, 12, 9],\n                [2017, 1, 13]]\n    for i in range(0, len(expected)):\n        d = datetime.date(expected[i][0], expected[i][1], expected[i][2])\n        assert d == actual[i].date()\n"],"entry_point":"f_36674519","intent":"create a DatetimeIndex containing 13 periods of the second friday of each month starting from date '2016-01-01'","library":["datetime","pandas"],"docs":[{"text":"pandas.bdate_range   pandas.bdate_range(start=None, end=None, periods=None, freq='B', tz=None, normalize=True, name=None, weekmask=None, holidays=None, closed=NoDefault.no_default, inclusive=None, **kwargs)[source]\n \nReturn a fixed frequency DatetimeIndex, with business day as the default frequency.  Parameters \n \nstart:str or datetime-like, default None\n\n\nLeft bound for generating dates.  \nend:str or datetime-like, default None\n\n\nRight bound for generating dates.  \nperiods:int, default None\n\n\nNumber of periods to generate.  \nfreq:str or DateOffset, default \u2018B\u2019 (business daily)\n\n\nFrequency strings can have multiples, e.g. \u20185H\u2019.  \ntz:str or None\n\n\nTime zone name for returning localized DatetimeIndex, for example Asia\/Beijing.  \nnormalize:bool, default False\n\n\nNormalize start\/end dates to midnight before generating date range.  \nname:str, default None\n\n\nName of the resulting DatetimeIndex.  \nweekmask:str or None, default None\n\n\nWeekmask of valid business days, passed to numpy.busdaycalendar, only used when custom frequency strings are passed. The default value None is equivalent to \u2018Mon Tue Wed Thu Fri\u2019.  \nholidays:list-like or None, default None\n\n\nDates to exclude from the set of valid business days, passed to numpy.busdaycalendar, only used when custom frequency strings are passed.  \nclosed:str, default None\n\n\nMake the interval closed with respect to the given frequency to the \u2018left\u2019, \u2018right\u2019, or both sides (None).  Deprecated since version 1.4.0: Argument closed has been deprecated to standardize boundary inputs. Use inclusive instead, to set each bound as closed or open.   \ninclusive:{\u201cboth\u201d, \u201cneither\u201d, \u201cleft\u201d, \u201cright\u201d}, default \u201cboth\u201d\n\n\nInclude boundaries; Whether to set each bound as closed or open.  New in version 1.4.0.   **kwargs\n\nFor compatibility. Has no effect on the result.    Returns \n DatetimeIndex\n   Notes Of the four parameters: start, end, periods, and freq, exactly three must be specified. Specifying freq is a requirement for bdate_range. Use date_range if specifying freq is not desired. To learn more about the frequency strings, please see this link. Examples Note how the two weekend days are skipped in the result. \n>>> pd.bdate_range(start='1\/1\/2018', end='1\/08\/2018')\nDatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04',\n           '2018-01-05', '2018-01-08'],\n          dtype='datetime64[ns]', freq='B')","title":"pandas.reference.api.pandas.bdate_range"},{"text":"pandas.tseries.offsets.SemiMonthEnd.name   SemiMonthEnd.name","title":"pandas.reference.api.pandas.tseries.offsets.semimonthend.name"},{"text":"pandas.DatetimeIndex   classpandas.DatetimeIndex(data=None, freq=NoDefault.no_default, tz=None, normalize=False, closed=None, ambiguous='raise', dayfirst=False, yearfirst=False, dtype=None, copy=False, name=None)[source]\n \nImmutable ndarray-like of datetime64 data. Represented internally as int64, and which can be boxed to Timestamp objects that are subclasses of datetime and carry metadata.  Parameters \n \ndata:array-like (1-dimensional), optional\n\n\nOptional datetime-like data to construct index with.  \nfreq:str or pandas offset object, optional\n\n\nOne of pandas date offset strings or corresponding objects. The string \u2018infer\u2019 can be passed in order to set the frequency of the index as the inferred frequency upon creation.  \ntz:pytz.timezone or dateutil.tz.tzfile or datetime.tzinfo or str\n\n\nSet the Timezone of the data.  \nnormalize:bool, default False\n\n\nNormalize start\/end dates to midnight before generating date range.  \nclosed:{\u2018left\u2019, \u2018right\u2019}, optional\n\n\nSet whether to include start and end that are on the boundary. The default includes boundary points on either end.  \nambiguous:\u2018infer\u2019, bool-ndarray, \u2018NaT\u2019, default \u2018raise\u2019\n\n\nWhen clocks moved backward due to DST, ambiguous times may arise. For example in Central European Time (UTC+01), when going from 03:00 DST to 02:00 non-DST, 02:30:00 local time occurs both at 00:30:00 UTC and at 01:30:00 UTC. In such a situation, the ambiguous parameter dictates how ambiguous times should be handled.  \u2018infer\u2019 will attempt to infer fall dst-transition hours based on order bool-ndarray where True signifies a DST time, False signifies a non-DST time (note that this flag is only applicable for ambiguous times) \u2018NaT\u2019 will return NaT where there are ambiguous times \u2018raise\u2019 will raise an AmbiguousTimeError if there are ambiguous times.   \ndayfirst:bool, default False\n\n\nIf True, parse dates in data with the day first order.  \nyearfirst:bool, default False\n\n\nIf True parse dates in data with the year first order.  \ndtype:numpy.dtype or DatetimeTZDtype or str, default None\n\n\nNote that the only NumPy dtype allowed is \u2018datetime64[ns]\u2019.  \ncopy:bool, default False\n\n\nMake a copy of input ndarray.  \nname:label, default None\n\n\nName to be stored in the index.      See also  Index\n\nThe base pandas Index type.  TimedeltaIndex\n\nIndex of timedelta64 data.  PeriodIndex\n\nIndex of Period data.  to_datetime\n\nConvert argument to datetime.  date_range\n\nCreate a fixed-frequency DatetimeIndex.    Notes To learn more about the frequency strings, please see this link. Attributes       \nyear The year of the datetime.  \nmonth The month as January=1, December=12.  \nday The day of the datetime.  \nhour The hours of the datetime.  \nminute The minutes of the datetime.  \nsecond The seconds of the datetime.  \nmicrosecond The microseconds of the datetime.  \nnanosecond The nanoseconds of the datetime.  \ndate Returns numpy array of python datetime.date objects.  \ntime Returns numpy array of datetime.time objects.  \ntimetz Returns numpy array of datetime.time objects with timezone information.  \ndayofyear The ordinal day of the year.  \nday_of_year The ordinal day of the year.  \nweekofyear (DEPRECATED) The week ordinal of the year.  \nweek (DEPRECATED) The week ordinal of the year.  \ndayofweek The day of the week with Monday=0, Sunday=6.  \nday_of_week The day of the week with Monday=0, Sunday=6.  \nweekday The day of the week with Monday=0, Sunday=6.  \nquarter The quarter of the date.  \ntz Return the timezone.  \nfreq Return the frequency object if it is set, otherwise None.  \nfreqstr Return the frequency object as a string if its set, otherwise None.  \nis_month_start Indicates whether the date is the first day of the month.  \nis_month_end Indicates whether the date is the last day of the month.  \nis_quarter_start Indicator for whether the date is the first day of a quarter.  \nis_quarter_end Indicator for whether the date is the last day of a quarter.  \nis_year_start Indicate whether the date is the first day of a year.  \nis_year_end Indicate whether the date is the last day of the year.  \nis_leap_year Boolean indicator if the date belongs to a leap year.  \ninferred_freq Tries to return a string representing a frequency guess, generated by infer_freq.    Methods       \nnormalize(*args, **kwargs) Convert times to midnight.  \nstrftime(*args, **kwargs) Convert to Index using specified date_format.  \nsnap([freq]) Snap time stamps to nearest occurring frequency.  \ntz_convert(tz) Convert tz-aware Datetime Array\/Index from one time zone to another.  \ntz_localize(tz[, ambiguous, nonexistent]) Localize tz-naive Datetime Array\/Index to tz-aware Datetime Array\/Index.  \nround(*args, **kwargs) Perform round operation on the data to the specified freq.  \nfloor(*args, **kwargs) Perform floor operation on the data to the specified freq.  \nceil(*args, **kwargs) Perform ceil operation on the data to the specified freq.  \nto_period(*args, **kwargs) Cast to PeriodArray\/Index at a particular frequency.  \nto_perioddelta(freq) Calculate TimedeltaArray of difference between index values and index converted to PeriodArray at specified freq.  \nto_pydatetime(*args, **kwargs) Return Datetime Array\/Index as object ndarray of datetime.datetime objects.  \nto_series([keep_tz, index, name]) Create a Series with both index and values equal to the index keys useful with map for returning an indexer based on an index.  \nto_frame([index, name]) Create a DataFrame with a column containing the Index.  \nmonth_name(*args, **kwargs) Return the month names of the DateTimeIndex with specified locale.  \nday_name(*args, **kwargs) Return the day names of the DateTimeIndex with specified locale.  \nmean(*args, **kwargs) Return the mean value of the Array.  \nstd(*args, **kwargs) Return sample standard deviation over requested axis.","title":"pandas.reference.api.pandas.datetimeindex"},{"text":"pandas.tseries.offsets.SemiMonthEnd.n   SemiMonthEnd.n","title":"pandas.reference.api.pandas.tseries.offsets.semimonthend.n"},{"text":"pandas.tseries.offsets.CustomBusinessMonthEnd.freqstr   CustomBusinessMonthEnd.freqstr","title":"pandas.reference.api.pandas.tseries.offsets.custombusinessmonthend.freqstr"},{"text":"pandas.tseries.offsets.BusinessMonthEnd.freqstr   BusinessMonthEnd.freqstr","title":"pandas.reference.api.pandas.tseries.offsets.businessmonthend.freqstr"},{"text":"pandas.PeriodIndex   classpandas.PeriodIndex(data=None, ordinal=None, freq=None, dtype=None, copy=False, name=None, **fields)[source]\n \nImmutable ndarray holding ordinal values indicating regular periods in time. Index keys are boxed to Period objects which carries the metadata (eg, frequency information).  Parameters \n \ndata:array-like (1d int np.ndarray or PeriodArray), optional\n\n\nOptional period-like data to construct index with.  \ncopy:bool\n\n\nMake a copy of input ndarray.  \nfreq:str or period object, optional\n\n\nOne of pandas period strings or corresponding objects.  \nyear:int, array, or Series, default None\n\n\nmonth:int, array, or Series, default None\n\n\nquarter:int, array, or Series, default None\n\n\nday:int, array, or Series, default None\n\n\nhour:int, array, or Series, default None\n\n\nminute:int, array, or Series, default None\n\n\nsecond:int, array, or Series, default None\n\n\ndtype:str or PeriodDtype, default None\n\n    See also  Index\n\nThe base pandas Index type.  Period\n\nRepresents a period of time.  DatetimeIndex\n\nIndex with datetime64 data.  TimedeltaIndex\n\nIndex of timedelta64 data.  period_range\n\nCreate a fixed-frequency PeriodIndex.    Examples \n>>> idx = pd.PeriodIndex(year=[2000, 2002], quarter=[1, 3])\n>>> idx\nPeriodIndex(['2000Q1', '2002Q3'], dtype='period[Q-DEC]')\n  Attributes       \nday The days of the period.  \ndayofweek The day of the week with Monday=0, Sunday=6.  \nday_of_week The day of the week with Monday=0, Sunday=6.  \ndayofyear The ordinal day of the year.  \nday_of_year The ordinal day of the year.  \ndays_in_month The number of days in the month.  \ndaysinmonth The number of days in the month.  \nfreq Return the frequency object if it is set, otherwise None.  \nfreqstr Return the frequency object as a string if its set, otherwise None.  \nhour The hour of the period.  \nis_leap_year Logical indicating if the date belongs to a leap year.  \nminute The minute of the period.  \nmonth The month as January=1, December=12.  \nquarter The quarter of the date.  \nsecond The second of the period.  \nweek The week ordinal of the year.  \nweekday The day of the week with Monday=0, Sunday=6.  \nweekofyear The week ordinal of the year.  \nyear The year of the period.          \nend_time   \nqyear   \nstart_time     Methods       \nasfreq([freq, how]) Convert the PeriodArray to the specified frequency freq.  \nstrftime(*args, **kwargs) Convert to Index using specified date_format.  \nto_timestamp([freq, how]) Cast to DatetimeArray\/Index.","title":"pandas.reference.api.pandas.periodindex"},{"text":"pandas.tseries.offsets.SemiMonthEnd.onOffset   SemiMonthEnd.onOffset()","title":"pandas.reference.api.pandas.tseries.offsets.semimonthend.onoffset"},{"text":"pandas.tseries.offsets.BusinessMonthEnd.name   BusinessMonthEnd.name","title":"pandas.reference.api.pandas.tseries.offsets.businessmonthend.name"},{"text":"pandas.PeriodIndex.month   propertyPeriodIndex.month\n \nThe month as January=1, December=12.","title":"pandas.reference.api.pandas.periodindex.month"}]}
{"task_id":508657,"prompt":"def f_508657():\n\t","suffix":"\n\treturn matrix","canonical_solution":"matrix = [['a', 'b'], ['c', 'd'], ['e', 'f']]","test_start":"\ndef check(candidate):","test":["\n    matrix = candidate()\n    assert len(matrix) == 3\n    assert all([len(row)==2 for row in matrix])\n"],"entry_point":"f_508657","intent":"Create multidimensional array `matrix` with 3 rows and 2 columns in python","library":[],"docs":[]}
{"task_id":1007481,"prompt":"def f_1007481(mystring):\n\treturn ","suffix":"","canonical_solution":"mystring.replace(' ', '_')","test_start":"\ndef check(candidate):","test":["\n    assert candidate(' ') == '_'\n","\n    assert candidate(' _ ') == '___'\n","\n    assert candidate('') == ''\n","\n    assert candidate('123123') == '123123'\n","\n    assert candidate('\\_ ') == '\\__'\n"],"entry_point":"f_1007481","intent":"replace spaces with underscore in string `mystring`","library":[],"docs":[]}
{"task_id":1249786,"prompt":"def f_1249786(my_string):\n\treturn ","suffix":"","canonical_solution":"\"\"\" \"\"\".join(my_string.split())","test_start":"\ndef check(candidate):","test":["\n    assert candidate('hello   world ') == 'hello world'\n","\n    assert candidate('') == ''\n","\n    assert candidate('    ') == ''\n","\n    assert candidate('  hello') == 'hello'\n","\n    assert candidate(' h  e  l  l  o   ') == 'h e l l o'\n"],"entry_point":"f_1249786","intent":"split string `my_string` on white spaces","library":[],"docs":[]}
{"task_id":4444923,"prompt":"def f_4444923(filename):\n\treturn ","suffix":"","canonical_solution":"os.path.splitext(filename)[0]","test_start":"\nimport os\n\ndef check(candidate):","test":["\n    assert candidate('\/Users\/test\/hello.txt') == '\/Users\/test\/hello'\n","\n    assert candidate('hello.txt') == 'hello'\n","\n    assert candidate('hello') == 'hello'\n","\n    assert candidate('.gitignore') == '.gitignore'\n"],"entry_point":"f_4444923","intent":"get filename without extension from file `filename`","library":["os"],"docs":[{"text":"str.removesuffix(suffix, \/)  \nIf the string ends with the suffix string and that suffix is not empty, return string[:-len(suffix)]. Otherwise, return a copy of the original string: >>> 'MiscTests'.removesuffix('Tests')\n'Misc'\n>>> 'TmpDirMixin'.removesuffix('Tests')\n'TmpDirMixin'\n  New in version 3.9.","title":"python.library.stdtypes#str.removesuffix"},{"text":"bytes.removesuffix(suffix, \/)  \nbytearray.removesuffix(suffix, \/)  \nIf the binary data ends with the suffix string and that suffix is not empty, return bytes[:-len(suffix)]. Otherwise, return a copy of the original binary data: >>> b'MiscTests'.removesuffix(b'Tests')\nb'Misc'\n>>> b'TmpDirMixin'.removesuffix(b'Tests')\nb'TmpDirMixin'\n The suffix may be any bytes-like object.  Note The bytearray version of this method does not operate in place - it always produces a new object, even if no changes were made.   New in version 3.9.","title":"python.library.stdtypes#bytearray.removesuffix"},{"text":"bytes.removesuffix(suffix, \/)  \nbytearray.removesuffix(suffix, \/)  \nIf the binary data ends with the suffix string and that suffix is not empty, return bytes[:-len(suffix)]. Otherwise, return a copy of the original binary data: >>> b'MiscTests'.removesuffix(b'Tests')\nb'Misc'\n>>> b'TmpDirMixin'.removesuffix(b'Tests')\nb'TmpDirMixin'\n The suffix may be any bytes-like object.  Note The bytearray version of this method does not operate in place - it always produces a new object, even if no changes were made.   New in version 3.9.","title":"python.library.stdtypes#bytes.removesuffix"},{"text":"clean_username(username)  \nPerforms any cleaning on the username (e.g. stripping LDAP DN information) prior to using it to get or create a user object. Returns the cleaned username.","title":"django.ref.contrib.auth#django.contrib.auth.backends.RemoteUserBackend.clean_username"},{"text":"username  \nThe username portion of the address, with all quoting removed.","title":"python.library.email.headerregistry#email.headerregistry.Address.username"},{"text":"torch.utils.cpp_extension.CppExtension(name, sources, *args, **kwargs) [source]\n \nCreates a setuptools.Extension for C++. Convenience method that creates a setuptools.Extension with the bare minimum (but often sufficient) arguments to build a C++ extension. All arguments are forwarded to the setuptools.Extension constructor. Example >>> from setuptools import setup\n>>> from torch.utils.cpp_extension import BuildExtension, CppExtension\n>>> setup(\n        name='extension',\n        ext_modules=[\n            CppExtension(\n                name='extension',\n                sources=['extension.cpp'],\n                extra_compile_args=['-g']),\n        ],\n        cmdclass={\n            'build_ext': BuildExtension\n        })","title":"torch.cpp_extension#torch.utils.cpp_extension.CppExtension"},{"text":"copy()  \nReturn new instance of the current font.","title":"python.library.tkinter.font#tkinter.font.Font.copy"},{"text":"os.path.basename(path)  \nReturn the base name of pathname path. This is the second element of the pair returned by passing path to the function split(). Note that the result of this function is different from the Unix basename program; where basename for '\/foo\/bar\/' returns 'bar', the basename() function returns an empty string ('').  Changed in version 3.6: Accepts a path-like object.","title":"python.library.os.path#os.path.basename"},{"text":"predict(X) [source]\n \nPredict using the linear model.  Parameters \n \nXarray-like or sparse matrix, shape (n_samples, n_features) \n\nSamples.    Returns \n \nCarray, shape (n_samples,) \n\nReturns predicted values.","title":"sklearn.modules.generated.sklearn.linear_model.lasso#sklearn.linear_model.Lasso.predict"},{"text":"filename\n \nAlias for field number 4","title":"matplotlib.dviread#matplotlib.dviread.PsFont.filename"}]}
{"task_id":13728486,"prompt":"def f_13728486(l):\n\treturn ","suffix":"","canonical_solution":"[sum(l[:i]) for i, _ in enumerate(l)]","test_start":"\ndef check(candidate):","test":["\n    assert candidate([1,2,3]) == [0,1,3]\n","\n    assert candidate([]) == []\n","\n    assert candidate([1]) == [0]\n"],"entry_point":"f_13728486","intent":"get a list containing the sum of each element `i` in list `l` plus the previous elements","library":[],"docs":[]}
{"task_id":9743134,"prompt":"def f_9743134():\n\treturn ","suffix":"","canonical_solution":"\"\"\"Docs\/src\/Scripts\/temp\"\"\".replace('\/', '\/\\x00\/').split('\\x00')","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == ['Docs\/', '\/src\/', '\/Scripts\/', '\/temp']\n","\n    assert candidate() != ['Docs', 'src', 'Scripts', 'temp']\n"],"entry_point":"f_9743134","intent":"split a string `Docs\/src\/Scripts\/temp` by `\/` keeping `\/` in the result","library":[],"docs":[]}
{"task_id":20546419,"prompt":"def f_20546419(r):\n\treturn ","suffix":"","canonical_solution":"np.random.shuffle(np.transpose(r))","test_start":"\nimport numpy as np\n\ndef check(candidate):","test":["\n    a1 = np.array([[ 1, 20], [ 2, 30]])\n    candidate(a1)\n    assert np.array_equal(a1, np.array([[ 1, 20],[ 2, 30]])) or np.array_equal(a1, np.array([[ 20, 1], [ 30, 2]]))\n","\n    a2 = np.array([[ 1], [ 2]])\n    candidate(a2)                       \n    assert np.array_equal(a2,np.array([[ 1], [ 2]]) )\n","\n    a3 = np.array([[ 1,2,3]])\n    candidate(a3)\n    assert np.array_equal(a3,np.array([[ 1,2,3]])) or np.array_equal(a3,np.array([[ 2,1,3]]))           or np.array_equal(a3,np.array([[ 1,3,2]]))            or np.array_equal(a3,np.array([[3,2,1]])) or np.array_equal(a3,np.array([[3,1,2]]))            or np.array_equal(a3,np.array([[2,3,1]])) \n","\n    a4 = np.zeros(shape=(5,2))\n    candidate(a4)\n    assert np.array_equal(a4, np.zeros(shape=(5,2)))\n"],"entry_point":"f_20546419","intent":"shuffle columns of an numpy array 'r'","library":["numpy"],"docs":[{"text":"sklearn.utils.shuffle(*arrays, random_state=None, n_samples=None) [source]\n \nShuffle arrays or sparse matrices in a consistent way. This is a convenience alias to resample(*arrays, replace=False) to do random permutations of the collections.  Parameters \n \n*arrayssequence of indexable data-structures \n\nIndexable data-structures can be arrays, lists, dataframes or scipy sparse matrices with consistent first dimension.  \nrandom_stateint, RandomState instance or None, default=None \n\nDetermines random number generation for shuffling the data. Pass an int for reproducible results across multiple function calls. See Glossary.  \nn_samplesint, default=None \n\nNumber of samples to generate. If left to None this is automatically set to the first dimension of the arrays. It should not be larger than the length of arrays.    Returns \n \nshuffled_arrayssequence of indexable data-structures \n\nSequence of shuffled copies of the collections. The original arrays are not impacted.      See also  \nresample\n\n  Examples It is possible to mix sparse and dense arrays in the same run: >>> X = np.array([[1., 0.], [2., 1.], [0., 0.]])\n>>> y = np.array([0, 1, 2])\n\n>>> from scipy.sparse import coo_matrix\n>>> X_sparse = coo_matrix(X)\n\n>>> from sklearn.utils import shuffle\n>>> X, X_sparse, y = shuffle(X, X_sparse, y, random_state=0)\n>>> X\narray([[0., 0.],\n       [2., 1.],\n       [1., 0.]])\n\n>>> X_sparse\n<3x2 sparse matrix of type '<... 'numpy.float64'>'\n    with 3 stored elements in Compressed Sparse Row format>\n\n>>> X_sparse.toarray()\narray([[0., 0.],\n       [2., 1.],\n       [1., 0.]])\n\n>>> y\narray([2, 1, 0])\n\n>>> shuffle(y, n_samples=2, random_state=0)\narray([0, 1])","title":"sklearn.modules.generated.sklearn.utils.shuffle#sklearn.utils.shuffle"},{"text":"sklearn.utils.shuffle  \nsklearn.utils.shuffle(*arrays, random_state=None, n_samples=None) [source]\n \nShuffle arrays or sparse matrices in a consistent way. This is a convenience alias to resample(*arrays, replace=False) to do random permutations of the collections.  Parameters \n \n*arrayssequence of indexable data-structures \n\nIndexable data-structures can be arrays, lists, dataframes or scipy sparse matrices with consistent first dimension.  \nrandom_stateint, RandomState instance or None, default=None \n\nDetermines random number generation for shuffling the data. Pass an int for reproducible results across multiple function calls. See Glossary.  \nn_samplesint, default=None \n\nNumber of samples to generate. If left to None this is automatically set to the first dimension of the arrays. It should not be larger than the length of arrays.    Returns \n \nshuffled_arrayssequence of indexable data-structures \n\nSequence of shuffled copies of the collections. The original arrays are not impacted.      See also  \nresample\n\n  Examples It is possible to mix sparse and dense arrays in the same run: >>> X = np.array([[1., 0.], [2., 1.], [0., 0.]])\n>>> y = np.array([0, 1, 2])\n\n>>> from scipy.sparse import coo_matrix\n>>> X_sparse = coo_matrix(X)\n\n>>> from sklearn.utils import shuffle\n>>> X, X_sparse, y = shuffle(X, X_sparse, y, random_state=0)\n>>> X\narray([[0., 0.],\n       [2., 1.],\n       [1., 0.]])\n\n>>> X_sparse\n<3x2 sparse matrix of type '<... 'numpy.float64'>'\n    with 3 stored elements in Compressed Sparse Row format>\n\n>>> X_sparse.toarray()\narray([[0., 0.],\n       [2., 1.],\n       [1., 0.]])\n\n>>> y\narray([2, 1, 0])\n\n>>> shuffle(y, n_samples=2, random_state=0)\narray([0, 1])\n \n Examples using sklearn.utils.shuffle\n \n  Color Quantization using K-Means  \n\n  Empirical evaluation of the impact of k-means initialization  \n\n  Combine predictors using stacking  \n\n  Model Complexity Influence  \n\n  Prediction Latency  \n\n  Early stopping of Stochastic Gradient Descent  \n\n  Approximate nearest neighbors in TSNE  \n\n  Effect of varying threshold for self-training","title":"sklearn.modules.generated.sklearn.utils.shuffle"},{"text":"random.shuffle(x[, random])  \nShuffle the sequence x in place. The optional argument random is a 0-argument function returning a random float in [0.0, 1.0); by default, this is the function random(). To shuffle an immutable sequence and return a new shuffled list, use sample(x, k=len(x)) instead. Note that even for small len(x), the total number of permutations of x can quickly grow larger than the period of most random number generators. This implies that most permutations of a long sequence can never be generated. For example, a sequence of length 2080 is the largest that can fit within the period of the Mersenne Twister random number generator.  Deprecated since version 3.9, will be removed in version 3.11: The optional parameter random.","title":"python.library.random#random.shuffle"},{"text":"itertools.permutations(iterable, r=None)  \nReturn successive r length permutations of elements in the iterable. If r is not specified or is None, then r defaults to the length of the iterable and all possible full-length permutations are generated. The permutation tuples are emitted in lexicographic ordering according to the order of the input iterable. So, if the input iterable is sorted, the combination tuples will be produced in sorted order. Elements are treated as unique based on their position, not on their value. So if the input elements are unique, there will be no repeat values in each permutation. Roughly equivalent to: def permutations(iterable, r=None):\n    # permutations('ABCD', 2) --> AB AC AD BA BC BD CA CB CD DA DB DC\n    # permutations(range(3)) --> 012 021 102 120 201 210\n    pool = tuple(iterable)\n    n = len(pool)\n    r = n if r is None else r\n    if r > n:\n        return\n    indices = list(range(n))\n    cycles = list(range(n, n-r, -1))\n    yield tuple(pool[i] for i in indices[:r])\n    while n:\n        for i in reversed(range(r)):\n            cycles[i] -= 1\n            if cycles[i] == 0:\n                indices[i:] = indices[i+1:] + indices[i:i+1]\n                cycles[i] = n - i\n            else:\n                j = cycles[i]\n                indices[i], indices[-j] = indices[-j], indices[i]\n                yield tuple(pool[i] for i in indices[:r])\n                break\n        else:\n            return\n The code for permutations() can be also expressed as a subsequence of product(), filtered to exclude entries with repeated elements (those from the same position in the input pool): def permutations(iterable, r=None):\n    pool = tuple(iterable)\n    n = len(pool)\n    r = n if r is None else r\n    for indices in product(range(n), repeat=r):\n        if len(set(indices)) == r:\n            yield tuple(pool[i] for i in indices)\n The number of items returned is n! \/ (n-r)! when 0 <= r <= n or zero when r > n.","title":"python.library.itertools#itertools.permutations"},{"text":"numpy.random.RandomState.shuffle method   random.RandomState.shuffle(x)\n \nModify a sequence in-place by shuffling its contents. This function only shuffles the array along the first axis of a multi-dimensional array. The order of sub-arrays is changed but their contents remains the same.  Note New code should use the shuffle method of a default_rng() instance instead; please see the Quick Start.   Parameters \n \nxndarray or MutableSequence\n\n\nThe array, list or mutable sequence to be shuffled.    Returns \n None\n    See also  Generator.shuffle\n\nwhich should be used for new code.    Examples >>> arr = np.arange(10)\n>>> np.random.shuffle(arr)\n>>> arr\n[1 7 5 2 9 4 3 6 0 8] # random\n Multi-dimensional arrays are only shuffled along the first axis: >>> arr = np.arange(9).reshape((3, 3))\n>>> np.random.shuffle(arr)\n>>> arr\narray([[3, 4, 5], # random\n       [6, 7, 8],\n       [0, 1, 2]])","title":"numpy.reference.random.generated.numpy.random.randomstate.shuffle"},{"text":"numpy.random.shuffle   random.shuffle(x)\n \nModify a sequence in-place by shuffling its contents. This function only shuffles the array along the first axis of a multi-dimensional array. The order of sub-arrays is changed but their contents remains the same.  Note New code should use the shuffle method of a default_rng() instance instead; please see the Quick Start.   Parameters \n \nxndarray or MutableSequence\n\n\nThe array, list or mutable sequence to be shuffled.    Returns \n None\n    See also  Generator.shuffle\n\nwhich should be used for new code.    Examples >>> arr = np.arange(10)\n>>> np.random.shuffle(arr)\n>>> arr\n[1 7 5 2 9 4 3 6 0 8] # random\n Multi-dimensional arrays are only shuffled along the first axis: >>> arr = np.arange(9).reshape((3, 3))\n>>> np.random.shuffle(arr)\n>>> arr\narray([[3, 4, 5], # random\n       [6, 7, 8],\n       [0, 1, 2]])","title":"numpy.reference.random.generated.numpy.random.shuffle"},{"text":"itertools.combinations_with_replacement(iterable, r)  \nReturn r length subsequences of elements from the input iterable allowing individual elements to be repeated more than once. The combination tuples are emitted in lexicographic ordering according to the order of the input iterable. So, if the input iterable is sorted, the combination tuples will be produced in sorted order. Elements are treated as unique based on their position, not on their value. So if the input elements are unique, the generated combinations will also be unique. Roughly equivalent to: def combinations_with_replacement(iterable, r):\n    # combinations_with_replacement('ABC', 2) --> AA AB AC BB BC CC\n    pool = tuple(iterable)\n    n = len(pool)\n    if not n and r:\n        return\n    indices = [0] * r\n    yield tuple(pool[i] for i in indices)\n    while True:\n        for i in reversed(range(r)):\n            if indices[i] != n - 1:\n                break\n        else:\n            return\n        indices[i:] = [indices[i] + 1] * (r - i)\n        yield tuple(pool[i] for i in indices)\n The code for combinations_with_replacement() can be also expressed as a subsequence of product() after filtering entries where the elements are not in sorted order (according to their position in the input pool): def combinations_with_replacement(iterable, r):\n    pool = tuple(iterable)\n    n = len(pool)\n    for indices in product(range(n), repeat=r):\n        if sorted(indices) == list(indices):\n            yield tuple(pool[i] for i in indices)\n The number of items returned is (n+r-1)! \/ r! \/ (n-1)! when n > 0.  New in version 3.1.","title":"python.library.itertools#itertools.combinations_with_replacement"},{"text":"numpy.random.Generator.shuffle method   random.Generator.shuffle(x, axis=0)\n \nModify an array or sequence in-place by shuffling its contents. The order of sub-arrays is changed but their contents remains the same.  Parameters \n \nxndarray or MutableSequence\n\n\nThe array, list or mutable sequence to be shuffled.  \naxisint, optional\n\n\nThe axis which x is shuffled along. Default is 0. It is only supported on ndarray objects.    Returns \n None\n   Examples >>> rng = np.random.default_rng()\n>>> arr = np.arange(10)\n>>> rng.shuffle(arr)\n>>> arr\n[1 7 5 2 9 4 3 6 0 8] # random\n >>> arr = np.arange(9).reshape((3, 3))\n>>> rng.shuffle(arr)\n>>> arr\narray([[3, 4, 5], # random\n       [6, 7, 8],\n       [0, 1, 2]])\n >>> arr = np.arange(9).reshape((3, 3))\n>>> rng.shuffle(arr, axis=1)\n>>> arr\narray([[2, 0, 1], # random\n       [5, 3, 4],\n       [8, 6, 7]])","title":"numpy.reference.random.generated.numpy.random.generator.shuffle"},{"text":"numpy.random.Generator.permuted method   random.Generator.permuted(x, axis=None, out=None)\n \nRandomly permute x along axis axis. Unlike shuffle, each slice along the given axis is shuffled independently of the others.  Parameters \n \nxarray_like, at least one-dimensional\n\n\nArray to be shuffled.  \naxisint, optional\n\n\nSlices of x in this axis are shuffled. Each slice is shuffled independently of the others. If axis is None, the flattened array is shuffled.  \noutndarray, optional\n\n\nIf given, this is the destinaton of the shuffled array. If out is None, a shuffled copy of the array is returned.    Returns \n ndarray\n\nIf out is None, a shuffled copy of x is returned. Otherwise, the shuffled array is stored in out, and out is returned      See also  shuffle\npermutation\n  Examples Create a numpy.random.Generator instance: >>> rng = np.random.default_rng()\n Create a test array: >>> x = np.arange(24).reshape(3, 8)\n>>> x\narray([[ 0,  1,  2,  3,  4,  5,  6,  7],\n       [ 8,  9, 10, 11, 12, 13, 14, 15],\n       [16, 17, 18, 19, 20, 21, 22, 23]])\n Shuffle the rows of x: >>> y = rng.permuted(x, axis=1)\n>>> y\narray([[ 4,  3,  6,  7,  1,  2,  5,  0],  # random\n       [15, 10, 14,  9, 12, 11,  8, 13],\n       [17, 16, 20, 21, 18, 22, 23, 19]])\n x has not been modified: >>> x\narray([[ 0,  1,  2,  3,  4,  5,  6,  7],\n       [ 8,  9, 10, 11, 12, 13, 14, 15],\n       [16, 17, 18, 19, 20, 21, 22, 23]])\n To shuffle the rows of x in-place, pass x as the out parameter: >>> y = rng.permuted(x, axis=1, out=x)\n>>> x\narray([[ 3,  0,  4,  7,  1,  6,  2,  5],  # random\n       [ 8, 14, 13,  9, 12, 11, 15, 10],\n       [17, 18, 16, 22, 19, 23, 20, 21]])\n Note that when the out parameter is given, the return value is out: >>> y is x\nTrue","title":"numpy.reference.random.generated.numpy.random.generator.permuted"},{"text":"itertools.combinations(iterable, r)  \nReturn r length subsequences of elements from the input iterable. The combination tuples are emitted in lexicographic ordering according to the order of the input iterable. So, if the input iterable is sorted, the combination tuples will be produced in sorted order. Elements are treated as unique based on their position, not on their value. So if the input elements are unique, there will be no repeat values in each combination. Roughly equivalent to: def combinations(iterable, r):\n    # combinations('ABCD', 2) --> AB AC AD BC BD CD\n    # combinations(range(4), 3) --> 012 013 023 123\n    pool = tuple(iterable)\n    n = len(pool)\n    if r > n:\n        return\n    indices = list(range(r))\n    yield tuple(pool[i] for i in indices)\n    while True:\n        for i in reversed(range(r)):\n            if indices[i] != i + n - r:\n                break\n        else:\n            return\n        indices[i] += 1\n        for j in range(i+1, r):\n            indices[j] = indices[j-1] + 1\n        yield tuple(pool[i] for i in indices)\n The code for combinations() can be also expressed as a subsequence of permutations() after filtering entries where the elements are not in sorted order (according to their position in the input pool): def combinations(iterable, r):\n    pool = tuple(iterable)\n    n = len(pool)\n    for indices in permutations(range(n), r):\n        if sorted(indices) == list(indices):\n            yield tuple(pool[i] for i in indices)\n The number of items returned is n! \/ r! \/ (n-r)! when 0 <= r <= n or zero when r > n.","title":"python.library.itertools#itertools.combinations"}]}
{"task_id":32675861,"prompt":"def f_32675861(df):\n\t","suffix":"\n\treturn df","canonical_solution":"df['D'] = df['B']","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    df_1 = pd.DataFrame({'A': [1,2,3], 'B': ['a', 'b', 'c']})\n    candidate(df_1)\n    assert (df_1['D'] == df_1['B']).all()\n","\n    df_2 = pd.DataFrame({'A': [1,2,3], 'B': [1, 'A', 'B']})\n    candidate(df_2)\n    assert (df_2['D'] == df_2['B']).all()\n","\n    df_3 = pd.DataFrame({'B': [1]})\n    candidate(df_3)\n    assert df_3['D'][0] == 1\n","\n    df_4 = pd.DataFrame({'B': []})\n    candidate(df_4)\n    assert len(df_4['D']) == 0\n"],"entry_point":"f_32675861","intent":"copy all values in a column 'B' to a new column 'D' in a pandas data frame 'df'","library":["pandas"],"docs":[]}
{"task_id":14227561,"prompt":"def f_14227561(data):\n\treturn ","suffix":"","canonical_solution":"list(data['A']['B'].values())[0]['maindata'][0]['Info']","test_start":"\nimport json\n\ndef check(candidate):","test":["\n    s1 = '{\"A\":{\"B\":{\"unknown\":{\"1\":\"F\",\"maindata\":[{\"Info\":\"TEXT\"}]}}}}'\n    data = json.loads(s1)\n    assert candidate(data) == 'TEXT'\n","\n    s2 = '{\"A\":{\"B\":{\"sample1\":{\"1\":\"F\",\"maindata\":[{\"Info\":\"TEXT!\"}]}}}}'\n    data = json.loads(s2)\n    assert candidate(data) == 'TEXT!'\n","\n    s3 = '{\"A\":{\"B\":{\"sample_weird_un\":{\"1\":\"F\",\"maindata\":[{\"Info\":\"!\"}]}}}}'\n    data = json.loads(s3)\n    assert candidate(data) == '!'\n","\n    s4 = '{\"A\":{\"B\":{\"sample_weird_un\":{\"1\":\"F\",\"maindata\":[{\"Info\":\"\"}]}}}}'\n    data = json.loads(s4)\n    assert candidate(data) == ''\n"],"entry_point":"f_14227561","intent":"find a value within nested json 'data' where the key inside another key 'B' is unknown.","library":["json"],"docs":[]}
{"task_id":14858916,"prompt":"def f_14858916(string, predicate):\n\treturn ","suffix":"","canonical_solution":"all(predicate(x) for x in string)","test_start":"\ndef check(candidate):","test":["\n    def predicate(x):\n        if x == 'a':\n            return True\n        else:\n            return False\n    assert candidate('aab', predicate) == False\n","\n    def predicate(x):\n        if x == 'a':\n            return True\n        else:\n            return False\n    assert candidate('aa', predicate) == True\n","\n    def predicate(x):\n        if x == 'a':\n            return True\n        else:\n            return False\n    assert candidate('', predicate) == True\n","\n    def predicate(x):\n        if x.islower():\n            return True\n        else:\n            return False\n    assert candidate('abc', predicate) == True\n","\n    def predicate(x):\n        if x.islower():\n            return True\n        else:\n            return False\n    assert candidate('Ab', predicate) == False\n","\n    def predicate(x):\n        if x.islower():\n            return True\n        else:\n            return False\n    assert candidate('ABCD', predicate) == False\n"],"entry_point":"f_14858916","intent":"check characters of string `string` are true predication of function `predicate`","library":[],"docs":[]}
{"task_id":574236,"prompt":"def f_574236():\n\treturn ","suffix":"","canonical_solution":"os.statvfs('\/').f_files - os.statvfs('\/').f_ffree","test_start":"\nimport os \n\ndef check(candidate):","test":["\n    assert candidate() == (os.statvfs('\/').f_files - os.statvfs('\/').f_ffree)\n"],"entry_point":"f_574236","intent":"determine number of files on a drive with python","library":["os"],"docs":[{"text":"test.support.fd_count()  \nCount the number of open file descriptors.","title":"python.library.test#test.support.fd_count"},{"text":"pygame.cdrom.get_count() \n number of cd drives on the system get_count() -> count  Return the number of cd drives on the system. When you create CD objects you need to pass an integer id that must be lower than this count. The count will be 0 if there are no drives on the system.","title":"pygame.ref.cdrom#pygame.cdrom.get_count"},{"text":"numpy.distutils.misc_util.get_num_build_jobs()[source]\n \nGet number of parallel build jobs set by the \u2013parallel command line argument of setup.py If the command did not receive a setting the environment variable NPY_NUM_BUILD_JOBS is checked. If that is unset, return the number of processors on the system, with a maximum of 8 (to prevent overloading the system if there a lot of CPUs).  Returns \n \noutint\n\n\nnumber of parallel jobs that can be run","title":"numpy.reference.distutils.misc_util#numpy.distutils.misc_util.get_num_build_jobs"},{"text":"driver_count","title":"django.ref.contrib.gis.gdal#django.contrib.gis.gdal.Driver.driver_count"},{"text":"fnmatch.fnmatch(filename, pattern)  \nTest whether the filename string matches the pattern string, returning True or False. Both parameters are case-normalized using os.path.normcase(). fnmatchcase() can be used to perform a case-sensitive comparison, regardless of whether that\u2019s standard for the operating system. This example will print all file names in the current directory with the extension .txt: import fnmatch\nimport os\n\nfor file in os.listdir('.'):\n    if fnmatch.fnmatch(file, '*.txt'):\n        print(file)","title":"python.library.fnmatch#fnmatch.fnmatch"},{"text":"numpy.char.join   char.join(sep, seq)[source]\n \nReturn a string which is the concatenation of the strings in the sequence seq. Calls str.join element-wise.  Parameters \n \nseparray_like of str or unicode\n\n\nseqarray_like of str or unicode\n\n  Returns \n \noutndarray\n\n\nOutput array of str or unicode, depending on input types      See also  str.join","title":"numpy.reference.generated.numpy.char.join"},{"text":"skimage.segmentation.disk_level_set(image_shape, *, center=None, radius=None) [source]\n \nCreate a disk level set with binary values.  Parameters \n \nimage_shapetuple of positive integers \n\nShape of the image  \ncentertuple of positive integers, optional \n\nCoordinates of the center of the disk given in (row, column). If not given, it defaults to the center of the image.  \nradiusfloat, optional \n\nRadius of the disk. If not given, it is set to the 75% of the smallest image dimension.    Returns \n \noutarray with shape image_shape \n\nBinary level set of the disk with the given radius and center.      See also  \ncheckerboard_level_set","title":"skimage.api.skimage.segmentation#skimage.segmentation.disk_level_set"},{"text":"resource.RLIMIT_NOFILE  \nThe maximum number of open file descriptors for the current process.","title":"python.library.resource#resource.RLIMIT_NOFILE"},{"text":"pandas.Int16Dtype   classpandas.Int16Dtype[source]\n \nAn ExtensionDtype for int16 integer data.  Changed in version 1.0.0: Now uses pandas.NA as its missing value, rather than numpy.nan.  Attributes       \nNone     Methods       \nNone","title":"pandas.reference.api.pandas.int16dtype"},{"text":"class numpy.ctypeslib.c_intp\n \nA ctypes signed integer type of the same size as numpy.intp. Depending on the platform, it can be an alias for either c_int, c_long or c_longlong.","title":"numpy.reference.routines.ctypeslib#numpy.ctypeslib.c_intp"}]}
{"task_id":7011291,"prompt":"def f_7011291(cursor):\n\treturn ","suffix":"","canonical_solution":"cursor.fetchone()[0]","test_start":"\nimport sqlite3\n\ndef check(candidate):","test":["\n    conn = sqlite3.connect('main')\n    cursor = conn.cursor()\n    cursor.execute(\"CREATE TABLE student (name VARCHAR(10))\")\n    cursor.execute(\"INSERT INTO student VALUES('abc')\")\n    cursor.execute(\"SELECT * FROM student\")\n    assert candidate(cursor) == 'abc'\n"],"entry_point":"f_7011291","intent":"how to get a single result from a SQLite query from `cursor`","library":["sqlite3"],"docs":[{"text":"fetchone()  \nFetches the next row of a query result set, returning a single sequence, or None when no more data is available.","title":"python.library.sqlite3#sqlite3.Cursor.fetchone"},{"text":"executescript(sql_script)  \nThis is a nonstandard shortcut that creates a cursor object by calling the cursor() method, calls the cursor\u2019s executescript() method with the given sql_script, and returns the cursor.","title":"python.library.sqlite3#sqlite3.Connection.executescript"},{"text":"fetchall()  \nFetches all (remaining) rows of a query result, returning a list. Note that the cursor\u2019s arraysize attribute can affect the performance of this operation. An empty list is returned when no rows are available.","title":"python.library.sqlite3#sqlite3.Cursor.fetchall"},{"text":"class sqlite3.Cursor  \nA Cursor instance has the following attributes and methods.  \nexecute(sql[, parameters])  \nExecutes an SQL statement. Values may be bound to the statement using placeholders. execute() will only execute a single SQL statement. If you try to execute more than one statement with it, it will raise a Warning. Use executescript() if you want to execute multiple SQL statements with one call. \n  \nexecutemany(sql, seq_of_parameters)  \nExecutes a parameterized SQL command against all parameter sequences or mappings found in the sequence seq_of_parameters. The sqlite3 module also allows using an iterator yielding parameters instead of a sequence. import sqlite3\n\nclass IterChars:\n    def __init__(self):\n        self.count = ord('a')\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if self.count > ord('z'):\n            raise StopIteration\n        self.count += 1\n        return (chr(self.count - 1),) # this is a 1-tuple\n\ncon = sqlite3.connect(\":memory:\")\ncur = con.cursor()\ncur.execute(\"create table characters(c)\")\n\ntheIter = IterChars()\ncur.executemany(\"insert into characters(c) values (?)\", theIter)\n\ncur.execute(\"select c from characters\")\nprint(cur.fetchall())\n\ncon.close()\n Here\u2019s a shorter example using a generator: import sqlite3\nimport string\n\ndef char_generator():\n    for c in string.ascii_lowercase:\n        yield (c,)\n\ncon = sqlite3.connect(\":memory:\")\ncur = con.cursor()\ncur.execute(\"create table characters(c)\")\n\ncur.executemany(\"insert into characters(c) values (?)\", char_generator())\n\ncur.execute(\"select c from characters\")\nprint(cur.fetchall())\n\ncon.close()\n \n  \nexecutescript(sql_script)  \nThis is a nonstandard convenience method for executing multiple SQL statements at once. It issues a COMMIT statement first, then executes the SQL script it gets as a parameter. sql_script can be an instance of str. Example: import sqlite3\n\ncon = sqlite3.connect(\":memory:\")\ncur = con.cursor()\ncur.executescript(\"\"\"\n    create table person(\n        firstname,\n        lastname,\n        age\n    );\n\n    create table book(\n        title,\n        author,\n        published\n    );\n\n    insert into book(title, author, published)\n    values (\n        'Dirk Gently''s Holistic Detective Agency',\n        'Douglas Adams',\n        1987\n    );\n    \"\"\")\ncon.close()\n \n  \nfetchone()  \nFetches the next row of a query result set, returning a single sequence, or None when no more data is available. \n  \nfetchmany(size=cursor.arraysize)  \nFetches the next set of rows of a query result, returning a list. An empty list is returned when no more rows are available. The number of rows to fetch per call is specified by the size parameter. If it is not given, the cursor\u2019s arraysize determines the number of rows to be fetched. The method should try to fetch as many rows as indicated by the size parameter. If this is not possible due to the specified number of rows not being available, fewer rows may be returned. Note there are performance considerations involved with the size parameter. For optimal performance, it is usually best to use the arraysize attribute. If the size parameter is used, then it is best for it to retain the same value from one fetchmany() call to the next. \n  \nfetchall()  \nFetches all (remaining) rows of a query result, returning a list. Note that the cursor\u2019s arraysize attribute can affect the performance of this operation. An empty list is returned when no rows are available. \n  \nclose()  \nClose the cursor now (rather than whenever __del__ is called). The cursor will be unusable from this point forward; a ProgrammingError exception will be raised if any operation is attempted with the cursor. \n  \nrowcount  \nAlthough the Cursor class of the sqlite3 module implements this attribute, the database engine\u2019s own support for the determination of \u201crows affected\u201d\/\u201drows selected\u201d is quirky. For executemany() statements, the number of modifications are summed up into rowcount. As required by the Python DB API Spec, the rowcount attribute \u201cis -1 in case no executeXX() has been performed on the cursor or the rowcount of the last operation is not determinable by the interface\u201d. This includes SELECT statements because we cannot determine the number of rows a query produced until all rows were fetched. With SQLite versions before 3.6.5, rowcount is set to 0 if you make a DELETE FROM table without any condition. \n  \nlastrowid  \nThis read-only attribute provides the rowid of the last modified row. It is only set if you issued an INSERT or a REPLACE statement using the execute() method. For operations other than INSERT or REPLACE or when executemany() is called, lastrowid is set to None. If the INSERT or REPLACE statement failed to insert the previous successful rowid is returned.  Changed in version 3.6: Added support for the REPLACE statement.  \n  \narraysize  \nRead\/write attribute that controls the number of rows returned by fetchmany(). The default value is 1 which means a single row would be fetched per call. \n  \ndescription  \nThis read-only attribute provides the column names of the last query. To remain compatible with the Python DB API, it returns a 7-tuple for each column where the last six items of each tuple are None. It is set for SELECT statements without any matching rows as well. \n  \nconnection  \nThis read-only attribute provides the SQLite database Connection used by the Cursor object. A Cursor object created by calling con.cursor() will have a connection attribute that refers to con: >>> con = sqlite3.connect(\":memory:\")\n>>> cur = con.cursor()\n>>> cur.connection == con\nTrue","title":"python.library.sqlite3#sqlite3.Cursor"},{"text":"execute(sql[, parameters])  \nThis is a nonstandard shortcut that creates a cursor object by calling the cursor() method, calls the cursor\u2019s execute() method with the parameters given, and returns the cursor.","title":"python.library.sqlite3#sqlite3.Connection.execute"},{"text":"csvreader.__next__()  \nReturn the next row of the reader\u2019s iterable object as a list (if the object was returned from reader()) or a dict (if it is a DictReader instance), parsed according to the current dialect. Usually you should call this as next(reader).","title":"python.library.csv#csv.csvreader.__next__"},{"text":"connection  \nThis read-only attribute provides the SQLite database Connection used by the Cursor object. A Cursor object created by calling con.cursor() will have a connection attribute that refers to con: >>> con = sqlite3.connect(\":memory:\")\n>>> cur = con.cursor()\n>>> cur.connection == con\nTrue","title":"python.library.sqlite3#sqlite3.Cursor.connection"},{"text":"close()  \nClose the cursor now (rather than whenever __del__ is called). The cursor will be unusable from this point forward; a ProgrammingError exception will be raised if any operation is attempted with the cursor.","title":"python.library.sqlite3#sqlite3.Cursor.close"},{"text":"fetchmany(size=cursor.arraysize)  \nFetches the next set of rows of a query result, returning a list. An empty list is returned when no more rows are available. The number of rows to fetch per call is specified by the size parameter. If it is not given, the cursor\u2019s arraysize determines the number of rows to be fetched. The method should try to fetch as many rows as indicated by the size parameter. If this is not possible due to the specified number of rows not being available, fewer rows may be returned. Note there are performance considerations involved with the size parameter. For optimal performance, it is usually best to use the arraysize attribute. If the size parameter is used, then it is best for it to retain the same value from one fetchmany() call to the next.","title":"python.library.sqlite3#sqlite3.Cursor.fetchmany"},{"text":"tf.config.set_logical_device_configuration Set the logical device configuration for a tf.config.PhysicalDevice.  View aliases  Main aliases \ntf.config.experimental.set_virtual_device_configuration Compat aliases for migration See Migration guide for more details. tf.compat.v1.config.experimental.set_virtual_device_configuration, tf.compat.v1.config.set_logical_device_configuration  \ntf.config.set_logical_device_configuration(\n    device, logical_devices\n)\n A visible tf.config.PhysicalDevice will by default have a single tf.config.LogicalDevice associated with it once the runtime is initialized. Specifying a list of tf.config.LogicalDeviceConfiguration objects allows multiple devices to be created on the same tf.config.PhysicalDevice. The following example splits the CPU into 2 logical devices: \nphysical_devices = tf.config.list_physical_devices('CPU')\nassert len(physical_devices) == 1, \"No CPUs found\"\n# Specify 2 virtual CPUs. Note currently memory limit is not supported.\ntry:\n  tf.config.set_logical_device_configuration(\n    physical_devices[0],\n    [tf.config.LogicalDeviceConfiguration(),\n     tf.config.LogicalDeviceConfiguration()])\n  logical_devices = tf.config.list_logical_devices('CPU')\n  assert len(logical_devices) == 2\n\n  tf.config.set_logical_device_configuration(\n    physical_devices[0],\n    [tf.config.LogicalDeviceConfiguration(),\n     tf.config.LogicalDeviceConfiguration(),\n     tf.config.LogicalDeviceConfiguration(),\n     tf.config.LogicalDeviceConfiguration()])\nexcept:\n  # Cannot modify logical devices once initialized.\n  pass\n The following example splits the GPU into 2 logical devices with 100 MB each: \nphysical_devices = tf.config.list_physical_devices('GPU')\ntry:\n  tf.config.set_logical_device_configuration(\n    physical_devices[0],\n    [tf.config.LogicalDeviceConfiguration(memory_limit=100),\n     tf.config.LogicalDeviceConfiguration(memory_limit=100)])\n\n  logical_devices = tf.config.list_logical_devices('GPU')\n  assert len(logical_devices) == len(physical_devices) + 1\n\n  tf.config.set_logical_device_configuration(\n    physical_devices[0],\n    [tf.config.LogicalDeviceConfiguration(memory_limit=10),\n     tf.config.LogicalDeviceConfiguration(memory_limit=10)])\nexcept:\n  # Invalid device or cannot modify logical devices once initialized.\n  pass\n\n \n\n\n Args\n  device   The PhysicalDevice to configure.  \n  logical_devices   (optional) List of tf.config.LogicalDeviceConfiguration objects to allocate for the specified PhysicalDevice. If None, the default configuration will be used.   \n \n\n\n Raises\n  ValueError   If argument validation fails.  \n  RuntimeError   Runtime is already initialized.","title":"tensorflow.config.set_logical_device_configuration"}]}
{"task_id":6378889,"prompt":"def f_6378889(user_input):\n\t","suffix":"\n\treturn user_list","canonical_solution":"user_list = [int(number) for number in user_input.split(',')]","test_start":"\ndef check(candidate):","test":["\n    assert candidate('0') == [0]\n","\n    assert candidate('12') == [12]\n","\n    assert candidate('12,33,223') == [12, 33, 223]\n"],"entry_point":"f_6378889","intent":"convert string `user_input` into a list of integers `user_list`","library":[],"docs":[]}
{"task_id":6378889,"prompt":"def f_6378889(user):\n\treturn ","suffix":"","canonical_solution":"[int(s) for s in user.split(',')]","test_start":"\ndef check(candidate):","test":["\n    assert candidate('0') == [0]\n","\n    assert candidate('12') == [12]\n","\n    assert candidate('12,33,223') == [12, 33, 223]\n"],"entry_point":"f_6378889","intent":"Get a list of integers by splitting  a string `user` with comma","library":[],"docs":[]}
{"task_id":5212870,"prompt":"def f_5212870(list):\n\treturn ","suffix":"","canonical_solution":"sorted(list, key=lambda x: (x[0], -x[1]))","test_start":"\ndef check(candidate):","test":["\n    list = [(9, 0), (9, 1), (9, -1), (8, 5), (4, 5)]\n    assert candidate(list) == [(4, 5), (8, 5), (9, 1), (9, 0), (9, -1)]\n"],"entry_point":"f_5212870","intent":"Sorting a Python list `list` by the first item ascending and last item descending","library":[],"docs":[]}
{"task_id":403421,"prompt":"def f_403421(ut, cmpfun):\n\t","suffix":"\n\treturn ut","canonical_solution":"ut.sort(key=cmpfun, reverse=True)","test_start":"\ndef check(candidate):","test":["\n    assert candidate([], lambda x: x) == []\n","\n    assert candidate(['a', 'b', 'c'], lambda x: x) == ['c', 'b', 'a']\n","\n    assert candidate([2, 1, 3], lambda x: -x) == [1, 2, 3]\n"],"entry_point":"f_403421","intent":"sort a list of objects `ut`, based on a function `cmpfun` in descending order","library":[],"docs":[]}
{"task_id":403421,"prompt":"def f_403421(ut):\n\t","suffix":"\n\treturn ut","canonical_solution":"ut.sort(key=lambda x: x.count, reverse=True)","test_start":"\nclass Tag: \n    def __init__(self, name, count): \n        self.name = name \n        self.count = count \n\n    def __str__(self):\n        return f\"[{self.name}]-[{self.count}]\"\n\ndef check(candidate):","test":["\n    result = candidate([Tag(\"red\", 1), Tag(\"blue\", 22), Tag(\"black\", 0)])\n    assert (result[0].name == \"blue\") and (result[0].count == 22)\n    assert (result[1].name == \"red\") and (result[1].count == 1)\n    assert (result[2].name == \"black\") and (result[2].count == 0)\n"],"entry_point":"f_403421","intent":"reverse list `ut` based on the `count` attribute of each object","library":[],"docs":[]}
{"task_id":3944876,"prompt":"def f_3944876(i):\n\treturn ","suffix":"","canonical_solution":"'ME' + str(i)","test_start":"\ndef check(candidate):","test":["\n    assert candidate(100) == \"ME100\"\n","\n    assert candidate(0.22) == \"ME0.22\"\n","\n    assert candidate(\"text\") == \"MEtext\"\n"],"entry_point":"f_3944876","intent":"cast an int `i` to a string and concat to string 'ME'","library":[],"docs":[]}
{"task_id":40903174,"prompt":"def f_40903174(df):\n\treturn ","suffix":"","canonical_solution":"df.sort_values(['System_num', 'Dis'])","test_start":"\nimport pandas as pd \n\ndef check(candidate):","test":["\n    df1 = pd.DataFrame([[6, 1, 1], [5, 1, 1], [4, 1, 1], [3, 2, 1], [2, 2, 1], [1, 2, 1]], columns = ['Dis', 'System_num', 'Energy'])\n    df_ans1 = pd.DataFrame([[4, 1, 1], [5, 1, 1], [6, 1, 1], [1, 2, 1], [2, 2, 1], [3, 2, 1]], columns = ['Dis', 'System_num', 'Energy'])\n    assert (df_ans1.equals(candidate(df1).reset_index(drop = True))) == True\n","\n    df2 = pd.DataFrame([[6, 3, 1], [5, 2, 1], [4, 1, 1]], columns = ['Dis', 'System_num', 'Energy'])\n    df_ans2 = pd.DataFrame([[4, 1, 1], [5, 2, 1], [6, 3, 1]], columns = ['Dis', 'System_num', 'Energy'])\n    assert (df_ans2.equals(candidate(df2).reset_index(drop = True))) == True\n","\n    df3 = pd.DataFrame([[1, 3, 1], [3, 3, 1], [2, 3, 1], [6, 1, 1], [4, 1, 1], [5, 2, 1], [3, 2, 1]], columns = ['Dis', 'System_num', 'Energy'])\n    df_ans3 = pd.DataFrame([[4, 1,1], [6, 1, 1], [3, 2, 1], [5, 2, 1], [1, 3, 1], [2, 3, 1], [3, 3, 1]], columns = ['Dis', 'System_num', 'Energy'])\n    assert (df_ans3.equals(candidate(df3).reset_index(drop = True))) == True \n","\n    df4 = pd.DataFrame([[1, 2, 3], [1, 2, 3], [4, 1, 3]], columns = ['Dis', 'System_num', 'Energy'])\n    df_ans4 = pd.DataFrame([[1, 2, 3], [1, 2, 3], [4, 1, 3]])\n    assert (df_ans4.equals(candidate(df4).reset_index(drop = True))) == False\n"],"entry_point":"f_40903174","intent":"Sorting data in Pandas DataFrame `df` with columns 'System_num' and 'Dis'","library":["pandas"],"docs":[{"text":"class secrets.SystemRandom  \nA class for generating random numbers using the highest-quality sources provided by the operating system. See random.SystemRandom for additional details.","title":"python.library.secrets#secrets.SystemRandom"},{"text":"class random.SystemRandom([seed])  \nClass that uses the os.urandom() function for generating random numbers from sources provided by the operating system. Not available on all systems. Does not rely on software state, and sequences are not reproducible. Accordingly, the seed() method has no effect and is ignored. The getstate() and setstate() methods raise NotImplementedError if called.","title":"python.library.random#random.SystemRandom"},{"text":"InputSource.setSystemId(id)  \nSets the system identifier of this InputSource.","title":"python.library.xml.sax.reader#xml.sax.xmlreader.InputSource.setSystemId"},{"text":"write_sys_ex() \n writes a timestamped system-exclusive midi message. write_sys_ex(when, msg) -> None  Writes a timestamped system-exclusive midi message.     \nParameters:\n\n \nmsg (list[int] or str) -- midi message \nwhen -- timestamp in milliseconds      Example: midi_output.write_sys_ex(0, '\\xF0\\x7D\\x10\\x11\\x12\\x13\\xF7')\n\n# is equivalent to\n\nmidi_output.write_sys_ex(pygame.midi.time(),\n                         [0xF0, 0x7D, 0x10, 0x11, 0x12, 0x13, 0xF7])","title":"pygame.ref.midi#pygame.midi.Output.write_sys_ex"},{"text":"dis.opname  \nSequence of operation names, indexable using the bytecode.","title":"python.library.dis#dis.opname"},{"text":"class xml.sax.xmlreader.InputSource(system_id=None)  \nEncapsulation of the information needed by the XMLReader to read entities. This class may include information about the public identifier, system identifier, byte stream (possibly with character encoding information) and\/or the character stream of an entity. Applications will create objects of this class for use in the XMLReader.parse() method and for returning from EntityResolver.resolveEntity. An InputSource belongs to the application, the XMLReader is not allowed to modify InputSource objects passed to it from the application, although it may make copies and modify those.","title":"python.library.xml.sax.reader#xml.sax.xmlreader.InputSource"},{"text":"exception SystemError  \nRaised when the interpreter finds an internal error, but the situation does not look so serious to cause it to abandon all hope. The associated value is a string indicating what went wrong (in low-level terms). You should report this to the author or maintainer of your Python interpreter. Be sure to report the version of the Python interpreter (sys.version; it is also printed at the start of an interactive Python session), the exact error message (the exception\u2019s associated value) and if possible the source of the program that triggered the error.","title":"python.library.exceptions#SystemError"},{"text":"sys.float_repr_style  \nA string indicating how the repr() function behaves for floats. If the string has value 'short' then for a finite float x, repr(x) aims to produce a short string with the property that float(repr(x)) == x. This is the usual behaviour in Python 3.1 and later. Otherwise, float_repr_style has value 'legacy' and repr(x) behaves in the same way as it did in versions of Python prior to 3.1.  New in version 3.1.","title":"python.library.sys#sys.float_repr_style"},{"text":"dis.hasfree  \nSequence of bytecodes that access a free variable (note that \u2018free\u2019 in this context refers to names in the current scope that are referenced by inner scopes or names in outer scopes that are referenced from this scope. It does not include references to global or builtin scopes).","title":"python.library.dis#dis.hasfree"},{"text":"dis \u2014 Disassembler for Python bytecode Source code: Lib\/dis.py The dis module supports the analysis of CPython bytecode by disassembling it. The CPython bytecode which this module takes as an input is defined in the file Include\/opcode.h and used by the compiler and the interpreter.  CPython implementation detail: Bytecode is an implementation detail of the CPython interpreter. No guarantees are made that bytecode will not be added, removed, or changed between versions of Python. Use of this module should not be considered to work across Python VMs or Python releases.  Changed in version 3.6: Use 2 bytes for each instruction. Previously the number of bytes varied by instruction.   Example: Given the function myfunc(): def myfunc(alist):\n    return len(alist)\n the following command can be used to display the disassembly of myfunc(): >>> dis.dis(myfunc)\n  2           0 LOAD_GLOBAL              0 (len)\n              2 LOAD_FAST                0 (alist)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n (The \u201c2\u201d is a line number). Bytecode analysis  New in version 3.4.  The bytecode analysis API allows pieces of Python code to be wrapped in a Bytecode object that provides easy access to details of the compiled code.  \nclass dis.Bytecode(x, *, first_line=None, current_offset=None)  \nAnalyse the bytecode corresponding to a function, generator, asynchronous generator, coroutine, method, string of source code, or a code object (as returned by compile()). This is a convenience wrapper around many of the functions listed below, most notably get_instructions(), as iterating over a Bytecode instance yields the bytecode operations as Instruction instances. If first_line is not None, it indicates the line number that should be reported for the first source line in the disassembled code. Otherwise, the source line information (if any) is taken directly from the disassembled code object. If current_offset is not None, it refers to an instruction offset in the disassembled code. Setting this means dis() will display a \u201ccurrent instruction\u201d marker against the specified opcode.  \nclassmethod from_traceback(tb)  \nConstruct a Bytecode instance from the given traceback, setting current_offset to the instruction responsible for the exception. \n  \ncodeobj  \nThe compiled code object. \n  \nfirst_line  \nThe first source line of the code object (if available) \n  \ndis()  \nReturn a formatted view of the bytecode operations (the same as printed by dis.dis(), but returned as a multi-line string). \n  \ninfo()  \nReturn a formatted multi-line string with detailed information about the code object, like code_info(). \n  Changed in version 3.7: This can now handle coroutine and asynchronous generator objects.  \n Example: >>> bytecode = dis.Bytecode(myfunc)\n>>> for instr in bytecode:\n...     print(instr.opname)\n...\nLOAD_GLOBAL\nLOAD_FAST\nCALL_FUNCTION\nRETURN_VALUE\n Analysis functions The dis module also defines the following analysis functions that convert the input directly to the desired output. They can be useful if only a single operation is being performed, so the intermediate analysis object isn\u2019t useful:  \ndis.code_info(x)  \nReturn a formatted multi-line string with detailed code object information for the supplied function, generator, asynchronous generator, coroutine, method, source code string or code object. Note that the exact contents of code info strings are highly implementation dependent and they may change arbitrarily across Python VMs or Python releases.  New in version 3.2.   Changed in version 3.7: This can now handle coroutine and asynchronous generator objects.  \n  \ndis.show_code(x, *, file=None)  \nPrint detailed code object information for the supplied function, method, source code string or code object to file (or sys.stdout if file is not specified). This is a convenient shorthand for print(code_info(x), file=file), intended for interactive exploration at the interpreter prompt.  New in version 3.2.   Changed in version 3.4: Added file parameter.  \n  \ndis.dis(x=None, *, file=None, depth=None)  \nDisassemble the x object. x can denote either a module, a class, a method, a function, a generator, an asynchronous generator, a coroutine, a code object, a string of source code or a byte sequence of raw bytecode. For a module, it disassembles all functions. For a class, it disassembles all methods (including class and static methods). For a code object or sequence of raw bytecode, it prints one line per bytecode instruction. It also recursively disassembles nested code objects (the code of comprehensions, generator expressions and nested functions, and the code used for building nested classes). Strings are first compiled to code objects with the compile() built-in function before being disassembled. If no object is provided, this function disassembles the last traceback. The disassembly is written as text to the supplied file argument if provided and to sys.stdout otherwise. The maximal depth of recursion is limited by depth unless it is None. depth=0 means no recursion.  Changed in version 3.4: Added file parameter.   Changed in version 3.7: Implemented recursive disassembling and added depth parameter.   Changed in version 3.7: This can now handle coroutine and asynchronous generator objects.  \n  \ndis.distb(tb=None, *, file=None)  \nDisassemble the top-of-stack function of a traceback, using the last traceback if none was passed. The instruction causing the exception is indicated. The disassembly is written as text to the supplied file argument if provided and to sys.stdout otherwise.  Changed in version 3.4: Added file parameter.  \n  \ndis.disassemble(code, lasti=-1, *, file=None)  \ndis.disco(code, lasti=-1, *, file=None)  \nDisassemble a code object, indicating the last instruction if lasti was provided. The output is divided in the following columns:  the line number, for the first instruction of each line the current instruction, indicated as -->, a labelled instruction, indicated with >>, the address of the instruction, the operation code name, operation parameters, and interpretation of the parameters in parentheses.  The parameter interpretation recognizes local and global variable names, constant values, branch targets, and compare operators. The disassembly is written as text to the supplied file argument if provided and to sys.stdout otherwise.  Changed in version 3.4: Added file parameter.  \n  \ndis.get_instructions(x, *, first_line=None)  \nReturn an iterator over the instructions in the supplied function, method, source code string or code object. The iterator generates a series of Instruction named tuples giving the details of each operation in the supplied code. If first_line is not None, it indicates the line number that should be reported for the first source line in the disassembled code. Otherwise, the source line information (if any) is taken directly from the disassembled code object.  New in version 3.4.  \n  \ndis.findlinestarts(code)  \nThis generator function uses the co_firstlineno and co_lnotab attributes of the code object code to find the offsets which are starts of lines in the source code. They are generated as (offset, lineno) pairs. See Objects\/lnotab_notes.txt for the co_lnotab format and how to decode it.  Changed in version 3.6: Line numbers can be decreasing. Before, they were always increasing.  \n  \ndis.findlabels(code)  \nDetect all offsets in the raw compiled bytecode string code which are jump targets, and return a list of these offsets. \n  \ndis.stack_effect(opcode, oparg=None, *, jump=None)  \nCompute the stack effect of opcode with argument oparg. If the code has a jump target and jump is True, stack_effect() will return the stack effect of jumping. If jump is False, it will return the stack effect of not jumping. And if jump is None (default), it will return the maximal stack effect of both cases.  New in version 3.4.   Changed in version 3.8: Added jump parameter.  \n Python Bytecode Instructions The get_instructions() function and Bytecode class provide details of bytecode instructions as Instruction instances:  \nclass dis.Instruction  \nDetails for a bytecode operation  \nopcode  \nnumeric code for operation, corresponding to the opcode values listed below and the bytecode values in the Opcode collections. \n  \nopname  \nhuman readable name for operation \n  \narg  \nnumeric argument to operation (if any), otherwise None \n  \nargval  \nresolved arg value (if known), otherwise same as arg \n  \nargrepr  \nhuman readable description of operation argument \n  \noffset  \nstart index of operation within bytecode sequence \n  \nstarts_line  \nline started by this opcode (if any), otherwise None \n  \nis_jump_target  \nTrue if other code jumps to here, otherwise False \n  New in version 3.4.  \n The Python compiler currently generates the following bytecode instructions. General instructions  \nNOP  \nDo nothing code. Used as a placeholder by the bytecode optimizer. \n  \nPOP_TOP  \nRemoves the top-of-stack (TOS) item. \n  \nROT_TWO  \nSwaps the two top-most stack items. \n  \nROT_THREE  \nLifts second and third stack item one position up, moves top down to position three. \n  \nROT_FOUR  \nLifts second, third and fourth stack items one position up, moves top down to position four.  New in version 3.8.  \n  \nDUP_TOP  \nDuplicates the reference on top of the stack.  New in version 3.2.  \n  \nDUP_TOP_TWO  \nDuplicates the two references on top of the stack, leaving them in the same order.  New in version 3.2.  \n Unary operations Unary operations take the top of the stack, apply the operation, and push the result back on the stack.  \nUNARY_POSITIVE  \nImplements TOS = +TOS. \n  \nUNARY_NEGATIVE  \nImplements TOS = -TOS. \n  \nUNARY_NOT  \nImplements TOS = not TOS. \n  \nUNARY_INVERT  \nImplements TOS = ~TOS. \n  \nGET_ITER  \nImplements TOS = iter(TOS). \n  \nGET_YIELD_FROM_ITER  \nIf TOS is a generator iterator or coroutine object it is left as is. Otherwise, implements TOS = iter(TOS).  New in version 3.5.  \n Binary operations Binary operations remove the top of the stack (TOS) and the second top-most stack item (TOS1) from the stack. They perform the operation, and put the result back on the stack.  \nBINARY_POWER  \nImplements TOS = TOS1 ** TOS. \n  \nBINARY_MULTIPLY  \nImplements TOS = TOS1 * TOS. \n  \nBINARY_MATRIX_MULTIPLY  \nImplements TOS = TOS1 @ TOS.  New in version 3.5.  \n  \nBINARY_FLOOR_DIVIDE  \nImplements TOS = TOS1 \/\/ TOS. \n  \nBINARY_TRUE_DIVIDE  \nImplements TOS = TOS1 \/ TOS. \n  \nBINARY_MODULO  \nImplements TOS = TOS1 % TOS. \n  \nBINARY_ADD  \nImplements TOS = TOS1 + TOS. \n  \nBINARY_SUBTRACT  \nImplements TOS = TOS1 - TOS. \n  \nBINARY_SUBSCR  \nImplements TOS = TOS1[TOS]. \n  \nBINARY_LSHIFT  \nImplements TOS = TOS1 << TOS. \n  \nBINARY_RSHIFT  \nImplements TOS = TOS1 >> TOS. \n  \nBINARY_AND  \nImplements TOS = TOS1 & TOS. \n  \nBINARY_XOR  \nImplements TOS = TOS1 ^ TOS. \n  \nBINARY_OR  \nImplements TOS = TOS1 | TOS. \n In-place operations In-place operations are like binary operations, in that they remove TOS and TOS1, and push the result back on the stack, but the operation is done in-place when TOS1 supports it, and the resulting TOS may be (but does not have to be) the original TOS1.  \nINPLACE_POWER  \nImplements in-place TOS = TOS1 ** TOS. \n  \nINPLACE_MULTIPLY  \nImplements in-place TOS = TOS1 * TOS. \n  \nINPLACE_MATRIX_MULTIPLY  \nImplements in-place TOS = TOS1 @ TOS.  New in version 3.5.  \n  \nINPLACE_FLOOR_DIVIDE  \nImplements in-place TOS = TOS1 \/\/ TOS. \n  \nINPLACE_TRUE_DIVIDE  \nImplements in-place TOS = TOS1 \/ TOS. \n  \nINPLACE_MODULO  \nImplements in-place TOS = TOS1 % TOS. \n  \nINPLACE_ADD  \nImplements in-place TOS = TOS1 + TOS. \n  \nINPLACE_SUBTRACT  \nImplements in-place TOS = TOS1 - TOS. \n  \nINPLACE_LSHIFT  \nImplements in-place TOS = TOS1 << TOS. \n  \nINPLACE_RSHIFT  \nImplements in-place TOS = TOS1 >> TOS. \n  \nINPLACE_AND  \nImplements in-place TOS = TOS1 & TOS. \n  \nINPLACE_XOR  \nImplements in-place TOS = TOS1 ^ TOS. \n  \nINPLACE_OR  \nImplements in-place TOS = TOS1 | TOS. \n  \nSTORE_SUBSCR  \nImplements TOS1[TOS] = TOS2. \n  \nDELETE_SUBSCR  \nImplements del TOS1[TOS]. \n Coroutine opcodes  \nGET_AWAITABLE  \nImplements TOS = get_awaitable(TOS), where get_awaitable(o) returns o if o is a coroutine object or a generator object with the CO_ITERABLE_COROUTINE flag, or resolves o.__await__.  New in version 3.5.  \n  \nGET_AITER  \nImplements TOS = TOS.__aiter__().  New in version 3.5.   Changed in version 3.7: Returning awaitable objects from __aiter__ is no longer supported.  \n  \nGET_ANEXT  \nImplements PUSH(get_awaitable(TOS.__anext__())). See GET_AWAITABLE for details about get_awaitable  New in version 3.5.  \n  \nEND_ASYNC_FOR  \nTerminates an async for loop. Handles an exception raised when awaiting a next item. If TOS is StopAsyncIteration pop 7 values from the stack and restore the exception state using the second three of them. Otherwise re-raise the exception using the three values from the stack. An exception handler block is removed from the block stack.  New in version 3.8.  \n  \nBEFORE_ASYNC_WITH  \nResolves __aenter__ and __aexit__ from the object on top of the stack. Pushes __aexit__ and result of __aenter__() to the stack.  New in version 3.5.  \n  \nSETUP_ASYNC_WITH  \nCreates a new frame object.  New in version 3.5.  \n Miscellaneous opcodes  \nPRINT_EXPR  \nImplements the expression statement for the interactive mode. TOS is removed from the stack and printed. In non-interactive mode, an expression statement is terminated with POP_TOP. \n  \nSET_ADD(i)  \nCalls set.add(TOS1[-i], TOS). Used to implement set comprehensions. \n  \nLIST_APPEND(i)  \nCalls list.append(TOS1[-i], TOS). Used to implement list comprehensions. \n  \nMAP_ADD(i)  \nCalls dict.__setitem__(TOS1[-i], TOS1, TOS). Used to implement dict comprehensions.  New in version 3.1.   Changed in version 3.8: Map value is TOS and map key is TOS1. Before, those were reversed.  \n For all of the SET_ADD, LIST_APPEND and MAP_ADD instructions, while the added value or key\/value pair is popped off, the container object remains on the stack so that it is available for further iterations of the loop.  \nRETURN_VALUE  \nReturns with TOS to the caller of the function. \n  \nYIELD_VALUE  \nPops TOS and yields it from a generator. \n  \nYIELD_FROM  \nPops TOS and delegates to it as a subiterator from a generator.  New in version 3.3.  \n  \nSETUP_ANNOTATIONS  \nChecks whether __annotations__ is defined in locals(), if not it is set up to an empty dict. This opcode is only emitted if a class or module body contains variable annotations statically.  New in version 3.6.  \n  \nIMPORT_STAR  \nLoads all symbols not starting with '_' directly from the module TOS to the local namespace. The module is popped after loading all names. This opcode implements from module import *. \n  \nPOP_BLOCK  \nRemoves one block from the block stack. Per frame, there is a stack of blocks, denoting try statements, and such. \n  \nPOP_EXCEPT  \nRemoves one block from the block stack. The popped block must be an exception handler block, as implicitly created when entering an except handler. In addition to popping extraneous values from the frame stack, the last three popped values are used to restore the exception state. \n  \nRERAISE  \nRe-raises the exception currently on top of the stack.  New in version 3.9.  \n  \nWITH_EXCEPT_START  \nCalls the function in position 7 on the stack with the top three items on the stack as arguments. Used to implement the call context_manager.__exit__(*exc_info()) when an exception has occurred in a with statement.  New in version 3.9.  \n  \nLOAD_ASSERTION_ERROR  \nPushes AssertionError onto the stack. Used by the assert statement.  New in version 3.9.  \n  \nLOAD_BUILD_CLASS  \nPushes builtins.__build_class__() onto the stack. It is later called by CALL_FUNCTION to construct a class. \n  \nSETUP_WITH(delta)  \nThis opcode performs several operations before a with block starts. First, it loads __exit__() from the context manager and pushes it onto the stack for later use by WITH_CLEANUP_START. Then, __enter__() is called, and a finally block pointing to delta is pushed. Finally, the result of calling the __enter__() method is pushed onto the stack. The next opcode will either ignore it (POP_TOP), or store it in (a) variable(s) (STORE_FAST, STORE_NAME, or UNPACK_SEQUENCE).  New in version 3.2.  \n All of the following opcodes use their arguments.  \nSTORE_NAME(namei)  \nImplements name = TOS. namei is the index of name in the attribute co_names of the code object. The compiler tries to use STORE_FAST or STORE_GLOBAL if possible. \n  \nDELETE_NAME(namei)  \nImplements del name, where namei is the index into co_names attribute of the code object. \n  \nUNPACK_SEQUENCE(count)  \nUnpacks TOS into count individual values, which are put onto the stack right-to-left. \n  \nUNPACK_EX(counts)  \nImplements assignment with a starred target: Unpacks an iterable in TOS into individual values, where the total number of values can be smaller than the number of items in the iterable: one of the new values will be a list of all leftover items. The low byte of counts is the number of values before the list value, the high byte of counts the number of values after it. The resulting values are put onto the stack right-to-left. \n  \nSTORE_ATTR(namei)  \nImplements TOS.name = TOS1, where namei is the index of name in co_names. \n  \nDELETE_ATTR(namei)  \nImplements del TOS.name, using namei as index into co_names. \n  \nSTORE_GLOBAL(namei)  \nWorks as STORE_NAME, but stores the name as a global. \n  \nDELETE_GLOBAL(namei)  \nWorks as DELETE_NAME, but deletes a global name. \n  \nLOAD_CONST(consti)  \nPushes co_consts[consti] onto the stack. \n  \nLOAD_NAME(namei)  \nPushes the value associated with co_names[namei] onto the stack. \n  \nBUILD_TUPLE(count)  \nCreates a tuple consuming count items from the stack, and pushes the resulting tuple onto the stack. \n  \nBUILD_LIST(count)  \nWorks as BUILD_TUPLE, but creates a list. \n  \nBUILD_SET(count)  \nWorks as BUILD_TUPLE, but creates a set. \n  \nBUILD_MAP(count)  \nPushes a new dictionary object onto the stack. Pops 2 * count items so that the dictionary holds count entries: {..., TOS3: TOS2, TOS1: TOS}.  Changed in version 3.5: The dictionary is created from stack items instead of creating an empty dictionary pre-sized to hold count items.  \n  \nBUILD_CONST_KEY_MAP(count)  \nThe version of BUILD_MAP specialized for constant keys. Pops the top element on the stack which contains a tuple of keys, then starting from TOS1, pops count values to form values in the built dictionary.  New in version 3.6.  \n  \nBUILD_STRING(count)  \nConcatenates count strings from the stack and pushes the resulting string onto the stack.  New in version 3.6.  \n  \nLIST_TO_TUPLE  \nPops a list from the stack and pushes a tuple containing the same values.  New in version 3.9.  \n  \nLIST_EXTEND(i)  \nCalls list.extend(TOS1[-i], TOS). Used to build lists.  New in version 3.9.  \n  \nSET_UPDATE(i)  \nCalls set.update(TOS1[-i], TOS). Used to build sets.  New in version 3.9.  \n  \nDICT_UPDATE(i)  \nCalls dict.update(TOS1[-i], TOS). Used to build dicts.  New in version 3.9.  \n  \nDICT_MERGE  \nLike DICT_UPDATE but raises an exception for duplicate keys.  New in version 3.9.  \n  \nLOAD_ATTR(namei)  \nReplaces TOS with getattr(TOS, co_names[namei]). \n  \nCOMPARE_OP(opname)  \nPerforms a Boolean operation. The operation name can be found in cmp_op[opname]. \n  \nIS_OP(invert)  \nPerforms is comparison, or is not if invert is 1.  New in version 3.9.  \n  \nCONTAINS_OP(invert)  \nPerforms in comparison, or not in if invert is 1.  New in version 3.9.  \n  \nIMPORT_NAME(namei)  \nImports the module co_names[namei]. TOS and TOS1 are popped and provide the fromlist and level arguments of __import__(). The module object is pushed onto the stack. The current namespace is not affected: for a proper import statement, a subsequent STORE_FAST instruction modifies the namespace. \n  \nIMPORT_FROM(namei)  \nLoads the attribute co_names[namei] from the module found in TOS. The resulting object is pushed onto the stack, to be subsequently stored by a STORE_FAST instruction. \n  \nJUMP_FORWARD(delta)  \nIncrements bytecode counter by delta. \n  \nPOP_JUMP_IF_TRUE(target)  \nIf TOS is true, sets the bytecode counter to target. TOS is popped.  New in version 3.1.  \n  \nPOP_JUMP_IF_FALSE(target)  \nIf TOS is false, sets the bytecode counter to target. TOS is popped.  New in version 3.1.  \n  \nJUMP_IF_NOT_EXC_MATCH(target)  \nTests whether the second value on the stack is an exception matching TOS, and jumps if it is not. Pops two values from the stack.  New in version 3.9.  \n  \nJUMP_IF_TRUE_OR_POP(target)  \nIf TOS is true, sets the bytecode counter to target and leaves TOS on the stack. Otherwise (TOS is false), TOS is popped.  New in version 3.1.  \n  \nJUMP_IF_FALSE_OR_POP(target)  \nIf TOS is false, sets the bytecode counter to target and leaves TOS on the stack. Otherwise (TOS is true), TOS is popped.  New in version 3.1.  \n  \nJUMP_ABSOLUTE(target)  \nSet bytecode counter to target. \n  \nFOR_ITER(delta)  \nTOS is an iterator. Call its __next__() method. If this yields a new value, push it on the stack (leaving the iterator below it). If the iterator indicates it is exhausted, TOS is popped, and the byte code counter is incremented by delta. \n  \nLOAD_GLOBAL(namei)  \nLoads the global named co_names[namei] onto the stack. \n  \nSETUP_FINALLY(delta)  \nPushes a try block from a try-finally or try-except clause onto the block stack. delta points to the finally block or the first except block. \n  \nLOAD_FAST(var_num)  \nPushes a reference to the local co_varnames[var_num] onto the stack. \n  \nSTORE_FAST(var_num)  \nStores TOS into the local co_varnames[var_num]. \n  \nDELETE_FAST(var_num)  \nDeletes local co_varnames[var_num]. \n  \nLOAD_CLOSURE(i)  \nPushes a reference to the cell contained in slot i of the cell and free variable storage. The name of the variable is co_cellvars[i] if i is less than the length of co_cellvars. Otherwise it is co_freevars[i -\nlen(co_cellvars)]. \n  \nLOAD_DEREF(i)  \nLoads the cell contained in slot i of the cell and free variable storage. Pushes a reference to the object the cell contains on the stack. \n  \nLOAD_CLASSDEREF(i)  \nMuch like LOAD_DEREF but first checks the locals dictionary before consulting the cell. This is used for loading free variables in class bodies.  New in version 3.4.  \n  \nSTORE_DEREF(i)  \nStores TOS into the cell contained in slot i of the cell and free variable storage. \n  \nDELETE_DEREF(i)  \nEmpties the cell contained in slot i of the cell and free variable storage. Used by the del statement.  New in version 3.2.  \n  \nRAISE_VARARGS(argc)  \nRaises an exception using one of the 3 forms of the raise statement, depending on the value of argc:  0: raise (re-raise previous exception) 1: raise TOS (raise exception instance or type at TOS) 2: raise TOS1 from TOS (raise exception instance or type at TOS1 with __cause__ set to TOS)  \n  \nCALL_FUNCTION(argc)  \nCalls a callable object with positional arguments. argc indicates the number of positional arguments. The top of the stack contains positional arguments, with the right-most argument on top. Below the arguments is a callable object to call. CALL_FUNCTION pops all arguments and the callable object off the stack, calls the callable object with those arguments, and pushes the return value returned by the callable object.  Changed in version 3.6: This opcode is used only for calls with positional arguments.  \n  \nCALL_FUNCTION_KW(argc)  \nCalls a callable object with positional (if any) and keyword arguments. argc indicates the total number of positional and keyword arguments. The top element on the stack contains a tuple with the names of the keyword arguments, which must be strings. Below that are the values for the keyword arguments, in the order corresponding to the tuple. Below that are positional arguments, with the right-most parameter on top. Below the arguments is a callable object to call. CALL_FUNCTION_KW pops all arguments and the callable object off the stack, calls the callable object with those arguments, and pushes the return value returned by the callable object.  Changed in version 3.6: Keyword arguments are packed in a tuple instead of a dictionary, argc indicates the total number of arguments.  \n  \nCALL_FUNCTION_EX(flags)  \nCalls a callable object with variable set of positional and keyword arguments. If the lowest bit of flags is set, the top of the stack contains a mapping object containing additional keyword arguments. Before the callable is called, the mapping object and iterable object are each \u201cunpacked\u201d and their contents passed in as keyword and positional arguments respectively. CALL_FUNCTION_EX pops all arguments and the callable object off the stack, calls the callable object with those arguments, and pushes the return value returned by the callable object.  New in version 3.6.  \n  \nLOAD_METHOD(namei)  \nLoads a method named co_names[namei] from the TOS object. TOS is popped. This bytecode distinguishes two cases: if TOS has a method with the correct name, the bytecode pushes the unbound method and TOS. TOS will be used as the first argument (self) by CALL_METHOD when calling the unbound method. Otherwise, NULL and the object return by the attribute lookup are pushed.  New in version 3.7.  \n  \nCALL_METHOD(argc)  \nCalls a method. argc is the number of positional arguments. Keyword arguments are not supported. This opcode is designed to be used with LOAD_METHOD. Positional arguments are on top of the stack. Below them, the two items described in LOAD_METHOD are on the stack (either self and an unbound method object or NULL and an arbitrary callable). All of them are popped and the return value is pushed.  New in version 3.7.  \n  \nMAKE_FUNCTION(flags)  \nPushes a new function object on the stack. From bottom to top, the consumed stack must consist of values if the argument carries a specified flag value  \n0x01 a tuple of default values for positional-only and positional-or-keyword parameters in positional order \n0x02 a dictionary of keyword-only parameters\u2019 default values \n0x04 an annotation dictionary \n0x08 a tuple containing cells for free variables, making a closure the code associated with the function (at TOS1) the qualified name of the function (at TOS)  \n  \nBUILD_SLICE(argc)  \nPushes a slice object on the stack. argc must be 2 or 3. If it is 2, slice(TOS1, TOS) is pushed; if it is 3, slice(TOS2, TOS1, TOS) is pushed. See the slice() built-in function for more information. \n  \nEXTENDED_ARG(ext)  \nPrefixes any opcode which has an argument too big to fit into the default one byte. ext holds an additional byte which act as higher bits in the argument. For each opcode, at most three prefixal EXTENDED_ARG are allowed, forming an argument from two-byte to four-byte. \n  \nFORMAT_VALUE(flags)  \nUsed for implementing formatted literal strings (f-strings). Pops an optional fmt_spec from the stack, then a required value. flags is interpreted as follows:  \n(flags & 0x03) == 0x00: value is formatted as-is. \n(flags & 0x03) == 0x01: call str() on value before formatting it. \n(flags & 0x03) == 0x02: call repr() on value before formatting it. \n(flags & 0x03) == 0x03: call ascii() on value before formatting it. \n(flags & 0x04) == 0x04: pop fmt_spec from the stack and use it, else use an empty fmt_spec.  Formatting is performed using PyObject_Format(). The result is pushed on the stack.  New in version 3.6.  \n  \nHAVE_ARGUMENT  \nThis is not really an opcode. It identifies the dividing line between opcodes which don\u2019t use their argument and those that do (< HAVE_ARGUMENT and >= HAVE_ARGUMENT, respectively).  Changed in version 3.6: Now every instruction has an argument, but opcodes < HAVE_ARGUMENT ignore it. Before, only opcodes >= HAVE_ARGUMENT had an argument.  \n Opcode collections These collections are provided for automatic introspection of bytecode instructions:  \ndis.opname  \nSequence of operation names, indexable using the bytecode. \n  \ndis.opmap  \nDictionary mapping operation names to bytecodes. \n  \ndis.cmp_op  \nSequence of all compare operation names. \n  \ndis.hasconst  \nSequence of bytecodes that access a constant. \n  \ndis.hasfree  \nSequence of bytecodes that access a free variable (note that \u2018free\u2019 in this context refers to names in the current scope that are referenced by inner scopes or names in outer scopes that are referenced from this scope. It does not include references to global or builtin scopes). \n  \ndis.hasname  \nSequence of bytecodes that access an attribute by name. \n  \ndis.hasjrel  \nSequence of bytecodes that have a relative jump target. \n  \ndis.hasjabs  \nSequence of bytecodes that have an absolute jump target. \n  \ndis.haslocal  \nSequence of bytecodes that access a local variable. \n  \ndis.hascompare  \nSequence of bytecodes of Boolean operations.","title":"python.library.dis"}]}
{"task_id":4454298,"prompt":"def f_4454298(infile, outfile):\n\t","suffix":"\n\treturn ","canonical_solution":"open(outfile, 'w').write('#test firstline\\n' + open(infile).read())","test_start":"\nimport filecmp\n\ndef check(candidate): ","test":["\n    open('test1.txt', 'w').write('test1')\n    candidate('test1.txt', 'test1_out.txt')\n    open('test1_ans.txt', 'w').write('#test firstline\\ntest1')\n    assert filecmp.cmp('test1_out.txt', 'test1_ans.txt') == True\n","\n    open('test2.txt', 'w').write('\\ntest2\\n')\n    candidate('test2.txt', 'test2_out.txt')\n    open('test2_ans.txt', 'w').write('#test firstline\\n\\ntest2\\n')\n    assert filecmp.cmp('test2_out.txt', 'test2_ans.txt') == True\n","\n    open('test3.txt', 'w').write(' \\n \\n')\n    candidate('test3.txt', 'test3_out.txt')\n    open('test3_ans.txt', 'w').write('#test firstline\\n \\n \\n')\n    assert filecmp.cmp('test3_out.txt', 'test3_ans.txt') == True\n","\n    open('test4.txt', 'w').write('hello')\n    candidate('test4.txt', 'test4_out.txt')\n    open('test4_ans.txt', 'w').write('hello')\n    assert filecmp.cmp('test4_out.txt', 'test4_ans.txt') == False\n"],"entry_point":"f_4454298","intent":"prepend the line '#test firstline\\n' to the contents of file 'infile' and save as the file 'outfile'","library":["filecmp"],"docs":[]}
{"task_id":19729928,"prompt":"def f_19729928(l):\n\t","suffix":"\n\treturn l","canonical_solution":"l.sort(key=lambda t: len(t[1]), reverse=True)","test_start":"\ndef check(candidate): ","test":["\n    assert candidate([(\"a\", [1]), (\"b\", [1,2]), (\"c\", [1,2,3])]) ==         [(\"c\", [1,2,3]), (\"b\", [1,2]), (\"a\", [1])]\n","\n    assert candidate([(\"a\", [1]), (\"b\", [2]), (\"c\", [1,2,3])]) ==         [(\"c\", [1,2,3]), (\"a\", [1]), (\"b\", [2])]\n","\n    assert candidate([(\"a\", [1]), (\"b\", [2]), (\"c\", [3])]) ==         [(\"a\", [1]), (\"b\", [2]), (\"c\", [3])]\n"],"entry_point":"f_19729928","intent":"sort a list `l` by length of value in tuple","library":[],"docs":[]}
{"task_id":31371879,"prompt":"def f_31371879(s):\n\treturn ","suffix":"","canonical_solution":"re.findall('\\\\b(\\\\w+)d\\\\b', s)","test_start":"\nimport re\n\ndef check(candidate): ","test":["\n    assert candidate(\"this is good\") == [\"goo\"]\n","\n    assert candidate(\"this is interesting\") == []\n","\n    assert candidate(\"good bad dd\") == [\"goo\", \"ba\", \"d\"]\n"],"entry_point":"f_31371879","intent":"split string `s` by words that ends with 'd'","library":["re"],"docs":[{"text":"fmt_d_m_partial='$%s%d^{\\\\circ}\\\\,%02d^{\\\\prime}\\\\,'","title":"matplotlib._as_gen.mpl_toolkits.axisartist.angle_helper.formatterdms#mpl_toolkits.axisartist.angle_helper.FormatterDMS.fmt_d_m_partial"},{"text":"matches(self, d, **kwargs)  \nTry to match a single dict with the supplied arguments.","title":"python.library.test#test.support.Matcher.matches"},{"text":"class test.support.Matcher  \n \nmatches(self, d, **kwargs)  \nTry to match a single dict with the supplied arguments. \n  \nmatch_value(self, k, dv, v)  \nTry to match a single stored value (dv) with a supplied value (v).","title":"python.library.test#test.support.Matcher"},{"text":"keyword.issoftkeyword(s)  \nReturn True if s is a Python soft keyword.  New in version 3.9.","title":"python.library.keyword#keyword.issoftkeyword"},{"text":"fmt_s_partial='%02d^{\\\\prime\\\\prime}$'","title":"matplotlib._as_gen.mpl_toolkits.axisartist.angle_helper.formatterdms#mpl_toolkits.axisartist.angle_helper.FormatterDMS.fmt_s_partial"},{"text":"fmt_d_m_partial='$%s%d^\\\\mathrm{h}\\\\,%02d^\\\\mathrm{m}\\\\,'","title":"matplotlib._as_gen.mpl_toolkits.axisartist.angle_helper.formatterhms#mpl_toolkits.axisartist.angle_helper.FormatterHMS.fmt_d_m_partial"},{"text":"fmt_d_m='$%s%d^{\\\\circ}\\\\,%02d^{\\\\prime}$'","title":"matplotlib._as_gen.mpl_toolkits.axisartist.angle_helper.formatterdms#mpl_toolkits.axisartist.angle_helper.FormatterDMS.fmt_d_m"},{"text":"fmt_d='$%d^{\\\\circ}$'","title":"matplotlib._as_gen.mpl_toolkits.axisartist.angle_helper.formatterdms#mpl_toolkits.axisartist.angle_helper.FormatterDMS.fmt_d"},{"text":"numpy.dsplit   numpy.dsplit(ary, indices_or_sections)[source]\n \nSplit array into multiple sub-arrays along the 3rd axis (depth). Please refer to the split documentation. dsplit is equivalent to split with axis=2, the array is always split along the third axis provided the array dimension is greater than or equal to 3.  See also  split\n\nSplit an array into multiple sub-arrays of equal size.    Examples >>> x = np.arange(16.0).reshape(2, 2, 4)\n>>> x\narray([[[ 0.,   1.,   2.,   3.],\n        [ 4.,   5.,   6.,   7.]],\n       [[ 8.,   9.,  10.,  11.],\n        [12.,  13.,  14.,  15.]]])\n>>> np.dsplit(x, 2)\n[array([[[ 0.,  1.],\n        [ 4.,  5.]],\n       [[ 8.,  9.],\n        [12., 13.]]]), array([[[ 2.,  3.],\n        [ 6.,  7.]],\n       [[10., 11.],\n        [14., 15.]]])]\n>>> np.dsplit(x, np.array([3, 6]))\n[array([[[ 0.,   1.,   2.],\n        [ 4.,   5.,   6.]],\n       [[ 8.,   9.,  10.],\n        [12.,  13.,  14.]]]),\n array([[[ 3.],\n        [ 7.]],\n       [[11.],\n        [15.]]]),\narray([], shape=(2, 2, 0), dtype=float64)]","title":"numpy.reference.generated.numpy.dsplit"},{"text":"keyword.iskeyword(s)  \nReturn True if s is a Python keyword.","title":"python.library.keyword#keyword.iskeyword"}]}
{"task_id":9012008,"prompt":"def f_9012008():\n\treturn ","suffix":"","canonical_solution":"bool(re.search('ba[rzd]', 'foobarrrr'))","test_start":"\nimport re\n\ndef check(candidate): ","test":["\n    assert candidate() == True\n"],"entry_point":"f_9012008","intent":"return `True` if string `foobarrrr` contains regex `ba[rzd]`","library":["re"],"docs":[{"text":"pandas.api.types.is_re_compilable   pandas.api.types.is_re_compilable(obj)[source]\n \nCheck if the object can be compiled into a regex pattern instance.  Parameters \n \nobj:The object to check\n\n  Returns \n \nis_regex_compilable:bool\n\n\nWhether obj can be compiled as a regex pattern.     Examples \n>>> is_re_compilable(\".*\")\nTrue\n>>> is_re_compilable(1)\nFalse","title":"pandas.reference.api.pandas.api.types.is_re_compilable"},{"text":"token.LSQB  \nToken value for \"[\".","title":"python.library.token#token.LSQB"},{"text":"matplotlib.cbook.is_math_text(s)[source]\n \nReturn whether the string s contains math expressions. This is done by checking whether s contains an even number of non-escaped dollar signs.","title":"matplotlib.cbook_api#matplotlib.cbook.is_math_text"},{"text":"pandas.api.types.is_re   pandas.api.types.is_re(obj)[source]\n \nCheck if the object is a regex pattern instance.  Parameters \n \nobj:The object to check\n\n  Returns \n \nis_regex:bool\n\n\nWhether obj is a regex pattern.     Examples \n>>> is_re(re.compile(\".*\"))\nTrue\n>>> is_re(\"foo\")\nFalse","title":"pandas.reference.api.pandas.api.types.is_re"},{"text":"numpy.ma.is_masked   ma.is_masked(x)[source]\n \nDetermine whether input has masked values. Accepts any object as input, but always returns False unless the input is a MaskedArray containing masked values.  Parameters \n \nxarray_like\n\n\nArray to check for masked values.    Returns \n \nresultbool\n\n\nTrue if x is a MaskedArray with masked values, False otherwise.     Examples >>> import numpy.ma as ma\n>>> x = ma.masked_equal([0, 1, 0, 2, 3], 0)\n>>> x\nmasked_array(data=[--, 1, --, 2, 3],\n             mask=[ True, False,  True, False, False],\n       fill_value=0)\n>>> ma.is_masked(x)\nTrue\n>>> x = ma.masked_equal([0, 1, 0, 2, 3], 42)\n>>> x\nmasked_array(data=[0, 1, 0, 2, 3],\n             mask=False,\n       fill_value=42)\n>>> ma.is_masked(x)\nFalse\n Always returns False if x isn\u2019t a MaskedArray. >>> x = [False, True, False]\n>>> ma.is_masked(x)\nFalse\n>>> x = 'a string'\n>>> ma.is_masked(x)\nFalse","title":"numpy.reference.generated.numpy.ma.is_masked"},{"text":"classBracketAB(widthA=1.0, lengthA=0.2, angleA=0, widthB=1.0, lengthB=0.2, angleB=0)[source]\n \nBases: matplotlib.patches.ArrowStyle._Curve An arrow with outward square brackets at both ends.  Parameters \n \nwidthA, widthBfloat, default: 1.0\n\n\nWidth of the bracket.  \nlengthA, lengthBfloat, default: 0.2\n\n\nLength of the bracket.  \nangleA, angleBfloat, default: 0 degrees\n\n\nOrientation of the bracket, as a counterclockwise angle. 0 degrees means perpendicular to the line.       arrow=']-['","title":"matplotlib._as_gen.matplotlib.patches.arrowstyle#matplotlib.patches.ArrowStyle.BracketAB"},{"text":"token.RSQB  \nToken value for \"]\".","title":"python.library.token#token.RSQB"},{"text":"contains() \n test if one rectangle is inside another contains(Rect) -> bool  Returns true when the argument is completely inside the Rect.","title":"pygame.ref.rect#pygame.Rect.contains"},{"text":"operator.contains(a, b)  \noperator.__contains__(a, b)  \nReturn the outcome of the test b in a. Note the reversed operands.","title":"python.library.operator#operator.contains"},{"text":"classBracketB(widthB=1.0, lengthB=0.2, angleB=0)[source]\n \nBases: matplotlib.patches.ArrowStyle._Curve An arrow with an outward square bracket at its end.  Parameters \n \nwidthBfloat, default: 1.0\n\n\nWidth of the bracket.  \nlengthBfloat, default: 0.2\n\n\nLength of the bracket.  \nangleBfloat, default: 0 degrees\n\n\nOrientation of the bracket, as a counterclockwise angle. 0 degrees means perpendicular to the line.       arrow='-['","title":"matplotlib._as_gen.matplotlib.patches.arrowstyle#matplotlib.patches.ArrowStyle.BracketB"}]}
{"task_id":7961363,"prompt":"def f_7961363(t):\n\treturn ","suffix":"","canonical_solution":"list(set(t))","test_start":"\ndef check(candidate): ","test":["\n    assert candidate([1,2,3]) == [1,2,3]\n","\n    assert candidate([1,1,1,1,1,1,1,1,1,1]) == [1] \n","\n    assert candidate([1,2,2,2,2,2,3,3,3,3,3]) == [1,2,3]\n","\n    assert (candidate([1, '1']) == [1, '1']) or (candidate([1, '1']) == ['1', 1])\n","\n    assert candidate([1.0, 1]) == [1.0] \n","\n    assert candidate([]) == [] \n","\n    assert candidate([None]) == [None] \n"],"entry_point":"f_7961363","intent":"Removing duplicates in list `t`","library":[],"docs":[]}
{"task_id":7961363,"prompt":"def f_7961363(source_list):\n\treturn ","suffix":"","canonical_solution":"list(set(source_list))","test_start":"\ndef check(candidate): ","test":["\n    assert candidate([1,2,3]) == [1,2,3]\n","\n    assert candidate([1,1,1,1,1,1,1,1,1,1]) == [1] \n","\n    assert candidate([1,2,2,2,2,2,3,3,3,3,3]) == [1,2,3]\n","\n    assert (candidate([1, '1']) == [1, '1']) or (candidate([1, '1']) == ['1', 1])\n","\n    assert candidate([1.0, 1]) == [1.0] \n","\n    assert candidate([]) == [] \n","\n    assert candidate([None]) == [None] \n"],"entry_point":"f_7961363","intent":"Removing duplicates in list `source_list`","library":[],"docs":[]}
{"task_id":7961363,"prompt":"def f_7961363():\n\treturn ","suffix":"","canonical_solution":"list(OrderedDict.fromkeys('abracadabra'))","test_start":"\nfrom collections import OrderedDict\n\ndef check(candidate):","test":["\n    assert candidate() == ['a', 'b', 'r', 'c', 'd']\n"],"entry_point":"f_7961363","intent":"Removing duplicates in list `abracadabra`","library":["collections"],"docs":[{"text":"commit(database)  \nGenerate a CAB file, add it as a stream to the MSI file, put it into the Media table, and remove the generated file from the disk.","title":"python.library.msilib#msilib.CAB.commit"},{"text":"lock()  \nunlock()  \nThree locking mechanisms are used\u2014dot locking and, if available, the flock() and lockf() system calls.","title":"python.library.mailbox#mailbox.Babyl.unlock"},{"text":"lock()  \nunlock()  \nThree locking mechanisms are used\u2014dot locking and, if available, the flock() and lockf() system calls.","title":"python.library.mailbox#mailbox.Babyl.lock"},{"text":"class msilib.CAB(name)  \nThe class CAB represents a CAB file. During MSI construction, files will be added simultaneously to the Files table, and to a CAB file. Then, when all files have been added, the CAB file can be written, then added to the MSI file. name is the name of the CAB file in the MSI file.  \nappend(full, file, logical)  \nAdd the file with the pathname full to the CAB file, under the name logical. If there is already a file named logical, a new file name is created. Return the index of the file in the CAB file, and the new name of the file inside the CAB file. \n  \ncommit(database)  \nGenerate a CAB file, add it as a stream to the MSI file, put it into the Media table, and remove the generated file from the disk.","title":"python.library.msilib#msilib.CAB"},{"text":"sys.abiflags  \nOn POSIX systems where Python was built with the standard configure script, this contains the ABI flags as specified by PEP 3149.  Changed in version 3.8: Default flags became an empty string (m flag for pymalloc has been removed).   New in version 3.2.","title":"python.library.sys#sys.abiflags"},{"text":"signal.SIGABRT  \nAbort signal from abort(3).","title":"python.library.signal#signal.SIGABRT"},{"text":"@abc.abstractproperty  \n Deprecated since version 3.3: It is now possible to use property, property.getter(), property.setter() and property.deleter() with abstractmethod(), making this decorator redundant.  A subclass of the built-in property(), indicating an abstract property. This special case is deprecated, as the property() decorator is now correctly identified as abstract when applied to an abstract method: class C(ABC):\n    @property\n    @abstractmethod\n    def my_abstract_property(self):\n        ...\n The above example defines a read-only property; you can also define a read-write abstract property by appropriately marking one or more of the underlying methods as abstract: class C(ABC):\n    @property\n    def x(self):\n        ...\n\n    @x.setter\n    @abstractmethod\n    def x(self, val):\n        ...\n If only some components are abstract, only those components need to be updated to create a concrete property in a subclass: class D(C):\n    @C.x.setter\n    def x(self, val):\n        ...","title":"python.library.abc#abc.abstractproperty"},{"text":"class mailbox.Babyl(path, factory=None, create=True)  \nA subclass of Mailbox for mailboxes in Babyl format. Parameter factory is a callable object that accepts a file-like message representation (which behaves as if opened in binary mode) and returns a custom representation. If factory is None, BabylMessage is used as the default message representation. If create is True, the mailbox is created if it does not exist. Babyl is a single-file mailbox format used by the Rmail mail user agent included with Emacs. The beginning of a message is indicated by a line containing the two characters Control-Underscore ('\\037') and Control-L ('\\014'). The end of a message is indicated by the start of the next message or, in the case of the last message, a line containing a Control-Underscore ('\\037') character. Messages in a Babyl mailbox have two sets of headers, original headers and so-called visible headers. Visible headers are typically a subset of the original headers that have been reformatted or abridged to be more attractive. Each message in a Babyl mailbox also has an accompanying list of labels, or short strings that record extra information about the message, and a list of all user-defined labels found in the mailbox is kept in the Babyl options section. Babyl instances have all of the methods of Mailbox in addition to the following:  \nget_labels()  \nReturn a list of the names of all user-defined labels used in the mailbox.  Note The actual messages are inspected to determine which labels exist in the mailbox rather than consulting the list of labels in the Babyl options section, but the Babyl section is updated whenever the mailbox is modified.  \n Some Mailbox methods implemented by Babyl deserve special remarks:  \nget_file(key)  \nIn Babyl mailboxes, the headers of a message are not stored contiguously with the body of the message. To generate a file-like representation, the headers and body are copied together into an io.BytesIO instance, which has an API identical to that of a file. As a result, the file-like object is truly independent of the underlying mailbox but does not save memory compared to a string representation. \n  \nlock()  \nunlock()  \nThree locking mechanisms are used\u2014dot locking and, if available, the flock() and lockf() system calls.","title":"python.library.mailbox#mailbox.Babyl"},{"text":"class mailbox.BabylMessage(message=None)  \nA message with Babyl-specific behaviors. Parameter message has the same meaning as with the Message constructor. Certain message labels, called attributes, are defined by convention to have special meanings. The attributes are as follows:   \nLabel Explanation   \nunseen Not read, but previously detected by MUA  \ndeleted Marked for subsequent deletion  \nfiled Copied to another file or mailbox  \nanswered Replied to  \nforwarded Forwarded  \nedited Modified by the user  \nresent Resent   By default, Rmail displays only visible headers. The BabylMessage class, though, uses the original headers because they are more complete. Visible headers may be accessed explicitly if desired. BabylMessage instances offer the following methods:  \nget_labels()  \nReturn a list of labels on the message. \n  \nset_labels(labels)  \nSet the list of labels on the message to labels. \n  \nadd_label(label)  \nAdd label to the list of labels on the message. \n  \nremove_label(label)  \nRemove label from the list of labels on the message. \n  \nget_visible()  \nReturn an Message instance whose headers are the message\u2019s visible headers and whose body is empty. \n  \nset_visible(visible)  \nSet the message\u2019s visible headers to be the same as the headers in message. Parameter visible should be a Message instance, an email.message.Message instance, a string, or a file-like object (which should be open in text mode). \n  \nupdate_visible()  \nWhen a BabylMessage instance\u2019s original headers are modified, the visible headers are not automatically modified to correspond. This method updates the visible headers as follows: each visible header with a corresponding original header is set to the value of the original header, each visible header without a corresponding original header is removed, and any of Date, From, Reply-To, To, CC, and Subject that are present in the original headers but not the visible headers are added to the visible headers.","title":"python.library.mailbox#mailbox.BabylMessage"},{"text":"sklearn.metrics.calinski_harabasz_score  \nsklearn.metrics.calinski_harabasz_score(X, labels) [source]\n \nCompute the Calinski and Harabasz score. It is also known as the Variance Ratio Criterion. The score is defined as ratio between the within-cluster dispersion and the between-cluster dispersion. Read more in the User Guide.  Parameters \n \nXarray-like of shape (n_samples, n_features) \n\nA list of n_features-dimensional data points. Each row corresponds to a single data point.  \nlabelsarray-like of shape (n_samples,) \n\nPredicted labels for each sample.    Returns \n \nscorefloat \n\nThe resulting Calinski-Harabasz score.     References  \n1  \nT. Calinski and J. Harabasz, 1974. \u201cA dendrite method for cluster analysis\u201d. Communications in Statistics","title":"sklearn.modules.generated.sklearn.metrics.calinski_harabasz_score"}]}
{"task_id":5183533,"prompt":"def f_5183533(a):\n\treturn ","suffix":"","canonical_solution":"numpy.array(a).reshape(-1).tolist()","test_start":"\nimport numpy\n\ndef check(candidate):","test":["\n    assert candidate([[1,2,3],[4,5,6]]) == [1,2,3,4,5,6]\n","\n    assert candidate(['a', 'aa', 'abc']) == ['a', 'aa', 'abc']\n"],"entry_point":"f_5183533","intent":"Convert array `a` into a list","library":["numpy"],"docs":[{"text":"array.tolist()  \nConvert the array to an ordinary list with the same items.","title":"python.library.array#array.array.tolist"},{"text":"numpy.ndarray.tolist method   ndarray.tolist()\n \nReturn the array as an a.ndim-levels deep nested list of Python scalars. Return a copy of the array data as a (nested) Python list. Data items are converted to the nearest compatible builtin Python type, via the item function. If a.ndim is 0, then since the depth of the nested list is 0, it will not be a list at all, but a simple Python scalar.  Parameters \n none\n  Returns \n \nyobject, or list of object, or list of list of object, or \u2026\n\n\nThe possibly nested list of array elements.     Notes The array may be recreated via a = np.array(a.tolist()), although this may sometimes lose precision. Examples For a 1D array, a.tolist() is almost the same as list(a), except that tolist changes numpy scalars to Python scalars: >>> a = np.uint32([1, 2])\n>>> a_list = list(a)\n>>> a_list\n[1, 2]\n>>> type(a_list[0])\n<class 'numpy.uint32'>\n>>> a_tolist = a.tolist()\n>>> a_tolist\n[1, 2]\n>>> type(a_tolist[0])\n<class 'int'>\n Additionally, for a 2D array, tolist applies recursively: >>> a = np.array([[1, 2], [3, 4]])\n>>> list(a)\n[array([1, 2]), array([3, 4])]\n>>> a.tolist()\n[[1, 2], [3, 4]]\n The base case for this recursion is a 0D array: >>> a = np.array(1)\n>>> list(a)\nTraceback (most recent call last):\n  ...\nTypeError: iteration over a 0-d array\n>>> a.tolist()\n1","title":"numpy.reference.generated.numpy.ndarray.tolist"},{"text":"numpy.char.chararray.tolist method   char.chararray.tolist()\n \nReturn the array as an a.ndim-levels deep nested list of Python scalars. Return a copy of the array data as a (nested) Python list. Data items are converted to the nearest compatible builtin Python type, via the item function. If a.ndim is 0, then since the depth of the nested list is 0, it will not be a list at all, but a simple Python scalar.  Parameters \n none\n  Returns \n \nyobject, or list of object, or list of list of object, or \u2026\n\n\nThe possibly nested list of array elements.     Notes The array may be recreated via a = np.array(a.tolist()), although this may sometimes lose precision. Examples For a 1D array, a.tolist() is almost the same as list(a), except that tolist changes numpy scalars to Python scalars: >>> a = np.uint32([1, 2])\n>>> a_list = list(a)\n>>> a_list\n[1, 2]\n>>> type(a_list[0])\n<class 'numpy.uint32'>\n>>> a_tolist = a.tolist()\n>>> a_tolist\n[1, 2]\n>>> type(a_tolist[0])\n<class 'int'>\n Additionally, for a 2D array, tolist applies recursively: >>> a = np.array([[1, 2], [3, 4]])\n>>> list(a)\n[array([1, 2]), array([3, 4])]\n>>> a.tolist()\n[[1, 2], [3, 4]]\n The base case for this recursion is a 0D array: >>> a = np.array(1)\n>>> list(a)\nTraceback (most recent call last):\n  ...\nTypeError: iteration over a 0-d array\n>>> a.tolist()\n1","title":"numpy.reference.generated.numpy.char.chararray.tolist"},{"text":"numpy.recarray.tolist method   recarray.tolist()\n \nReturn the array as an a.ndim-levels deep nested list of Python scalars. Return a copy of the array data as a (nested) Python list. Data items are converted to the nearest compatible builtin Python type, via the item function. If a.ndim is 0, then since the depth of the nested list is 0, it will not be a list at all, but a simple Python scalar.  Parameters \n none\n  Returns \n \nyobject, or list of object, or list of list of object, or \u2026\n\n\nThe possibly nested list of array elements.     Notes The array may be recreated via a = np.array(a.tolist()), although this may sometimes lose precision. Examples For a 1D array, a.tolist() is almost the same as list(a), except that tolist changes numpy scalars to Python scalars: >>> a = np.uint32([1, 2])\n>>> a_list = list(a)\n>>> a_list\n[1, 2]\n>>> type(a_list[0])\n<class 'numpy.uint32'>\n>>> a_tolist = a.tolist()\n>>> a_tolist\n[1, 2]\n>>> type(a_tolist[0])\n<class 'int'>\n Additionally, for a 2D array, tolist applies recursively: >>> a = np.array([[1, 2], [3, 4]])\n>>> list(a)\n[array([1, 2]), array([3, 4])]\n>>> a.tolist()\n[[1, 2], [3, 4]]\n The base case for this recursion is a 0D array: >>> a = np.array(1)\n>>> list(a)\nTraceback (most recent call last):\n  ...\nTypeError: iteration over a 0-d array\n>>> a.tolist()\n1","title":"numpy.reference.generated.numpy.recarray.tolist"},{"text":"pandas.Index.tolist   Index.tolist()[source]\n \nReturn a list of the values. These are each a scalar type, which is a Python scalar (for str, int, float) or a pandas scalar (for Timestamp\/Timedelta\/Interval\/Period)  Returns \n list\n    See also  numpy.ndarray.tolist\n\nReturn the array as an a.ndim-levels deep nested list of Python scalars.","title":"pandas.reference.api.pandas.index.tolist"},{"text":"pandas.Index.to_list   Index.to_list()[source]\n \nReturn a list of the values. These are each a scalar type, which is a Python scalar (for str, int, float) or a pandas scalar (for Timestamp\/Timedelta\/Interval\/Period)  Returns \n list\n    See also  numpy.ndarray.tolist\n\nReturn the array as an a.ndim-levels deep nested list of Python scalars.","title":"pandas.reference.api.pandas.index.to_list"},{"text":"numpy.chararray.tolist method   chararray.tolist()\n \nReturn the array as an a.ndim-levels deep nested list of Python scalars. Return a copy of the array data as a (nested) Python list. Data items are converted to the nearest compatible builtin Python type, via the item function. If a.ndim is 0, then since the depth of the nested list is 0, it will not be a list at all, but a simple Python scalar.  Parameters \n none\n  Returns \n \nyobject, or list of object, or list of list of object, or \u2026\n\n\nThe possibly nested list of array elements.     Notes The array may be recreated via a = np.array(a.tolist()), although this may sometimes lose precision. Examples For a 1D array, a.tolist() is almost the same as list(a), except that tolist changes numpy scalars to Python scalars: >>> a = np.uint32([1, 2])\n>>> a_list = list(a)\n>>> a_list\n[1, 2]\n>>> type(a_list[0])\n<class 'numpy.uint32'>\n>>> a_tolist = a.tolist()\n>>> a_tolist\n[1, 2]\n>>> type(a_tolist[0])\n<class 'int'>\n Additionally, for a 2D array, tolist applies recursively: >>> a = np.array([[1, 2], [3, 4]])\n>>> list(a)\n[array([1, 2]), array([3, 4])]\n>>> a.tolist()\n[[1, 2], [3, 4]]\n The base case for this recursion is a 0D array: >>> a = np.array(1)\n>>> list(a)\nTraceback (most recent call last):\n  ...\nTypeError: iteration over a 0-d array\n>>> a.tolist()\n1","title":"numpy.reference.generated.numpy.chararray.tolist"},{"text":"pandas.Series.to_list   Series.to_list()[source]\n \nReturn a list of the values. These are each a scalar type, which is a Python scalar (for str, int, float) or a pandas scalar (for Timestamp\/Timedelta\/Interval\/Period)  Returns \n list\n    See also  numpy.ndarray.tolist\n\nReturn the array as an a.ndim-levels deep nested list of Python scalars.","title":"pandas.reference.api.pandas.series.to_list"},{"text":"pandas.Series.tolist   Series.tolist()[source]\n \nReturn a list of the values. These are each a scalar type, which is a Python scalar (for str, int, float) or a pandas scalar (for Timestamp\/Timedelta\/Interval\/Period)  Returns \n list\n    See also  numpy.ndarray.tolist\n\nReturn the array as an a.ndim-levels deep nested list of Python scalars.","title":"pandas.reference.api.pandas.series.tolist"},{"text":"numpy.distutils.misc_util.as_list(seq)[source]","title":"numpy.reference.distutils.misc_util#numpy.distutils.misc_util.as_list"}]}
{"task_id":5183533,"prompt":"def f_5183533(a):\n\treturn ","suffix":"","canonical_solution":"numpy.array(a)[0].tolist()","test_start":"\nimport numpy\n\ndef check(candidate):","test":["\n    assert candidate([[1,2,3],[4,5,6]]) == [1,2,3]\n","\n    assert candidate(['a', 'aa', 'abc']) == 'a'\n"],"entry_point":"f_5183533","intent":"Convert the first row of numpy matrix `a` to a list","library":["numpy"],"docs":[{"text":"numpy.matrix.tolist method   matrix.tolist()[source]\n \nReturn the matrix as a (possibly nested) list. See ndarray.tolist for full documentation.  See also  ndarray.tolist\n  Examples >>> x = np.matrix(np.arange(12).reshape((3,4))); x\nmatrix([[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]])\n>>> x.tolist()\n[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11]]","title":"numpy.reference.generated.numpy.matrix.tolist"},{"text":"numpy.ndarray.tolist method   ndarray.tolist()\n \nReturn the array as an a.ndim-levels deep nested list of Python scalars. Return a copy of the array data as a (nested) Python list. Data items are converted to the nearest compatible builtin Python type, via the item function. If a.ndim is 0, then since the depth of the nested list is 0, it will not be a list at all, but a simple Python scalar.  Parameters \n none\n  Returns \n \nyobject, or list of object, or list of list of object, or \u2026\n\n\nThe possibly nested list of array elements.     Notes The array may be recreated via a = np.array(a.tolist()), although this may sometimes lose precision. Examples For a 1D array, a.tolist() is almost the same as list(a), except that tolist changes numpy scalars to Python scalars: >>> a = np.uint32([1, 2])\n>>> a_list = list(a)\n>>> a_list\n[1, 2]\n>>> type(a_list[0])\n<class 'numpy.uint32'>\n>>> a_tolist = a.tolist()\n>>> a_tolist\n[1, 2]\n>>> type(a_tolist[0])\n<class 'int'>\n Additionally, for a 2D array, tolist applies recursively: >>> a = np.array([[1, 2], [3, 4]])\n>>> list(a)\n[array([1, 2]), array([3, 4])]\n>>> a.tolist()\n[[1, 2], [3, 4]]\n The base case for this recursion is a 0D array: >>> a = np.array(1)\n>>> list(a)\nTraceback (most recent call last):\n  ...\nTypeError: iteration over a 0-d array\n>>> a.tolist()\n1","title":"numpy.reference.generated.numpy.ndarray.tolist"},{"text":"pandas.Index.to_list   Index.to_list()[source]\n \nReturn a list of the values. These are each a scalar type, which is a Python scalar (for str, int, float) or a pandas scalar (for Timestamp\/Timedelta\/Interval\/Period)  Returns \n list\n    See also  numpy.ndarray.tolist\n\nReturn the array as an a.ndim-levels deep nested list of Python scalars.","title":"pandas.reference.api.pandas.index.to_list"},{"text":"numpy.matrix.getA1 method   matrix.getA1()[source]\n \nReturn self as a flattened ndarray. Equivalent to np.asarray(x).ravel()  Parameters \n None\n  Returns \n \nretndarray\n\n\nself, 1-D, as an ndarray     Examples >>> x = np.matrix(np.arange(12).reshape((3,4))); x\nmatrix([[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]])\n>>> x.getA1()\narray([ 0,  1,  2, ...,  9, 10, 11])","title":"numpy.reference.generated.numpy.matrix.geta1"},{"text":"pandas.Index.tolist   Index.tolist()[source]\n \nReturn a list of the values. These are each a scalar type, which is a Python scalar (for str, int, float) or a pandas scalar (for Timestamp\/Timedelta\/Interval\/Period)  Returns \n list\n    See also  numpy.ndarray.tolist\n\nReturn the array as an a.ndim-levels deep nested list of Python scalars.","title":"pandas.reference.api.pandas.index.tolist"},{"text":"array.tolist()  \nConvert the array to an ordinary list with the same items.","title":"python.library.array#array.array.tolist"},{"text":"pandas.Series.to_list   Series.to_list()[source]\n \nReturn a list of the values. These are each a scalar type, which is a Python scalar (for str, int, float) or a pandas scalar (for Timestamp\/Timedelta\/Interval\/Period)  Returns \n list\n    See also  numpy.ndarray.tolist\n\nReturn the array as an a.ndim-levels deep nested list of Python scalars.","title":"pandas.reference.api.pandas.series.to_list"},{"text":"numpy.recarray.tolist method   recarray.tolist()\n \nReturn the array as an a.ndim-levels deep nested list of Python scalars. Return a copy of the array data as a (nested) Python list. Data items are converted to the nearest compatible builtin Python type, via the item function. If a.ndim is 0, then since the depth of the nested list is 0, it will not be a list at all, but a simple Python scalar.  Parameters \n none\n  Returns \n \nyobject, or list of object, or list of list of object, or \u2026\n\n\nThe possibly nested list of array elements.     Notes The array may be recreated via a = np.array(a.tolist()), although this may sometimes lose precision. Examples For a 1D array, a.tolist() is almost the same as list(a), except that tolist changes numpy scalars to Python scalars: >>> a = np.uint32([1, 2])\n>>> a_list = list(a)\n>>> a_list\n[1, 2]\n>>> type(a_list[0])\n<class 'numpy.uint32'>\n>>> a_tolist = a.tolist()\n>>> a_tolist\n[1, 2]\n>>> type(a_tolist[0])\n<class 'int'>\n Additionally, for a 2D array, tolist applies recursively: >>> a = np.array([[1, 2], [3, 4]])\n>>> list(a)\n[array([1, 2]), array([3, 4])]\n>>> a.tolist()\n[[1, 2], [3, 4]]\n The base case for this recursion is a 0D array: >>> a = np.array(1)\n>>> list(a)\nTraceback (most recent call last):\n  ...\nTypeError: iteration over a 0-d array\n>>> a.tolist()\n1","title":"numpy.reference.generated.numpy.recarray.tolist"},{"text":"pandas.Series.tolist   Series.tolist()[source]\n \nReturn a list of the values. These are each a scalar type, which is a Python scalar (for str, int, float) or a pandas scalar (for Timestamp\/Timedelta\/Interval\/Period)  Returns \n list\n    See also  numpy.ndarray.tolist\n\nReturn the array as an a.ndim-levels deep nested list of Python scalars.","title":"pandas.reference.api.pandas.series.tolist"},{"text":"numpy.polynomial.polyutils.as_series   polynomial.polyutils.as_series(alist, trim=True)[source]\n \nReturn argument as a list of 1-d arrays. The returned list contains array(s) of dtype double, complex double, or object. A 1-d argument of shape (N,) is parsed into N arrays of size one; a 2-d argument of shape (M,N) is parsed into M arrays of size N (i.e., is \u201cparsed by row\u201d); and a higher dimensional array raises a Value Error if it is not first reshaped into either a 1-d or 2-d array.  Parameters \n \nalistarray_like\n\n\nA 1- or 2-d array_like  \ntrimboolean, optional\n\n\nWhen True, trailing zeros are removed from the inputs. When False, the inputs are passed through intact.    Returns \n \n[a1, a2,\u2026]list of 1-D arrays\n\n\nA copy of the input data as a list of 1-d arrays.    Raises \n ValueError\n\nRaised when as_series cannot convert its input to 1-d arrays, or at least one of the resulting arrays is empty.     Examples >>> from numpy.polynomial import polyutils as pu\n>>> a = np.arange(4)\n>>> pu.as_series(a)\n[array([0.]), array([1.]), array([2.]), array([3.])]\n>>> b = np.arange(6).reshape((2,3))\n>>> pu.as_series(b)\n[array([0., 1., 2.]), array([3., 4., 5.])]\n >>> pu.as_series((1, np.arange(3), np.arange(2, dtype=np.float16)))\n[array([1.]), array([0., 1., 2.]), array([0., 1.])]\n >>> pu.as_series([2, [1.1, 0.]])\n[array([2.]), array([1.1])]\n >>> pu.as_series([2, [1.1, 0.]], trim=False)\n[array([2.]), array([1.1, 0. ])]","title":"numpy.reference.generated.numpy.polynomial.polyutils.as_series"}]}
{"task_id":5999747,"prompt":"def f_5999747(soup):\n\treturn ","suffix":"","canonical_solution":"soup.find(text='Address:').findNext('td').contents[0]","test_start":"\nfrom bs4 import BeautifulSoup\n\ndef check(candidate):","test":["\n    assert candidate(BeautifulSoup(\"<td><b>Address:<\/b><\/td><td>My home address<\/td>\")) == \"My home address\"\n","\n    assert candidate(BeautifulSoup(\"<td><b>Address:<\/b><\/td><td>This is my home address<\/td><td>Not my home address<\/td>\")) == \"This is my home address\"\n","\n    assert candidate(BeautifulSoup(\"<td><b>Address:<\/b><\/td><td>My home address<li>My home address in a list<\/li><\/td>\")) == \"My home address\"\n"],"entry_point":"f_5999747","intent":"In `soup`, get the content of the sibling of the `td`  tag with text content `Address:`","library":["bs4"],"docs":[]}
{"task_id":4284648,"prompt":"def f_4284648(l):\n\treturn ","suffix":"","canonical_solution":"\"\"\" \"\"\".join([('%d@%d' % t) for t in l])","test_start":"\ndef check(candidate):","test":["\n    assert candidate([(1, 2), (3, 4)]) == \"1@2 3@4\"\n","\n    assert candidate([(10, 11), (12, 13)]) == \"10@11 12@13\"\n","\n    assert candidate([(10.2, 11.4), (12.14, 13.13)]) == \"10@11 12@13\"\n"],"entry_point":"f_4284648","intent":"convert elements of each tuple in list `l` into a string  separated by character `@`","library":[],"docs":[]}
{"task_id":4284648,"prompt":"def f_4284648(l):\n\treturn ","suffix":"","canonical_solution":"\"\"\" \"\"\".join([('%d@%d' % (t[0], t[1])) for t in l])","test_start":"\ndef check(candidate):","test":["\n    assert candidate([(1, 2), (3, 4)]) == \"1@2 3@4\"\n","\n    assert candidate([(10, 11), (12, 13)]) == \"10@11 12@13\"\n","\n    assert candidate([(10.2, 11.4), (12.14, 13.13)]) == \"10@11 12@13\"\n"],"entry_point":"f_4284648","intent":"convert each tuple in list `l` to a string with '@' separating the tuples' elements","library":[],"docs":[]}
{"task_id":29696641,"prompt":"def f_29696641(teststr):\n\treturn ","suffix":"","canonical_solution":"[i for i in teststr if re.search('\\\\d+[xX]', i)]","test_start":"\nimport re\n\ndef check(candidate):","test":["\n    assert candidate(['1 FirstString', '2x Sec String', '3rd String', 'x forString', '5X fifth']) == ['2x Sec String', '5X fifth']\n","\n    assert candidate(['1x', '2', '3X', '4x random', '5X random']) == ['1x', '3X', '4x random', '5X random']\n","\n    assert candidate(['1x', '2', '3X', '4xrandom', '5Xrandom']) == ['1x', '3X', '4xrandom', '5Xrandom']\n"],"entry_point":"f_29696641","intent":"Get all matches with regex pattern `\\\\d+[xX]` in list of string `teststr`","library":["re"],"docs":[{"text":"decode(doc) [source]\n \nDecode the input into a string of unicode symbols. The decoding strategy depends on the vectorizer parameters.  Parameters \n \ndocstr \n\nThe string to decode.    Returns \n doc: str\n\nA string of unicode symbols.","title":"sklearn.modules.generated.sklearn.feature_extraction.text.tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer.decode"},{"text":"decode(doc) [source]\n \nDecode the input into a string of unicode symbols. The decoding strategy depends on the vectorizer parameters.  Parameters \n \ndocstr \n\nThe string to decode.    Returns \n doc: str\n\nA string of unicode symbols.","title":"sklearn.modules.generated.sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer.decode"},{"text":"decode(doc) [source]\n \nDecode the input into a string of unicode symbols. The decoding strategy depends on the vectorizer parameters.  Parameters \n \ndocstr \n\nThe string to decode.    Returns \n doc: str\n\nA string of unicode symbols.","title":"sklearn.modules.generated.sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer.decode"},{"text":"pandas.Series.str.extractall   Series.str.extractall(pat, flags=0)[source]\n \nExtract capture groups in the regex pat as columns in DataFrame. For each subject string in the Series, extract groups from all matches of regular expression pat. When each subject string in the Series has exactly one match, extractall(pat).xs(0, level=\u2019match\u2019) is the same as extract(pat).  Parameters \n \npat:str\n\n\nRegular expression pattern with capturing groups.  \nflags:int, default 0 (no flags)\n\n\nA re module flag, for example re.IGNORECASE. These allow to modify regular expression matching for things like case, spaces, etc. Multiple flags can be combined with the bitwise OR operator, for example re.IGNORECASE | re.MULTILINE.    Returns \n DataFrame\n\nA DataFrame with one row for each match, and one column for each group. Its rows have a MultiIndex with first levels that come from the subject Series. The last level is named \u2018match\u2019 and indexes the matches in each item of the Series. Any capture group names in regular expression pat will be used for column names; otherwise capture group numbers will be used.      See also  extract\n\nReturns first match only (not all matches).    Examples A pattern with one group will return a DataFrame with one column. Indices with no matches will not appear in the result. \n>>> s = pd.Series([\"a1a2\", \"b1\", \"c1\"], index=[\"A\", \"B\", \"C\"])\n>>> s.str.extractall(r\"[ab](\\d)\")\n        0\nmatch\nA 0      1\n  1      2\nB 0      1\n  Capture group names are used for column names of the result. \n>>> s.str.extractall(r\"[ab](?P<digit>\\d)\")\n        digit\nmatch\nA 0         1\n  1         2\nB 0         1\n  A pattern with two groups will return a DataFrame with two columns. \n>>> s.str.extractall(r\"(?P<letter>[ab])(?P<digit>\\d)\")\n        letter digit\nmatch\nA 0          a     1\n  1          a     2\nB 0          b     1\n  Optional groups that do not match are NaN in the result. \n>>> s.str.extractall(r\"(?P<letter>[ab])?(?P<digit>\\d)\")\n        letter digit\nmatch\nA 0          a     1\n  1          a     2\nB 0          b     1\nC 0        NaN     1","title":"pandas.reference.api.pandas.series.str.extractall"},{"text":"skimage.lookfor(what) [source]\n \nDo a keyword search on scikit-image docstrings.  Parameters \n \nwhatstr \n\nWords to look for.     Examples >>> import skimage\n>>> skimage.lookfor('regular_grid')\nSearch results for 'regular_grid'\n---------------------------------\nskimage.lookfor\n    Do a keyword search on scikit-image docstrings.\nskimage.util.regular_grid\n    Find `n_points` regularly spaced along `ar_shape`.","title":"skimage.api.skimage#skimage.lookfor"},{"text":"get_depth(texstr, dpi=120, fontsize=14)[source]\n \n[Deprecated] Get the depth of a mathtext string.  Parameters \n \ntexstrstr\n\n\nA valid mathtext string, e.g., r'IQ: $sigma_i=15$'.  \ndpifloat\n\n\nThe dots-per-inch setting used to render the text.    Returns \n int\n\nOffset of the baseline from the bottom of the image, in pixels.     Notes  Deprecated since version 3.4.","title":"matplotlib.mathtext_api#matplotlib.mathtext.MathTextParser.get_depth"},{"text":"pandas.Series.str.findall   Series.str.findall(pat, flags=0)[source]\n \nFind all occurrences of pattern or regular expression in the Series\/Index. Equivalent to applying re.findall() to all the elements in the Series\/Index.  Parameters \n \npat:str\n\n\nPattern or regular expression.  \nflags:int, default 0\n\n\nFlags from re module, e.g. re.IGNORECASE (default is 0, which means no flags).    Returns \n Series\/Index of lists of strings\n\nAll non-overlapping matches of pattern or regular expression in each string of this Series\/Index.      See also  count\n\nCount occurrences of pattern or regular expression in each string of the Series\/Index.  extractall\n\nFor each string in the Series, extract groups from all matches of regular expression and return a DataFrame with one row for each match and one column for each group.  re.findall\n\nThe equivalent re function to all non-overlapping matches of pattern or regular expression in string, as a list of strings.    Examples \n>>> s = pd.Series(['Lion', 'Monkey', 'Rabbit'])\n  The search for the pattern \u2018Monkey\u2019 returns one match: \n>>> s.str.findall('Monkey')\n0          []\n1    [Monkey]\n2          []\ndtype: object\n  On the other hand, the search for the pattern \u2018MONKEY\u2019 doesn\u2019t return any match: \n>>> s.str.findall('MONKEY')\n0    []\n1    []\n2    []\ndtype: object\n  Flags can be added to the pattern or regular expression. For instance, to find the pattern \u2018MONKEY\u2019 ignoring the case: \n>>> import re\n>>> s.str.findall('MONKEY', flags=re.IGNORECASE)\n0          []\n1    [Monkey]\n2          []\ndtype: object\n  When the pattern matches more than one string in the Series, all matches are returned: \n>>> s.str.findall('on')\n0    [on]\n1    [on]\n2      []\ndtype: object\n  Regular expressions are supported too. For instance, the search for all the strings ending with the word \u2018on\u2019 is shown next: \n>>> s.str.findall('on$')\n0    [on]\n1      []\n2      []\ndtype: object\n  If the pattern is found more than once in the same string, then a list of multiple strings is returned: \n>>> s.str.findall('b')\n0        []\n1        []\n2    [b, b]\ndtype: object","title":"pandas.reference.api.pandas.series.str.findall"},{"text":"numpy.distutils.ccompiler.simple_version_match   distutils.ccompiler.simple_version_match(pat='[-.\\\\d]+', ignore='', start='')[source]\n \nSimple matching of version numbers, for use in CCompiler and FCompiler.  Parameters \n \npatstr, optional\n\n\nA regular expression matching version numbers. Default is r'[-.\\d]+'.  \nignorestr, optional\n\n\nA regular expression matching patterns to skip. Default is '', in which case nothing is skipped.  \nstartstr, optional\n\n\nA regular expression matching the start of where to start looking for version numbers. Default is '', in which case searching is started at the beginning of the version string given to matcher.    Returns \n \nmatchercallable\n\n\nA function that is appropriate to use as the .version_match attribute of a CCompiler class. matcher takes a single parameter, a version string.","title":"numpy.reference.generated.numpy.distutils.ccompiler.simple_version_match"},{"text":"to_mask(texstr, dpi=120, fontsize=14)[source]\n \n[Deprecated] Convert a mathtext string to a grayscale array and depth.  Parameters \n \ntexstrstr\n\n\nA valid mathtext string, e.g., r'IQ: $sigma_i=15$'.  \ndpifloat\n\n\nThe dots-per-inch setting used to render the text.  \nfontsizeint\n\n\nThe font size in points    Returns \n \narray2D uint8 alpha\n\n\nMask array of rasterized tex.  \ndepthint\n\n\nOffset of the baseline from the bottom of the image, in pixels.     Notes  Deprecated since version 3.4.","title":"matplotlib.mathtext_api#matplotlib.mathtext.MathTextParser.to_mask"},{"text":"pandas.Series.str.extract   Series.str.extract(pat, flags=0, expand=True)[source]\n \nExtract capture groups in the regex pat as columns in a DataFrame. For each subject string in the Series, extract groups from the first match of regular expression pat.  Parameters \n \npat:str\n\n\nRegular expression pattern with capturing groups.  \nflags:int, default 0 (no flags)\n\n\nFlags from the re module, e.g. re.IGNORECASE, that modify regular expression matching for things like case, spaces, etc. For more details, see re.  \nexpand:bool, default True\n\n\nIf True, return DataFrame with one column per capture group. If False, return a Series\/Index if there is one capture group or DataFrame if there are multiple capture groups.    Returns \n DataFrame or Series or Index\n\nA DataFrame with one row for each subject string, and one column for each group. Any capture group names in regular expression pat will be used for column names; otherwise capture group numbers will be used. The dtype of each result column is always object, even when no match is found. If expand=False and pat has only one capture group, then return a Series (if subject is a Series) or Index (if subject is an Index).      See also  extractall\n\nReturns all matches (not just the first match).    Examples A pattern with two groups will return a DataFrame with two columns. Non-matches will be NaN. \n>>> s = pd.Series(['a1', 'b2', 'c3'])\n>>> s.str.extract(r'([ab])(\\d)')\n    0    1\n0    a    1\n1    b    2\n2  NaN  NaN\n  A pattern may contain optional groups. \n>>> s.str.extract(r'([ab])?(\\d)')\n    0  1\n0    a  1\n1    b  2\n2  NaN  3\n  Named groups will become column names in the result. \n>>> s.str.extract(r'(?P<letter>[ab])(?P<digit>\\d)')\nletter digit\n0      a     1\n1      b     2\n2    NaN   NaN\n  A pattern with one group will return a DataFrame with one column if expand=True. \n>>> s.str.extract(r'[ab](\\d)', expand=True)\n    0\n0    1\n1    2\n2  NaN\n  A pattern with one group will return a Series if expand=False. \n>>> s.str.extract(r'[ab](\\d)', expand=False)\n0      1\n1      2\n2    NaN\ndtype: object","title":"pandas.reference.api.pandas.series.str.extract"}]}
{"task_id":15315452,"prompt":"def f_15315452(df):\n\treturn ","suffix":"","canonical_solution":"df['A'][(df['B'] > 50) & (df['C'] == 900)]","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    df = pd.DataFrame({'A': [7, 7, 4, 4, 7, 7, 3, 9, 6, 3], 'B': [20, 80, 90, 30, 80, 60, 80, 40, 40 ,10], 'C': [300, 700, 100, 900, 200, 800, 900, 100, 100, 600]})\n    assert candidate(df).to_dict() == {6: 3}\n","\n    df1 = pd.DataFrame({'A': [9, 9, 5, 8, 7, 9, 2, 2, 5, 7], 'B': [40, 70, 70, 80, 50, 30, 80, 80, 80, 70], 'C': [300, 700, 900, 900, 200, 900, 700, 400, 300, 800]})\n    assert candidate(df1).to_dict() == {2: 5, 3: 8}\n","\n    df2 = pd.DataFrame({'A': [3, 4, 5, 6], 'B': [-10, 50, 20, 10], 'C': [900, 800, 900, 900]})\n    assert candidate(df2).to_dict() == {}\n"],"entry_point":"f_15315452","intent":"select values from column 'A' for which corresponding values in column 'B' will be greater than 50, and in column 'C' - equal 900 in dataframe `df`","library":["pandas"],"docs":[]}
{"task_id":4642501,"prompt":"def f_4642501(o):\n\treturn ","suffix":"","canonical_solution":"sorted(o.items())","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    assert candidate({1:\"abc\", 5:\"klm\", 2:\"pqr\"}) == [(1, \"abc\"), (2, \"pqr\"), (5, \"klm\")]\n","\n    assert candidate({4.221:\"uwv\", -1.009:\"pow\"}) == [(-1.009, 'pow'), (4.221, 'uwv')]\n","\n    assert candidate({\"as2q\":\"piqr\", \"#wwq\":\"say\", \"Rwc\":\"koala\", \"35\":\"kangaroo\"}) == [('#wwq', 'say'), ('35', 'kangaroo'), ('Rwc', 'koala'), ('as2q', 'piqr')]\n"],"entry_point":"f_4642501","intent":"Sort dictionary `o` in ascending order based on its keys and items","library":["pandas"],"docs":[]}
{"task_id":4642501,"prompt":"def f_4642501(d):\n\treturn ","suffix":"","canonical_solution":"sorted(d)","test_start":"\ndef check(candidate):","test":["\n    assert candidate({1:\"abc\", 5:\"klm\", 2:\"pqr\"}) == [1, 2, 5]\n","\n    assert candidate({4.221:\"uwv\", -1.009:\"pow\"}) == [-1.009, 4.221]\n","\n    assert candidate({\"as2q\":\"piqr\", \"#wwq\":\"say\", \"Rwc\":\"koala\", \"35\":\"kangaroo\"}) == ['#wwq', '35', 'Rwc', 'as2q']\n"],"entry_point":"f_4642501","intent":"get sorted list of keys of dict `d`","library":[],"docs":[]}
{"task_id":4642501,"prompt":"def f_4642501(d):\n\treturn ","suffix":"","canonical_solution":"sorted(d.items())","test_start":"\ndef check(candidate):","test":["\n    d = {'a': [1, 2, 3], 'c': ['one', 'two'], 'b': ['blah', 'bhasdf', 'asdf'], 'd': ['asdf', 'wer', 'asdf', 'zxcv']}\n    assert candidate(d) == [('a', [1, 2, 3]), ('b', ['blah', 'bhasdf', 'asdf']), ('c', ['one', 'two']), ('d', ['asdf', 'wer', 'asdf', 'zxcv'])]\n"],"entry_point":"f_4642501","intent":"sort dictionaries `d` by keys","library":[],"docs":[]}
{"task_id":642154,"prompt":"def f_642154():\n\treturn ","suffix":"","canonical_solution":"int('1')","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == 1\n","\n    assert candidate() + 1 == 2\n"],"entry_point":"f_642154","intent":"convert string \"1\" into integer","library":[],"docs":[]}
{"task_id":642154,"prompt":"def f_642154(T1):\n\treturn ","suffix":"","canonical_solution":"[list(map(int, x)) for x in T1]","test_start":"\ndef check(candidate):","test":["\n    T1 = (('13', '17', '18', '21', '32'), ('07', '11', '13', '14', '28'), ('01', '05', '06', '08', '15', '16'))\n    assert candidate(T1) == [[13, 17, 18, 21, 32], [7, 11, 13, 14, 28], [1, 5, 6, 8, 15, 16]]\n"],"entry_point":"f_642154","intent":"convert items in `T1` to integers","library":[],"docs":[]}
{"task_id":3777301,"prompt":"def f_3777301():\n\t","suffix":"\n\treturn ","canonical_solution":"subprocess.call(['.\/test.sh'])","test_start":"\nimport subprocess\nfrom unittest.mock import Mock\n\ndef check(candidate):","test":["\n    subprocess.call = Mock()\n    try:\n        candidate()\n    except:\n        assert False\n"],"entry_point":"f_3777301","intent":"call a shell script `.\/test.sh` using subprocess","library":["subprocess"],"docs":[{"text":"cgi.test()  \nRobust test CGI script, usable as main program. Writes minimal HTTP headers and formats all information provided to the script in HTML form.","title":"python.library.cgi#cgi.test"},{"text":"user_passes_test(test_func, login_url=None, redirect_field_name='next')  \nAs a shortcut, you can use the convenient user_passes_test decorator which performs a redirect when the callable returns False: from django.contrib.auth.decorators import user_passes_test\n\ndef email_check(user):\n    return user.email.endswith('@example.com')\n\n@user_passes_test(email_check)\ndef my_view(request):\n    ...\n user_passes_test() takes a required argument: a callable that takes a User object and returns True if the user is allowed to view the page. Note that user_passes_test() does not automatically check that the User is not anonymous. user_passes_test() takes two optional arguments:  \nlogin_url  Lets you specify the URL that users who don\u2019t pass the test will be redirected to. It may be a login page and defaults to settings.LOGIN_URL if you don\u2019t specify one. \nredirect_field_name  Same as for login_required(). Setting it to None removes it from the URL, which you may want to do if you are redirecting users that don\u2019t pass the test to a non-login page where there\u2019s no \u201cnext page\u201d.  For example: @user_passes_test(email_check, login_url='\/login\/')\ndef my_view(request):\n    ...","title":"django.topics.auth.default#django.contrib.auth.decorators.user_passes_test"},{"text":"test.support.script_helper.spawn_python(*args, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, **kw)  \nRun a Python subprocess with the given arguments. kw is extra keyword args to pass to subprocess.Popen(). Returns a subprocess.Popen object.","title":"python.library.test#test.support.script_helper.spawn_python"},{"text":"test.support.script_helper.assert_python_failure(*args, **env_vars)  \nAssert that running the interpreter with args and optional environment variables env_vars fails (rc != 0) and return a (return code,\nstdout, stderr) tuple. See assert_python_ok() for more options.  Changed in version 3.9: The function no longer strips whitespaces from stderr.","title":"python.library.test#test.support.script_helper.assert_python_failure"},{"text":"pandas.Series.get   Series.get(key, default=None)[source]\n \nGet item from object for given key (ex: DataFrame column). Returns default value if not found.  Parameters \n \nkey:object\n\n  Returns \n \nvalue:same type as items contained in object\n\n   Examples \n>>> df = pd.DataFrame(\n...     [\n...         [24.3, 75.7, \"high\"],\n...         [31, 87.8, \"high\"],\n...         [22, 71.6, \"medium\"],\n...         [35, 95, \"medium\"],\n...     ],\n...     columns=[\"temp_celsius\", \"temp_fahrenheit\", \"windspeed\"],\n...     index=pd.date_range(start=\"2014-02-12\", end=\"2014-02-15\", freq=\"D\"),\n... )\n  \n>>> df\n            temp_celsius  temp_fahrenheit windspeed\n2014-02-12          24.3             75.7      high\n2014-02-13          31.0             87.8      high\n2014-02-14          22.0             71.6    medium\n2014-02-15          35.0             95.0    medium\n  \n>>> df.get([\"temp_celsius\", \"windspeed\"])\n            temp_celsius windspeed\n2014-02-12          24.3      high\n2014-02-13          31.0      high\n2014-02-14          22.0    medium\n2014-02-15          35.0    medium\n  If the key isn\u2019t found, the default value will be used. \n>>> df.get([\"temp_celsius\", \"temp_kelvin\"], default=\"default_value\")\n'default_value'","title":"pandas.reference.api.pandas.series.get"},{"text":"pandas.test   pandas.test(extra_args=None)[source]\n \nRun the pandas test suite using pytest.","title":"pandas.reference.api.pandas.test"},{"text":"Subprocesses Source code: Lib\/asyncio\/subprocess.py, Lib\/asyncio\/base_subprocess.py This section describes high-level async\/await asyncio APIs to create and manage subprocesses. Here\u2019s an example of how asyncio can run a shell command and obtain its result: import asyncio\n\nasync def run(cmd):\n    proc = await asyncio.create_subprocess_shell(\n        cmd,\n        stdout=asyncio.subprocess.PIPE,\n        stderr=asyncio.subprocess.PIPE)\n\n    stdout, stderr = await proc.communicate()\n\n    print(f'[{cmd!r} exited with {proc.returncode}]')\n    if stdout:\n        print(f'[stdout]\\n{stdout.decode()}')\n    if stderr:\n        print(f'[stderr]\\n{stderr.decode()}')\n\nasyncio.run(run('ls \/zzz'))\n will print: ['ls \/zzz' exited with 1]\n[stderr]\nls: \/zzz: No such file or directory\n Because all asyncio subprocess functions are asynchronous and asyncio provides many tools to work with such functions, it is easy to execute and monitor multiple subprocesses in parallel. It is indeed trivial to modify the above example to run several commands simultaneously: async def main():\n    await asyncio.gather(\n        run('ls \/zzz'),\n        run('sleep 1; echo \"hello\"'))\n\nasyncio.run(main())\n See also the Examples subsection. Creating Subprocesses  \ncoroutine asyncio.create_subprocess_exec(program, *args, stdin=None, stdout=None, stderr=None, loop=None, limit=None, **kwds)  \nCreate a subprocess. The limit argument sets the buffer limit for StreamReader wrappers for Process.stdout and Process.stderr (if subprocess.PIPE is passed to stdout and stderr arguments). Return a Process instance. See the documentation of loop.subprocess_exec() for other parameters.  Deprecated since version 3.8, will be removed in version 3.10: The loop parameter.  \n  \ncoroutine asyncio.create_subprocess_shell(cmd, stdin=None, stdout=None, stderr=None, loop=None, limit=None, **kwds)  \nRun the cmd shell command. The limit argument sets the buffer limit for StreamReader wrappers for Process.stdout and Process.stderr (if subprocess.PIPE is passed to stdout and stderr arguments). Return a Process instance. See the documentation of loop.subprocess_shell() for other parameters.  Important It is the application\u2019s responsibility to ensure that all whitespace and special characters are quoted appropriately to avoid shell injection vulnerabilities. The shlex.quote() function can be used to properly escape whitespace and special shell characters in strings that are going to be used to construct shell commands.   Deprecated since version 3.8, will be removed in version 3.10: The loop parameter.  \n  Note Subprocesses are available for Windows if a ProactorEventLoop is used. See Subprocess Support on Windows for details.   See also asyncio also has the following low-level APIs to work with subprocesses: loop.subprocess_exec(), loop.subprocess_shell(), loop.connect_read_pipe(), loop.connect_write_pipe(), as well as the Subprocess Transports and Subprocess Protocols.  Constants  \nasyncio.subprocess.PIPE  \nCan be passed to the stdin, stdout or stderr parameters. If PIPE is passed to stdin argument, the Process.stdin attribute will point to a StreamWriter instance. If PIPE is passed to stdout or stderr arguments, the Process.stdout and Process.stderr attributes will point to StreamReader instances. \n  \nasyncio.subprocess.STDOUT  \nSpecial value that can be used as the stderr argument and indicates that standard error should be redirected into standard output. \n  \nasyncio.subprocess.DEVNULL  \nSpecial value that can be used as the stdin, stdout or stderr argument to process creation functions. It indicates that the special file os.devnull will be used for the corresponding subprocess stream. \n Interacting with Subprocesses Both create_subprocess_exec() and create_subprocess_shell() functions return instances of the Process class. Process is a high-level wrapper that allows communicating with subprocesses and watching for their completion.  \nclass asyncio.subprocess.Process  \nAn object that wraps OS processes created by the create_subprocess_exec() and create_subprocess_shell() functions. This class is designed to have a similar API to the subprocess.Popen class, but there are some notable differences:  unlike Popen, Process instances do not have an equivalent to the poll() method; the communicate() and wait() methods don\u2019t have a timeout parameter: use the wait_for() function; the Process.wait() method is asynchronous, whereas subprocess.Popen.wait() method is implemented as a blocking busy loop; the universal_newlines parameter is not supported.  This class is not thread safe. See also the Subprocess and Threads section.  \ncoroutine wait()  \nWait for the child process to terminate. Set and return the returncode attribute.  Note This method can deadlock when using stdout=PIPE or stderr=PIPE and the child process generates so much output that it blocks waiting for the OS pipe buffer to accept more data. Use the communicate() method when using pipes to avoid this condition.  \n  \ncoroutine communicate(input=None)  \nInteract with process:  send data to stdin (if input is not None); read data from stdout and stderr, until EOF is reached; wait for process to terminate.  The optional input argument is the data (bytes object) that will be sent to the child process. Return a tuple (stdout_data, stderr_data). If either BrokenPipeError or ConnectionResetError exception is raised when writing input into stdin, the exception is ignored. This condition occurs when the process exits before all data are written into stdin. If it is desired to send data to the process\u2019 stdin, the process needs to be created with stdin=PIPE. Similarly, to get anything other than None in the result tuple, the process has to be created with stdout=PIPE and\/or stderr=PIPE arguments. Note, that the data read is buffered in memory, so do not use this method if the data size is large or unlimited. \n  \nsend_signal(signal)  \nSends the signal signal to the child process.  Note On Windows, SIGTERM is an alias for terminate(). CTRL_C_EVENT and CTRL_BREAK_EVENT can be sent to processes started with a creationflags parameter which includes CREATE_NEW_PROCESS_GROUP.  \n  \nterminate()  \nStop the child process. On POSIX systems this method sends signal.SIGTERM to the child process. On Windows the Win32 API function TerminateProcess() is called to stop the child process. \n  \nkill()  \nKill the child process. On POSIX systems this method sends SIGKILL to the child process. On Windows this method is an alias for terminate(). \n  \nstdin  \nStandard input stream (StreamWriter) or None if the process was created with stdin=None. \n  \nstdout  \nStandard output stream (StreamReader) or None if the process was created with stdout=None. \n  \nstderr  \nStandard error stream (StreamReader) or None if the process was created with stderr=None. \n  Warning Use the communicate() method rather than process.stdin.write(), await process.stdout.read() or await process.stderr.read. This avoids deadlocks due to streams pausing reading or writing and blocking the child process.   \npid  \nProcess identification number (PID). Note that for processes created by the create_subprocess_shell() function, this attribute is the PID of the spawned shell. \n  \nreturncode  \nReturn code of the process when it exits. A None value indicates that the process has not terminated yet. A negative value -N indicates that the child was terminated by signal N (POSIX only). \n \n Subprocess and Threads Standard asyncio event loop supports running subprocesses from different threads by default. On Windows subprocesses are provided by ProactorEventLoop only (default), SelectorEventLoop has no subprocess support. On UNIX child watchers are used for subprocess finish waiting, see Process Watchers for more info.  Changed in version 3.8: UNIX switched to use ThreadedChildWatcher for spawning subprocesses from different threads without any limitation. Spawning a subprocess with inactive current child watcher raises RuntimeError.  Note that alternative event loop implementations might have own limitations; please refer to their documentation.  See also The Concurrency and multithreading in asyncio section.  Examples An example using the Process class to control a subprocess and the StreamReader class to read from its standard output. The subprocess is created by the create_subprocess_exec() function: import asyncio\nimport sys\n\nasync def get_date():\n    code = 'import datetime; print(datetime.datetime.now())'\n\n    # Create the subprocess; redirect the standard output\n    # into a pipe.\n    proc = await asyncio.create_subprocess_exec(\n        sys.executable, '-c', code,\n        stdout=asyncio.subprocess.PIPE)\n\n    # Read one line of output.\n    data = await proc.stdout.readline()\n    line = data.decode('ascii').rstrip()\n\n    # Wait for the subprocess exit.\n    await proc.wait()\n    return line\n\ndate = asyncio.run(get_date())\nprint(f\"Current date: {date}\")\n See also the same example written using low-level APIs.","title":"python.library.asyncio-subprocess"},{"text":"pty \u2014 Pseudo-terminal utilities Source code: Lib\/pty.py The pty module defines operations for handling the pseudo-terminal concept: starting another process and being able to write to and read from its controlling terminal programmatically. Because pseudo-terminal handling is highly platform dependent, there is code to do it only for Linux. (The Linux code is supposed to work on other platforms, but hasn\u2019t been tested yet.) The pty module defines the following functions:  \npty.fork()  \nFork. Connect the child\u2019s controlling terminal to a pseudo-terminal. Return value is (pid, fd). Note that the child gets pid 0, and the fd is invalid. The parent\u2019s return value is the pid of the child, and fd is a file descriptor connected to the child\u2019s controlling terminal (and also to the child\u2019s standard input and output). \n  \npty.openpty()  \nOpen a new pseudo-terminal pair, using os.openpty() if possible, or emulation code for generic Unix systems. Return a pair of file descriptors (master, slave), for the master and the slave end, respectively. \n  \npty.spawn(argv[, master_read[, stdin_read]])  \nSpawn a process, and connect its controlling terminal with the current process\u2019s standard io. This is often used to baffle programs which insist on reading from the controlling terminal. It is expected that the process spawned behind the pty will eventually terminate, and when it does spawn will return. The functions master_read and stdin_read are passed a file descriptor which they should read from, and they should always return a byte string. In order to force spawn to return before the child process exits an OSError should be thrown. The default implementation for both functions will read and return up to 1024 bytes each time the function is called. The master_read callback is passed the pseudoterminal\u2019s master file descriptor to read output from the child process, and stdin_read is passed file descriptor 0, to read from the parent process\u2019s standard input. Returning an empty byte string from either callback is interpreted as an end-of-file (EOF) condition, and that callback will not be called after that. If stdin_read signals EOF the controlling terminal can no longer communicate with the parent process OR the child process. Unless the child process will quit without any input, spawn will then loop forever. If master_read signals EOF the same behavior results (on linux at least). If both callbacks signal EOF then spawn will probably never return, unless select throws an error on your platform when passed three empty lists. This is a bug, documented in issue 26228. Return the exit status value from os.waitpid() on the child process. waitstatus_to_exitcode() can be used to convert the exit status into an exit code. Raises an auditing event pty.spawn with argument argv.  Changed in version 3.4: spawn() now returns the status value from os.waitpid() on the child process.  \n Example The following program acts like the Unix command script(1), using a pseudo-terminal to record all input and output of a terminal session in a \u201ctypescript\u201d. import argparse\nimport os\nimport pty\nimport sys\nimport time\n\nparser = argparse.ArgumentParser()\nparser.add_argument('-a', dest='append', action='store_true')\nparser.add_argument('-p', dest='use_python', action='store_true')\nparser.add_argument('filename', nargs='?', default='typescript')\noptions = parser.parse_args()\n\nshell = sys.executable if options.use_python else os.environ.get('SHELL', 'sh')\nfilename = options.filename\nmode = 'ab' if options.append else 'wb'\n\nwith open(filename, mode) as script:\n    def read(fd):\n        data = os.read(fd, 1024)\n        script.write(data)\n        return data\n\n    print('Script started, file is', filename)\n    script.write(('Script started on %s\\n' % time.asctime()).encode())\n\n    pty.spawn(shell, read)\n\n    script.write(('Script done on %s\\n' % time.asctime()).encode())\n    print('Script done, file is', filename)","title":"python.library.pty"},{"text":"test.support.script_helper.assert_python_ok(*args, **env_vars)  \nAssert that running the interpreter with args and optional environment variables env_vars succeeds (rc == 0) and return a (return code,\nstdout, stderr) tuple. If the __cleanenv keyword is set, env_vars is used as a fresh environment. Python is started in isolated mode (command line option -I), except if the __isolated keyword is set to False.  Changed in version 3.9: The function no longer strips whitespaces from stderr.","title":"python.library.test#test.support.script_helper.assert_python_ok"},{"text":"pandas.DataFrame.get   DataFrame.get(key, default=None)[source]\n \nGet item from object for given key (ex: DataFrame column). Returns default value if not found.  Parameters \n \nkey:object\n\n  Returns \n \nvalue:same type as items contained in object\n\n   Examples \n>>> df = pd.DataFrame(\n...     [\n...         [24.3, 75.7, \"high\"],\n...         [31, 87.8, \"high\"],\n...         [22, 71.6, \"medium\"],\n...         [35, 95, \"medium\"],\n...     ],\n...     columns=[\"temp_celsius\", \"temp_fahrenheit\", \"windspeed\"],\n...     index=pd.date_range(start=\"2014-02-12\", end=\"2014-02-15\", freq=\"D\"),\n... )\n  \n>>> df\n            temp_celsius  temp_fahrenheit windspeed\n2014-02-12          24.3             75.7      high\n2014-02-13          31.0             87.8      high\n2014-02-14          22.0             71.6    medium\n2014-02-15          35.0             95.0    medium\n  \n>>> df.get([\"temp_celsius\", \"windspeed\"])\n            temp_celsius windspeed\n2014-02-12          24.3      high\n2014-02-13          31.0      high\n2014-02-14          22.0    medium\n2014-02-15          35.0    medium\n  If the key isn\u2019t found, the default value will be used. \n>>> df.get([\"temp_celsius\", \"temp_kelvin\"], default=\"default_value\")\n'default_value'","title":"pandas.reference.api.pandas.dataframe.get"}]}
{"task_id":3777301,"prompt":"def f_3777301():\n\t","suffix":"\n\treturn ","canonical_solution":"subprocess.call(['notepad'])","test_start":"\nimport subprocess\nfrom unittest.mock import Mock\n\ndef check(candidate):","test":["\n    subprocess.call = Mock()\n    try:\n        candidate()\n    except:\n        assert False\n"],"entry_point":"f_3777301","intent":"call a shell script `notepad` using subprocess","library":["subprocess"],"docs":[{"text":"class ast.UAdd  \nclass ast.USub  \nclass ast.Not  \nclass ast.Invert  \nUnary operator tokens. Not is the not keyword, Invert is the ~ operator. >>> print(ast.dump(ast.parse('not x', mode='eval'), indent=4))\nExpression(\n    body=UnaryOp(\n        op=Not(),\n        operand=Name(id='x', ctx=Load())))","title":"python.library.ast#ast.Not"},{"text":"matplotlib.axis.Tick.get_pad   Tick.get_pad()[source]\n \nGet the value of the tick label pad in points.","title":"matplotlib._as_gen.matplotlib.axis.tick.get_pad"},{"text":"matplotlib.axis.Tick.get_pad_pixels   Tick.get_pad_pixels()[source]","title":"matplotlib._as_gen.matplotlib.axis.tick.get_pad_pixels"},{"text":"class ast.UAdd  \nclass ast.USub  \nclass ast.Not  \nclass ast.Invert  \nUnary operator tokens. Not is the not keyword, Invert is the ~ operator. >>> print(ast.dump(ast.parse('not x', mode='eval'), indent=4))\nExpression(\n    body=UnaryOp(\n        op=Not(),\n        operand=Name(id='x', ctx=Load())))","title":"python.library.ast#ast.UAdd"},{"text":"matplotlib.axis.Axis.OFFSETTEXTPAD   Axis.OFFSETTEXTPAD=3","title":"matplotlib._as_gen.matplotlib.axis.axis.offsettextpad"},{"text":"os.P_NOWAIT  \nos.P_NOWAITO  \nPossible values for the mode parameter to the spawn* family of functions. If either of these values is given, the spawn*() functions will return as soon as the new process has been created, with the process id as the return value. Availability: Unix, Windows.","title":"python.library.os#os.P_NOWAIT"},{"text":"class ast.UAdd  \nclass ast.USub  \nclass ast.Not  \nclass ast.Invert  \nUnary operator tokens. Not is the not keyword, Invert is the ~ operator. >>> print(ast.dump(ast.parse('not x', mode='eval'), indent=4))\nExpression(\n    body=UnaryOp(\n        op=Not(),\n        operand=Name(id='x', ctx=Load())))","title":"python.library.ast#ast.USub"},{"text":"epoll.unregister(fd)  \nRemove a registered file descriptor from the epoll object.  Changed in version 3.9: The method no longer ignores the EBADF error.","title":"python.library.select#select.epoll.unregister"},{"text":"get_pad()[source]\n \nReturn the internal pad in points. See set_pad for more details.","title":"matplotlib._as_gen.mpl_toolkits.axisartist.axis_artist.axislabel#mpl_toolkits.axisartist.axis_artist.AxisLabel.get_pad"},{"text":"class ast.UAdd  \nclass ast.USub  \nclass ast.Not  \nclass ast.Invert  \nUnary operator tokens. Not is the not keyword, Invert is the ~ operator. >>> print(ast.dump(ast.parse('not x', mode='eval'), indent=4))\nExpression(\n    body=UnaryOp(\n        op=Not(),\n        operand=Name(id='x', ctx=Load())))","title":"python.library.ast#ast.Invert"}]}
{"task_id":7946798,"prompt":"def f_7946798(l1, l2):\n\treturn ","suffix":"","canonical_solution":"[val for pair in zip(l1, l2) for val in pair]","test_start":"\ndef check(candidate):","test":["\n    assert candidate([1,2,3], [10,20,30]) == [1,10,2,20,3,30]\n","\n    assert candidate([1,2,3], ['c','b','a']) == [1,'c',2,'b',3,'a']\n","\n    assert candidate([1,2,3], ['c','b']) == [1,'c',2,'b']\n"],"entry_point":"f_7946798","intent":"combine lists `l1` and `l2`  by alternating their elements","library":[],"docs":[]}
{"task_id":8908287,"prompt":"def f_8908287():\n\treturn ","suffix":"","canonical_solution":"base64.b64encode(b'data to be encoded')","test_start":"\nimport base64\n\ndef check(candidate):","test":["\n    assert candidate() == b'ZGF0YSB0byBiZSBlbmNvZGVk'\n"],"entry_point":"f_8908287","intent":"encode string 'data to be encoded'","library":["base64"],"docs":[{"text":"base64.b32encode(s)  \nEncode the bytes-like object s using Base32 and return the encoded bytes.","title":"python.library.base64#base64.b32encode"},{"text":"binascii.b2a_base64(data, *, newline=True)  \nConvert binary data to a line of ASCII characters in base64 coding. The return value is the converted line, including a newline char if newline is true. The output of this function conforms to RFC 3548.  Changed in version 3.6: Added the newline parameter.","title":"python.library.binascii#binascii.b2a_base64"},{"text":"base64.b16encode(s)  \nEncode the bytes-like object s using Base16 and return the encoded bytes.","title":"python.library.base64#base64.b16encode"},{"text":"werkzeug.http.generate_etag(data)  \nGenerate an etag for some data.  Changed in version 2.0: Use SHA-1. MD5 may not be available in some environments.   Parameters \ndata (bytes) \u2013   Return type \nstr","title":"werkzeug.http.index#werkzeug.http.generate_etag"},{"text":"base64.b85encode(b, pad=False)  \nEncode the bytes-like object b using base85 (as used in e.g. git-style binary diffs) and return the encoded bytes. If pad is true, the input is padded with b'\\0' so its length is a multiple of 4 bytes before encoding.  New in version 3.4.","title":"python.library.base64#base64.b85encode"},{"text":"compress(data)  \nProvide data to the compressor object. Returns a chunk of compressed data if possible, or an empty byte string otherwise. When you have finished providing data to the compressor, call the flush() method to finish the compression process.","title":"python.library.bz2#bz2.BZ2Compressor.compress"},{"text":"Compress.compress(data)  \nCompress data, returning a bytes object containing compressed data for at least part of the data in data. This data should be concatenated to the output produced by any preceding calls to the compress() method. Some input may be kept in internal buffers for later processing.","title":"python.library.zlib#zlib.Compress.compress"},{"text":"header_encode(string)  \nHeader-encode the string string. The type of encoding (base64 or quoted-printable) will be based on the header_encoding attribute.","title":"python.library.email.charset#email.charset.Charset.header_encode"},{"text":"compress(data)  \nCompress data (a bytes object), returning a bytes object containing compressed data for at least part of the input. Some of data may be buffered internally, for use in later calls to compress() and flush(). The returned data should be concatenated with the output of any previous calls to compress().","title":"python.library.lzma#lzma.LZMACompressor.compress"},{"text":"quopri.encodestring(s, quotetabs=False, header=False)  \nLike encode(), except that it accepts a source bytes and returns the corresponding encoded bytes. By default, it sends a False value to quotetabs parameter of the encode() function.","title":"python.library.quopri#quopri.encodestring"}]}
{"task_id":8908287,"prompt":"def f_8908287():\n\treturn ","suffix":"","canonical_solution":"'data to be encoded'.encode('ascii')","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == b'data to be encoded'\n"],"entry_point":"f_8908287","intent":"encode a string `data to be encoded` to `ascii` encoding","library":[],"docs":[]}
{"task_id":7856296,"prompt":"def f_7856296():\n\treturn ","suffix":"","canonical_solution":"list(csv.reader(open('text.txt', 'r'), delimiter='\\t'))","test_start":"\nimport csv \n\ndef check(candidate):","test":["\n    with open('text.txt', 'w', newline='') as csvfile:\n        spamwriter = csv.writer(csvfile, delimiter='\t')\n        spamwriter.writerow(['Spam', 'Lovely Spam', 'Wonderful Spam'])\n        spamwriter.writerow(['hello', 'world', '!'])\n\n    assert candidate() == [['Spam', 'Lovely Spam', 'Wonderful Spam'], ['hello', 'world', '!']]\n"],"entry_point":"f_7856296","intent":"parse tab-delimited CSV file 'text.txt' into a list","library":["csv"],"docs":[{"text":"Importing data with genfromtxt NumPy provides several functions to create arrays from tabular data. We focus here on the genfromtxt function. In a nutshell, genfromtxt runs two main loops. The first loop converts each line of the file in a sequence of strings. The second loop converts each string to the appropriate data type. This mechanism is slower than a single loop, but gives more flexibility. In particular, genfromtxt is able to take missing data into account, when other faster and simpler functions like loadtxt cannot.  Note When giving examples, we will use the following conventions: >>> import numpy as np\n>>> from io import StringIO\n   Defining the input The only mandatory argument of genfromtxt is the source of the data. It can be a string, a list of strings, a generator or an open file-like object with a read method, for example, a file or io.StringIO object. If a single string is provided, it is assumed to be the name of a local or remote file. If a list of strings or a generator returning strings is provided, each string is treated as one line in a file. When the URL of a remote file is passed, the file is automatically downloaded to the current directory and opened. Recognized file types are text files and archives. Currently, the function recognizes gzip and bz2 (bzip2) archives. The type of the archive is determined from the extension of the file: if the filename ends with '.gz', a gzip archive is expected; if it ends with 'bz2', a bzip2 archive is assumed.   Splitting the lines into columns  The delimiter argument Once the file is defined and open for reading, genfromtxt splits each non-empty line into a sequence of strings. Empty or commented lines are just skipped. The delimiter keyword is used to define how the splitting should take place. Quite often, a single character marks the separation between columns. For example, comma-separated files (CSV) use a comma (,) or a semicolon (;) as delimiter: >>> data = u\"1, 2, 3\\n4, 5, 6\"\n>>> np.genfromtxt(StringIO(data), delimiter=\",\")\narray([[ 1.,  2.,  3.],\n       [ 4.,  5.,  6.]])\n Another common separator is \"\\t\", the tabulation character. However, we are not limited to a single character, any string will do. By default, genfromtxt assumes delimiter=None, meaning that the line is split along white spaces (including tabs) and that consecutive white spaces are considered as a single white space. Alternatively, we may be dealing with a fixed-width file, where columns are defined as a given number of characters. In that case, we need to set delimiter to a single integer (if all the columns have the same size) or to a sequence of integers (if columns can have different sizes): >>> data = u\"  1  2  3\\n  4  5 67\\n890123  4\"\n>>> np.genfromtxt(StringIO(data), delimiter=3)\narray([[   1.,    2.,    3.],\n       [   4.,    5.,   67.],\n       [ 890.,  123.,    4.]])\n>>> data = u\"123456789\\n   4  7 9\\n   4567 9\"\n>>> np.genfromtxt(StringIO(data), delimiter=(4, 3, 2))\narray([[ 1234.,   567.,    89.],\n       [    4.,     7.,     9.],\n       [    4.,   567.,     9.]])\n   The autostrip argument By default, when a line is decomposed into a series of strings, the individual entries are not stripped of leading nor trailing white spaces. This behavior can be overwritten by setting the optional argument autostrip to a value of True: >>> data = u\"1, abc , 2\\n 3, xxx, 4\"\n>>> # Without autostrip\n>>> np.genfromtxt(StringIO(data), delimiter=\",\", dtype=\"|U5\")\narray([['1', ' abc ', ' 2'],\n       ['3', ' xxx', ' 4']], dtype='<U5')\n>>> # With autostrip\n>>> np.genfromtxt(StringIO(data), delimiter=\",\", dtype=\"|U5\", autostrip=True)\narray([['1', 'abc', '2'],\n       ['3', 'xxx', '4']], dtype='<U5')\n   The comments argument The optional argument comments is used to define a character string that marks the beginning of a comment. By default, genfromtxt assumes comments='#'. The comment marker may occur anywhere on the line. Any character present after the comment marker(s) is simply ignored: >>> data = u\"\"\"#\n... # Skip me !\n... # Skip me too !\n... 1, 2\n... 3, 4\n... 5, 6 #This is the third line of the data\n... 7, 8\n... # And here comes the last line\n... 9, 0\n... \"\"\"\n>>> np.genfromtxt(StringIO(data), comments=\"#\", delimiter=\",\")\narray([[1., 2.],\n       [3., 4.],\n       [5., 6.],\n       [7., 8.],\n       [9., 0.]])\n  New in version 1.7.0: When comments is set to None, no lines are treated as comments.   Note There is one notable exception to this behavior: if the optional argument names=True, the first commented line will be examined for names.     Skipping lines and choosing columns  The skip_header and skip_footer arguments The presence of a header in the file can hinder data processing. In that case, we need to use the skip_header optional argument. The values of this argument must be an integer which corresponds to the number of lines to skip at the beginning of the file, before any other action is performed. Similarly, we can skip the last n lines of the file by using the skip_footer attribute and giving it a value of n: >>> data = u\"\\n\".join(str(i) for i in range(10))\n>>> np.genfromtxt(StringIO(data),)\narray([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.])\n>>> np.genfromtxt(StringIO(data),\n...               skip_header=3, skip_footer=5)\narray([ 3.,  4.])\n By default, skip_header=0 and skip_footer=0, meaning that no lines are skipped.   The usecols argument In some cases, we are not interested in all the columns of the data but only a few of them. We can select which columns to import with the usecols argument. This argument accepts a single integer or a sequence of integers corresponding to the indices of the columns to import. Remember that by convention, the first column has an index of 0. Negative integers behave the same as regular Python negative indexes. For example, if we want to import only the first and the last columns, we can use usecols=(0, -1): >>> data = u\"1 2 3\\n4 5 6\"\n>>> np.genfromtxt(StringIO(data), usecols=(0, -1))\narray([[ 1.,  3.],\n       [ 4.,  6.]])\n If the columns have names, we can also select which columns to import by giving their name to the usecols argument, either as a sequence of strings or a comma-separated string: >>> data = u\"1 2 3\\n4 5 6\"\n>>> np.genfromtxt(StringIO(data),\n...               names=\"a, b, c\", usecols=(\"a\", \"c\"))\narray([(1.0, 3.0), (4.0, 6.0)],\n      dtype=[('a', '<f8'), ('c', '<f8')])\n>>> np.genfromtxt(StringIO(data),\n...               names=\"a, b, c\", usecols=(\"a, c\"))\n    array([(1.0, 3.0), (4.0, 6.0)],\n          dtype=[('a', '<f8'), ('c', '<f8')])\n    Choosing the data type The main way to control how the sequences of strings we have read from the file are converted to other types is to set the dtype argument. Acceptable values for this argument are:  a single type, such as dtype=float. The output will be 2D with the given dtype, unless a name has been associated with each column with the use of the names argument (see below). Note that dtype=float is the default for genfromtxt. a sequence of types, such as dtype=(int, float, float). a comma-separated string, such as dtype=\"i4,f8,|U3\". a dictionary with two keys 'names' and 'formats'. a sequence of tuples (name, type), such as dtype=[('A', int), ('B', float)]. an existing numpy.dtype object. the special value None. In that case, the type of the columns will be determined from the data itself (see below).  In all the cases but the first one, the output will be a 1D array with a structured dtype. This dtype has as many fields as items in the sequence. The field names are defined with the names keyword. When dtype=None, the type of each column is determined iteratively from its data. We start by checking whether a string can be converted to a boolean (that is, if the string matches true or false in lower cases); then whether it can be converted to an integer, then to a float, then to a complex and eventually to a string. This behavior may be changed by modifying the default mapper of the StringConverter class. The option dtype=None is provided for convenience. However, it is significantly slower than setting the dtype explicitly.   Setting the names  The names argument A natural approach when dealing with tabular data is to allocate a name to each column. A first possibility is to use an explicit structured dtype, as mentioned previously: >>> data = StringIO(\"1 2 3\\n 4 5 6\")\n>>> np.genfromtxt(data, dtype=[(_, int) for _ in \"abc\"])\narray([(1, 2, 3), (4, 5, 6)],\n      dtype=[('a', '<i8'), ('b', '<i8'), ('c', '<i8')])\n Another simpler possibility is to use the names keyword with a sequence of strings or a comma-separated string: >>> data = StringIO(\"1 2 3\\n 4 5 6\")\n>>> np.genfromtxt(data, names=\"A, B, C\")\narray([(1.0, 2.0, 3.0), (4.0, 5.0, 6.0)],\n      dtype=[('A', '<f8'), ('B', '<f8'), ('C', '<f8')])\n In the example above, we used the fact that by default, dtype=float. By giving a sequence of names, we are forcing the output to a structured dtype. We may sometimes need to define the column names from the data itself. In that case, we must use the names keyword with a value of True. The names will then be read from the first line (after the skip_header ones), even if the line is commented out: >>> data = StringIO(\"So it goes\\n#a b c\\n1 2 3\\n 4 5 6\")\n>>> np.genfromtxt(data, skip_header=1, names=True)\narray([(1.0, 2.0, 3.0), (4.0, 5.0, 6.0)],\n      dtype=[('a', '<f8'), ('b', '<f8'), ('c', '<f8')])\n The default value of names is None. If we give any other value to the keyword, the new names will overwrite the field names we may have defined with the dtype: >>> data = StringIO(\"1 2 3\\n 4 5 6\")\n>>> ndtype=[('a',int), ('b', float), ('c', int)]\n>>> names = [\"A\", \"B\", \"C\"]\n>>> np.genfromtxt(data, names=names, dtype=ndtype)\narray([(1, 2.0, 3), (4, 5.0, 6)],\n      dtype=[('A', '<i8'), ('B', '<f8'), ('C', '<i8')])\n   The defaultfmt argument If names=None but a structured dtype is expected, names are defined with the standard NumPy default of \"f%i\", yielding names like f0, f1 and so forth: >>> data = StringIO(\"1 2 3\\n 4 5 6\")\n>>> np.genfromtxt(data, dtype=(int, float, int))\narray([(1, 2.0, 3), (4, 5.0, 6)],\n      dtype=[('f0', '<i8'), ('f1', '<f8'), ('f2', '<i8')])\n In the same way, if we don\u2019t give enough names to match the length of the dtype, the missing names will be defined with this default template: >>> data = StringIO(\"1 2 3\\n 4 5 6\")\n>>> np.genfromtxt(data, dtype=(int, float, int), names=\"a\")\narray([(1, 2.0, 3), (4, 5.0, 6)],\n      dtype=[('a', '<i8'), ('f0', '<f8'), ('f1', '<i8')])\n We can overwrite this default with the defaultfmt argument, that takes any format string: >>> data = StringIO(\"1 2 3\\n 4 5 6\")\n>>> np.genfromtxt(data, dtype=(int, float, int), defaultfmt=\"var_%02i\")\narray([(1, 2.0, 3), (4, 5.0, 6)],\n      dtype=[('var_00', '<i8'), ('var_01', '<f8'), ('var_02', '<i8')])\n  Note We need to keep in mind that defaultfmt is used only if some names are expected but not defined.    Validating names NumPy arrays with a structured dtype can also be viewed as recarray, where a field can be accessed as if it were an attribute. For that reason, we may need to make sure that the field name doesn\u2019t contain any space or invalid character, or that it does not correspond to the name of a standard attribute (like size or shape), which would confuse the interpreter. genfromtxt accepts three optional arguments that provide a finer control on the names:  deletechars\n\nGives a string combining all the characters that must be deleted from the name. By default, invalid characters are ~!@#$%^&*()-=+~\\|]}[{';:\n\/?.>,<.  excludelist\n\nGives a list of the names to exclude, such as return, file, print\u2026 If one of the input name is part of this list, an underscore character ('_') will be appended to it.  case_sensitive\n\nWhether the names should be case-sensitive (case_sensitive=True), converted to upper case (case_sensitive=False or case_sensitive='upper') or to lower case (case_sensitive='lower').      Tweaking the conversion  The converters argument Usually, defining a dtype is sufficient to define how the sequence of strings must be converted. However, some additional control may sometimes be required. For example, we may want to make sure that a date in a format YYYY\/MM\/DD is converted to a datetime object, or that a string like xx% is properly converted to a float between 0 and 1. In such cases, we should define conversion functions with the converters arguments. The value of this argument is typically a dictionary with column indices or column names as keys and a conversion functions as values. These conversion functions can either be actual functions or lambda functions. In any case, they should accept only a string as input and output only a single element of the wanted type. In the following example, the second column is converted from as string representing a percentage to a float between 0 and 1: >>> convertfunc = lambda x: float(x.strip(b\"%\"))\/100.\n>>> data = u\"1, 2.3%, 45.\\n6, 78.9%, 0\"\n>>> names = (\"i\", \"p\", \"n\")\n>>> # General case .....\n>>> np.genfromtxt(StringIO(data), delimiter=\",\", names=names)\narray([(1., nan, 45.), (6., nan, 0.)],\n      dtype=[('i', '<f8'), ('p', '<f8'), ('n', '<f8')])\n We need to keep in mind that by default, dtype=float. A float is therefore expected for the second column. However, the strings ' 2.3%' and ' 78.9%' cannot be converted to float and we end up having np.nan instead. Let\u2019s now use a converter: >>> # Converted case ...\n>>> np.genfromtxt(StringIO(data), delimiter=\",\", names=names,\n...               converters={1: convertfunc})\narray([(1.0, 0.023, 45.0), (6.0, 0.78900000000000003, 0.0)],\n      dtype=[('i', '<f8'), ('p', '<f8'), ('n', '<f8')])\n The same results can be obtained by using the name of the second column (\"p\") as key instead of its index (1): >>> # Using a name for the converter ...\n>>> np.genfromtxt(StringIO(data), delimiter=\",\", names=names,\n...               converters={\"p\": convertfunc})\narray([(1.0, 0.023, 45.0), (6.0, 0.78900000000000003, 0.0)],\n      dtype=[('i', '<f8'), ('p', '<f8'), ('n', '<f8')])\n Converters can also be used to provide a default for missing entries. In the following example, the converter convert transforms a stripped string into the corresponding float or into -999 if the string is empty. We need to explicitly strip the string from white spaces as it is not done by default: >>> data = u\"1, , 3\\n 4, 5, 6\"\n>>> convert = lambda x: float(x.strip() or -999)\n>>> np.genfromtxt(StringIO(data), delimiter=\",\",\n...               converters={1: convert})\narray([[   1., -999.,    3.],\n       [   4.,    5.,    6.]])\n   Using missing and filling values Some entries may be missing in the dataset we are trying to import. In a previous example, we used a converter to transform an empty string into a float. However, user-defined converters may rapidly become cumbersome to manage. The genfromtxt function provides two other complementary mechanisms: the missing_values argument is used to recognize missing data and a second argument, filling_values, is used to process these missing data.   missing_values By default, any empty string is marked as missing. We can also consider more complex strings, such as \"N\/A\" or \"???\" to represent missing or invalid data. The missing_values argument accepts three kinds of values:  a string or a comma-separated string\n\nThis string will be used as the marker for missing data for all the columns  a sequence of strings\n\nIn that case, each item is associated to a column, in order.  a dictionary\n\nValues of the dictionary are strings or sequence of strings. The corresponding keys can be column indices (integers) or column names (strings). In addition, the special key None can be used to define a default applicable to all columns.     filling_values We know how to recognize missing data, but we still need to provide a value for these missing entries. By default, this value is determined from the expected dtype according to this table:   \nExpected type Default   \nbool False  \nint -1  \nfloat np.nan  \ncomplex np.nan+0j  \nstring '???'   We can get a finer control on the conversion of missing values with the filling_values optional argument. Like missing_values, this argument accepts different kind of values:  a single value\n\nThis will be the default for all columns  a sequence of values\n\nEach entry will be the default for the corresponding column  a dictionary\n\nEach key can be a column index or a column name, and the corresponding value should be a single object. We can use the special key None to define a default for all columns.   In the following example, we suppose that the missing values are flagged with \"N\/A\" in the first column and by \"???\" in the third column. We wish to transform these missing values to 0 if they occur in the first and second column, and to -999 if they occur in the last column: >>> data = u\"N\/A, 2, 3\\n4, ,???\"\n>>> kwargs = dict(delimiter=\",\",\n...               dtype=int,\n...               names=\"a,b,c\",\n...               missing_values={0:\"N\/A\", 'b':\" \", 2:\"???\"},\n...               filling_values={0:0, 'b':0, 2:-999})\n>>> np.genfromtxt(StringIO(data), **kwargs)\narray([(0, 2, 3), (4, 0, -999)],\n      dtype=[('a', '<i8'), ('b', '<i8'), ('c', '<i8')])\n   usemask We may also want to keep track of the occurrence of missing data by constructing a boolean mask, with True entries where data was missing and False otherwise. To do that, we just have to set the optional argument usemask to True (the default is False). The output array will then be a MaskedArray.    Shortcut functions In addition to genfromtxt, the numpy.lib.npyio module provides several convenience functions derived from genfromtxt. These functions work the same way as the original, but they have different default values.  recfromtxt\n\nReturns a standard numpy.recarray (if usemask=False) or a MaskedRecords array (if usemaske=True). The default dtype is dtype=None, meaning that the types of each column will be automatically determined.  recfromcsv\n\nLike recfromtxt, but with a default delimiter=\",\".","title":"numpy.user.basics.io.genfromtxt"},{"text":"numpy.loadtxt   numpy.loadtxt(fname, dtype=<class 'float'>, comments='#', delimiter=None, converters=None, skiprows=0, usecols=None, unpack=False, ndmin=0, encoding='bytes', max_rows=None, *, like=None)[source]\n \nLoad data from a text file. Each row in the text file must have the same number of values.  Parameters \n \nfnamefile, str, pathlib.Path, list of str, generator\n\n\nFile, filename, list, or generator to read. If the filename extension is .gz or .bz2, the file is first decompressed. Note that generators must return bytes or strings. The strings in a list or produced by a generator are treated as lines.  \ndtypedata-type, optional\n\n\nData-type of the resulting array; default: float. If this is a structured data-type, the resulting array will be 1-dimensional, and each row will be interpreted as an element of the array. In this case, the number of columns used must match the number of fields in the data-type.  \ncommentsstr or sequence of str, optional\n\n\nThe characters or list of characters used to indicate the start of a comment. None implies no comments. For backwards compatibility, byte strings will be decoded as \u2018latin1\u2019. The default is \u2018#\u2019.  \ndelimiterstr, optional\n\n\nThe string used to separate values. For backwards compatibility, byte strings will be decoded as \u2018latin1\u2019. The default is whitespace.  \nconvertersdict, optional\n\n\nA dictionary mapping column number to a function that will parse the column string into the desired value. E.g., if column 0 is a date string: converters = {0: datestr2num}. Converters can also be used to provide a default value for missing data (but see also genfromtxt): converters = {3: lambda s: float(s.strip() or 0)}. Default: None.  \nskiprowsint, optional\n\n\nSkip the first skiprows lines, including comments; default: 0.  \nusecolsint or sequence, optional\n\n\nWhich columns to read, with 0 being the first. For example, usecols = (1,4,5) will extract the 2nd, 5th and 6th columns. The default, None, results in all columns being read.  Changed in version 1.11.0: When a single column has to be read it is possible to use an integer instead of a tuple. E.g usecols = 3 reads the fourth column the same way as usecols = (3,) would.   \nunpackbool, optional\n\n\nIf True, the returned array is transposed, so that arguments may be unpacked using x, y, z = loadtxt(...). When used with a structured data-type, arrays are returned for each field. Default is False.  \nndminint, optional\n\n\nThe returned array will have at least ndmin dimensions. Otherwise mono-dimensional axes will be squeezed. Legal values: 0 (default), 1 or 2.  New in version 1.6.0.   \nencodingstr, optional\n\n\nEncoding used to decode the inputfile. Does not apply to input streams. The special value \u2018bytes\u2019 enables backward compatibility workarounds that ensures you receive byte arrays as results if possible and passes \u2018latin1\u2019 encoded strings to converters. Override this value to receive unicode arrays and pass strings as input to converters. If set to None the system default is used. The default value is \u2018bytes\u2019.  New in version 1.14.0.   \nmax_rowsint, optional\n\n\nRead max_rows lines of content after skiprows lines. The default is to read all the lines.  New in version 1.16.0.   \nlikearray_like\n\n\nReference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as like supports the __array_function__ protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.  New in version 1.20.0.     Returns \n \noutndarray\n\n\nData read from the text file.      See also  \nload, fromstring, fromregex\n\ngenfromtxt\n\nLoad data with missing values handled as specified.  scipy.io.loadmat\n\nreads MATLAB data files    Notes This function aims to be a fast reader for simply formatted files. The genfromtxt function provides more sophisticated handling of, e.g., lines with missing values.  New in version 1.10.0.  The strings produced by the Python float.hex method can be used as input for floats. Examples >>> from io import StringIO   # StringIO behaves like a file object\n>>> c = StringIO(\"0 1\\n2 3\")\n>>> np.loadtxt(c)\narray([[0., 1.],\n       [2., 3.]])\n >>> d = StringIO(\"M 21 72\\nF 35 58\")\n>>> np.loadtxt(d, dtype={'names': ('gender', 'age', 'weight'),\n...                      'formats': ('S1', 'i4', 'f4')})\narray([(b'M', 21, 72.), (b'F', 35, 58.)],\n      dtype=[('gender', 'S1'), ('age', '<i4'), ('weight', '<f4')])\n >>> c = StringIO(\"1,0,2\\n3,0,4\")\n>>> x, y = np.loadtxt(c, delimiter=',', usecols=(0, 2), unpack=True)\n>>> x\narray([1., 3.])\n>>> y\narray([2., 4.])\n This example shows how converters can be used to convert a field with a trailing minus sign into a negative number. >>> s = StringIO('10.01 31.25-\\n19.22 64.31\\n17.57- 63.94')\n>>> def conv(fld):\n...     return -float(fld[:-1]) if fld.endswith(b'-') else float(fld)\n...\n>>> np.loadtxt(s, converters={0: conv, 1: conv})\narray([[ 10.01, -31.25],\n       [ 19.22,  64.31],\n       [-17.57,  63.94]])","title":"numpy.reference.generated.numpy.loadtxt"},{"text":"numpy.genfromtxt   numpy.genfromtxt(fname, dtype=<class 'float'>, comments='#', delimiter=None, skip_header=0, skip_footer=0, converters=None, missing_values=None, filling_values=None, usecols=None, names=None, excludelist=None, deletechars=\" !#$%&'()*+, -.\/:;<=>?@[\\\\]^{|}~\", replace_space='_', autostrip=False, case_sensitive=True, defaultfmt='f%i', unpack=None, usemask=False, loose=True, invalid_raise=True, max_rows=None, encoding='bytes', *, like=None)[source]\n \nLoad data from a text file, with missing values handled as specified. Each line past the first skip_header lines is split at the delimiter character, and characters following the comments character are discarded.  Parameters \n \nfnamefile, str, pathlib.Path, list of str, generator\n\n\nFile, filename, list, or generator to read. If the filename extension is .gz or .bz2, the file is first decompressed. Note that generators must return bytes or strings. The strings in a list or produced by a generator are treated as lines.  \ndtypedtype, optional\n\n\nData type of the resulting array. If None, the dtypes will be determined by the contents of each column, individually.  \ncommentsstr, optional\n\n\nThe character used to indicate the start of a comment. All the characters occurring on a line after a comment are discarded.  \ndelimiterstr, int, or sequence, optional\n\n\nThe string used to separate values. By default, any consecutive whitespaces act as delimiter. An integer or sequence of integers can also be provided as width(s) of each field.  \nskiprowsint, optional\n\n\nskiprows was removed in numpy 1.10. Please use skip_header instead.  \nskip_headerint, optional\n\n\nThe number of lines to skip at the beginning of the file.  \nskip_footerint, optional\n\n\nThe number of lines to skip at the end of the file.  \nconvertersvariable, optional\n\n\nThe set of functions that convert the data of a column to a value. The converters can also be used to provide a default value for missing data: converters = {3: lambda s: float(s or 0)}.  \nmissingvariable, optional\n\n\nmissing was removed in numpy 1.10. Please use missing_values instead.  \nmissing_valuesvariable, optional\n\n\nThe set of strings corresponding to missing data.  \nfilling_valuesvariable, optional\n\n\nThe set of values to be used as default when the data are missing.  \nusecolssequence, optional\n\n\nWhich columns to read, with 0 being the first. For example, usecols = (1, 4, 5) will extract the 2nd, 5th and 6th columns.  \nnames{None, True, str, sequence}, optional\n\n\nIf names is True, the field names are read from the first line after the first skip_header lines. This line can optionally be preceded by a comment delimiter. If names is a sequence or a single-string of comma-separated names, the names will be used to define the field names in a structured dtype. If names is None, the names of the dtype fields will be used, if any.  \nexcludelistsequence, optional\n\n\nA list of names to exclude. This list is appended to the default list [\u2018return\u2019,\u2019file\u2019,\u2019print\u2019]. Excluded names are appended with an underscore: for example, file would become file_.  \ndeletecharsstr, optional\n\n\nA string combining invalid characters that must be deleted from the names.  \ndefaultfmtstr, optional\n\n\nA format used to define default field names, such as \u201cf%i\u201d or \u201cf_%02i\u201d.  \nautostripbool, optional\n\n\nWhether to automatically strip white spaces from the variables.  \nreplace_spacechar, optional\n\n\nCharacter(s) used in replacement of white spaces in the variable names. By default, use a \u2018_\u2019.  \ncase_sensitive{True, False, \u2018upper\u2019, \u2018lower\u2019}, optional\n\n\nIf True, field names are case sensitive. If False or \u2018upper\u2019, field names are converted to upper case. If \u2018lower\u2019, field names are converted to lower case.  \nunpackbool, optional\n\n\nIf True, the returned array is transposed, so that arguments may be unpacked using x, y, z = genfromtxt(...). When used with a structured data-type, arrays are returned for each field. Default is False.  \nusemaskbool, optional\n\n\nIf True, return a masked array. If False, return a regular array.  \nloosebool, optional\n\n\nIf True, do not raise errors for invalid values.  \ninvalid_raisebool, optional\n\n\nIf True, an exception is raised if an inconsistency is detected in the number of columns. If False, a warning is emitted and the offending lines are skipped.  \nmax_rowsint, optional\n\n\nThe maximum number of rows to read. Must not be used with skip_footer at the same time. If given, the value must be at least 1. Default is to read the entire file.  New in version 1.10.0.   \nencodingstr, optional\n\n\nEncoding used to decode the inputfile. Does not apply when fname is a file object. The special value \u2018bytes\u2019 enables backward compatibility workarounds that ensure that you receive byte arrays when possible and passes latin1 encoded strings to converters. Override this value to receive unicode arrays and pass strings as input to converters. If set to None the system default is used. The default value is \u2018bytes\u2019.  New in version 1.14.0.   \nlikearray_like\n\n\nReference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as like supports the __array_function__ protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.  New in version 1.20.0.     Returns \n \noutndarray\n\n\nData read from the text file. If usemask is True, this is a masked array.      See also  numpy.loadtxt\n\nequivalent function when no data is missing.    Notes  When spaces are used as delimiters, or when no delimiter has been given as input, there should not be any missing data between two fields. When the variables are named (either by a flexible dtype or with names), there must not be any header in the file (else a ValueError exception is raised). Individual values are not stripped of spaces by default. When using a custom converter, make sure the function does remove spaces.  References  1 \nNumPy User Guide, section I\/O with NumPy.   Examples >>> from io import StringIO\n>>> import numpy as np\n Comma delimited file with mixed dtype >>> s = StringIO(u\"1,1.3,abcde\")\n>>> data = np.genfromtxt(s, dtype=[('myint','i8'),('myfloat','f8'),\n... ('mystring','S5')], delimiter=\",\")\n>>> data\narray((1, 1.3, b'abcde'),\n      dtype=[('myint', '<i8'), ('myfloat', '<f8'), ('mystring', 'S5')])\n Using dtype = None >>> _ = s.seek(0) # needed for StringIO example only\n>>> data = np.genfromtxt(s, dtype=None,\n... names = ['myint','myfloat','mystring'], delimiter=\",\")\n>>> data\narray((1, 1.3, b'abcde'),\n      dtype=[('myint', '<i8'), ('myfloat', '<f8'), ('mystring', 'S5')])\n Specifying dtype and names >>> _ = s.seek(0)\n>>> data = np.genfromtxt(s, dtype=\"i8,f8,S5\",\n... names=['myint','myfloat','mystring'], delimiter=\",\")\n>>> data\narray((1, 1.3, b'abcde'),\n      dtype=[('myint', '<i8'), ('myfloat', '<f8'), ('mystring', 'S5')])\n An example with fixed-width columns >>> s = StringIO(u\"11.3abcde\")\n>>> data = np.genfromtxt(s, dtype=None, names=['intvar','fltvar','strvar'],\n...     delimiter=[1,3,5])\n>>> data\narray((1, 1.3, b'abcde'),\n      dtype=[('intvar', '<i8'), ('fltvar', '<f8'), ('strvar', 'S5')])\n An example to show comments >>> f = StringIO('''\n... text,# of chars\n... hello world,11\n... numpy,5''')\n>>> np.genfromtxt(f, dtype='S12,S12', delimiter=',')\narray([(b'text', b''), (b'hello world', b'11'), (b'numpy', b'5')],\n  dtype=[('f0', 'S12'), ('f1', 'S12')])","title":"numpy.reference.generated.numpy.genfromtxt"},{"text":"textwrap.dedent(text)  \nRemove any common leading whitespace from every line in text. This can be used to make triple-quoted strings line up with the left edge of the display, while still presenting them in the source code in indented form. Note that tabs and spaces are both treated as whitespace, but they are not equal: the lines \"\u00a0 hello\" and \"\\thello\" are considered to have no common leading whitespace. Lines containing only whitespace are ignored in the input and normalized to a single newline character in the output. For example: def test():\n    # end first line with \\ to avoid the empty line!\n    s = '''\\\n    hello\n      world\n    '''\n    print(repr(s))          # prints '    hello\\n      world\\n    '\n    print(repr(dedent(s)))  # prints 'hello\\n  world\\n'","title":"python.library.textwrap#textwrap.dedent"},{"text":"tf.raw_ops.TextLineDataset Creates a dataset that emits the lines of one or more text files.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.raw_ops.TextLineDataset  \ntf.raw_ops.TextLineDataset(\n    filenames, compression_type, buffer_size, name=None\n)\n\n \n\n\n Args\n  filenames   A Tensor of type string. A scalar or a vector containing the name(s) of the file(s) to be read.  \n  compression_type   A Tensor of type string. A scalar containing either (i) the empty string (no compression), (ii) \"ZLIB\", or (iii) \"GZIP\".  \n  buffer_size   A Tensor of type int64. A scalar containing the number of bytes to buffer.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor of type variant.","title":"tensorflow.raw_ops.textlinedataset"},{"text":"tf.raw_ops.InitializeTableFromTextFile Initializes a table from a text file.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.raw_ops.InitializeTableFromTextFile  \ntf.raw_ops.InitializeTableFromTextFile(\n    table_handle, filename, key_index, value_index, vocab_size=-1,\n    delimiter='\\t', name=None\n)\n It inserts one key-value pair into the table for each line of the file. The key and value is extracted from the whole line content, elements from the split line based on delimiter or the line number (starting from zero). Where to extract the key and value from a line is specified by key_index and value_index.  A value of -1 means use the line number(starting from zero), expects int64. A value of -2 means use the whole line content, expects string. A value >= 0 means use the index (starting at zero) of the split line based on delimiter. \n \n\n\n Args\n  table_handle   A Tensor of type mutable string. Handle to a table which will be initialized.  \n  filename   A Tensor of type string. Filename of a vocabulary text file.  \n  key_index   An int that is >= -2. Column index in a line to get the table key values from.  \n  value_index   An int that is >= -2. Column index that represents information of a line to get the table value values from.  \n  vocab_size   An optional int that is >= -1. Defaults to -1. Number of elements of the file, use -1 if unknown.  \n  delimiter   An optional string. Defaults to \"\\t\". Delimiter to separate fields in a line.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   The created Operation.","title":"tensorflow.raw_ops.initializetablefromtextfile"},{"text":"tf.raw_ops.InitializeTableFromTextFileV2 Initializes a table from a text file.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.raw_ops.InitializeTableFromTextFileV2  \ntf.raw_ops.InitializeTableFromTextFileV2(\n    table_handle, filename, key_index, value_index, vocab_size=-1,\n    delimiter='\\t', name=None\n)\n It inserts one key-value pair into the table for each line of the file. The key and value is extracted from the whole line content, elements from the split line based on delimiter or the line number (starting from zero). Where to extract the key and value from a line is specified by key_index and value_index.  A value of -1 means use the line number(starting from zero), expects int64. A value of -2 means use the whole line content, expects string. A value >= 0 means use the index (starting at zero) of the split line based on delimiter. \n \n\n\n Args\n  table_handle   A Tensor of type resource. Handle to a table which will be initialized.  \n  filename   A Tensor of type string. Filename of a vocabulary text file.  \n  key_index   An int that is >= -2. Column index in a line to get the table key values from.  \n  value_index   An int that is >= -2. Column index that represents information of a line to get the table value values from.  \n  vocab_size   An optional int that is >= -1. Defaults to -1. Number of elements of the file, use -1 if unknown.  \n  delimiter   An optional string. Defaults to \"\\t\". Delimiter to separate fields in a line.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   The created Operation.","title":"tensorflow.raw_ops.initializetablefromtextfilev2"},{"text":"tf.raw_ops.TextLineReaderV2 A Reader that outputs the lines of a file delimited by '\\n'.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.raw_ops.TextLineReaderV2  \ntf.raw_ops.TextLineReaderV2(\n    skip_header_lines=0, container='', shared_name='', name=None\n)\n\n \n\n\n Args\n  skip_header_lines   An optional int. Defaults to 0. Number of lines to skip from the beginning of every file.  \n  container   An optional string. Defaults to \"\". If non-empty, this reader is placed in the given container. Otherwise, a default container is used.  \n  shared_name   An optional string. Defaults to \"\". If non-empty, this reader is named in the given bucket with this shared_name. Otherwise, the node name is used instead.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor of type resource.","title":"tensorflow.raw_ops.textlinereaderv2"},{"text":"tf.compat.v1.TextLineReader A Reader that outputs the lines of a file delimited by newlines. Inherits From: ReaderBase \ntf.compat.v1.TextLineReader(\n    skip_header_lines=None, name=None\n)\n Newlines are stripped from the output. See ReaderBase for supported methods.\n \n\n\n Args\n  skip_header_lines   An optional int. Defaults to 0. Number of lines to skip from the beginning of every file.  \n  name   A name for the operation (optional).    Eager Compatibility Readers are not compatible with eager execution. Instead, please use tf.data to get data into your model.\n \n\n\n Attributes\n  reader_ref   Op that implements the reader.  \n  supports_serialize   Whether the Reader implementation can serialize its state.    Methods num_records_produced View source \nnum_records_produced(\n    name=None\n)\n Returns the number of records this reader has produced. This is the same as the number of Read executions that have succeeded.\n \n\n\n Args\n  name   A name for the operation (optional).   \n \n\n\n Returns   An int64 Tensor.  \n num_work_units_completed View source \nnum_work_units_completed(\n    name=None\n)\n Returns the number of work units this reader has finished processing.\n \n\n\n Args\n  name   A name for the operation (optional).   \n \n\n\n Returns   An int64 Tensor.  \n read View source \nread(\n    queue, name=None\n)\n Returns the next record (key, value) pair produced by a reader. Will dequeue a work unit from queue if necessary (e.g. when the Reader needs to start reading from a new file since it has finished with the previous file).\n \n\n\n Args\n  queue   A Queue or a mutable string Tensor representing a handle to a Queue, with string work items.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A tuple of Tensors (key, value).     key   A string scalar Tensor.  \n  value   A string scalar Tensor.    read_up_to View source \nread_up_to(\n    queue, num_records, name=None\n)\n Returns up to num_records (key, value) pairs produced by a reader. Will dequeue a work unit from queue if necessary (e.g., when the Reader needs to start reading from a new file since it has finished with the previous file). It may return less than num_records even before the last batch.\n \n\n\n Args\n  queue   A Queue or a mutable string Tensor representing a handle to a Queue, with string work items.  \n  num_records   Number of records to read.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A tuple of Tensors (keys, values).     keys   A 1-D string Tensor.  \n  values   A 1-D string Tensor.    reset View source \nreset(\n    name=None\n)\n Restore a reader to its initial clean state.\n \n\n\n Args\n  name   A name for the operation (optional).   \n \n\n\n Returns   The created Operation.  \n restore_state View source \nrestore_state(\n    state, name=None\n)\n Restore a reader to a previously saved state. Not all Readers support being restored, so this can produce an Unimplemented error.\n \n\n\n Args\n  state   A string Tensor. Result of a SerializeState of a Reader with matching type.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   The created Operation.  \n serialize_state View source \nserialize_state(\n    name=None\n)\n Produce a string tensor that encodes the state of a reader. Not all Readers support being serialized, so this can produce an Unimplemented error.\n \n\n\n Args\n  name   A name for the operation (optional).   \n \n\n\n Returns   A string Tensor.","title":"tensorflow.compat.v1.textlinereader"},{"text":"class typing.Text  \nText is an alias for str. It is provided to supply a forward compatible path for Python 2 code: in Python 2, Text is an alias for unicode. Use Text to indicate that a value must contain a unicode string in a manner that is compatible with both Python 2 and Python 3: def add_unicode_checkmark(text: Text) -> Text:\n    return text + u' \\u2713'\n  New in version 3.5.2.","title":"python.library.typing#typing.Text"}]}
{"task_id":9035479,"prompt":"def f_9035479(my_object, my_str):\n\treturn ","suffix":"","canonical_solution":"getattr(my_object, my_str)","test_start":"\ndef check(candidate):","test":["\n    class Student:\n        id = 9\n        name = \"abc\"\n        grade = 97.08\n\n    s = Student()\n    \n    assert candidate(s, \"name\") == \"abc\"\n","\n    class Student:\n        id = 9\n        name = \"abc\"\n        grade = 97.08\n\n    s = Student()\n    \n    assert (candidate(s, \"grade\") - 97.08) < 1e-6\n","\n    class Student:\n        id = 9\n        name = \"abc\"\n        grade = 97.08\n\n    s = Student()\n    \n    assert (candidate(s, \"grade\") - 97.07) > 1e-6\n","\n    class Student:\n        id = 9\n        name = \"abc\"\n        grade = 97.08\n\n    s = Student()\n    \n    assert candidate(s, \"id\") == 9\n"],"entry_point":"f_9035479","intent":"Get attribute `my_str` of object `my_object`","library":[],"docs":[]}
{"task_id":5558418,"prompt":"def f_5558418(LD):\n\treturn ","suffix":"","canonical_solution":"dict(zip(LD[0], zip(*[list(d.values()) for d in LD])))","test_start":"\nimport collections\n\ndef check(candidate):","test":["\n    employees = [{'name' : 'apple', 'id': 60}, {'name' : 'orange', 'id': 65}]\n    exp_result = {'name': ('apple', 'orange'), 'id': (60, 65)}\n    actual_result = candidate(employees)\n    for key in actual_result:\n        assert collections.Counter(list(exp_result[key])) == collections.Counter(list(actual_result[key]))\n"],"entry_point":"f_5558418","intent":"group a list of dicts `LD` into one dict by key","library":["collections"],"docs":[]}
{"task_id":638048,"prompt":"def f_638048(list_of_pairs):\n\treturn ","suffix":"","canonical_solution":"sum([pair[0] for pair in list_of_pairs])","test_start":"\ndef check(candidate):","test":["\n    assert candidate([(5, 9), (-1, -2), (4, 2)]) == 8\n"],"entry_point":"f_638048","intent":"sum the first value in each tuple in a list of tuples `list_of_pairs` in python","library":[],"docs":[]}
{"task_id":14950260,"prompt":"def f_14950260():\n\treturn ","suffix":"","canonical_solution":"ast.literal_eval(\"{'code1':1,'code2':1}\")","test_start":"\nimport ast\n\ndef check(candidate):","test":["\n    d = candidate()\n    exp_result = {'code1' : 1, 'code2': 1}\n    for key in d:\n        if key not in exp_result:\n            assert False\n        else:\n            assert d[key] == exp_result[key]\n"],"entry_point":"f_14950260","intent":"convert unicode string u\"{'code1':1,'code2':1}\" into dictionary","library":["ast"],"docs":[{"text":"html.entities.name2codepoint  \nA dictionary that maps HTML entity names to the Unicode code points.","title":"python.library.html.entities#html.entities.name2codepoint"},{"text":"html.entities.codepoint2name  \nA dictionary that maps Unicode code points to HTML entity names.","title":"python.library.html.entities#html.entities.codepoint2name"},{"text":"NUM_VERTICES_FOR_CODE={0: 1, 1: 1, 2: 1, 3: 2, 4: 3, 79: 1}\n \nA dictionary mapping Path codes to the number of vertices that the code expects.","title":"matplotlib.path_api#matplotlib.path.Path.NUM_VERTICES_FOR_CODE"},{"text":"html.entities.html5  \nA dictionary that maps HTML5 named character references 1 to the equivalent Unicode character(s), e.g. html5['gt;'] == '>'. Note that the trailing semicolon is included in the name (e.g. 'gt;'), however some of the names are accepted by the standard even without the semicolon: in this case the name is present with and without the ';'. See also html.unescape().  New in version 3.3.","title":"python.library.html.entities#html.entities.html5"},{"text":"get_glyphs_mathtext(prop, s, glyph_map=None, return_new_glyphs_only=False)[source]\n \nParse mathtext string s and convert it to a (vertices, codes) pair.","title":"matplotlib.textpath_api#matplotlib.textpath.TextToPath.get_glyphs_mathtext"},{"text":"class ast.Dict(keys, values)  \nA dictionary. keys and values hold lists of nodes representing the keys and the values respectively, in matching order (what would be returned when calling dictionary.keys() and dictionary.values()). When doing dictionary unpacking using dictionary literals the expression to be expanded goes in the values list, with a None at the corresponding position in keys. >>> print(ast.dump(ast.parse('{\"a\":1, **d}', mode='eval'), indent=4))\nExpression(\n    body=Dict(\n        keys=[\n            Constant(value='a'),\n            None],\n        values=[\n            Constant(value=1),\n            Name(id='d', ctx=Load())]))","title":"python.library.ast#ast.Dict"},{"text":"raw_decode(s)  \nDecode a JSON document from s (a str beginning with a JSON document) and return a 2-tuple of the Python representation and the index in s where the document ended. This can be used to decode a JSON document from a string that may have extraneous data at the end.","title":"python.library.json#json.JSONDecoder.raw_decode"},{"text":"encode  \ndecode  \nThe stateless encoding and decoding functions. These must be functions or methods which have the same interface as the encode() and decode() methods of Codec instances (see Codec Interface). The functions or methods are expected to work in a stateless mode.","title":"python.library.codecs#codecs.CodecInfo.decode"},{"text":"set_verts_and_codes(verts, codes)[source]\n \nSet 3D vertices with path codes.","title":"matplotlib._as_gen.mpl_toolkits.mplot3d.art3d.poly3dcollection#mpl_toolkits.mplot3d.art3d.Poly3DCollection.set_verts_and_codes"},{"text":"decode(s)  \nReturn the Python representation of s (a str instance containing a JSON document). JSONDecodeError will be raised if the given JSON document is not valid.","title":"python.library.json#json.JSONDecoder.decode"}]}
{"task_id":11416772,"prompt":"def f_11416772(mystring):\n\treturn ","suffix":"","canonical_solution":"[word for word in mystring.split() if word.startswith('$')]","test_start":"\ndef check(candidate):","test":["\n    str = \"$abc def $efg $hij klm $\"\n    exp_result = ['$abc', '$efg', '$hij', '$']\n    assert sorted(candidate(str)) == sorted(exp_result)\n"],"entry_point":"f_11416772","intent":"find all words in a string `mystring` that start with the `$` sign","library":[],"docs":[]}
{"task_id":11331982,"prompt":"def f_11331982(text):\n\t","suffix":"\n\treturn text","canonical_solution":"text = re.sub('^https?:\\\\\/\\\\\/.*[\\\\r\\\\n]*', '', text, flags=re.MULTILINE)","test_start":"\nimport re\n\ndef check(candidate):","test":["\n    assert candidate(\"https:\/\/www.wikipedia.org\/ click at\") == \"\"\n"],"entry_point":"f_11331982","intent":"remove any url within string `text`","library":["re"],"docs":[{"text":"werkzeug.urls.url_fix(s, charset='utf-8')  \nSometimes you get an URL by a user that just isn\u2019t a real URL because it contains unsafe characters like \u2018 \u2018 and so on. This function can fix some of the problems in a similar way browsers handle data entered by the user: >>> url_fix('http:\/\/de.wikipedia.org\/wiki\/Elf (Begriffskl\\xe4rung)')\n'http:\/\/de.wikipedia.org\/wiki\/Elf%20(Begriffskl%C3%A4rung)'\n  Parameters \n \ns (str) \u2013 the string with the URL to fix. \ncharset (str) \u2013 The target charset for the URL if the url was given as a string.   Return type \nstr","title":"werkzeug.urls.index#werkzeug.urls.url_fix"},{"text":"urllib.parse.unwrap(url)  \nExtract the url from a wrapped URL (that is, a string formatted as <URL:scheme:\/\/host\/path>, <scheme:\/\/host\/path>, URL:scheme:\/\/host\/path or scheme:\/\/host\/path). If url is not a wrapped URL, it is returned without changes.","title":"python.library.urllib.parse#urllib.parse.unwrap"},{"text":"urllib.parse.unquote_plus(string, encoding='utf-8', errors='replace')  \nLike unquote(), but also replace plus signs with spaces, as required for unquoting HTML form values. string must be a str. Example: unquote_plus('\/El+Ni%C3%B1o\/') yields '\/El Ni\u00f1o\/'.","title":"python.library.urllib.parse#urllib.parse.unquote_plus"},{"text":"set_urls(urls)[source]\n \n Parameters \n \nurlslist of str or None\n\n   Notes URLs are currently only implemented by the SVG backend. They are ignored by all other backends.","title":"matplotlib.collections_api#matplotlib.collections.LineCollection.set_urls"},{"text":"set_urls(urls)[source]\n \n Parameters \n \nurlslist of str or None\n\n   Notes URLs are currently only implemented by the SVG backend. They are ignored by all other backends.","title":"matplotlib.collections_api#matplotlib.collections.StarPolygonCollection.set_urls"},{"text":"set_urls(urls)[source]\n \n Parameters \n \nurlslist of str or None\n\n   Notes URLs are currently only implemented by the SVG backend. They are ignored by all other backends.","title":"matplotlib.collections_api#matplotlib.collections.RegularPolyCollection.set_urls"},{"text":"set_urls(urls)[source]\n \n Parameters \n \nurlslist of str or None\n\n   Notes URLs are currently only implemented by the SVG backend. They are ignored by all other backends.","title":"matplotlib.collections_api#matplotlib.collections.AsteriskPolygonCollection.set_urls"},{"text":"set_urls(urls)[source]\n \n Parameters \n \nurlslist of str or None\n\n   Notes URLs are currently only implemented by the SVG backend. They are ignored by all other backends.","title":"matplotlib.collections_api#matplotlib.collections.PolyCollection.set_urls"},{"text":"set_urls(urls)[source]\n \n Parameters \n \nurlslist of str or None\n\n   Notes URLs are currently only implemented by the SVG backend. They are ignored by all other backends.","title":"matplotlib.collections_api#matplotlib.collections.PathCollection.set_urls"},{"text":"class skimage.viewer.widgets.Text(name=None, text='') [source]\n \nBases: skimage.viewer.widgets.core.BaseWidget  \n__init__(name=None, text='') [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nproperty text","title":"skimage.api.skimage.viewer.widgets#skimage.viewer.widgets.Text"}]}
{"task_id":34945274,"prompt":"def f_34945274(A):\n\treturn ","suffix":"","canonical_solution":"np.where(np.in1d(A, [1, 3, 4]).reshape(A.shape), A, 0)","test_start":"\nimport numpy as np\n\ndef check(candidate):","test":["\n    A = np.array([[6, 8, 1, 3, 4], [-1, 0, 3, 5, 1]])\n    B = np.array([[0, 0, 1, 3, 4], [0, 0, 3, 0, 1]])\n    assert np.array_equal(candidate(A), B)\n"],"entry_point":"f_34945274","intent":"replace all elements in array `A` that are not present in array `[1, 3, 4]` with zeros","library":["numpy"],"docs":[{"text":"numpy.ma.zeros_like   ma.zeros_like(*args, **kwargs) = <numpy.ma.core._convert2ma object>\n \nReturn an array of zeros with the same shape and type as a given array.  Parameters \n \naarray_like\n\n\nThe shape and data-type of a define these same attributes of the returned array.  \ndtypedata-type, optional\n\n\nOverrides the data type of the result.  New in version 1.6.0.   \norder{\u2018C\u2019, \u2018F\u2019, \u2018A\u2019, or \u2018K\u2019}, optional\n\n\nOverrides the memory layout of the result. \u2018C\u2019 means C-order, \u2018F\u2019 means F-order, \u2018A\u2019 means \u2018F\u2019 if a is Fortran contiguous, \u2018C\u2019 otherwise. \u2018K\u2019 means match the layout of a as closely as possible.  New in version 1.6.0.   \nsubokbool, optional.\n\n\nIf True, then the newly created array will use the sub-class type of a, otherwise it will be a base-class array. Defaults to True.  \nshapeint or sequence of ints, optional.\n\n\nOverrides the shape of the result. If order=\u2019K\u2019 and the number of dimensions is unchanged, will try to keep order, otherwise, order=\u2019C\u2019 is implied.  New in version 1.17.0.     Returns \n \noutMaskedArray\n\n\nArray of zeros with the same shape and type as a.      See also  empty_like\n\nReturn an empty array with shape and type of input.  ones_like\n\nReturn an array of ones with shape and type of input.  full_like\n\nReturn a new array with shape of input filled with value.  zeros\n\nReturn a new array setting values to zero.    Examples >>> x = np.arange(6)\n>>> x = x.reshape((2, 3))\n>>> x\narray([[0, 1, 2],\n       [3, 4, 5]])\n>>> np.zeros_like(x)\narray([[0, 0, 0],\n       [0, 0, 0]])\n >>> y = np.arange(3, dtype=float)\n>>> y\narray([0., 1., 2.])\n>>> np.zeros_like(y)\narray([0.,  0.,  0.])","title":"numpy.reference.generated.numpy.ma.zeros_like"},{"text":"numpy.zeros_like   numpy.zeros_like(a, dtype=None, order='K', subok=True, shape=None)[source]\n \nReturn an array of zeros with the same shape and type as a given array.  Parameters \n \naarray_like\n\n\nThe shape and data-type of a define these same attributes of the returned array.  \ndtypedata-type, optional\n\n\nOverrides the data type of the result.  New in version 1.6.0.   \norder{\u2018C\u2019, \u2018F\u2019, \u2018A\u2019, or \u2018K\u2019}, optional\n\n\nOverrides the memory layout of the result. \u2018C\u2019 means C-order, \u2018F\u2019 means F-order, \u2018A\u2019 means \u2018F\u2019 if a is Fortran contiguous, \u2018C\u2019 otherwise. \u2018K\u2019 means match the layout of a as closely as possible.  New in version 1.6.0.   \nsubokbool, optional.\n\n\nIf True, then the newly created array will use the sub-class type of a, otherwise it will be a base-class array. Defaults to True.  \nshapeint or sequence of ints, optional.\n\n\nOverrides the shape of the result. If order=\u2019K\u2019 and the number of dimensions is unchanged, will try to keep order, otherwise, order=\u2019C\u2019 is implied.  New in version 1.17.0.     Returns \n \noutndarray\n\n\nArray of zeros with the same shape and type as a.      See also  empty_like\n\nReturn an empty array with shape and type of input.  ones_like\n\nReturn an array of ones with shape and type of input.  full_like\n\nReturn a new array with shape of input filled with value.  zeros\n\nReturn a new array setting values to zero.    Examples >>> x = np.arange(6)\n>>> x = x.reshape((2, 3))\n>>> x\narray([[0, 1, 2],\n       [3, 4, 5]])\n>>> np.zeros_like(x)\narray([[0, 0, 0],\n       [0, 0, 0]])\n >>> y = np.arange(3, dtype=float)\n>>> y\narray([0., 1., 2.])\n>>> np.zeros_like(y)\narray([0.,  0.,  0.])","title":"numpy.reference.generated.numpy.zeros_like"},{"text":"tf.experimental.numpy.zeros_like TensorFlow variant of NumPy's zeros_like. \ntf.experimental.numpy.zeros_like(\n    a, dtype=None\n)\n Unsupported arguments: order, subok, shape. See the NumPy documentation for numpy.zeros_like.","title":"tensorflow.experimental.numpy.zeros_like"},{"text":"tf.sparse.fill_empty_rows     View source on GitHub    Fills empty rows in the input 2-D SparseTensor with a default value.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.sparse.fill_empty_rows, tf.compat.v1.sparse_fill_empty_rows  \ntf.sparse.fill_empty_rows(\n    sp_input, default_value, name=None\n)\n This op adds entries with the specified default_value at index [row, 0] for any row in the input that does not already have a value. For example, suppose sp_input has shape [5, 6] and non-empty values: [0, 1]: a\n[0, 3]: b\n[2, 0]: c\n[3, 1]: d\n Rows 1 and 4 are empty, so the output will be of shape [5, 6] with values: [0, 1]: a\n[0, 3]: b\n[1, 0]: default_value\n[2, 0]: c\n[3, 1]: d\n[4, 0]: default_value\n Note that the input may have empty columns at the end, with no effect on this op. The output SparseTensor will be in row-major order and will have the same shape as the input. This op also returns an indicator vector such that empty_row_indicator[i] = True iff row i was an empty row.\n\n \n\n\n Args\n  sp_input   A SparseTensor with shape [N, M].  \n  default_value   The value to fill for empty rows, with the same type as sp_input.  \n  name   A name prefix for the returned tensors (optional)   \n \n\n\n Returns\n  sp_ordered_output   A SparseTensor with shape [N, M], and with all empty rows filled in with default_value.  \n  empty_row_indicator   A bool vector of length N indicating whether each input row was empty.   \n \n\n\n Raises\n  TypeError   If sp_input is not a SparseTensor.","title":"tensorflow.sparse.fill_empty_rows"},{"text":"set_array(A)[source]\n \nSet the value array from array-like A.  Parameters \n \nAarray-like or None\n\n\nThe values that are mapped to colors. The base class ScalarMappable does not make any assumptions on the dimensionality and shape of the value array A.","title":"matplotlib.cm_api#matplotlib.cm.ScalarMappable.set_array"},{"text":"numpy.copy   numpy.copy(a, order='K', subok=False)[source]\n \nReturn an array copy of the given object.  Parameters \n \naarray_like\n\n\nInput data.  \norder{\u2018C\u2019, \u2018F\u2019, \u2018A\u2019, \u2018K\u2019}, optional\n\n\nControls the memory layout of the copy. \u2018C\u2019 means C-order, \u2018F\u2019 means F-order, \u2018A\u2019 means \u2018F\u2019 if a is Fortran contiguous, \u2018C\u2019 otherwise. \u2018K\u2019 means match the layout of a as closely as possible. (Note that this function and ndarray.copy are very similar, but have different default values for their order= arguments.)  \nsubokbool, optional\n\n\nIf True, then sub-classes will be passed-through, otherwise the returned array will be forced to be a base-class array (defaults to False).  New in version 1.19.0.     Returns \n \narrndarray\n\n\nArray interpretation of a.      See also  ndarray.copy\n\nPreferred method for creating an array copy    Notes This is equivalent to: >>> np.array(a, copy=True)  \n Examples Create an array x, with a reference y and a copy z: >>> x = np.array([1, 2, 3])\n>>> y = x\n>>> z = np.copy(x)\n Note that, when we modify x, y changes, but not z: >>> x[0] = 10\n>>> x[0] == y[0]\nTrue\n>>> x[0] == z[0]\nFalse\n Note that, np.copy clears previously set WRITEABLE=False flag. >>> a = np.array([1, 2, 3])\n>>> a.flags[\"WRITEABLE\"] = False\n>>> b = np.copy(a)\n>>> b.flags[\"WRITEABLE\"]\nTrue\n>>> b[0] = 3\n>>> b\narray([3, 2, 3])\n Note that np.copy is a shallow copy and will not copy object elements within arrays. This is mainly important for arrays containing Python objects. The new array will contain the same object which may lead to surprises if that object can be modified (is mutable): >>> a = np.array([1, 'm', [2, 3, 4]], dtype=object)\n>>> b = np.copy(a)\n>>> b[2][0] = 10\n>>> a\narray([1, 'm', list([10, 3, 4])], dtype=object)\n To ensure all elements within an object array are copied, use copy.deepcopy: >>> import copy\n>>> a = np.array([1, 'm', [2, 3, 4]], dtype=object)\n>>> c = copy.deepcopy(a)\n>>> c[2][0] = 10\n>>> c\narray([1, 'm', list([10, 3, 4])], dtype=object)\n>>> a\narray([1, 'm', list([2, 3, 4])], dtype=object)","title":"numpy.reference.generated.numpy.copy"},{"text":"set_array(A)[source]\n \nSet the value array from array-like A.  Parameters \n \nAarray-like or None\n\n\nThe values that are mapped to colors. The base class ScalarMappable does not make any assumptions on the dimensionality and shape of the value array A.","title":"matplotlib.collections_api#matplotlib.collections.RegularPolyCollection.set_array"},{"text":"autoscale(A)[source]\n \nSet vmin, vmax to min, max of A.","title":"matplotlib._as_gen.matplotlib.colors.normalize#matplotlib.colors.Normalize.autoscale"},{"text":"set_array(A)[source]\n \nSet the value array from array-like A.  Parameters \n \nAarray-like or None\n\n\nThe values that are mapped to colors. The base class ScalarMappable does not make any assumptions on the dimensionality and shape of the value array A.","title":"matplotlib.collections_api#matplotlib.collections.PolyCollection.set_array"},{"text":"numpy.char.zfill   char.zfill(a, width)[source]\n \nReturn the numeric string left-filled with zeros Calls str.zfill element-wise.  Parameters \n \naarray_like, {str, unicode}\n\n\nInput array.  \nwidthint\n\n\nWidth of string to left-fill elements in a.    Returns \n \noutndarray, {str, unicode}\n\n\nOutput array of str or unicode, depending on input type      See also  str.zfill","title":"numpy.reference.generated.numpy.char.zfill"}]}
{"task_id":15819980,"prompt":"def f_15819980(a):\n\treturn ","suffix":"","canonical_solution":"np.mean(a, axis=1)","test_start":"\nimport numpy as np\n\ndef check(candidate):","test":["\n    A = np.array([[6, 8, 1, 3, 4], [-1, 0, 3, 5, 1]])\n    B = np.array([4.4, 1.6])\n    assert np.array_equal(candidate(A), B)\n"],"entry_point":"f_15819980","intent":"calculate mean across dimension in a 2d array `a`","library":["numpy"],"docs":[{"text":"numpy.matrix.mean method   matrix.mean(axis=None, dtype=None, out=None)[source]\n \nReturns the average of the matrix elements along the given axis. Refer to numpy.mean for full documentation.  See also  numpy.mean\n  Notes Same as ndarray.mean except that, where that returns an ndarray, this returns a matrix object. Examples >>> x = np.matrix(np.arange(12).reshape((3, 4)))\n>>> x\nmatrix([[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]])\n>>> x.mean()\n5.5\n>>> x.mean(0)\nmatrix([[4., 5., 6., 7.]])\n>>> x.mean(1)\nmatrix([[ 1.5],\n        [ 5.5],\n        [ 9.5]])","title":"numpy.reference.generated.numpy.matrix.mean"},{"text":"numpy.ndarray.mean method   ndarray.mean(axis=None, dtype=None, out=None, keepdims=False, *, where=True)\n \nReturns the average of the array elements along given axis. Refer to numpy.mean for full documentation.  See also  numpy.mean\n\nequivalent function","title":"numpy.reference.generated.numpy.ndarray.mean"},{"text":"tf.experimental.numpy.mean TensorFlow variant of NumPy's mean. \ntf.experimental.numpy.mean(\n    a, axis=None, dtype=None, keepdims=None\n)\n Unsupported arguments: out. See the NumPy documentation for numpy.mean.","title":"tensorflow.experimental.numpy.mean"},{"text":"numpy.recarray.mean method   recarray.mean(axis=None, dtype=None, out=None, keepdims=False, *, where=True)\n \nReturns the average of the array elements along given axis. Refer to numpy.mean for full documentation.  See also  numpy.mean\n\nequivalent function","title":"numpy.reference.generated.numpy.recarray.mean"},{"text":"numpy.mean   numpy.mean(a, axis=None, dtype=None, out=None, keepdims=<no value>, *, where=<no value>)[source]\n \nCompute the arithmetic mean along the specified axis. Returns the average of the array elements. The average is taken over the flattened array by default, otherwise over the specified axis. float64 intermediate and return values are used for integer inputs.  Parameters \n \naarray_like\n\n\nArray containing numbers whose mean is desired. If a is not an array, a conversion is attempted.  \naxisNone or int or tuple of ints, optional\n\n\nAxis or axes along which the means are computed. The default is to compute the mean of the flattened array.  New in version 1.7.0.  If this is a tuple of ints, a mean is performed over multiple axes, instead of a single axis or all the axes as before.  \ndtypedata-type, optional\n\n\nType to use in computing the mean. For integer inputs, the default is float64; for floating point inputs, it is the same as the input dtype.  \noutndarray, optional\n\n\nAlternate output array in which to place the result. The default is None; if provided, it must have the same shape as the expected output, but the type will be cast if necessary. See Output type determination for more details.  \nkeepdimsbool, optional\n\n\nIf this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array. If the default value is passed, then keepdims will not be passed through to the mean method of sub-classes of ndarray, however any non-default value will be. If the sub-class\u2019 method does not implement keepdims any exceptions will be raised.  \nwherearray_like of bool, optional\n\n\nElements to include in the mean. See reduce for details.  New in version 1.20.0.     Returns \n \nmndarray, see dtype parameter above\n\n\nIf out=None, returns a new array containing the mean values, otherwise a reference to the output array is returned.      See also  average\n\nWeighted average  \nstd, var, nanmean, nanstd, nanvar\n\n  Notes The arithmetic mean is the sum of the elements along the axis divided by the number of elements. Note that for floating-point input, the mean is computed using the same precision the input has. Depending on the input data, this can cause the results to be inaccurate, especially for float32 (see example below). Specifying a higher-precision accumulator using the dtype keyword can alleviate this issue. By default, float16 results are computed using float32 intermediates for extra precision. Examples >>> a = np.array([[1, 2], [3, 4]])\n>>> np.mean(a)\n2.5\n>>> np.mean(a, axis=0)\narray([2., 3.])\n>>> np.mean(a, axis=1)\narray([1.5, 3.5])\n In single precision, mean can be inaccurate: >>> a = np.zeros((2, 512*512), dtype=np.float32)\n>>> a[0, :] = 1.0\n>>> a[1, :] = 0.1\n>>> np.mean(a)\n0.54999924\n Computing the mean in float64 is more accurate: >>> np.mean(a, dtype=np.float64)\n0.55000000074505806 # may vary\n Specifying a where argument: >>> a = np.array([[5, 9, 13], [14, 10, 12], [11, 15, 19]]) >>> np.mean(a) 12.0 >>> np.mean(a, where=[[True], [False], [False]]) 9.0","title":"numpy.reference.generated.numpy.mean"},{"text":"tf.experimental.numpy.average TensorFlow variant of NumPy's average. \ntf.experimental.numpy.average(\n    a, axis=None, weights=None, returned=False\n)\n See the NumPy documentation for numpy.average.","title":"tensorflow.experimental.numpy.average"},{"text":"numpy.nanmean   numpy.nanmean(a, axis=None, dtype=None, out=None, keepdims=<no value>, *, where=<no value>)[source]\n \nCompute the arithmetic mean along the specified axis, ignoring NaNs. Returns the average of the array elements. The average is taken over the flattened array by default, otherwise over the specified axis. float64 intermediate and return values are used for integer inputs. For all-NaN slices, NaN is returned and a RuntimeWarning is raised.  New in version 1.8.0.   Parameters \n \naarray_like\n\n\nArray containing numbers whose mean is desired. If a is not an array, a conversion is attempted.  \naxis{int, tuple of int, None}, optional\n\n\nAxis or axes along which the means are computed. The default is to compute the mean of the flattened array.  \ndtypedata-type, optional\n\n\nType to use in computing the mean. For integer inputs, the default is float64; for inexact inputs, it is the same as the input dtype.  \noutndarray, optional\n\n\nAlternate output array in which to place the result. The default is None; if provided, it must have the same shape as the expected output, but the type will be cast if necessary. See Output type determination for more details.  \nkeepdimsbool, optional\n\n\nIf this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the original a. If the value is anything but the default, then keepdims will be passed through to the mean or sum methods of sub-classes of ndarray. If the sub-classes methods does not implement keepdims any exceptions will be raised.  \nwherearray_like of bool, optional\n\n\nElements to include in the mean. See reduce for details.  New in version 1.22.0.     Returns \n \nmndarray, see dtype parameter above\n\n\nIf out=None, returns a new array containing the mean values, otherwise a reference to the output array is returned. Nan is returned for slices that contain only NaNs.      See also  average\n\nWeighted average  mean\n\nArithmetic mean taken while not ignoring NaNs  \nvar, nanvar\n\n  Notes The arithmetic mean is the sum of the non-NaN elements along the axis divided by the number of non-NaN elements. Note that for floating-point input, the mean is computed using the same precision the input has. Depending on the input data, this can cause the results to be inaccurate, especially for float32. Specifying a higher-precision accumulator using the dtype keyword can alleviate this issue. Examples >>> a = np.array([[1, np.nan], [3, 4]])\n>>> np.nanmean(a)\n2.6666666666666665\n>>> np.nanmean(a, axis=0)\narray([2.,  4.])\n>>> np.nanmean(a, axis=1)\narray([1.,  3.5]) # may vary","title":"numpy.reference.generated.numpy.nanmean"},{"text":"tf.experimental.numpy.nanmean TensorFlow variant of NumPy's nanmean. \ntf.experimental.numpy.nanmean(\n    a, axis=None, dtype=None, keepdims=None\n)\n Unsupported arguments: out. See the NumPy documentation for numpy.nanmean.","title":"tensorflow.experimental.numpy.nanmean"},{"text":"numpy.ma.MaskedArray.mean method   ma.MaskedArray.mean(axis=None, dtype=None, out=None, keepdims=<no value>)[source]\n \nReturns the average of the array elements along given axis. Masked entries are ignored, and result elements which are not finite will be masked. Refer to numpy.mean for full documentation.  See also  numpy.ndarray.mean\n\ncorresponding function for ndarrays  numpy.mean\n\nEquivalent function  numpy.ma.average\n\nWeighted average.    Examples >>> a = np.ma.array([1,2,3], mask=[False, False, True])\n>>> a\nmasked_array(data=[1, 2, --],\n             mask=[False, False,  True],\n       fill_value=999999)\n>>> a.mean()\n1.5","title":"numpy.reference.generated.numpy.ma.maskedarray.mean"},{"text":"numpy.ma.mean   ma.mean(self, axis=None, dtype=None, out=None, keepdims=<no value>) = <numpy.ma.core._frommethod object>\n \nReturns the average of the array elements along given axis. Masked entries are ignored, and result elements which are not finite will be masked. Refer to numpy.mean for full documentation.  See also  numpy.ndarray.mean\n\ncorresponding function for ndarrays  numpy.mean\n\nEquivalent function  numpy.ma.average\n\nWeighted average.    Examples >>> a = np.ma.array([1,2,3], mask=[False, False, True])\n>>> a\nmasked_array(data=[1, 2, --],\n             mask=[False, False,  True],\n       fill_value=999999)\n>>> a.mean()\n1.5","title":"numpy.reference.generated.numpy.ma.mean"}]}
{"task_id":19894365,"prompt":"def f_19894365():\n\treturn ","suffix":"","canonical_solution":"subprocess.call(['\/usr\/bin\/Rscript', '--vanilla', '\/pathto\/MyrScript.r'])","test_start":"\nfrom unittest.mock import Mock\nimport subprocess\n\ndef check(candidate):","test":["\n    subprocess.call = Mock(return_value = 0)\n    assert candidate() == 0\n"],"entry_point":"f_19894365","intent":"running r script '\/pathto\/MyrScript.r' from python","library":["subprocess"],"docs":[{"text":"remote(self: torch._C._distributed_rpc.PyRRef, timeout: float = -1.0) \u2192 object  \nCreate a helper proxy to easily launch a remote using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, rref.remote().func_name(*args, **kwargs) is the same as the following: >>> def run(rref, func_name, args, kwargs):\n>>>   return getattr(rref.local_value(), func_name)(*args, **kwargs)\n>>>\n>>> rpc.remote(rref.owner(), run, args=(rref, func_name, args, kwargs))\n  Parameters \ntimeout (float, optional) \u2013 Timeout for rref.remote(). If the creation of this RRef is not successfully completed within the timeout, then the next time there is an attempt to use the RRef (such as to_here), a timeout will be raised. If not provided, the default RPC timeout will be used. Please see rpc.remote() for specific timeout semantics for RRef.    Example::\n\n>>> from torch.distributed import rpc\n>>> rref = rpc.remote(\"worker1\", torch.add, args=(torch.zeros(2, 2), 1))\n>>> rref.remote().size().to_here()  # returns torch.Size([2, 2])\n>>> rref.remote().view(1, 4).to_here()  # returns tensor([[1., 1., 1., 1.]])","title":"torch.rpc#torch.distributed.rpc.RRef.remote"},{"text":"class torch.distributed.rpc.RRef [source]\n \n \nbackward(self: torch._C._distributed_rpc.PyRRef, dist_autograd_ctx_id: int = -1, retain_graph: bool = False) \u2192 None   Runs the backward pass using the RRef as the root of the backward pass. If dist_autograd_ctx_id is provided, we perform a distributed backward pass using the provided ctx_id starting from the owner of the RRef. In this case, get_gradients() should be used to retrieve the gradients. If dist_autograd_ctx_id is None, it is assumed that this is a local autograd graph and we only perform a local backward pass. In the local case, the node calling this API has to be the owner of the RRef. The value of the RRef is expected to be a scalar Tensor.  Parameters \n \ndist_autograd_ctx_id (int, optional) \u2013 The distributed autograd context id for which we should retrieve the gradients (default: -1). \nretain_graph (bool, optional) \u2013 If False, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Usually, you need to set this to True to run backward multiple times (default: False).     Example::\n\n>>> import torch.distributed.autograd as dist_autograd\n>>> with dist_autograd.context() as context_id:\n>>>     rref.backward(context_id)\n   \n  \nconfirmed_by_owner(self: torch._C._distributed_rpc.PyRRef) \u2192 bool  \nReturns whether this RRef has been confirmed by the owner. OwnerRRef always returns true, while UserRRef only returns true when the owner knowns about this UserRRef. \n  \nis_owner(self: torch._C._distributed_rpc.PyRRef) \u2192 bool  \nReturns whether or not the current node is the owner of this RRef. \n  \nlocal_value(self: torch._C._distributed_rpc.PyRRef) \u2192 object  \nIf the current node is the owner, returns a reference to the local value. Otherwise, throws an exception. \n  \nowner(self: torch._C._distributed_rpc.PyRRef) \u2192 torch._C._distributed_rpc.WorkerInfo  \nReturns worker information of the node that owns this RRef. \n  \nowner_name(self: torch._C._distributed_rpc.PyRRef) \u2192 str  \nReturns worker name of the node that owns this RRef. \n  \nremote(self: torch._C._distributed_rpc.PyRRef, timeout: float = -1.0) \u2192 object  \nCreate a helper proxy to easily launch a remote using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, rref.remote().func_name(*args, **kwargs) is the same as the following: >>> def run(rref, func_name, args, kwargs):\n>>>   return getattr(rref.local_value(), func_name)(*args, **kwargs)\n>>>\n>>> rpc.remote(rref.owner(), run, args=(rref, func_name, args, kwargs))\n  Parameters \ntimeout (float, optional) \u2013 Timeout for rref.remote(). If the creation of this RRef is not successfully completed within the timeout, then the next time there is an attempt to use the RRef (such as to_here), a timeout will be raised. If not provided, the default RPC timeout will be used. Please see rpc.remote() for specific timeout semantics for RRef.    Example::\n\n>>> from torch.distributed import rpc\n>>> rref = rpc.remote(\"worker1\", torch.add, args=(torch.zeros(2, 2), 1))\n>>> rref.remote().size().to_here()  # returns torch.Size([2, 2])\n>>> rref.remote().view(1, 4).to_here()  # returns tensor([[1., 1., 1., 1.]])\n   \n  \nrpc_async(self: torch._C._distributed_rpc.PyRRef, timeout: float = -1.0) \u2192 object  \nCreate a helper proxy to easily launch an rpc_async using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, rref.rpc_async().func_name(*args, **kwargs) is the same as the following: >>> def run(rref, func_name, args, kwargs):\n>>>   return getattr(rref.local_value(), func_name)(*args, **kwargs)\n>>>\n>>> rpc.rpc_async(rref.owner(), run, args=(rref, func_name, args, kwargs))\n  Parameters \ntimeout (float, optional) \u2013 Timeout for rref.rpc_async(). If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout will be used.    Example::\n\n>>> from torch.distributed import rpc\n>>> rref = rpc.remote(\"worker1\", torch.add, args=(torch.zeros(2, 2), 1))\n>>> rref.rpc_async().size().wait()  # returns torch.Size([2, 2])\n>>> rref.rpc_async().view(1, 4).wait()  # returns tensor([[1., 1., 1., 1.]])\n   \n  \nrpc_sync(self: torch._C._distributed_rpc.PyRRef, timeout: float = -1.0) \u2192 object  \nCreate a helper proxy to easily launch an rpc_sync using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, rref.rpc_sync().func_name(*args, **kwargs) is the same as the following: >>> def run(rref, func_name, args, kwargs):\n>>>   return getattr(rref.local_value(), func_name)(*args, **kwargs)\n>>>\n>>> rpc.rpc_sync(rref.owner(), run, args=(rref, func_name, args, kwargs))\n  Parameters \ntimeout (float, optional) \u2013 Timeout for rref.rpc_sync(). If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout will be used.    Example::\n\n>>> from torch.distributed import rpc\n>>> rref = rpc.remote(\"worker1\", torch.add, args=(torch.zeros(2, 2), 1))\n>>> rref.rpc_sync().size()  # returns torch.Size([2, 2])\n>>> rref.rpc_sync().view(1, 4)  # returns tensor([[1., 1., 1., 1.]])\n   \n  \nto_here(self: torch._C._distributed_rpc.PyRRef, timeout: float = -1.0) \u2192 object  \nBlocking call that copies the value of the RRef from the owner to the local node and returns it. If the current node is the owner, returns a reference to the local value.  Parameters \ntimeout (float, optional) \u2013 Timeout for to_here. If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout (60s) will be used.","title":"torch.rpc#torch.distributed.rpc.RRef"},{"text":"rfind(sub[, start[, end]])  \nReturns the highest index in the object where the subsequence sub is found, such that sub is contained in the range [start, end]. Optional arguments start and end are interpreted as in slice notation. Returns -1 on failure.  Changed in version 3.5: Writable bytes-like object is now accepted.","title":"python.library.mmap#mmap.mmap.rfind"},{"text":"str.rfind(sub[, start[, end]])  \nReturn the highest index in the string where substring sub is found, such that sub is contained within s[start:end]. Optional arguments start and end are interpreted as in slice notation. Return -1 on failure.","title":"python.library.stdtypes#str.rfind"},{"text":"to_here(self: torch._C._distributed_rpc.PyRRef, timeout: float = -1.0) \u2192 object  \nBlocking call that copies the value of the RRef from the owner to the local node and returns it. If the current node is the owner, returns a reference to the local value.  Parameters \ntimeout (float, optional) \u2013 Timeout for to_here. If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout (60s) will be used.","title":"torch.rpc#torch.distributed.rpc.RRef.to_here"},{"text":"sys.argv  \nThe list of command line arguments passed to a Python script. argv[0] is the script name (it is operating system dependent whether this is a full pathname or not). If the command was executed using the -c command line option to the interpreter, argv[0] is set to the string '-c'. If no script name was passed to the Python interpreter, argv[0] is the empty string. To loop over the standard input, or the list of files given on the command line, see the fileinput module.  Note On Unix, command line arguments are passed by bytes from OS. Python decodes them with filesystem encoding and \u201csurrogateescape\u201d error handler. When you need original bytes, you can get it by [os.fsencode(arg) for arg in sys.argv].","title":"python.library.sys#sys.argv"},{"text":"run_script(pathname)  \nAnalyze the contents of the pathname file, which must contain Python code.","title":"python.library.modulefinder#modulefinder.ModuleFinder.run_script"},{"text":"bytes.rfind(sub[, start[, end]])  \nbytearray.rfind(sub[, start[, end]])  \nReturn the highest index in the sequence where the subsequence sub is found, such that sub is contained within s[start:end]. Optional arguments start and end are interpreted as in slice notation. Return -1 on failure. The subsequence to search for may be any bytes-like object or an integer in the range 0 to 255.  Changed in version 3.3: Also accept an integer in the range 0 to 255 as the subsequence.","title":"python.library.stdtypes#bytes.rfind"},{"text":"bytes.rfind(sub[, start[, end]])  \nbytearray.rfind(sub[, start[, end]])  \nReturn the highest index in the sequence where the subsequence sub is found, such that sub is contained within s[start:end]. Optional arguments start and end are interpreted as in slice notation. Return -1 on failure. The subsequence to search for may be any bytes-like object or an integer in the range 0 to 255.  Changed in version 3.3: Also accept an integer in the range 0 to 255 as the subsequence.","title":"python.library.stdtypes#bytearray.rfind"},{"text":"modulefinder \u2014 Find modules used by a script Source code: Lib\/modulefinder.py This module provides a ModuleFinder class that can be used to determine the set of modules imported by a script. modulefinder.py can also be run as a script, giving the filename of a Python script as its argument, after which a report of the imported modules will be printed.  \nmodulefinder.AddPackagePath(pkg_name, path)  \nRecord that the package named pkg_name can be found in the specified path. \n  \nmodulefinder.ReplacePackage(oldname, newname)  \nAllows specifying that the module named oldname is in fact the package named newname. \n  \nclass modulefinder.ModuleFinder(path=None, debug=0, excludes=[], replace_paths=[])  \nThis class provides run_script() and report() methods to determine the set of modules imported by a script. path can be a list of directories to search for modules; if not specified, sys.path is used. debug sets the debugging level; higher values make the class print debugging messages about what it\u2019s doing. excludes is a list of module names to exclude from the analysis. replace_paths is a list of (oldpath, newpath) tuples that will be replaced in module paths.  \nreport()  \nPrint a report to standard output that lists the modules imported by the script and their paths, as well as modules that are missing or seem to be missing. \n  \nrun_script(pathname)  \nAnalyze the contents of the pathname file, which must contain Python code. \n  \nmodules  \nA dictionary mapping module names to modules. See Example usage of ModuleFinder. \n \n Example usage of ModuleFinder The script that is going to get analyzed later on (bacon.py): import re, itertools\n\ntry:\n    import baconhameggs\nexcept ImportError:\n    pass\n\ntry:\n    import guido.python.ham\nexcept ImportError:\n    pass\n The script that will output the report of bacon.py: from modulefinder import ModuleFinder\n\nfinder = ModuleFinder()\nfinder.run_script('bacon.py')\n\nprint('Loaded modules:')\nfor name, mod in finder.modules.items():\n    print('%s: ' % name, end='')\n    print(','.join(list(mod.globalnames.keys())[:3]))\n\nprint('-'*50)\nprint('Modules not imported:')\nprint('\\n'.join(finder.badmodules.keys()))\n Sample output (may vary depending on the architecture): Loaded modules:\n_types:\ncopyreg:  _inverted_registry,_slotnames,__all__\nsre_compile:  isstring,_sre,_optimize_unicode\n_sre:\nsre_constants:  REPEAT_ONE,makedict,AT_END_LINE\nsys:\nre:  __module__,finditer,_expand\nitertools:\n__main__:  re,itertools,baconhameggs\nsre_parse:  _PATTERNENDERS,SRE_FLAG_UNICODE\narray:\ntypes:  __module__,IntType,TypeType\n---------------------------------------------------\nModules not imported:\nguido.python.ham\nbaconhameggs","title":"python.library.modulefinder"}]}
{"task_id":19894365,"prompt":"def f_19894365():\n\treturn ","suffix":"","canonical_solution":"subprocess.call('\/usr\/bin\/Rscript --vanilla \/pathto\/MyrScript.r', shell=True)","test_start":"\nfrom unittest.mock import Mock\nimport subprocess\n\ndef check(candidate):","test":["\n    subprocess.call = Mock(return_value = 0)\n    assert candidate() == 0\n"],"entry_point":"f_19894365","intent":"run r script '\/usr\/bin\/Rscript --vanilla \/pathto\/MyrScript.r'","library":["subprocess"],"docs":[{"text":"rfind(sub[, start[, end]])  \nReturns the highest index in the object where the subsequence sub is found, such that sub is contained in the range [start, end]. Optional arguments start and end are interpreted as in slice notation. Returns -1 on failure.  Changed in version 3.5: Writable bytes-like object is now accepted.","title":"python.library.mmap#mmap.mmap.rfind"},{"text":"str.rfind(sub[, start[, end]])  \nReturn the highest index in the string where substring sub is found, such that sub is contained within s[start:end]. Optional arguments start and end are interpreted as in slice notation. Return -1 on failure.","title":"python.library.stdtypes#str.rfind"},{"text":"class torch.distributed.rpc.RRef [source]\n \n \nbackward(self: torch._C._distributed_rpc.PyRRef, dist_autograd_ctx_id: int = -1, retain_graph: bool = False) \u2192 None   Runs the backward pass using the RRef as the root of the backward pass. If dist_autograd_ctx_id is provided, we perform a distributed backward pass using the provided ctx_id starting from the owner of the RRef. In this case, get_gradients() should be used to retrieve the gradients. If dist_autograd_ctx_id is None, it is assumed that this is a local autograd graph and we only perform a local backward pass. In the local case, the node calling this API has to be the owner of the RRef. The value of the RRef is expected to be a scalar Tensor.  Parameters \n \ndist_autograd_ctx_id (int, optional) \u2013 The distributed autograd context id for which we should retrieve the gradients (default: -1). \nretain_graph (bool, optional) \u2013 If False, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Usually, you need to set this to True to run backward multiple times (default: False).     Example::\n\n>>> import torch.distributed.autograd as dist_autograd\n>>> with dist_autograd.context() as context_id:\n>>>     rref.backward(context_id)\n   \n  \nconfirmed_by_owner(self: torch._C._distributed_rpc.PyRRef) \u2192 bool  \nReturns whether this RRef has been confirmed by the owner. OwnerRRef always returns true, while UserRRef only returns true when the owner knowns about this UserRRef. \n  \nis_owner(self: torch._C._distributed_rpc.PyRRef) \u2192 bool  \nReturns whether or not the current node is the owner of this RRef. \n  \nlocal_value(self: torch._C._distributed_rpc.PyRRef) \u2192 object  \nIf the current node is the owner, returns a reference to the local value. Otherwise, throws an exception. \n  \nowner(self: torch._C._distributed_rpc.PyRRef) \u2192 torch._C._distributed_rpc.WorkerInfo  \nReturns worker information of the node that owns this RRef. \n  \nowner_name(self: torch._C._distributed_rpc.PyRRef) \u2192 str  \nReturns worker name of the node that owns this RRef. \n  \nremote(self: torch._C._distributed_rpc.PyRRef, timeout: float = -1.0) \u2192 object  \nCreate a helper proxy to easily launch a remote using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, rref.remote().func_name(*args, **kwargs) is the same as the following: >>> def run(rref, func_name, args, kwargs):\n>>>   return getattr(rref.local_value(), func_name)(*args, **kwargs)\n>>>\n>>> rpc.remote(rref.owner(), run, args=(rref, func_name, args, kwargs))\n  Parameters \ntimeout (float, optional) \u2013 Timeout for rref.remote(). If the creation of this RRef is not successfully completed within the timeout, then the next time there is an attempt to use the RRef (such as to_here), a timeout will be raised. If not provided, the default RPC timeout will be used. Please see rpc.remote() for specific timeout semantics for RRef.    Example::\n\n>>> from torch.distributed import rpc\n>>> rref = rpc.remote(\"worker1\", torch.add, args=(torch.zeros(2, 2), 1))\n>>> rref.remote().size().to_here()  # returns torch.Size([2, 2])\n>>> rref.remote().view(1, 4).to_here()  # returns tensor([[1., 1., 1., 1.]])\n   \n  \nrpc_async(self: torch._C._distributed_rpc.PyRRef, timeout: float = -1.0) \u2192 object  \nCreate a helper proxy to easily launch an rpc_async using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, rref.rpc_async().func_name(*args, **kwargs) is the same as the following: >>> def run(rref, func_name, args, kwargs):\n>>>   return getattr(rref.local_value(), func_name)(*args, **kwargs)\n>>>\n>>> rpc.rpc_async(rref.owner(), run, args=(rref, func_name, args, kwargs))\n  Parameters \ntimeout (float, optional) \u2013 Timeout for rref.rpc_async(). If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout will be used.    Example::\n\n>>> from torch.distributed import rpc\n>>> rref = rpc.remote(\"worker1\", torch.add, args=(torch.zeros(2, 2), 1))\n>>> rref.rpc_async().size().wait()  # returns torch.Size([2, 2])\n>>> rref.rpc_async().view(1, 4).wait()  # returns tensor([[1., 1., 1., 1.]])\n   \n  \nrpc_sync(self: torch._C._distributed_rpc.PyRRef, timeout: float = -1.0) \u2192 object  \nCreate a helper proxy to easily launch an rpc_sync using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, rref.rpc_sync().func_name(*args, **kwargs) is the same as the following: >>> def run(rref, func_name, args, kwargs):\n>>>   return getattr(rref.local_value(), func_name)(*args, **kwargs)\n>>>\n>>> rpc.rpc_sync(rref.owner(), run, args=(rref, func_name, args, kwargs))\n  Parameters \ntimeout (float, optional) \u2013 Timeout for rref.rpc_sync(). If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout will be used.    Example::\n\n>>> from torch.distributed import rpc\n>>> rref = rpc.remote(\"worker1\", torch.add, args=(torch.zeros(2, 2), 1))\n>>> rref.rpc_sync().size()  # returns torch.Size([2, 2])\n>>> rref.rpc_sync().view(1, 4)  # returns tensor([[1., 1., 1., 1.]])\n   \n  \nto_here(self: torch._C._distributed_rpc.PyRRef, timeout: float = -1.0) \u2192 object  \nBlocking call that copies the value of the RRef from the owner to the local node and returns it. If the current node is the owner, returns a reference to the local value.  Parameters \ntimeout (float, optional) \u2013 Timeout for to_here. If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout (60s) will be used.","title":"torch.rpc#torch.distributed.rpc.RRef"},{"text":"has_rsample = True","title":"torch.distributions#torch.distributions.fishersnedecor.FisherSnedecor.has_rsample"},{"text":"has_rsample = True","title":"torch.distributions#torch.distributions.kumaraswamy.Kumaraswamy.has_rsample"},{"text":"has_rsample = True","title":"torch.distributions#torch.distributions.multivariate_normal.MultivariateNormal.has_rsample"},{"text":"rfile  \nAn io.BufferedIOBase input stream, ready to read from the start of the optional input data.","title":"python.library.http.server#http.server.BaseHTTPRequestHandler.rfile"},{"text":"remote(self: torch._C._distributed_rpc.PyRRef, timeout: float = -1.0) \u2192 object  \nCreate a helper proxy to easily launch a remote using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, rref.remote().func_name(*args, **kwargs) is the same as the following: >>> def run(rref, func_name, args, kwargs):\n>>>   return getattr(rref.local_value(), func_name)(*args, **kwargs)\n>>>\n>>> rpc.remote(rref.owner(), run, args=(rref, func_name, args, kwargs))\n  Parameters \ntimeout (float, optional) \u2013 Timeout for rref.remote(). If the creation of this RRef is not successfully completed within the timeout, then the next time there is an attempt to use the RRef (such as to_here), a timeout will be raised. If not provided, the default RPC timeout will be used. Please see rpc.remote() for specific timeout semantics for RRef.    Example::\n\n>>> from torch.distributed import rpc\n>>> rref = rpc.remote(\"worker1\", torch.add, args=(torch.zeros(2, 2), 1))\n>>> rref.remote().size().to_here()  # returns torch.Size([2, 2])\n>>> rref.remote().view(1, 4).to_here()  # returns tensor([[1., 1., 1., 1.]])","title":"torch.rpc#torch.distributed.rpc.RRef.remote"},{"text":"has_rsample = False","title":"torch.distributions#torch.distributions.von_mises.VonMises.has_rsample"},{"text":"has_rsample = True","title":"torch.distributions#torch.distributions.studentT.StudentT.has_rsample"}]}
{"task_id":33058590,"prompt":"def f_33058590(df):\n\treturn ","suffix":"","canonical_solution":"df.fillna(df.mean(axis=0))","test_start":"\nimport pandas as pd\nimport numpy as np\n\ndef check(candidate):","test":["\n    df = pd.DataFrame([[1,2,3],[4,5,6],[7.0,np.nan,9.0]], columns=[\"c1\",\"c2\",\"c3\"]) \n    res = pd.DataFrame([[1,2,3],[4,5,6],[7.0,3.5,9.0]], columns=[\"c1\",\"c2\",\"c3\"])\n    assert candidate(df).equals(res)\n"],"entry_point":"f_33058590","intent":"replacing nan in the dataframe `df` with row average","library":["numpy","pandas"],"docs":[{"text":"class RegrAvgY(y, x, filter=None, default=None)  \nReturns the average of the dependent variable (sum(y)\/N) as a float, or default if there aren\u2019t any matching rows.","title":"django.ref.contrib.postgres.aggregates#django.contrib.postgres.aggregates.RegrAvgY"},{"text":"class RegrAvgX(y, x, filter=None, default=None)  \nReturns the average of the independent variable (sum(x)\/N) as a float, or default if there aren\u2019t any matching rows.","title":"django.ref.contrib.postgres.aggregates#django.contrib.postgres.aggregates.RegrAvgX"},{"text":"class Avg(expression, output_field=None, distinct=False, filter=None, default=None, **extra)  \nReturns the mean value of the given expression, which must be numeric unless you specify a different output_field.  Default alias: <field>__avg\n Return type: float if input is int, otherwise same as input field, or output_field if supplied  Has one optional argument:  \ndistinct  \nIf distinct=True, Avg returns the mean value of unique values. This is the SQL equivalent of AVG(DISTINCT <field>). The default value is False.","title":"django.ref.models.querysets#django.db.models.Avg"},{"text":"class RegrSXY(y, x, filter=None, default=None)  \nReturns sum(x*y) - sum(x) * sum(y)\/N (\u201csum of products\u201d of independent times dependent variable) as a float, or default if there aren\u2019t any matching rows.","title":"django.ref.contrib.postgres.aggregates#django.contrib.postgres.aggregates.RegrSXY"},{"text":"tf.estimator.NanLossDuringTrainingError Unspecified run-time error.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.estimator.NanLossDuringTrainingError, tf.compat.v1.train.NanLossDuringTrainingError  \ntf.estimator.NanLossDuringTrainingError(\n    *args, **kwargs\n)","title":"tensorflow.estimator.nanlossduringtrainingerror"},{"text":"pandas.core.window.rolling.Rolling.mean   Rolling.mean(*args, engine=None, engine_kwargs=None, **kwargs)[source]\n \nCalculate the rolling mean.  Parameters \n *args\n\nFor NumPy compatibility and will not have an effect on the result.  \nengine:str, default None\n\n\n 'cython' : Runs the operation through C-extensions from cython. 'numba' : Runs the operation through JIT compiled code from numba. \nNone : Defaults to 'cython' or globally setting compute.use_numba  New in version 1.3.0.     \nengine_kwargs:dict, default None\n\n\n For 'cython' engine, there are no accepted engine_kwargs \nFor 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False}  New in version 1.3.0.     **kwargs\n\nFor NumPy compatibility and will not have an effect on the result.    Returns \n Series or DataFrame\n\nReturn type is the same as the original object with np.float64 dtype.      See also  pandas.Series.rolling\n\nCalling rolling with Series data.  pandas.DataFrame.rolling\n\nCalling rolling with DataFrames.  pandas.Series.mean\n\nAggregating mean for Series.  pandas.DataFrame.mean\n\nAggregating mean for DataFrame.    Notes See Numba engine and Numba (JIT compilation) for extended documentation and performance considerations for the Numba engine. Examples The below examples will show rolling mean calculations with window sizes of two and three, respectively. \n>>> s = pd.Series([1, 2, 3, 4])\n>>> s.rolling(2).mean()\n0    NaN\n1    1.5\n2    2.5\n3    3.5\ndtype: float64\n  \n>>> s.rolling(3).mean()\n0    NaN\n1    NaN\n2    2.0\n3    3.0\ndtype: float64","title":"pandas.reference.api.pandas.core.window.rolling.rolling.mean"},{"text":"sklearn.utils.sparsefuncs_fast.inplace_csr_row_normalize_l2()  \nInplace row normalize using the l2 norm","title":"sklearn.modules.generated.sklearn.utils.sparsefuncs_fast.inplace_csr_row_normalize_l2#sklearn.utils.sparsefuncs_fast.inplace_csr_row_normalize_l2"},{"text":"window.noutrefresh()  \nMark for refresh but wait. This function updates the data structure representing the desired state of the window, but does not force an update of the physical screen. To accomplish that, call doupdate().","title":"python.library.curses#curses.window.noutrefresh"},{"text":"distinct  \nIf distinct=True, Avg returns the mean value of unique values. This is the SQL equivalent of AVG(DISTINCT <field>). The default value is False.","title":"django.ref.models.querysets#django.db.models.Avg.distinct"},{"text":"pandas.core.window.rolling.Window.mean   Window.mean(*args, **kwargs)[source]\n \nCalculate the rolling weighted window mean.  Parameters \n **kwargs\n\nKeyword arguments to configure the SciPy weighted window type.    Returns \n Series or DataFrame\n\nReturn type is the same as the original object with np.float64 dtype.      See also  pandas.Series.rolling\n\nCalling rolling with Series data.  pandas.DataFrame.rolling\n\nCalling rolling with DataFrames.  pandas.Series.mean\n\nAggregating mean for Series.  pandas.DataFrame.mean\n\nAggregating mean for DataFrame.","title":"pandas.reference.api.pandas.core.window.rolling.window.mean"}]}
{"task_id":12400256,"prompt":"def f_12400256():\n\treturn ","suffix":"","canonical_solution":"time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(1347517370))","test_start":"\nimport time\n\ndef check(candidate):","test":["\n    assert candidate() == \"2012-09-13 06:22:50\"\n"],"entry_point":"f_12400256","intent":"Convert unix timestamp '1347517370' to formatted string '%Y-%m-%d %H:%M:%S'","library":["time"],"docs":[{"text":"time.ctime([secs])  \nConvert a time expressed in seconds since the epoch to a string of a form: 'Sun Jun 20 23:21:05 1993' representing local time. The day field is two characters long and is space padded if the day is a single digit, e.g.: 'Wed Jun\u00a0 9 04:26:40 1993'. If secs is not provided or None, the current time as returned by time() is used. ctime(secs) is equivalent to asctime(localtime(secs)). Locale information is not used by ctime().","title":"python.library.time#time.ctime"},{"text":"time.asctime([t])  \nConvert a tuple or struct_time representing a time as returned by gmtime() or localtime() to a string of the following form: 'Sun Jun 20 23:21:05 1993'. The day field is two characters long and is space padded if the day is a single digit, e.g.: 'Wed Jun\u00a0 9 04:26:40 1993'. If t is not provided, the current time as returned by localtime() is used. Locale information is not used by asctime().  Note Unlike the C function of the same name, asctime() does not add a trailing newline.","title":"python.library.time#time.asctime"},{"text":"werkzeug.http.http_date(timestamp=None)  \nFormat a datetime object or timestamp into an RFC 2822 date string. This is a wrapper for email.utils.format_datetime(). It assumes naive datetime objects are in UTC instead of raising an exception.  Parameters \ntimestamp (Optional[Union[datetime.datetime, datetime.date, int, float, time.struct_time]]) \u2013 The datetime or timestamp to format. Defaults to the current time.  Return type \nstr    Changed in version 2.0: Use email.utils.format_datetime. Accept date objects.","title":"werkzeug.http.index#werkzeug.http.http_date"},{"text":"ascii(object)  \nAs repr(), return a string containing a printable representation of an object, but escape the non-ASCII characters in the string returned by repr() using \\x, \\u or \\U escapes. This generates a string similar to that returned by repr() in Python 2.","title":"python.library.functions#ascii"},{"text":"st_ctime  \nPlatform dependent:  the time of most recent metadata change on Unix, the time of creation on Windows, expressed in seconds.","title":"python.library.os#os.stat_result.st_ctime"},{"text":"st_atime  \nTime of most recent access expressed in seconds.","title":"python.library.os#os.stat_result.st_atime"},{"text":"st_atime_ns  \nTime of most recent access expressed in nanoseconds as an integer.","title":"python.library.os#os.stat_result.st_atime_ns"},{"text":"set_unixfrom(unixfrom)  \nSet the message\u2019s envelope header to unixfrom, which should be a string. (See mboxMessage for a brief description of this header.)","title":"python.library.email.message#email.message.EmailMessage.set_unixfrom"},{"text":"stat.ST_ATIME  \nTime of last access.","title":"python.library.stat#stat.ST_ATIME"},{"text":"set_unixfrom(unixfrom)  \nSet the message\u2019s envelope header to unixfrom, which should be a string.","title":"python.library.email.compat32-message#email.message.Message.set_unixfrom"}]}
{"task_id":23359886,"prompt":"def f_23359886(a):\n\treturn ","suffix":"","canonical_solution":"a[np.where((a[:, (0)] == 0) * (a[:, (1)] == 1))]","test_start":"\nimport numpy as np\n\ndef check(candidate):","test":["\n    a = np.array([[ 0,  1,  2], [ 3,  4,  5], [ 6,  7,  8], [ 9, 10, 11], [12, 13, 14]])\n    res = np.array([[0, 1, 2]])\n    assert np.array_equal(candidate(a), res)\n"],"entry_point":"f_23359886","intent":"selecting rows in Numpy ndarray 'a', where the value in the first column is 0 and value in the second column is 1","library":["numpy"],"docs":[{"text":"numpy.argwhere   numpy.argwhere(a)[source]\n \nFind the indices of array elements that are non-zero, grouped by element.  Parameters \n \naarray_like\n\n\nInput data.    Returns \n \nindex_array(N, a.ndim) ndarray\n\n\nIndices of elements that are non-zero. Indices are grouped by element. This array will have shape (N, a.ndim) where N is the number of non-zero items.      See also  \nwhere, nonzero\n\n  Notes np.argwhere(a) is almost the same as np.transpose(np.nonzero(a)), but produces a result of the correct shape for a 0D array. The output of argwhere is not suitable for indexing arrays. For this purpose use nonzero(a) instead. Examples >>> x = np.arange(6).reshape(2,3)\n>>> x\narray([[0, 1, 2],\n       [3, 4, 5]])\n>>> np.argwhere(x>1)\narray([[0, 2],\n       [1, 0],\n       [1, 1],\n       [1, 2]])","title":"numpy.reference.generated.numpy.argwhere"},{"text":"numpy.ma.ones_like   ma.ones_like(*args, **kwargs) = <numpy.ma.core._convert2ma object>\n \nReturn an array of ones with the same shape and type as a given array.  Parameters \n \naarray_like\n\n\nThe shape and data-type of a define these same attributes of the returned array.  \ndtypedata-type, optional\n\n\nOverrides the data type of the result.  New in version 1.6.0.   \norder{\u2018C\u2019, \u2018F\u2019, \u2018A\u2019, or \u2018K\u2019}, optional\n\n\nOverrides the memory layout of the result. \u2018C\u2019 means C-order, \u2018F\u2019 means F-order, \u2018A\u2019 means \u2018F\u2019 if a is Fortran contiguous, \u2018C\u2019 otherwise. \u2018K\u2019 means match the layout of a as closely as possible.  New in version 1.6.0.   \nsubokbool, optional.\n\n\nIf True, then the newly created array will use the sub-class type of a, otherwise it will be a base-class array. Defaults to True.  \nshapeint or sequence of ints, optional.\n\n\nOverrides the shape of the result. If order=\u2019K\u2019 and the number of dimensions is unchanged, will try to keep order, otherwise, order=\u2019C\u2019 is implied.  New in version 1.17.0.     Returns \n \noutMaskedArray\n\n\nArray of ones with the same shape and type as a.      See also  empty_like\n\nReturn an empty array with shape and type of input.  zeros_like\n\nReturn an array of zeros with shape and type of input.  full_like\n\nReturn a new array with shape of input filled with value.  ones\n\nReturn a new array setting values to one.    Examples >>> x = np.arange(6)\n>>> x = x.reshape((2, 3))\n>>> x\narray([[0, 1, 2],\n       [3, 4, 5]])\n>>> np.ones_like(x)\narray([[1, 1, 1],\n       [1, 1, 1]])\n >>> y = np.arange(3, dtype=float)\n>>> y\narray([0., 1., 2.])\n>>> np.ones_like(y)\narray([1.,  1.,  1.])","title":"numpy.reference.generated.numpy.ma.ones_like"},{"text":"numpy.ones_like   numpy.ones_like(a, dtype=None, order='K', subok=True, shape=None)[source]\n \nReturn an array of ones with the same shape and type as a given array.  Parameters \n \naarray_like\n\n\nThe shape and data-type of a define these same attributes of the returned array.  \ndtypedata-type, optional\n\n\nOverrides the data type of the result.  New in version 1.6.0.   \norder{\u2018C\u2019, \u2018F\u2019, \u2018A\u2019, or \u2018K\u2019}, optional\n\n\nOverrides the memory layout of the result. \u2018C\u2019 means C-order, \u2018F\u2019 means F-order, \u2018A\u2019 means \u2018F\u2019 if a is Fortran contiguous, \u2018C\u2019 otherwise. \u2018K\u2019 means match the layout of a as closely as possible.  New in version 1.6.0.   \nsubokbool, optional.\n\n\nIf True, then the newly created array will use the sub-class type of a, otherwise it will be a base-class array. Defaults to True.  \nshapeint or sequence of ints, optional.\n\n\nOverrides the shape of the result. If order=\u2019K\u2019 and the number of dimensions is unchanged, will try to keep order, otherwise, order=\u2019C\u2019 is implied.  New in version 1.17.0.     Returns \n \noutndarray\n\n\nArray of ones with the same shape and type as a.      See also  empty_like\n\nReturn an empty array with shape and type of input.  zeros_like\n\nReturn an array of zeros with shape and type of input.  full_like\n\nReturn a new array with shape of input filled with value.  ones\n\nReturn a new array setting values to one.    Examples >>> x = np.arange(6)\n>>> x = x.reshape((2, 3))\n>>> x\narray([[0, 1, 2],\n       [3, 4, 5]])\n>>> np.ones_like(x)\narray([[1, 1, 1],\n       [1, 1, 1]])\n >>> y = np.arange(3, dtype=float)\n>>> y\narray([0., 1., 2.])\n>>> np.ones_like(y)\narray([1.,  1.,  1.])","title":"numpy.reference.generated.numpy.ones_like"},{"text":"numpy.ma.compress_rows   ma.compress_rows(a)[source]\n \nSuppress whole rows of a 2-D array that contain masked values. This is equivalent to np.ma.compress_rowcols(a, 0), see compress_rowcols for details.  See also  compress_rowcols","title":"numpy.reference.generated.numpy.ma.compress_rows"},{"text":"tf.experimental.numpy.ones_like TensorFlow variant of NumPy's ones_like. \ntf.experimental.numpy.ones_like(\n    a, dtype=None\n)\n Unsupported arguments: order, subok, shape. See the NumPy documentation for numpy.ones_like.","title":"tensorflow.experimental.numpy.ones_like"},{"text":"numpy.nonzero   numpy.nonzero(a)[source]\n \nReturn the indices of the elements that are non-zero. Returns a tuple of arrays, one for each dimension of a, containing the indices of the non-zero elements in that dimension. The values in a are always tested and returned in row-major, C-style order. To group the indices by element, rather than dimension, use argwhere, which returns a row for each non-zero element.  Note When called on a zero-d array or scalar, nonzero(a) is treated as nonzero(atleast_1d(a)).  Deprecated since version 1.17.0: Use atleast_1d explicitly if this behavior is deliberate.    Parameters \n \naarray_like\n\n\nInput array.    Returns \n \ntuple_of_arraystuple\n\n\nIndices of elements that are non-zero.      See also  flatnonzero\n\nReturn indices that are non-zero in the flattened version of the input array.  ndarray.nonzero\n\nEquivalent ndarray method.  count_nonzero\n\nCounts the number of non-zero elements in the input array.    Notes While the nonzero values can be obtained with a[nonzero(a)], it is recommended to use x[x.astype(bool)] or x[x != 0] instead, which will correctly handle 0-d arrays. Examples >>> x = np.array([[3, 0, 0], [0, 4, 0], [5, 6, 0]])\n>>> x\narray([[3, 0, 0],\n       [0, 4, 0],\n       [5, 6, 0]])\n>>> np.nonzero(x)\n(array([0, 1, 2, 2]), array([0, 1, 0, 1]))\n >>> x[np.nonzero(x)]\narray([3, 4, 5, 6])\n>>> np.transpose(np.nonzero(x))\narray([[0, 0],\n       [1, 1],\n       [2, 0],\n       [2, 1]])\n A common use for nonzero is to find the indices of an array, where a condition is True. Given an array a, the condition a > 3 is a boolean array and since False is interpreted as 0, np.nonzero(a > 3) yields the indices of the a where the condition is true. >>> a = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n>>> a > 3\narray([[False, False, False],\n       [ True,  True,  True],\n       [ True,  True,  True]])\n>>> np.nonzero(a > 3)\n(array([1, 1, 1, 2, 2, 2]), array([0, 1, 2, 0, 1, 2]))\n Using this result to index a is equivalent to using the mask directly: >>> a[np.nonzero(a > 3)]\narray([4, 5, 6, 7, 8, 9])\n>>> a[a > 3]  # prefer this spelling\narray([4, 5, 6, 7, 8, 9])\n nonzero can also be called as a method of the array. >>> (a > 3).nonzero()\n(array([1, 1, 1, 2, 2, 2]), array([0, 1, 2, 0, 1, 2]))","title":"numpy.reference.generated.numpy.nonzero"},{"text":"numpy.flatnonzero   numpy.flatnonzero(a)[source]\n \nReturn indices that are non-zero in the flattened version of a. This is equivalent to np.nonzero(np.ravel(a))[0].  Parameters \n \naarray_like\n\n\nInput data.    Returns \n \nresndarray\n\n\nOutput array, containing the indices of the elements of a.ravel() that are non-zero.      See also  nonzero\n\nReturn the indices of the non-zero elements of the input array.  ravel\n\nReturn a 1-D array containing the elements of the input array.    Examples >>> x = np.arange(-2, 3)\n>>> x\narray([-2, -1,  0,  1,  2])\n>>> np.flatnonzero(x)\narray([0, 1, 3, 4])\n Use the indices of the non-zero elements as an index array to extract these elements: >>> x.ravel()[np.flatnonzero(x)]\narray([-2, -1,  1,  2])","title":"numpy.reference.generated.numpy.flatnonzero"},{"text":"numpy.ma.compress_cols   ma.compress_cols(a)[source]\n \nSuppress whole columns of a 2-D array that contain masked values. This is equivalent to np.ma.compress_rowcols(a, 1), see compress_rowcols for details.  See also  compress_rowcols","title":"numpy.reference.generated.numpy.ma.compress_cols"},{"text":"numpy.ma.mask_cols   ma.mask_cols(a, axis=<no value>)[source]\n \nMask columns of a 2D array that contain masked values. This function is a shortcut to mask_rowcols with axis equal to 1.  See also  mask_rowcols\n\nMask rows and\/or columns of a 2D array.  masked_where\n\nMask where a condition is met.    Examples >>> import numpy.ma as ma\n>>> a = np.zeros((3, 3), dtype=int)\n>>> a[1, 1] = 1\n>>> a\narray([[0, 0, 0],\n       [0, 1, 0],\n       [0, 0, 0]])\n>>> a = ma.masked_equal(a, 1)\n>>> a\nmasked_array(\n  data=[[0, 0, 0],\n        [0, --, 0],\n        [0, 0, 0]],\n  mask=[[False, False, False],\n        [False,  True, False],\n        [False, False, False]],\n  fill_value=1)\n>>> ma.mask_cols(a)\nmasked_array(\n  data=[[0, --, 0],\n        [0, --, 0],\n        [0, --, 0]],\n  mask=[[False,  True, False],\n        [False,  True, False],\n        [False,  True, False]],\n  fill_value=1)","title":"numpy.reference.generated.numpy.ma.mask_cols"},{"text":"numpy.ma.mask_rows   ma.mask_rows(a, axis=<no value>)[source]\n \nMask rows of a 2D array that contain masked values. This function is a shortcut to mask_rowcols with axis equal to 0.  See also  mask_rowcols\n\nMask rows and\/or columns of a 2D array.  masked_where\n\nMask where a condition is met.    Examples >>> import numpy.ma as ma\n>>> a = np.zeros((3, 3), dtype=int)\n>>> a[1, 1] = 1\n>>> a\narray([[0, 0, 0],\n       [0, 1, 0],\n       [0, 0, 0]])\n>>> a = ma.masked_equal(a, 1)\n>>> a\nmasked_array(\n  data=[[0, 0, 0],\n        [0, --, 0],\n        [0, 0, 0]],\n  mask=[[False, False, False],\n        [False,  True, False],\n        [False, False, False]],\n  fill_value=1)\n >>> ma.mask_rows(a)\nmasked_array(\n  data=[[0, 0, 0],\n        [--, --, --],\n        [0, 0, 0]],\n  mask=[[False, False, False],\n        [ True,  True,  True],\n        [False, False, False]],\n  fill_value=1)","title":"numpy.reference.generated.numpy.ma.mask_rows"}]}
{"task_id":4383082,"prompt":"def f_4383082(words):\n\treturn ","suffix":"","canonical_solution":"re.split(' +', words)","test_start":"\nimport regex as re\n\ndef check(candidate):","test":["\n    s = \"hello world sample text\"\n    res = [\"hello\", \"world\", \"sample\", \"text\"]\n    assert candidate(s) == res\n"],"entry_point":"f_4383082","intent":"separate words delimited by one or more spaces into a list","library":["regex"],"docs":[{"text":"asyncio.run_coroutine_threadsafe(coro, loop)  \nSubmit a coroutine to the given event loop. Thread-safe. Return a concurrent.futures.Future to wait for the result from another OS thread. This function is meant to be called from a different OS thread than the one where the event loop is running. Example: # Create a coroutine\ncoro = asyncio.sleep(1, result=3)\n\n# Submit the coroutine to a given loop\nfuture = asyncio.run_coroutine_threadsafe(coro, loop)\n\n# Wait for the result with an optional timeout argument\nassert future.result(timeout) == 3\n If an exception is raised in the coroutine, the returned Future will be notified. It can also be used to cancel the task in the event loop: try:\n    result = future.result(timeout)\nexcept asyncio.TimeoutError:\n    print('The coroutine took too long, cancelling the task...')\n    future.cancel()\nexcept Exception as exc:\n    print(f'The coroutine raised an exception: {exc!r}')\nelse:\n    print(f'The coroutine returned: {result!r}')\n See the concurrency and multithreading section of the documentation. Unlike other asyncio functions this function requires the loop argument to be passed explicitly.  New in version 3.5.1.","title":"python.library.asyncio-task#asyncio.run_coroutine_threadsafe"},{"text":"pandas.Series.clip   Series.clip(lower=None, upper=None, axis=None, inplace=False, *args, **kwargs)[source]\n \nTrim values at input threshold(s). Assigns values outside boundary to boundary values. Thresholds can be singular values or array like, and in the latter case the clipping is performed element-wise in the specified axis.  Parameters \n \nlower:float or array-like, default None\n\n\nMinimum threshold value. All values below this threshold will be set to it. A missing threshold (e.g NA) will not clip the value.  \nupper:float or array-like, default None\n\n\nMaximum threshold value. All values above this threshold will be set to it. A missing threshold (e.g NA) will not clip the value.  \naxis:int or str axis name, optional\n\n\nAlign object with lower and upper along the given axis.  \ninplace:bool, default False\n\n\nWhether to perform the operation in place on the data.  *args, **kwargs\n\nAdditional keywords have no effect but might be accepted for compatibility with numpy.    Returns \n Series or DataFrame or None\n\nSame type as calling object with the values outside the clip boundaries replaced or None if inplace=True.      See also  Series.clip\n\nTrim values at input threshold in series.  DataFrame.clip\n\nTrim values at input threshold in dataframe.  numpy.clip\n\nClip (limit) the values in an array.    Examples \n>>> data = {'col_0': [9, -3, 0, -1, 5], 'col_1': [-2, -7, 6, 8, -5]}\n>>> df = pd.DataFrame(data)\n>>> df\n   col_0  col_1\n0      9     -2\n1     -3     -7\n2      0      6\n3     -1      8\n4      5     -5\n  Clips per column using lower and upper thresholds: \n>>> df.clip(-4, 6)\n   col_0  col_1\n0      6     -2\n1     -3     -4\n2      0      6\n3     -1      6\n4      5     -4\n  Clips using specific lower and upper thresholds per column element: \n>>> t = pd.Series([2, -4, -1, 6, 3])\n>>> t\n0    2\n1   -4\n2   -1\n3    6\n4    3\ndtype: int64\n  \n>>> df.clip(t, t + 4, axis=0)\n   col_0  col_1\n0      6      2\n1     -3     -4\n2      0      3\n3      6      8\n4      5      3\n  Clips using specific lower threshold per column element, with missing values: \n>>> t = pd.Series([2, -4, np.NaN, 6, 3])\n>>> t\n0    2.0\n1   -4.0\n2    NaN\n3    6.0\n4    3.0\ndtype: float64\n  \n>>> df.clip(t, axis=0)\ncol_0  col_1\n0      9      2\n1     -3     -4\n2      0      6\n3      6      8\n4      5      3","title":"pandas.reference.api.pandas.series.clip"},{"text":"pandas.DataFrame.clip   DataFrame.clip(lower=None, upper=None, axis=None, inplace=False, *args, **kwargs)[source]\n \nTrim values at input threshold(s). Assigns values outside boundary to boundary values. Thresholds can be singular values or array like, and in the latter case the clipping is performed element-wise in the specified axis.  Parameters \n \nlower:float or array-like, default None\n\n\nMinimum threshold value. All values below this threshold will be set to it. A missing threshold (e.g NA) will not clip the value.  \nupper:float or array-like, default None\n\n\nMaximum threshold value. All values above this threshold will be set to it. A missing threshold (e.g NA) will not clip the value.  \naxis:int or str axis name, optional\n\n\nAlign object with lower and upper along the given axis.  \ninplace:bool, default False\n\n\nWhether to perform the operation in place on the data.  *args, **kwargs\n\nAdditional keywords have no effect but might be accepted for compatibility with numpy.    Returns \n Series or DataFrame or None\n\nSame type as calling object with the values outside the clip boundaries replaced or None if inplace=True.      See also  Series.clip\n\nTrim values at input threshold in series.  DataFrame.clip\n\nTrim values at input threshold in dataframe.  numpy.clip\n\nClip (limit) the values in an array.    Examples \n>>> data = {'col_0': [9, -3, 0, -1, 5], 'col_1': [-2, -7, 6, 8, -5]}\n>>> df = pd.DataFrame(data)\n>>> df\n   col_0  col_1\n0      9     -2\n1     -3     -7\n2      0      6\n3     -1      8\n4      5     -5\n  Clips per column using lower and upper thresholds: \n>>> df.clip(-4, 6)\n   col_0  col_1\n0      6     -2\n1     -3     -4\n2      0      6\n3     -1      6\n4      5     -4\n  Clips using specific lower and upper thresholds per column element: \n>>> t = pd.Series([2, -4, -1, 6, 3])\n>>> t\n0    2\n1   -4\n2   -1\n3    6\n4    3\ndtype: int64\n  \n>>> df.clip(t, t + 4, axis=0)\n   col_0  col_1\n0      6      2\n1     -3     -4\n2      0      3\n3      6      8\n4      5      3\n  Clips using specific lower threshold per column element, with missing values: \n>>> t = pd.Series([2, -4, np.NaN, 6, 3])\n>>> t\n0    2.0\n1   -4.0\n2    NaN\n3    6.0\n4    3.0\ndtype: float64\n  \n>>> df.clip(t, axis=0)\ncol_0  col_1\n0      9      2\n1     -3     -4\n2      0      6\n3      6      8\n4      5      3","title":"pandas.reference.api.pandas.dataframe.clip"},{"text":"tf.image.non_max_suppression_with_scores     View source on GitHub    Greedily selects a subset of bounding boxes in descending order of score.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.image.non_max_suppression_with_scores  \ntf.image.non_max_suppression_with_scores(\n    boxes, scores, max_output_size, iou_threshold=0.5,\n    score_threshold=float('-inf'), soft_nms_sigma=0.0, name=None\n)\n Prunes away boxes that have high intersection-over-union (IOU) overlap with previously selected boxes. Bounding boxes are supplied as [y1, x1, y2, x2], where (y1, x1) and (y2, x2) are the coordinates of any diagonal pair of box corners and the coordinates can be provided as normalized (i.e., lying in the interval [0, 1]) or absolute. Note that this algorithm is agnostic to where the origin is in the coordinate system. Note that this algorithm is invariant to orthogonal transformations and translations of the coordinate system; thus translating or reflections of the coordinate system result in the same boxes being selected by the algorithm. The output of this operation is a set of integers indexing into the input collection of bounding boxes representing the selected boxes. The bounding box coordinates corresponding to the selected indices can then be obtained using the tf.gather operation. For example: selected_indices, selected_scores = tf.image.non_max_suppression_padded(\n    boxes, scores, max_output_size, iou_threshold=1.0, score_threshold=0.1,\n    soft_nms_sigma=0.5)\nselected_boxes = tf.gather(boxes, selected_indices)\n This function generalizes the tf.image.non_max_suppression op by also supporting a Soft-NMS (with Gaussian weighting) mode (c.f. Bodla et al, https:\/\/arxiv.org\/abs\/1704.04503) where boxes reduce the score of other overlapping boxes instead of directly causing them to be pruned. Consequently, in contrast to tf.image.non_max_suppression, tf.image.non_max_suppression_padded returns the new scores of each input box in the second output, selected_scores. To enable this Soft-NMS mode, set the soft_nms_sigma parameter to be larger than 0. When soft_nms_sigma equals 0, the behavior of tf.image.non_max_suppression_padded is identical to that of tf.image.non_max_suppression (except for the extra output) both in function and in running time.\n \n\n\n Args\n  boxes   A 2-D float Tensor of shape [num_boxes, 4].  \n  scores   A 1-D float Tensor of shape [num_boxes] representing a single score corresponding to each box (each row of boxes).  \n  max_output_size   A scalar integer Tensor representing the maximum number of boxes to be selected by non-max suppression.  \n  iou_threshold   A float representing the threshold for deciding whether boxes overlap too much with respect to IOU.  \n  score_threshold   A float representing the threshold for deciding when to remove boxes based on score.  \n  soft_nms_sigma   A scalar float representing the Soft NMS sigma parameter; See Bodla et al, https:\/\/arxiv.org\/abs\/1704.04503). When soft_nms_sigma=0.0 (which is default), we fall back to standard (hard) NMS.  \n  name   A name for the operation (optional).   \n \n\n\n Returns\n  selected_indices   A 1-D integer Tensor of shape [M] representing the selected indices from the boxes tensor, where M <= max_output_size.  \n  selected_scores   A 1-D float tensor of shape [M] representing the corresponding scores for each selected box, where M <= max_output_size. Scores only differ from corresponding input scores when using Soft NMS (i.e. when soft_nms_sigma>0)","title":"tensorflow.image.non_max_suppression_with_scores"},{"text":"get_rect() \n Returns a Rect based on the size of the mask get_rect(**kwargs) -> Rect  Returns a new pygame.Rect() object based on the size of this mask. The rect's default position will be (0, 0) and its default width and height will be the same as this mask's. The rect's attributes can be altered via pygame.Rect() attribute keyword arguments\/values passed into this method. As an example, a_mask.get_rect(center=(10, 5)) would create a pygame.Rect() based on the mask's size centered at the given position.     \nParameters:\n\nkwargs (dict) -- pygame.Rect() attribute keyword arguments\/values that will be applied to the rect  \nReturns:\na new pygame.Rect() object based on the size of this mask with any pygame.Rect() attribute keyword arguments\/values applied to it  \nReturn type:\nRect     New in pygame 2.0.0.","title":"pygame.ref.mask#pygame.mask.Mask.get_rect"},{"text":"str.split(sep=None, maxsplit=-1)  \nReturn a list of the words in the string, using sep as the delimiter string. If maxsplit is given, at most maxsplit splits are done (thus, the list will have at most maxsplit+1 elements). If maxsplit is not specified or -1, then there is no limit on the number of splits (all possible splits are made). If sep is given, consecutive delimiters are not grouped together and are deemed to delimit empty strings (for example, '1,,2'.split(',') returns ['1', '', '2']). The sep argument may consist of multiple characters (for example, '1<>2<>3'.split('<>') returns ['1', '2', '3']). Splitting an empty string with a specified separator returns ['']. For example: >>> '1,2,3'.split(',')\n['1', '2', '3']\n>>> '1,2,3'.split(',', maxsplit=1)\n['1', '2,3']\n>>> '1,2,,3,'.split(',')\n['1', '2', '', '3', '']\n If sep is not specified or is None, a different splitting algorithm is applied: runs of consecutive whitespace are regarded as a single separator, and the result will contain no empty strings at the start or end if the string has leading or trailing whitespace. Consequently, splitting an empty string or a string consisting of just whitespace with a None separator returns []. For example: >>> '1 2 3'.split()\n['1', '2', '3']\n>>> '1 2 3'.split(maxsplit=1)\n['1', '2 3']\n>>> '   1   2   3   '.split()\n['1', '2', '3']","title":"python.library.stdtypes#str.split"},{"text":"torch.distributions.constraints.independent  \nalias of torch.distributions.constraints._IndependentConstraint","title":"torch.distributions#torch.distributions.constraints.independent"},{"text":"torch.nn.functional.avg_pool3d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None) \u2192 Tensor  \nApplies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kW  regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sW  steps. The number of output features is equal to \u230ainput planessT\u230b\\lfloor\\frac{\\text{input planes}}{sT}\\rfloor . See AvgPool3d for details and output shape.  Parameters \n \ninput \u2013 input tensor (minibatch,in_channels,iT\u00d7iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iT \\times iH , iW) \n \nkernel_size \u2013 size of the pooling region. Can be a single number or a tuple (kT, kH, kW)\n \nstride \u2013 stride of the pooling operation. Can be a single number or a tuple (sT, sH, sW). Default: kernel_size\n \npadding \u2013 implicit zero paddings on both sides of the input. Can be a single number or a tuple (padT, padH, padW), Default: 0 \nceil_mode \u2013 when True, will use ceil instead of floor in the formula to compute the output shape \ncount_include_pad \u2013 when True, will include the zero-padding in the averaging calculation \ndivisor_override \u2013 if specified, it will be used as divisor, otherwise size of the pooling region will be used. Default: None","title":"torch.nn.functional#torch.nn.functional.avg_pool3d"},{"text":"ContentHandler.endDocument()  \nReceive notification of the end of a document. The SAX parser will invoke this method only once, and it will be the last method invoked during the parse. The parser shall not invoke this method until it has either abandoned parsing (because of an unrecoverable error) or reached the end of input.","title":"python.library.xml.sax.handler#xml.sax.handler.ContentHandler.endDocument"},{"text":"expand(batch_shape, _instance=None) [source]","title":"torch.distributions#torch.distributions.cauchy.Cauchy.expand"}]}
{"task_id":14637696,"prompt":"def f_14637696(words):\n\treturn ","suffix":"","canonical_solution":"len(max(words, key=len))","test_start":"\ndef check(candidate):","test":["\n    assert candidate([\"hello\", \"world\", \"sample\", \"text\", \"superballer\"]) == 11\n"],"entry_point":"f_14637696","intent":"length of longest element in list `words`","library":[],"docs":[]}
{"task_id":3933478,"prompt":"def f_3933478(result):\n\treturn ","suffix":"","canonical_solution":"result[0]['from_user']","test_start":"\ndef check(candidate):","test":["\n    Contents = [{\"hi\": 7, \"bye\": 4, \"from_user\": 0}, {1: 2, 3: 4, 5: 6}]\n    assert candidate(Contents) == 0\n"],"entry_point":"f_3933478","intent":"get the value associated with unicode key 'from_user' of first dictionary in list `result`","library":[],"docs":[]}
{"task_id":39112645,"prompt":"def f_39112645():\n\treturn ","suffix":"","canonical_solution":"[line.split() for line in open('File.txt')]","test_start":"\ndef check(candidate):","test":["\n    with open('File.txt','w') as fw:\n        fw.write(\"hi hello cat dog\")\n    assert candidate() == [['hi', 'hello', 'cat', 'dog']]\n"],"entry_point":"f_39112645","intent":"Retrieve each line from a file 'File.txt' as a list","library":[],"docs":[]}
{"task_id":1031851,"prompt":"def f_1031851(a):\n\treturn ","suffix":"","canonical_solution":"dict((v, k) for k, v in a.items())","test_start":"\ndef check(candidate):","test":["\n    a = {\"one\": 1, \"two\": 2}\n    assert candidate(a) == {1: \"one\", 2: \"two\"}\n"],"entry_point":"f_1031851","intent":"swap keys with values in a dictionary `a`","library":[],"docs":[]}
{"task_id":8577137,"prompt":"def f_8577137():\n\treturn ","suffix":"","canonical_solution":"open('path\/to\/FILE_NAME.ext', 'w')","test_start":"\nimport os\n\ndef check(candidate):","test":["\n    path1 = os.path.join(\"\", \"path\")\n    os.mkdir(path1)\n    path2 = os.path.join(\"path\", \"to\")\n    os.mkdir(path2)\n    candidate()\n    assert os.path.exists('path\/to\/FILE_NAME.ext')\n"],"entry_point":"f_8577137","intent":"Open a file `path\/to\/FILE_NAME.ext` in write mode","library":["os"],"docs":[{"text":"Path.touch(mode=0o666, exist_ok=True)  \nCreate a file at this given path. If mode is given, it is combined with the process\u2019 umask value to determine the file mode and access flags. If the file already exists, the function succeeds if exist_ok is true (and its modification time is updated to the current time), otherwise FileExistsError is raised.","title":"python.library.pathlib#pathlib.Path.touch"},{"text":"test.support.create_empty_file(filename)  \nCreate an empty file with filename. If it already exists, truncate it.","title":"python.library.test#test.support.create_empty_file"},{"text":"os.path.splitext(path)  \nSplit the pathname path into a pair (root, ext) such that root + ext ==\npath, and ext is empty or begins with a period and contains at most one period. Leading periods on the basename are ignored; splitext('.cshrc') returns ('.cshrc', '').  Changed in version 3.6: Accepts a path-like object.","title":"python.library.os.path#os.path.splitext"},{"text":"tkinter.filedialog.asksaveasfile(mode=\"w\", **options)  \nCreate a SaveAs dialog and return a file object opened in write-only mode.","title":"python.library.dialog#tkinter.filedialog.asksaveasfile"},{"text":"_open(name, mode='rb')","title":"django.howto.custom-file-storage#django.core.files.storage._open"},{"text":"stat.UF_APPEND  \nThe file may only be appended to.","title":"python.library.stat#stat.UF_APPEND"},{"text":"os.mkfifo(path, mode=0o666, *, dir_fd=None)  \nCreate a FIFO (a named pipe) named path with numeric mode mode. The current umask value is first masked out from the mode. This function can also support paths relative to directory descriptors. FIFOs are pipes that can be accessed like regular files. FIFOs exist until they are deleted (for example with os.unlink()). Generally, FIFOs are used as rendezvous between \u201cclient\u201d and \u201cserver\u201d type processes: the server opens the FIFO for reading, and the client opens it for writing. Note that mkfifo() doesn\u2019t open the FIFO \u2014 it just creates the rendezvous point. Availability: Unix.  New in version 3.3: The dir_fd argument.   Changed in version 3.6: Accepts a path-like object.","title":"python.library.os#os.mkfifo"},{"text":"msilib.add_stream(database, name, path)  \nAdd the file path into the _Stream table of database, with the stream name name.","title":"python.library.msilib#msilib.add_stream"},{"text":"io.open(file, mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None)  \nThis is an alias for the builtin open() function.\nThis function raises an auditing event open with arguments path, mode and flags. The mode and flags arguments may have been modified or inferred from the original call.","title":"python.library.io#io.open"},{"text":"PurePath.with_suffix(suffix)  \nReturn a new path with the suffix changed. If the original path doesn\u2019t have a suffix, the new suffix is appended instead. If the suffix is an empty string, the original suffix is removed: >>> p = PureWindowsPath('c:\/Downloads\/pathlib.tar.gz')\n>>> p.with_suffix('.bz2')\nPureWindowsPath('c:\/Downloads\/pathlib.tar.bz2')\n>>> p = PureWindowsPath('README')\n>>> p.with_suffix('.txt')\nPureWindowsPath('README.txt')\n>>> p = PureWindowsPath('README.txt')\n>>> p.with_suffix('')\nPureWindowsPath('README')","title":"python.library.pathlib#pathlib.PurePath.with_suffix"}]}
{"task_id":17926273,"prompt":"def f_17926273(df):\n\treturn ","suffix":"","canonical_solution":"df.groupby(['col1', 'col2'])['col3'].nunique().reset_index()","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    data = [[1, 1, 1], [1, 1, 1], [1, 1, 2], [1, 2, 3], [1, 2, 3], [1, 2, 3], \n            [2, 1, 1], [2, 1, 2], [2, 1, 3], [2, 2, 3], [2, 2, 3], [2, 2, 3]]\n    expected = [[1, 1, 2], [1, 2, 1], [2, 1, 3], [2, 2, 1]]\n    df = pd.DataFrame(data, columns = ['col1', 'col2', 'col3'])\n    expected_df = pd.DataFrame(expected, columns = ['col1', 'col2', 'col3'])\n    df1 = candidate(df)\n    assert pd.DataFrame.equals(expected_df, df1)\n"],"entry_point":"f_17926273","intent":"count distinct values in a column 'col3' of a pandas dataframe `df` group by objects in 'col1' and 'col2'","library":["pandas"],"docs":[{"text":"class RegrCount(y, x, filter=None)  \nReturns an int of the number of input rows in which both expressions are not null.  Note The default argument is not supported.","title":"django.ref.contrib.postgres.aggregates#django.contrib.postgres.aggregates.RegrCount"},{"text":"pandas.core.groupby.DataFrameGroupBy.nunique   DataFrameGroupBy.nunique(dropna=True)[source]\n \nReturn DataFrame with counts of unique elements in each position.  Parameters \n \ndropna:bool, default True\n\n\nDon\u2019t include NaN in the counts.    Returns \n nunique: DataFrame\n   Examples \n>>> df = pd.DataFrame({'id': ['spam', 'egg', 'egg', 'spam',\n...                           'ham', 'ham'],\n...                    'value1': [1, 5, 5, 2, 5, 5],\n...                    'value2': list('abbaxy')})\n>>> df\n     id  value1 value2\n0  spam       1      a\n1   egg       5      b\n2   egg       5      b\n3  spam       2      a\n4   ham       5      x\n5   ham       5      y\n  \n>>> df.groupby('id').nunique()\n      value1  value2\nid\negg        1       1\nham        1       2\nspam       2       1\n  Check for rows with the same id but conflicting values: \n>>> df.groupby('id').filter(lambda g: (g.nunique() > 1).any())\n     id  value1 value2\n0  spam       1      a\n3  spam       2      a\n4   ham       5      x\n5   ham       5      y","title":"pandas.reference.api.pandas.core.groupby.dataframegroupby.nunique"},{"text":"get_group_by_cols(alias=None)  \nResponsible for returning the list of columns references by this expression. get_group_by_cols() should be called on any nested expressions. F() objects, in particular, hold a reference to a column. The alias parameter will be None unless the expression has been annotated and is used for grouping.","title":"django.ref.models.expressions#django.db.models.Expression.get_group_by_cols"},{"text":"pandas.core.groupby.SeriesGroupBy.nunique   SeriesGroupBy.nunique(dropna=True)[source]\n \nReturn number of unique elements in the group.  Returns \n Series\n\nNumber of unique values within each group.","title":"pandas.reference.api.pandas.core.groupby.seriesgroupby.nunique"},{"text":"pandas.core.resample.Resampler.nunique   Resampler.nunique(_method='nunique')[source]\n \nReturn number of unique elements in the group.  Returns \n Series\n\nNumber of unique values within each group.","title":"pandas.reference.api.pandas.core.resample.resampler.nunique"},{"text":"pandas.core.groupby.DataFrameGroupBy.count   DataFrameGroupBy.count()[source]\n \nCompute count of group, excluding missing values.  Returns \n Series or DataFrame\n\nCount of values within each group.      See also  Series.groupby\n\nApply a function groupby to a Series.  DataFrame.groupby\n\nApply a function groupby to each row or column of a DataFrame.","title":"pandas.reference.api.pandas.core.groupby.dataframegroupby.count"},{"text":"pandas.core.resample.Resampler.count   Resampler.count()[source]\n \nCompute count of group, excluding missing values.  Returns \n Series or DataFrame\n\nCount of values within each group.      See also  Series.groupby\n\nApply a function groupby to a Series.  DataFrame.groupby\n\nApply a function groupby to each row or column of a DataFrame.","title":"pandas.reference.api.pandas.core.resample.resampler.count"},{"text":"pandas.core.resample.Resampler.size   Resampler.size()[source]\n \nCompute group sizes.  Returns \n DataFrame or Series\n\nNumber of rows in each group as a Series if as_index is True or a DataFrame if as_index is False.      See also  Series.groupby\n\nApply a function groupby to a Series.  DataFrame.groupby\n\nApply a function groupby to each row or column of a DataFrame.","title":"pandas.reference.api.pandas.core.resample.resampler.size"},{"text":"pandas.core.groupby.DataFrameGroupBy.size   DataFrameGroupBy.size()[source]\n \nCompute group sizes.  Returns \n DataFrame or Series\n\nNumber of rows in each group as a Series if as_index is True or a DataFrame if as_index is False.      See also  Series.groupby\n\nApply a function groupby to a Series.  DataFrame.groupby\n\nApply a function groupby to each row or column of a DataFrame.","title":"pandas.reference.api.pandas.core.groupby.dataframegroupby.size"},{"text":"pandas.core.groupby.GroupBy.count   finalGroupBy.count()[source]\n \nCompute count of group, excluding missing values.  Returns \n Series or DataFrame\n\nCount of values within each group.      See also  Series.groupby\n\nApply a function groupby to a Series.  DataFrame.groupby\n\nApply a function groupby to each row or column of a DataFrame.","title":"pandas.reference.api.pandas.core.groupby.groupby.count"}]}
{"task_id":3735814,"prompt":"def f_3735814(dict1):\n\treturn ","suffix":"","canonical_solution":"any(key.startswith('EMP$$') for key in dict1)","test_start":"\ndef check(candidate):","test":["\n    assert candidate({'EMP$$': 1, 'EMP$$112': 4}) == True\n","\n    assert candidate({'EMP$$': 1, 'EM$$112': 4}) == True\n","\n    assert candidate({'EMP$33': 0}) == False\n"],"entry_point":"f_3735814","intent":"Check if any key in the dictionary `dict1` starts with the string `EMP$$`","library":[],"docs":[]}
{"task_id":3735814,"prompt":"def f_3735814(dict1):\n\treturn ","suffix":"","canonical_solution":"[value for key, value in list(dict1.items()) if key.startswith('EMP$$')]","test_start":"\ndef check(candidate):","test":["\n    assert sorted(candidate({'EMP$$': 1, 'EMP$$112': 4})) == [1, 4]\n","\n    assert sorted(candidate({'EMP$$': 1, 'EM$$112': 4})) == [1]\n","\n    assert sorted(candidate({'EMP$33': 0})) == []\n"],"entry_point":"f_3735814","intent":"create list of values from dictionary `dict1` that have a key that starts with 'EMP$$'","library":[],"docs":[]}
{"task_id":26097916,"prompt":"def f_26097916(sf):\n\t","suffix":"\n\treturn df","canonical_solution":"df = pd.DataFrame({'email': sf.index, 'list': sf.values})","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    dict = {'email1': [1.0, 5.0, 7.0], 'email2': [4.2, 3.6, -0.9]}\n    sf = pd.Series(dict)\n    k = [['email1', [1.0, 5.0, 7.0]], ['email2', [4.2, 3.6, -0.9]]]\n    df1 = pd.DataFrame(k, columns=['email', 'list'])\n    df2 = candidate(sf)\n    assert pd.DataFrame.equals(df1, df2)\n"],"entry_point":"f_26097916","intent":"convert a pandas series `sf` into a pandas dataframe `df` with columns `email` and `list`","library":["pandas"],"docs":[]}
{"task_id":4048964,"prompt":"def f_4048964(list):\n\treturn ","suffix":"","canonical_solution":"'\\t'.join(map(str, list))","test_start":"\ndef check(candidate):","test":["\n    assert candidate(['hello', 'world', '!']) == 'hello\\tworld\\t!'\n","\n    assert candidate([]) == \"\"\n","\n    assert candidate([\"mconala\"]) == \"mconala\"\n","\n    assert candidate([\"MCoNaLa\"]) == \"MCoNaLa\"\n"],"entry_point":"f_4048964","intent":"concatenate elements of list `list` by tabs `\t`","library":[],"docs":[]}
{"task_id":3182716,"prompt":"def f_3182716():\n\treturn ","suffix":"","canonical_solution":"'\\xd0\\xbf\\xd1\\x80\\xd0\\xb8'.encode('raw_unicode_escape')","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == b'\\xd0\\xbf\\xd1\\x80\\xd0\\xb8'\n"],"entry_point":"f_3182716","intent":"print unicode string '\\xd0\\xbf\\xd1\\x80\\xd0\\xb8' with utf-8","library":[],"docs":[]}
{"task_id":3182716,"prompt":"def f_3182716():\n\treturn ","suffix":"","canonical_solution":"'Sopet\\xc3\\xb3n'.encode('latin-1').decode('utf-8')","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == \"Sopet\u00f3n\"\n"],"entry_point":"f_3182716","intent":"Encode a latin character in string `Sopet\\xc3\\xb3n` properly","library":[],"docs":[]}
{"task_id":35622945,"prompt":"def f_35622945(s):\n\treturn ","suffix":"","canonical_solution":"re.findall('n(?<=[^n]n)n+(?=[^n])(?i)', s)","test_start":"\nimport re \n\ndef check(candidate):","test":["\n    assert candidate(\"ncnnnne\") == ['nnnn']\n","\n    assert candidate(\"nn\") == []\n","\n    assert candidate(\"ask\") == []\n"],"entry_point":"f_35622945","intent":"regex, find \"n\"s only in the middle of string `s`","library":["re"],"docs":[{"text":"Pattern.subn(repl, string, count=0)  \nIdentical to the subn() function, using the compiled pattern.","title":"python.library.re#re.Pattern.subn"},{"text":"pandas.Series.str.rsplit   Series.str.rsplit(pat=None, n=- 1, expand=False)[source]\n \nSplit strings around given separator\/delimiter. Splits the string in the Series\/Index from the end, at the specified delimiter string.  Parameters \n \npat:str or compiled regex, optional\n\n\nString or regular expression to split on. If not specified, split on whitespace.  \nn:int, default -1 (all)\n\n\nLimit number of splits in output. None, 0 and -1 will be interpreted as return all splits.  \nexpand:bool, default False\n\n\nExpand the split strings into separate columns.  If True, return DataFrame\/MultiIndex expanding dimensionality. If False, return Series\/Index, containing lists of strings.   \nregex:bool, default None\n\n\nDetermines if the passed-in pattern is a regular expression:  If True, assumes the passed-in pattern is a regular expression If False, treats the pattern as a literal string. If None and pat length is 1, treats pat as a literal string. If None and pat length is not 1, treats pat as a regular expression. Cannot be set to False if pat is a compiled regex   New in version 1.4.0.     Returns \n Series, Index, DataFrame or MultiIndex\n\nType matches caller unless expand=True (see Notes).    Raises \n ValueError\n\n if regex is False and pat is a compiled regex       See also  Series.str.split\n\nSplit strings around given separator\/delimiter.  Series.str.rsplit\n\nSplits string around given separator\/delimiter, starting from the right.  Series.str.join\n\nJoin lists contained as elements in the Series\/Index with passed delimiter.  str.split\n\nStandard library version for split.  str.rsplit\n\nStandard library version for rsplit.    Notes The handling of the n keyword depends on the number of found splits:  If found splits > n, make first n splits only If found splits <= n, make all splits If for a certain row the number of found splits < n, append None for padding up to n if expand=True  If using expand=True, Series and Index callers return DataFrame and MultiIndex objects, respectively. Use of regex=False with a pat as a compiled regex will raise an error. Examples \n>>> s = pd.Series(\n...     [\n...         \"this is a regular sentence\",\n...         \"https:\/\/docs.python.org\/3\/tutorial\/index.html\",\n...         np.nan\n...     ]\n... )\n>>> s\n0                       this is a regular sentence\n1    https:\/\/docs.python.org\/3\/tutorial\/index.html\n2                                              NaN\ndtype: object\n  In the default setting, the string is split by whitespace. \n>>> s.str.split()\n0                   [this, is, a, regular, sentence]\n1    [https:\/\/docs.python.org\/3\/tutorial\/index.html]\n2                                                NaN\ndtype: object\n  Without the n parameter, the outputs of rsplit and split are identical. \n>>> s.str.rsplit()\n0                   [this, is, a, regular, sentence]\n1    [https:\/\/docs.python.org\/3\/tutorial\/index.html]\n2                                                NaN\ndtype: object\n  The n parameter can be used to limit the number of splits on the delimiter. The outputs of split and rsplit are different. \n>>> s.str.split(n=2)\n0                     [this, is, a regular sentence]\n1    [https:\/\/docs.python.org\/3\/tutorial\/index.html]\n2                                                NaN\ndtype: object\n  \n>>> s.str.rsplit(n=2)\n0                     [this is a, regular, sentence]\n1    [https:\/\/docs.python.org\/3\/tutorial\/index.html]\n2                                                NaN\ndtype: object\n  The pat parameter can be used to split by other characters. \n>>> s.str.split(pat=\"\/\")\n0                         [this is a regular sentence]\n1    [https:, , docs.python.org, 3, tutorial, index...\n2                                                  NaN\ndtype: object\n  When using expand=True, the split elements will expand out into separate columns. If NaN is present, it is propagated throughout the columns during the split. \n>>> s.str.split(expand=True)\n                                               0     1     2        3         4\n0                                           this    is     a  regular  sentence\n1  https:\/\/docs.python.org\/3\/tutorial\/index.html  None  None     None      None\n2                                            NaN   NaN   NaN      NaN       NaN\n  For slightly more complex use cases like splitting the html document name from a url, a combination of parameter settings can be used. \n>>> s.str.rsplit(\"\/\", n=1, expand=True)\n                                    0           1\n0          this is a regular sentence        None\n1  https:\/\/docs.python.org\/3\/tutorial  index.html\n2                                 NaN         NaN\n  Remember to escape special characters when explicitly using regular expressions. \n>>> s = pd.Series([\"foo and bar plus baz\"])\n>>> s.str.split(r\"and|plus\", expand=True)\n    0   1   2\n0 foo bar baz\n  Regular expressions can be used to handle urls or file names. When pat is a string and regex=None (the default), the given pat is compiled as a regex only if len(pat) != 1. \n>>> s = pd.Series(['foojpgbar.jpg'])\n>>> s.str.split(r\".\", expand=True)\n           0    1\n0  foojpgbar  jpg\n  \n>>> s.str.split(r\"\\.jpg\", expand=True)\n           0 1\n0  foojpgbar\n  When regex=True, pat is interpreted as a regex \n>>> s.str.split(r\"\\.jpg\", regex=True, expand=True)\n           0 1\n0  foojpgbar\n  A compiled regex can be passed as pat \n>>> import re\n>>> s.str.split(re.compile(r\"\\.jpg\"), expand=True)\n           0 1\n0  foojpgbar\n  When regex=False, pat is interpreted as the string itself \n>>> s.str.split(r\"\\.jpg\", regex=False, expand=True)\n               0\n0  foojpgbar.jpg","title":"pandas.reference.api.pandas.series.str.rsplit"},{"text":"pandas.Series.str.split   Series.str.split(pat=None, n=- 1, expand=False, *, regex=None)[source]\n \nSplit strings around given separator\/delimiter. Splits the string in the Series\/Index from the beginning, at the specified delimiter string.  Parameters \n \npat:str or compiled regex, optional\n\n\nString or regular expression to split on. If not specified, split on whitespace.  \nn:int, default -1 (all)\n\n\nLimit number of splits in output. None, 0 and -1 will be interpreted as return all splits.  \nexpand:bool, default False\n\n\nExpand the split strings into separate columns.  If True, return DataFrame\/MultiIndex expanding dimensionality. If False, return Series\/Index, containing lists of strings.   \nregex:bool, default None\n\n\nDetermines if the passed-in pattern is a regular expression:  If True, assumes the passed-in pattern is a regular expression If False, treats the pattern as a literal string. If None and pat length is 1, treats pat as a literal string. If None and pat length is not 1, treats pat as a regular expression. Cannot be set to False if pat is a compiled regex   New in version 1.4.0.     Returns \n Series, Index, DataFrame or MultiIndex\n\nType matches caller unless expand=True (see Notes).    Raises \n ValueError\n\n if regex is False and pat is a compiled regex       See also  Series.str.split\n\nSplit strings around given separator\/delimiter.  Series.str.rsplit\n\nSplits string around given separator\/delimiter, starting from the right.  Series.str.join\n\nJoin lists contained as elements in the Series\/Index with passed delimiter.  str.split\n\nStandard library version for split.  str.rsplit\n\nStandard library version for rsplit.    Notes The handling of the n keyword depends on the number of found splits:  If found splits > n, make first n splits only If found splits <= n, make all splits If for a certain row the number of found splits < n, append None for padding up to n if expand=True  If using expand=True, Series and Index callers return DataFrame and MultiIndex objects, respectively. Use of regex=False with a pat as a compiled regex will raise an error. Examples \n>>> s = pd.Series(\n...     [\n...         \"this is a regular sentence\",\n...         \"https:\/\/docs.python.org\/3\/tutorial\/index.html\",\n...         np.nan\n...     ]\n... )\n>>> s\n0                       this is a regular sentence\n1    https:\/\/docs.python.org\/3\/tutorial\/index.html\n2                                              NaN\ndtype: object\n  In the default setting, the string is split by whitespace. \n>>> s.str.split()\n0                   [this, is, a, regular, sentence]\n1    [https:\/\/docs.python.org\/3\/tutorial\/index.html]\n2                                                NaN\ndtype: object\n  Without the n parameter, the outputs of rsplit and split are identical. \n>>> s.str.rsplit()\n0                   [this, is, a, regular, sentence]\n1    [https:\/\/docs.python.org\/3\/tutorial\/index.html]\n2                                                NaN\ndtype: object\n  The n parameter can be used to limit the number of splits on the delimiter. The outputs of split and rsplit are different. \n>>> s.str.split(n=2)\n0                     [this, is, a regular sentence]\n1    [https:\/\/docs.python.org\/3\/tutorial\/index.html]\n2                                                NaN\ndtype: object\n  \n>>> s.str.rsplit(n=2)\n0                     [this is a, regular, sentence]\n1    [https:\/\/docs.python.org\/3\/tutorial\/index.html]\n2                                                NaN\ndtype: object\n  The pat parameter can be used to split by other characters. \n>>> s.str.split(pat=\"\/\")\n0                         [this is a regular sentence]\n1    [https:, , docs.python.org, 3, tutorial, index...\n2                                                  NaN\ndtype: object\n  When using expand=True, the split elements will expand out into separate columns. If NaN is present, it is propagated throughout the columns during the split. \n>>> s.str.split(expand=True)\n                                               0     1     2        3         4\n0                                           this    is     a  regular  sentence\n1  https:\/\/docs.python.org\/3\/tutorial\/index.html  None  None     None      None\n2                                            NaN   NaN   NaN      NaN       NaN\n  For slightly more complex use cases like splitting the html document name from a url, a combination of parameter settings can be used. \n>>> s.str.rsplit(\"\/\", n=1, expand=True)\n                                    0           1\n0          this is a regular sentence        None\n1  https:\/\/docs.python.org\/3\/tutorial  index.html\n2                                 NaN         NaN\n  Remember to escape special characters when explicitly using regular expressions. \n>>> s = pd.Series([\"foo and bar plus baz\"])\n>>> s.str.split(r\"and|plus\", expand=True)\n    0   1   2\n0 foo bar baz\n  Regular expressions can be used to handle urls or file names. When pat is a string and regex=None (the default), the given pat is compiled as a regex only if len(pat) != 1. \n>>> s = pd.Series(['foojpgbar.jpg'])\n>>> s.str.split(r\".\", expand=True)\n           0    1\n0  foojpgbar  jpg\n  \n>>> s.str.split(r\"\\.jpg\", expand=True)\n           0 1\n0  foojpgbar\n  When regex=True, pat is interpreted as a regex \n>>> s.str.split(r\"\\.jpg\", regex=True, expand=True)\n           0 1\n0  foojpgbar\n  A compiled regex can be passed as pat \n>>> import re\n>>> s.str.split(re.compile(r\"\\.jpg\"), expand=True)\n           0 1\n0  foojpgbar\n  When regex=False, pat is interpreted as the string itself \n>>> s.str.split(r\"\\.jpg\", regex=False, expand=True)\n               0\n0  foojpgbar.jpg","title":"pandas.reference.api.pandas.series.str.split"},{"text":"Match.group([group1, ...])  \nReturns one or more subgroups of the match. If there is a single argument, the result is a single string; if there are multiple arguments, the result is a tuple with one item per argument. Without arguments, group1 defaults to zero (the whole match is returned). If a groupN argument is zero, the corresponding return value is the entire matching string; if it is in the inclusive range [1..99], it is the string matching the corresponding parenthesized group. If a group number is negative or larger than the number of groups defined in the pattern, an IndexError exception is raised. If a group is contained in a part of the pattern that did not match, the corresponding result is None. If a group is contained in a part of the pattern that matched multiple times, the last match is returned. >>> m = re.match(r\"(\\w+) (\\w+)\", \"Isaac Newton, physicist\")\n>>> m.group(0)       # The entire match\n'Isaac Newton'\n>>> m.group(1)       # The first parenthesized subgroup.\n'Isaac'\n>>> m.group(2)       # The second parenthesized subgroup.\n'Newton'\n>>> m.group(1, 2)    # Multiple arguments give us a tuple.\n('Isaac', 'Newton')\n If the regular expression uses the (?P<name>...) syntax, the groupN arguments may also be strings identifying groups by their group name. If a string argument is not used as a group name in the pattern, an IndexError exception is raised. A moderately complicated example: >>> m = re.match(r\"(?P<first_name>\\w+) (?P<last_name>\\w+)\", \"Malcolm Reynolds\")\n>>> m.group('first_name')\n'Malcolm'\n>>> m.group('last_name')\n'Reynolds'\n Named groups can also be referred to by their index: >>> m.group(1)\n'Malcolm'\n>>> m.group(2)\n'Reynolds'\n If a group matches multiple times, only the last match is accessible: >>> m = re.match(r\"(..)+\", \"a1b2c3\")  # Matches 3 times.\n>>> m.group(1)                        # Returns only the last match.\n'c3'","title":"python.library.re#re.Match.group"},{"text":"Pattern.groups  \nThe number of capturing groups in the pattern.","title":"python.library.re#re.Pattern.groups"},{"text":"Pattern.split(string, maxsplit=0)  \nIdentical to the split() function, using the compiled pattern.","title":"python.library.re#re.Pattern.split"},{"text":"Match.groups(default=None)  \nReturn a tuple containing all the subgroups of the match, from 1 up to however many groups are in the pattern. The default argument is used for groups that did not participate in the match; it defaults to None. For example: >>> m = re.match(r\"(\\d+)\\.(\\d+)\", \"24.1632\")\n>>> m.groups()\n('24', '1632')\n If we make the decimal place and everything after it optional, not all groups might participate in the match. These groups will default to None unless the default argument is given: >>> m = re.match(r\"(\\d+)\\.?(\\d+)?\", \"24\")\n>>> m.groups()      # Second group defaults to None.\n('24', None)\n>>> m.groups('0')   # Now, the second group defaults to '0'.\n('24', '0')","title":"python.library.re#re.Match.groups"},{"text":"re.subn(pattern, repl, string, count=0, flags=0)  \nPerform the same operation as sub(), but return a tuple (new_string,\nnumber_of_subs_made).  Changed in version 3.1: Added the optional flags argument.   Changed in version 3.5: Unmatched groups are replaced with an empty string.","title":"python.library.re#re.subn"},{"text":"pandas.Series.str.extract   Series.str.extract(pat, flags=0, expand=True)[source]\n \nExtract capture groups in the regex pat as columns in a DataFrame. For each subject string in the Series, extract groups from the first match of regular expression pat.  Parameters \n \npat:str\n\n\nRegular expression pattern with capturing groups.  \nflags:int, default 0 (no flags)\n\n\nFlags from the re module, e.g. re.IGNORECASE, that modify regular expression matching for things like case, spaces, etc. For more details, see re.  \nexpand:bool, default True\n\n\nIf True, return DataFrame with one column per capture group. If False, return a Series\/Index if there is one capture group or DataFrame if there are multiple capture groups.    Returns \n DataFrame or Series or Index\n\nA DataFrame with one row for each subject string, and one column for each group. Any capture group names in regular expression pat will be used for column names; otherwise capture group numbers will be used. The dtype of each result column is always object, even when no match is found. If expand=False and pat has only one capture group, then return a Series (if subject is a Series) or Index (if subject is an Index).      See also  extractall\n\nReturns all matches (not just the first match).    Examples A pattern with two groups will return a DataFrame with two columns. Non-matches will be NaN. \n>>> s = pd.Series(['a1', 'b2', 'c3'])\n>>> s.str.extract(r'([ab])(\\d)')\n    0    1\n0    a    1\n1    b    2\n2  NaN  NaN\n  A pattern may contain optional groups. \n>>> s.str.extract(r'([ab])?(\\d)')\n    0  1\n0    a  1\n1    b  2\n2  NaN  3\n  Named groups will become column names in the result. \n>>> s.str.extract(r'(?P<letter>[ab])(?P<digit>\\d)')\nletter digit\n0      a     1\n1      b     2\n2    NaN   NaN\n  A pattern with one group will return a DataFrame with one column if expand=True. \n>>> s.str.extract(r'[ab](\\d)', expand=True)\n    0\n0    1\n1    2\n2  NaN\n  A pattern with one group will return a Series if expand=False. \n>>> s.str.extract(r'[ab](\\d)', expand=False)\n0      1\n1      2\n2    NaN\ndtype: object","title":"pandas.reference.api.pandas.series.str.extract"},{"text":"pandas.Series.str.count   Series.str.count(pat, flags=0)[source]\n \nCount occurrences of pattern in each string of the Series\/Index. This function is used to count the number of times a particular regex pattern is repeated in each of the string elements of the Series.  Parameters \n \npat:str\n\n\nValid regular expression.  \nflags:int, default 0, meaning no flags\n\n\nFlags for the re module. For a complete list, see here.  **kwargs\n\nFor compatibility with other string methods. Not used.    Returns \n Series or Index\n\nSame type as the calling object containing the integer counts.      See also  re\n\nStandard library module for regular expressions.  str.count\n\nStandard library version, without regular expression support.    Notes Some characters need to be escaped when passing in pat. eg. '$' has a special meaning in regex and must be escaped when finding this literal character. Examples \n>>> s = pd.Series(['A', 'B', 'Aaba', 'Baca', np.nan, 'CABA', 'cat'])\n>>> s.str.count('a')\n0    0.0\n1    0.0\n2    2.0\n3    2.0\n4    NaN\n5    0.0\n6    1.0\ndtype: float64\n  Escape '$' to find the literal dollar sign. \n>>> s = pd.Series(['$', 'B', 'Aab$', '$$ca', 'C$B$', 'cat'])\n>>> s.str.count('\\\\$')\n0    1\n1    0\n2    1\n3    2\n4    2\n5    0\ndtype: int64\n  This is also available on Index \n>>> pd.Index(['A', 'A', 'Aaba', 'cat']).str.count('a')\nInt64Index([0, 0, 2, 1], dtype='int64')","title":"pandas.reference.api.pandas.series.str.count"}]}
{"task_id":5306756,"prompt":"def f_5306756():\n\treturn ","suffix":"","canonical_solution":"'{0:.0f}%'.format(1.0 \/ 3 * 100)","test_start":"\ndef check(candidate):","test":["\n    assert(candidate() == \"33%\")\n"],"entry_point":"f_5306756","intent":"display the float `1\/3*100` as a percentage","library":[],"docs":[]}
{"task_id":2878084,"prompt":"def f_2878084(mylist):\n\t","suffix":"\n\treturn mylist","canonical_solution":"mylist.sort(key=lambda x: x['title'])","test_start":"\ndef check(candidate):","test":["\n    input = [\n        {'title':'New York Times', 'title_url':'New_York_Times','id':4}, \n        {'title':'USA Today','title_url':'USA_Today','id':6}, \n        {'title':'Apple News','title_url':'Apple_News','id':2}\n    ]\n    res = [\n        {'title': 'Apple News', 'title_url': 'Apple_News', 'id': 2}, \n        {'title': 'New York Times', 'title_url': 'New_York_Times', 'id': 4},\n        {'title': 'USA Today', 'title_url': 'USA_Today', 'id': 6}\n    ]\n    assert candidate(input) == res\n"],"entry_point":"f_2878084","intent":"sort a list of dictionary `mylist` by the key `title`","library":[],"docs":[]}
{"task_id":2878084,"prompt":"def f_2878084(l):\n\t","suffix":"\n\treturn l","canonical_solution":"l.sort(key=lambda x: x['title'])","test_start":"\ndef check(candidate):","test":["\n    input = [\n        {'title':'New York Times', 'title_url':'New_York_Times','id':4}, \n        {'title':'USA Today','title_url':'USA_Today','id':6}, \n        {'title':'Apple News','title_url':'Apple_News','id':2}\n    ]\n    res = [\n        {'title': 'Apple News', 'title_url': 'Apple_News', 'id': 2}, \n        {'title': 'New York Times', 'title_url': 'New_York_Times', 'id': 4},\n        {'title': 'USA Today', 'title_url': 'USA_Today', 'id': 6}\n    ]\n    assert candidate(input) == res\n"],"entry_point":"f_2878084","intent":"sort a list `l` of dicts by dict value 'title'","library":[],"docs":[]}
{"task_id":2878084,"prompt":"def f_2878084(l):\n\t","suffix":"\n\treturn l","canonical_solution":"l.sort(key=lambda x: (x['title'], x['title_url'], x['id']))","test_start":"\ndef check(candidate):","test":["\n    input = [\n        {'title':'New York Times', 'title_url':'New_York_Times','id':4}, \n        {'title':'USA Today','title_url':'USA_Today','id':6}, \n        {'title':'Apple News','title_url':'Apple_News','id':2}\n    ]\n    res = [\n        {'title': 'Apple News', 'title_url': 'Apple_News', 'id': 2}, \n        {'title': 'New York Times', 'title_url': 'New_York_Times', 'id': 4},\n        {'title': 'USA Today', 'title_url': 'USA_Today', 'id': 6}\n    ]\n    assert candidate(input) == res\n"],"entry_point":"f_2878084","intent":"sort a list of dictionaries by the value of keys 'title', 'title_url', 'id' in ascending order.","library":[],"docs":[]}
{"task_id":9323159,"prompt":"def f_9323159(l1, l2):\n\treturn ","suffix":"","canonical_solution":"heapq.nlargest(10, range(len(l1)), key=lambda i: abs(l1[i] - l2[i]))","test_start":"\nimport heapq\n\ndef check(candidate):","test":["\n    l1 = [99, 86, 90, 70, 86, 95, 56, 98, 80, 81]\n    l2 = [21, 11, 21, 1, 26, 40, 4, 50, 34, 37]\n    res = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n    assert candidate(l1, l2) == res\n"],"entry_point":"f_9323159","intent":"find 10 largest differences between each respective elements of list `l1` and list `l2`","library":["heapq"],"docs":[{"text":"tf.raw_ops.ListDiff Computes the difference between two lists of numbers or strings.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.raw_ops.ListDiff  \ntf.raw_ops.ListDiff(\n    x, y, out_idx=tf.dtypes.int32, name=None\n)\n Given a list x and a list y, this operation returns a list out that represents all values that are in x but not in y. The returned list out is sorted in the same order that the numbers appear in x (duplicates are preserved). This operation also returns a list idx that represents the position of each out element in x. In other words: out[i] = x[idx[i]] for i in [0, 1, ..., len(out) - 1] For example, given this input: x = [1, 2, 3, 4, 5, 6]\ny = [1, 3, 5]\n This operation would return: out ==> [2, 4, 6]\nidx ==> [1, 3, 5]\n\n \n\n\n Args\n  x   A Tensor. 1-D. Values to keep.  \n  y   A Tensor. Must have the same type as x. 1-D. Values to remove.  \n  out_idx   An optional tf.DType from: tf.int32, tf.int64. Defaults to tf.int32.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A tuple of Tensor objects (out, idx).     out   A Tensor. Has the same type as x.  \n  idx   A Tensor of type out_idx.","title":"tensorflow.raw_ops.listdiff"},{"text":"lstsq(A) -> (Tensor, Tensor)  \nSee torch.lstsq()","title":"torch.tensors#torch.Tensor.lstsq"},{"text":"skimage.color.deltaE_cmc(lab1, lab2, kL=1, kC=1) [source]\n \nColor difference from the CMC l:c standard. This color difference was developed by the Colour Measurement Committee (CMC) of the Society of Dyers and Colourists (United Kingdom). It is intended for use in the textile industry. The scale factors kL, kC set the weight given to differences in lightness and chroma relative to differences in hue. The usual values are kL=2, kC=1 for \u201cacceptability\u201d and kL=1, kC=1 for \u201cimperceptibility\u201d. Colors with dE > 1 are \u201cdifferent\u201d for the given scale factors.  Parameters \n \nlab1array_like \n\nreference color (Lab colorspace)  \nlab2array_like \n\ncomparison color (Lab colorspace)    Returns \n \ndEarray_like \n\ndistance between colors lab1 and lab2     Notes deltaE_cmc the defines the scales for the lightness, hue, and chroma in terms of the first color. Consequently deltaE_cmc(lab1, lab2) != deltaE_cmc(lab2, lab1) References  \n1  \nhttps:\/\/en.wikipedia.org\/wiki\/Color_difference  \n2  \nhttp:\/\/www.brucelindbloom.com\/index.html?Eqn_DeltaE_CIE94.html  \n3  \nF. J. J. Clarke, R. McDonald, and B. Rigg, \u201cModification to the JPC79 colour-difference formula,\u201d J. Soc. Dyers Colour. 100, 128-132 (1984).","title":"skimage.api.skimage.color#skimage.color.deltaE_cmc"},{"text":"plistlib.FMT_BINARY  \nThe binary format for plist files  New in version 3.4.","title":"python.library.plistlib#plistlib.FMT_BINARY"},{"text":"matplotlib.axis.Axis.get_tightbbox   Axis.get_tightbbox(renderer, *, for_layout_only=False)[source]\n \nReturn a bounding box that encloses the axis. It only accounts tick labels, axis label, and offsetText. If for_layout_only is True, then the width of the label (if this is an x-axis) or the height of the label (if this is a y-axis) is collapsed to near zero. This allows tight\/constrained_layout to ignore too-long labels when doing their layout.","title":"matplotlib._as_gen.matplotlib.axis.axis.get_tightbbox"},{"text":"class werkzeug.local.LocalManager(locals=None, ident_func=None)  \nLocal objects cannot manage themselves. For that you need a local manager. You can pass a local manager multiple locals or add them later y appending them to manager.locals. Every time the manager cleans up, it will clean up all the data left in the locals for this context.  Changed in version 2.0: ident_func is deprecated and will be removed in Werkzeug 2.1.   Changelog Changed in version 0.7: The ident_func parameter was added.   Changed in version 0.6.1: The release_local() function can be used instead of a manager.   Parameters \n \nlocals (Optional[Iterable[Union[werkzeug.local.Local, werkzeug.local.LocalStack]]]) \u2013  \nident_func (None) \u2013    Return type \nNone    \ncleanup()  \nManually clean up the data in the locals for this context. Call this at the end of the request or use make_middleware().  Return type \nNone   \n  \nget_ident()  \nReturn the context identifier the local objects use internally for this context. You cannot override this method to change the behavior but use it to link other context local objects (such as SQLAlchemy\u2019s scoped sessions) to the Werkzeug locals.  Deprecated since version 2.0: Will be removed in Werkzeug 2.1.   Changelog Changed in version 0.7: You can pass a different ident function to the local manager that will then be propagated to all the locals passed to the constructor.   Return type \nint   \n  \nmake_middleware(app)  \nWrap a WSGI application so that cleaning up happens after request end.  Parameters \napp (WSGIApplication) \u2013   Return type \nWSGIApplication   \n  \nmiddleware(func)  \nLike make_middleware but for decorating functions. Example usage: @manager.middleware\ndef application(environ, start_response):\n    ...\n The difference to make_middleware is that the function passed will have all the arguments copied from the inner application (name, docstring, module).  Parameters \nfunc (WSGIApplication) \u2013   Return type \nWSGIApplication","title":"werkzeug.local.index#werkzeug.local.LocalManager"},{"text":"class werkzeug.local.LocalStack  \nThis class works similar to a Local but keeps a stack of objects instead. This is best explained with an example: >>> ls = LocalStack()\n>>> ls.push(42)\n>>> ls.top\n42\n>>> ls.push(23)\n>>> ls.top\n23\n>>> ls.pop()\n23\n>>> ls.top\n42\n They can be force released by using a LocalManager or with the release_local() function but the correct way is to pop the item from the stack after using. When the stack is empty it will no longer be bound to the current context (and as such released). By calling the stack without arguments it returns a proxy that resolves to the topmost item on the stack.  Changelog New in version 0.6.1.   Return type \nNone    \npop()  \nRemoves the topmost item from the stack, will return the old value or None if the stack was already empty.  Return type \nAny   \n  \npush(obj)  \nPushes a new item to the stack  Parameters \nobj (Any) \u2013   Return type \nList[Any]   \n  \nproperty top: Any  \nThe topmost item on the stack. If the stack is empty, None is returned.","title":"werkzeug.local.index#werkzeug.local.LocalStack"},{"text":"close()  \nCloses associated resources of this request object. This closes all file handles explicitly. You can also use the request object in a with statement which will automatically close it.  Changelog New in version 0.9.   Return type \nNone","title":"flask.api.index#flask.Request.close"},{"text":"close()  \nCloses associated resources of this request object. This closes all file handles explicitly. You can also use the request object in a with statement which will automatically close it.  Changelog New in version 0.9.   Return type \nNone","title":"werkzeug.wrappers.index#werkzeug.wrappers.Request.close"},{"text":"get_tightbbox(renderer, *, for_layout_only=False)[source]\n \nReturn a bounding box that encloses the axis. It only accounts tick labels, axis label, and offsetText. If for_layout_only is True, then the width of the label (if this is an x-axis) or the height of the label (if this is a y-axis) is collapsed to near zero. This allows tight\/constrained_layout to ignore too-long labels when doing their layout.","title":"matplotlib._as_gen.mpl_toolkits.mplot3d.axis3d.axis#mpl_toolkits.mplot3d.axis3d.Axis.get_tightbbox"}]}
{"task_id":29877663,"prompt":"def f_29877663(soup):\n\treturn ","suffix":"","canonical_solution":"soup.find_all('span', {'class': 'starGryB sp'})","test_start":"\nimport bs4\n\ndef check(candidate):","test":["\n    html = '''<span class=\"starBig sp\">4.1<\/span>\n             <span class=\"starGryB sp\">2.9<\/span>\n             <span class=\"sp starGryB\">2.9<\/span>\n             <span class=\"sp starBig\">22<\/span>'''\n    soup = bs4.BeautifulSoup(html, features=\"html5lib\")\n    res = '''[<span class=\"starGryB sp\">2.9<\/span>]'''\n    assert(str(candidate(soup)) == res)\n"],"entry_point":"f_29877663","intent":"BeautifulSoup find all 'span' elements in HTML string `soup` with class of 'starGryB sp'","library":["bs4"],"docs":[]}
{"task_id":24189150,"prompt":"def f_24189150(df, engine):\n\t","suffix":"\n\treturn ","canonical_solution":"df.to_sql('test', engine)","test_start":"\nimport pandas as pd\nfrom sqlalchemy import create_engine\n\ndef check(candidate):","test":["\n    engine = create_engine('sqlite:\/\/', echo=False)\n    df = pd.DataFrame({'name' : ['User 1', 'User 2', 'User 3']})\n    candidate(df, engine)\n    result = pd.read_sql('SELECT name FROM test', engine)\n    assert result.equals(df)\n"],"entry_point":"f_24189150","intent":"write records in dataframe `df` to table 'test' in schema 'a_schema' with `engine`","library":["pandas","sqlalchemy"],"docs":[{"text":"pandas.DataFrame.to_sql   DataFrame.to_sql(name, con, schema=None, if_exists='fail', index=True, index_label=None, chunksize=None, dtype=None, method=None)[source]\n \nWrite records stored in a DataFrame to a SQL database. Databases supported by SQLAlchemy [1] are supported. Tables can be newly created, appended to, or overwritten.  Parameters \n \nname:str\n\n\nName of SQL table.  \ncon:sqlalchemy.engine.(Engine or Connection) or sqlite3.Connection\n\n\nUsing SQLAlchemy makes it possible to use any DB supported by that library. Legacy support is provided for sqlite3.Connection objects. The user is responsible for engine disposal and connection closure for the SQLAlchemy connectable See here.  \nschema:str, optional\n\n\nSpecify the schema (if database flavor supports this). If None, use default schema.  \nif_exists:{\u2018fail\u2019, \u2018replace\u2019, \u2018append\u2019}, default \u2018fail\u2019\n\n\nHow to behave if the table already exists.  fail: Raise a ValueError. replace: Drop the table before inserting new values. append: Insert new values to the existing table.   \nindex:bool, default True\n\n\nWrite DataFrame index as a column. Uses index_label as the column name in the table.  \nindex_label:str or sequence, default None\n\n\nColumn label for index column(s). If None is given (default) and index is True, then the index names are used. A sequence should be given if the DataFrame uses MultiIndex.  \nchunksize:int, optional\n\n\nSpecify the number of rows in each batch to be written at a time. By default, all rows will be written at once.  \ndtype:dict or scalar, optional\n\n\nSpecifying the datatype for columns. If a dictionary is used, the keys should be the column names and the values should be the SQLAlchemy types or strings for the sqlite3 legacy mode. If a scalar is provided, it will be applied to all columns.  \nmethod:{None, \u2018multi\u2019, callable}, optional\n\n\nControls the SQL insertion clause used:  None : Uses standard SQL INSERT clause (one per row). \u2018multi\u2019: Pass multiple values in a single INSERT clause. callable with signature (pd_table, conn, keys, data_iter).  Details and a sample callable implementation can be found in the section insert method.    Returns \n None or int\n\nNumber of rows affected by to_sql. None is returned if the callable passed into method does not return the number of rows. The number of returned rows affected is the sum of the rowcount attribute of sqlite3.Cursor or SQLAlchemy connectable which may not reflect the exact number of written rows as stipulated in the sqlite3 or SQLAlchemy.  New in version 1.4.0.     Raises \n ValueError\n\nWhen the table already exists and if_exists is \u2018fail\u2019 (the default).      See also  read_sql\n\nRead a DataFrame from a table.    Notes Timezone aware datetime columns will be written as Timestamp with timezone type with SQLAlchemy if supported by the database. Otherwise, the datetimes will be stored as timezone unaware timestamps local to the original timezone. References  1 \nhttps:\/\/docs.sqlalchemy.org  2 \nhttps:\/\/www.python.org\/dev\/peps\/pep-0249\/   Examples Create an in-memory SQLite database. \n>>> from sqlalchemy import create_engine\n>>> engine = create_engine('sqlite:\/\/', echo=False)\n  Create a table from scratch with 3 rows. \n>>> df = pd.DataFrame({'name' : ['User 1', 'User 2', 'User 3']})\n>>> df\n     name\n0  User 1\n1  User 2\n2  User 3\n  \n>>> df.to_sql('users', con=engine)\n3\n>>> engine.execute(\"SELECT * FROM users\").fetchall()\n[(0, 'User 1'), (1, 'User 2'), (2, 'User 3')]\n  An sqlalchemy.engine.Connection can also be passed to con: \n>>> with engine.begin() as connection:\n...     df1 = pd.DataFrame({'name' : ['User 4', 'User 5']})\n...     df1.to_sql('users', con=connection, if_exists='append')\n2\n  This is allowed to support operations that require that the same DBAPI connection is used for the entire operation. \n>>> df2 = pd.DataFrame({'name' : ['User 6', 'User 7']})\n>>> df2.to_sql('users', con=engine, if_exists='append')\n2\n>>> engine.execute(\"SELECT * FROM users\").fetchall()\n[(0, 'User 1'), (1, 'User 2'), (2, 'User 3'),\n (0, 'User 4'), (1, 'User 5'), (0, 'User 6'),\n (1, 'User 7')]\n  Overwrite the table with just df2. \n>>> df2.to_sql('users', con=engine, if_exists='replace',\n...            index_label='id')\n2\n>>> engine.execute(\"SELECT * FROM users\").fetchall()\n[(0, 'User 6'), (1, 'User 7')]\n  Specify the dtype (especially useful for integers with missing values). Notice that while pandas is forced to store the data as floating point, the database supports nullable integers. When fetching the data with Python, we get back integer scalars. \n>>> df = pd.DataFrame({\"A\": [1, None, 2]})\n>>> df\n     A\n0  1.0\n1  NaN\n2  2.0\n  \n>>> from sqlalchemy.types import Integer\n>>> df.to_sql('integers', con=engine, index=False,\n...           dtype={\"A\": Integer()})\n3\n  \n>>> engine.execute(\"SELECT * FROM integers\").fetchall()\n[(1,), (None,), (2,)]","title":"pandas.reference.api.pandas.dataframe.to_sql"},{"text":"pandas.Series.to_sql   Series.to_sql(name, con, schema=None, if_exists='fail', index=True, index_label=None, chunksize=None, dtype=None, method=None)[source]\n \nWrite records stored in a DataFrame to a SQL database. Databases supported by SQLAlchemy [1] are supported. Tables can be newly created, appended to, or overwritten.  Parameters \n \nname:str\n\n\nName of SQL table.  \ncon:sqlalchemy.engine.(Engine or Connection) or sqlite3.Connection\n\n\nUsing SQLAlchemy makes it possible to use any DB supported by that library. Legacy support is provided for sqlite3.Connection objects. The user is responsible for engine disposal and connection closure for the SQLAlchemy connectable See here.  \nschema:str, optional\n\n\nSpecify the schema (if database flavor supports this). If None, use default schema.  \nif_exists:{\u2018fail\u2019, \u2018replace\u2019, \u2018append\u2019}, default \u2018fail\u2019\n\n\nHow to behave if the table already exists.  fail: Raise a ValueError. replace: Drop the table before inserting new values. append: Insert new values to the existing table.   \nindex:bool, default True\n\n\nWrite DataFrame index as a column. Uses index_label as the column name in the table.  \nindex_label:str or sequence, default None\n\n\nColumn label for index column(s). If None is given (default) and index is True, then the index names are used. A sequence should be given if the DataFrame uses MultiIndex.  \nchunksize:int, optional\n\n\nSpecify the number of rows in each batch to be written at a time. By default, all rows will be written at once.  \ndtype:dict or scalar, optional\n\n\nSpecifying the datatype for columns. If a dictionary is used, the keys should be the column names and the values should be the SQLAlchemy types or strings for the sqlite3 legacy mode. If a scalar is provided, it will be applied to all columns.  \nmethod:{None, \u2018multi\u2019, callable}, optional\n\n\nControls the SQL insertion clause used:  None : Uses standard SQL INSERT clause (one per row). \u2018multi\u2019: Pass multiple values in a single INSERT clause. callable with signature (pd_table, conn, keys, data_iter).  Details and a sample callable implementation can be found in the section insert method.    Returns \n None or int\n\nNumber of rows affected by to_sql. None is returned if the callable passed into method does not return the number of rows. The number of returned rows affected is the sum of the rowcount attribute of sqlite3.Cursor or SQLAlchemy connectable which may not reflect the exact number of written rows as stipulated in the sqlite3 or SQLAlchemy.  New in version 1.4.0.     Raises \n ValueError\n\nWhen the table already exists and if_exists is \u2018fail\u2019 (the default).      See also  read_sql\n\nRead a DataFrame from a table.    Notes Timezone aware datetime columns will be written as Timestamp with timezone type with SQLAlchemy if supported by the database. Otherwise, the datetimes will be stored as timezone unaware timestamps local to the original timezone. References  1 \nhttps:\/\/docs.sqlalchemy.org  2 \nhttps:\/\/www.python.org\/dev\/peps\/pep-0249\/   Examples Create an in-memory SQLite database. \n>>> from sqlalchemy import create_engine\n>>> engine = create_engine('sqlite:\/\/', echo=False)\n  Create a table from scratch with 3 rows. \n>>> df = pd.DataFrame({'name' : ['User 1', 'User 2', 'User 3']})\n>>> df\n     name\n0  User 1\n1  User 2\n2  User 3\n  \n>>> df.to_sql('users', con=engine)\n3\n>>> engine.execute(\"SELECT * FROM users\").fetchall()\n[(0, 'User 1'), (1, 'User 2'), (2, 'User 3')]\n  An sqlalchemy.engine.Connection can also be passed to con: \n>>> with engine.begin() as connection:\n...     df1 = pd.DataFrame({'name' : ['User 4', 'User 5']})\n...     df1.to_sql('users', con=connection, if_exists='append')\n2\n  This is allowed to support operations that require that the same DBAPI connection is used for the entire operation. \n>>> df2 = pd.DataFrame({'name' : ['User 6', 'User 7']})\n>>> df2.to_sql('users', con=engine, if_exists='append')\n2\n>>> engine.execute(\"SELECT * FROM users\").fetchall()\n[(0, 'User 1'), (1, 'User 2'), (2, 'User 3'),\n (0, 'User 4'), (1, 'User 5'), (0, 'User 6'),\n (1, 'User 7')]\n  Overwrite the table with just df2. \n>>> df2.to_sql('users', con=engine, if_exists='replace',\n...            index_label='id')\n2\n>>> engine.execute(\"SELECT * FROM users\").fetchall()\n[(0, 'User 6'), (1, 'User 7')]\n  Specify the dtype (especially useful for integers with missing values). Notice that while pandas is forced to store the data as floating point, the database supports nullable integers. When fetching the data with Python, we get back integer scalars. \n>>> df = pd.DataFrame({\"A\": [1, None, 2]})\n>>> df\n     A\n0  1.0\n1  NaN\n2  2.0\n  \n>>> from sqlalchemy.types import Integer\n>>> df.to_sql('integers', con=engine, index=False,\n...           dtype={\"A\": Integer()})\n3\n  \n>>> engine.execute(\"SELECT * FROM integers\").fetchall()\n[(1,), (None,), (2,)]","title":"pandas.reference.api.pandas.series.to_sql"},{"text":"pandas.ExcelWriter   classpandas.ExcelWriter(path, engine=None, date_format=None, datetime_format=None, mode='w', storage_options=None, if_sheet_exists=None, engine_kwargs=None, **kwargs)[source]\n \nClass for writing DataFrame objects into excel sheets. Default is to use : * xlwt for xls * xlsxwriter for xlsx if xlsxwriter is installed otherwise openpyxl * odf for ods. See DataFrame.to_excel for typical usage. The writer should be used as a context manager. Otherwise, call close() to save and close any opened file handles.  Parameters \n \npath:str or typing.BinaryIO\n\n\nPath to xls or xlsx or ods file.  \nengine:str (optional)\n\n\nEngine to use for writing. If None, defaults to io.excel.<extension>.writer. NOTE: can only be passed as a keyword argument.  Deprecated since version 1.2.0: As the xlwt package is no longer maintained, the xlwt engine will be removed in a future version of pandas.   \ndate_format:str, default None\n\n\nFormat string for dates written into Excel files (e.g. \u2018YYYY-MM-DD\u2019).  \ndatetime_format:str, default None\n\n\nFormat string for datetime objects written into Excel files. (e.g. \u2018YYYY-MM-DD HH:MM:SS\u2019).  \nmode:{\u2018w\u2019, \u2018a\u2019}, default \u2018w\u2019\n\n\nFile mode to use (write or append). Append does not work with fsspec URLs.  \nstorage_options:dict, optional\n\n\nExtra options that make sense for a particular storage connection, e.g. host, port, username, password, etc., if using a URL that will be parsed by fsspec, e.g., starting \u201cs3:\/\/\u201d, \u201cgcs:\/\/\u201d.  New in version 1.2.0.   \nif_sheet_exists:{\u2018error\u2019, \u2018new\u2019, \u2018replace\u2019, \u2018overlay\u2019}, default \u2018error\u2019\n\n\nHow to behave when trying to write to a sheet that already exists (append mode only).  error: raise a ValueError. new: Create a new sheet, with a name determined by the engine. replace: Delete the contents of the sheet before writing to it. overlay: Write contents to the existing sheet without removing the old contents.   New in version 1.3.0.   Changed in version 1.4.0: Added overlay option   \nengine_kwargs:dict, optional\n\n\nKeyword arguments to be passed into the engine. These will be passed to the following functions of the respective engines:  xlsxwriter: xlsxwriter.Workbook(file, **engine_kwargs) openpyxl (write mode): openpyxl.Workbook(**engine_kwargs) openpyxl (append mode): openpyxl.load_workbook(file, **engine_kwargs) odswriter: odf.opendocument.OpenDocumentSpreadsheet(**engine_kwargs)   New in version 1.3.0.   \n**kwargs:dict, optional\n\n\nKeyword arguments to be passed into the engine.  Deprecated since version 1.3.0: Use engine_kwargs instead.      Notes None of the methods and properties are considered public. For compatibility with CSV writers, ExcelWriter serializes lists and dicts to strings before writing. Examples Default usage: \n>>> df = pd.DataFrame([[\"ABC\", \"XYZ\"]], columns=[\"Foo\", \"Bar\"])  \n>>> with pd.ExcelWriter(\"path_to_file.xlsx\") as writer:\n...     df.to_excel(writer)  \n  To write to separate sheets in a single file: \n>>> df1 = pd.DataFrame([[\"AAA\", \"BBB\"]], columns=[\"Spam\", \"Egg\"])  \n>>> df2 = pd.DataFrame([[\"ABC\", \"XYZ\"]], columns=[\"Foo\", \"Bar\"])  \n>>> with pd.ExcelWriter(\"path_to_file.xlsx\") as writer:\n...     df1.to_excel(writer, sheet_name=\"Sheet1\")  \n...     df2.to_excel(writer, sheet_name=\"Sheet2\")  \n  You can set the date format or datetime format: \n>>> from datetime import date, datetime  \n>>> df = pd.DataFrame(\n...     [\n...         [date(2014, 1, 31), date(1999, 9, 24)],\n...         [datetime(1998, 5, 26, 23, 33, 4), datetime(2014, 2, 28, 13, 5, 13)],\n...     ],\n...     index=[\"Date\", \"Datetime\"],\n...     columns=[\"X\", \"Y\"],\n... )  \n>>> with pd.ExcelWriter(\n...     \"path_to_file.xlsx\",\n...     date_format=\"YYYY-MM-DD\",\n...     datetime_format=\"YYYY-MM-DD HH:MM:SS\"\n... ) as writer:\n...     df.to_excel(writer)  \n  You can also append to an existing Excel file: \n>>> with pd.ExcelWriter(\"path_to_file.xlsx\", mode=\"a\", engine=\"openpyxl\") as writer:\n...     df.to_excel(writer, sheet_name=\"Sheet3\")  \n  Here, the if_sheet_exists parameter can be set to replace a sheet if it already exists: \n>>> with ExcelWriter(\n...     \"path_to_file.xlsx\",\n...     mode=\"a\",\n...     engine=\"openpyxl\",\n...     if_sheet_exists=\"replace\",\n... ) as writer:\n...     df.to_excel(writer, sheet_name=\"Sheet1\")  \n  You can also write multiple DataFrames to a single sheet. Note that the if_sheet_exists parameter needs to be set to overlay: \n>>> with ExcelWriter(\"path_to_file.xlsx\",\n...     mode=\"a\",\n...     engine=\"openpyxl\",\n...     if_sheet_exists=\"overlay\",\n... ) as writer:\n...     df1.to_excel(writer, sheet_name=\"Sheet1\")\n...     df2.to_excel(writer, sheet_name=\"Sheet1\", startcol=3)  \n  You can store Excel file in RAM: \n>>> import io\n>>> df = pd.DataFrame([[\"ABC\", \"XYZ\"]], columns=[\"Foo\", \"Bar\"])\n>>> buffer = io.BytesIO()\n>>> with pd.ExcelWriter(buffer) as writer:\n...     df.to_excel(writer)\n  You can pack Excel file into zip archive: \n>>> import zipfile  \n>>> df = pd.DataFrame([[\"ABC\", \"XYZ\"]], columns=[\"Foo\", \"Bar\"])  \n>>> with zipfile.ZipFile(\"path_to_file.zip\", \"w\") as zf:\n...     with zf.open(\"filename.xlsx\", \"w\") as buffer:\n...         with pd.ExcelWriter(buffer) as writer:\n...             df.to_excel(writer)  \n  You can specify additional arguments to the underlying engine: \n>>> with pd.ExcelWriter(\n...     \"path_to_file.xlsx\",\n...     engine=\"xlsxwriter\",\n...     engine_kwargs={\"options\": {\"nan_inf_to_errors\": True}}\n... ) as writer:\n...     df.to_excel(writer)  \n  In append mode, engine_kwargs are passed through to openpyxl\u2019s load_workbook: \n>>> with pd.ExcelWriter(\n...     \"path_to_file.xlsx\",\n...     engine=\"openpyxl\",\n...     mode=\"a\",\n...     engine_kwargs={\"keep_vba\": True}\n... ) as writer:\n...     df.to_excel(writer, sheet_name=\"Sheet2\")  \n  Attributes       \nNone     Methods       \nNone","title":"pandas.reference.api.pandas.excelwriter"},{"text":"renorm_(p, dim, maxnorm) \u2192 Tensor  \nIn-place version of renorm()","title":"torch.tensors#torch.Tensor.renorm_"},{"text":"pandas.read_sql_query   pandas.read_sql_query(sql, con, index_col=None, coerce_float=True, params=None, parse_dates=None, chunksize=None, dtype=None)[source]\n \nRead SQL query into a DataFrame. Returns a DataFrame corresponding to the result set of the query string. Optionally provide an index_col parameter to use one of the columns as the index, otherwise default integer index will be used.  Parameters \n \nsql:str SQL query or SQLAlchemy Selectable (select or text object)\n\n\nSQL query to be executed.  \ncon:SQLAlchemy connectable, str, or sqlite3 connection\n\n\nUsing SQLAlchemy makes it possible to use any DB supported by that library. If a DBAPI2 object, only sqlite3 is supported.  \nindex_col:str or list of str, optional, default: None\n\n\nColumn(s) to set as index(MultiIndex).  \ncoerce_float:bool, default True\n\n\nAttempts to convert values of non-string, non-numeric objects (like decimal.Decimal) to floating point. Useful for SQL result sets.  \nparams:list, tuple or dict, optional, default: None\n\n\nList of parameters to pass to execute method. The syntax used to pass parameters is database driver dependent. Check your database driver documentation for which of the five syntax styles, described in PEP 249\u2019s paramstyle, is supported. Eg. for psycopg2, uses %(name)s so use params={\u2018name\u2019 : \u2018value\u2019}.  \nparse_dates:list or dict, default: None\n\n\n List of column names to parse as dates. Dict of {column_name: format string} where format string is strftime compatible in case of parsing string times, or is one of (D, s, ns, ms, us) in case of parsing integer timestamps. Dict of {column_name: arg dict}, where the arg dict corresponds to the keyword arguments of pandas.to_datetime() Especially useful with databases without native Datetime support, such as SQLite.   \nchunksize:int, default None\n\n\nIf specified, return an iterator where chunksize is the number of rows to include in each chunk.  \ndtype:Type name or dict of columns\n\n\nData type for data or columns. E.g. np.float64 or {\u2018a\u2019: np.float64, \u2018b\u2019: np.int32, \u2018c\u2019: \u2018Int64\u2019}.  New in version 1.3.0.     Returns \n DataFrame or Iterator[DataFrame]\n    See also  read_sql_table\n\nRead SQL database table into a DataFrame.  read_sql\n\nRead SQL query or database table into a DataFrame.    Notes Any datetime values with time zone information parsed via the parse_dates parameter will be converted to UTC.","title":"pandas.reference.api.pandas.read_sql_query"},{"text":"pandas.DataFrame.to_gbq   DataFrame.to_gbq(destination_table, project_id=None, chunksize=None, reauth=False, if_exists='fail', auth_local_webserver=False, table_schema=None, location=None, progress_bar=True, credentials=None)[source]\n \nWrite a DataFrame to a Google BigQuery table. This function requires the pandas-gbq package. See the How to authenticate with Google BigQuery guide for authentication instructions.  Parameters \n \ndestination_table:str\n\n\nName of table to be written, in the form dataset.tablename.  \nproject_id:str, optional\n\n\nGoogle BigQuery Account project ID. Optional when available from the environment.  \nchunksize:int, optional\n\n\nNumber of rows to be inserted in each chunk from the dataframe. Set to None to load the whole dataframe at once.  \nreauth:bool, default False\n\n\nForce Google BigQuery to re-authenticate the user. This is useful if multiple accounts are used.  \nif_exists:str, default \u2018fail\u2019\n\n\nBehavior when the destination table exists. Value can be one of:  'fail'\n\nIf table exists raise pandas_gbq.gbq.TableCreationError.  'replace'\n\nIf table exists, drop it, recreate it, and insert data.  'append'\n\nIf table exists, insert data. Create if does not exist.    \nauth_local_webserver:bool, default False\n\n\nUse the local webserver flow instead of the console flow when getting user credentials. New in version 0.2.0 of pandas-gbq.  \ntable_schema:list of dicts, optional\n\n\nList of BigQuery table fields to which according DataFrame columns conform to, e.g. [{'name': 'col1', 'type':\n'STRING'},...]. If schema is not provided, it will be generated according to dtypes of DataFrame columns. See BigQuery API documentation on available names of a field. New in version 0.3.1 of pandas-gbq.  \nlocation:str, optional\n\n\nLocation where the load job should run. See the BigQuery locations documentation for a list of available locations. The location must match that of the target dataset. New in version 0.5.0 of pandas-gbq.  \nprogress_bar:bool, default True\n\n\nUse the library tqdm to show the progress bar for the upload, chunk by chunk. New in version 0.5.0 of pandas-gbq.  \ncredentials:google.auth.credentials.Credentials, optional\n\n\nCredentials for accessing Google APIs. Use this parameter to override default credentials, such as to use Compute Engine google.auth.compute_engine.Credentials or Service Account google.oauth2.service_account.Credentials directly. New in version 0.8.0 of pandas-gbq.      See also  pandas_gbq.to_gbq\n\nThis function in the pandas-gbq library.  read_gbq\n\nRead a DataFrame from Google BigQuery.","title":"pandas.reference.api.pandas.dataframe.to_gbq"},{"text":"renorm(p, dim, maxnorm) \u2192 Tensor  \nSee torch.renorm()","title":"torch.tensors#torch.Tensor.renorm"},{"text":"pandas.read_sql   pandas.read_sql(sql, con, index_col=None, coerce_float=True, params=None, parse_dates=None, columns=None, chunksize=None)[source]\n \nRead SQL query or database table into a DataFrame. This function is a convenience wrapper around read_sql_table and read_sql_query (for backward compatibility). It will delegate to the specific function depending on the provided input. A SQL query will be routed to read_sql_query, while a database table name will be routed to read_sql_table. Note that the delegated function might have more specific notes about their functionality not listed here.  Parameters \n \nsql:str or SQLAlchemy Selectable (select or text object)\n\n\nSQL query to be executed or a table name.  \ncon:SQLAlchemy connectable, str, or sqlite3 connection\n\n\nUsing SQLAlchemy makes it possible to use any DB supported by that library. If a DBAPI2 object, only sqlite3 is supported. The user is responsible for engine disposal and connection closure for the SQLAlchemy connectable; str connections are closed automatically. See here.  \nindex_col:str or list of str, optional, default: None\n\n\nColumn(s) to set as index(MultiIndex).  \ncoerce_float:bool, default True\n\n\nAttempts to convert values of non-string, non-numeric objects (like decimal.Decimal) to floating point, useful for SQL result sets.  \nparams:list, tuple or dict, optional, default: None\n\n\nList of parameters to pass to execute method. The syntax used to pass parameters is database driver dependent. Check your database driver documentation for which of the five syntax styles, described in PEP 249\u2019s paramstyle, is supported. Eg. for psycopg2, uses %(name)s so use params={\u2018name\u2019 : \u2018value\u2019}.  \nparse_dates:list or dict, default: None\n\n\n List of column names to parse as dates. Dict of {column_name: format string} where format string is strftime compatible in case of parsing string times, or is one of (D, s, ns, ms, us) in case of parsing integer timestamps. Dict of {column_name: arg dict}, where the arg dict corresponds to the keyword arguments of pandas.to_datetime() Especially useful with databases without native Datetime support, such as SQLite.   \ncolumns:list, default: None\n\n\nList of column names to select from SQL table (only used when reading a table).  \nchunksize:int, default None\n\n\nIf specified, return an iterator where chunksize is the number of rows to include in each chunk.    Returns \n DataFrame or Iterator[DataFrame]\n    See also  read_sql_table\n\nRead SQL database table into a DataFrame.  read_sql_query\n\nRead SQL query into a DataFrame.    Examples Read data from SQL via either a SQL query or a SQL tablename. When using a SQLite database only SQL queries are accepted, providing only the SQL tablename will result in an error. \n>>> from sqlite3 import connect\n>>> conn = connect(':memory:')\n>>> df = pd.DataFrame(data=[[0, '10\/11\/12'], [1, '12\/11\/10']],\n...                   columns=['int_column', 'date_column'])\n>>> df.to_sql('test_data', conn)\n2\n  \n>>> pd.read_sql('SELECT int_column, date_column FROM test_data', conn)\n   int_column date_column\n0           0    10\/11\/12\n1           1    12\/11\/10\n  \n>>> pd.read_sql('test_data', 'postgres:\/\/\/db_name')  \n  Apply date parsing to columns through the parse_dates argument \n>>> pd.read_sql('SELECT int_column, date_column FROM test_data',\n...             conn,\n...             parse_dates=[\"date_column\"])\n   int_column date_column\n0           0  2012-10-11\n1           1  2010-12-11\n  The parse_dates argument calls pd.to_datetime on the provided columns. Custom argument values for applying pd.to_datetime on a column are specified via a dictionary format: 1. Ignore errors while parsing the values of \u201cdate_column\u201d \n>>> pd.read_sql('SELECT int_column, date_column FROM test_data',\n...             conn,\n...             parse_dates={\"date_column\": {\"errors\": \"ignore\"}})\n   int_column date_column\n0           0  2012-10-11\n1           1  2010-12-11\n   Apply a dayfirst date parsing order on the values of \u201cdate_column\u201d  \n>>> pd.read_sql('SELECT int_column, date_column FROM test_data',\n...             conn,\n...             parse_dates={\"date_column\": {\"dayfirst\": True}})\n   int_column date_column\n0           0  2012-11-10\n1           1  2010-11-12\n   Apply custom formatting when date parsing the values of \u201cdate_column\u201d  \n>>> pd.read_sql('SELECT int_column, date_column FROM test_data',\n...             conn,\n...             parse_dates={\"date_column\": {\"format\": \"%d\/%m\/%y\"}})\n   int_column date_column\n0           0  2012-11-10\n1           1  2010-11-12","title":"pandas.reference.api.pandas.read_sql"},{"text":"pandas.read_sql_table   pandas.read_sql_table(table_name, con, schema=None, index_col=None, coerce_float=True, parse_dates=None, columns=None, chunksize=None)[source]\n \nRead SQL database table into a DataFrame. Given a table name and a SQLAlchemy connectable, returns a DataFrame. This function does not support DBAPI connections.  Parameters \n \ntable_name:str\n\n\nName of SQL table in database.  \ncon:SQLAlchemy connectable or str\n\n\nA database URI could be provided as str. SQLite DBAPI connection mode not supported.  \nschema:str, default None\n\n\nName of SQL schema in database to query (if database flavor supports this). Uses default schema if None (default).  \nindex_col:str or list of str, optional, default: None\n\n\nColumn(s) to set as index(MultiIndex).  \ncoerce_float:bool, default True\n\n\nAttempts to convert values of non-string, non-numeric objects (like decimal.Decimal) to floating point. Can result in loss of Precision.  \nparse_dates:list or dict, default None\n\n\n List of column names to parse as dates. Dict of {column_name: format string} where format string is strftime compatible in case of parsing string times or is one of (D, s, ns, ms, us) in case of parsing integer timestamps. Dict of {column_name: arg dict}, where the arg dict corresponds to the keyword arguments of pandas.to_datetime() Especially useful with databases without native Datetime support, such as SQLite.   \ncolumns:list, default None\n\n\nList of column names to select from SQL table.  \nchunksize:int, default None\n\n\nIf specified, returns an iterator where chunksize is the number of rows to include in each chunk.    Returns \n DataFrame or Iterator[DataFrame]\n\nA SQL table is returned as two-dimensional data structure with labeled axes.      See also  read_sql_query\n\nRead SQL query into a DataFrame.  read_sql\n\nRead SQL query or database table into a DataFrame.    Notes Any datetime values with time zone information will be converted to UTC. Examples \n>>> pd.read_sql_table('table_name', 'postgres:\/\/\/db_name')","title":"pandas.reference.api.pandas.read_sql_table"},{"text":"pandas.DataFrame.to_excel   DataFrame.to_excel(excel_writer, sheet_name='Sheet1', na_rep='', float_format=None, columns=None, header=True, index=True, index_label=None, startrow=0, startcol=0, engine=None, merge_cells=True, encoding=None, inf_rep='inf', verbose=True, freeze_panes=None, storage_options=None)[source]\n \nWrite object to an Excel sheet. To write a single object to an Excel .xlsx file it is only necessary to specify a target file name. To write to multiple sheets it is necessary to create an ExcelWriter object with a target file name, and specify a sheet in the file to write to. Multiple sheets may be written to by specifying unique sheet_name. With all data written to the file it is necessary to save the changes. Note that creating an ExcelWriter object with a file name that already exists will result in the contents of the existing file being erased.  Parameters \n \nexcel_writer:path-like, file-like, or ExcelWriter object\n\n\nFile path or existing ExcelWriter.  \nsheet_name:str, default \u2018Sheet1\u2019\n\n\nName of sheet which will contain DataFrame.  \nna_rep:str, default \u2018\u2019\n\n\nMissing data representation.  \nfloat_format:str, optional\n\n\nFormat string for floating point numbers. For example float_format=\"%.2f\" will format 0.1234 to 0.12.  \ncolumns:sequence or list of str, optional\n\n\nColumns to write.  \nheader:bool or list of str, default True\n\n\nWrite out the column names. If a list of string is given it is assumed to be aliases for the column names.  \nindex:bool, default True\n\n\nWrite row names (index).  \nindex_label:str or sequence, optional\n\n\nColumn label for index column(s) if desired. If not specified, and header and index are True, then the index names are used. A sequence should be given if the DataFrame uses MultiIndex.  \nstartrow:int, default 0\n\n\nUpper left cell row to dump data frame.  \nstartcol:int, default 0\n\n\nUpper left cell column to dump data frame.  \nengine:str, optional\n\n\nWrite engine to use, \u2018openpyxl\u2019 or \u2018xlsxwriter\u2019. You can also set this via the options io.excel.xlsx.writer, io.excel.xls.writer, and io.excel.xlsm.writer.  Deprecated since version 1.2.0: As the xlwt package is no longer maintained, the xlwt engine will be removed in a future version of pandas.   \nmerge_cells:bool, default True\n\n\nWrite MultiIndex and Hierarchical Rows as merged cells.  \nencoding:str, optional\n\n\nEncoding of the resulting excel file. Only necessary for xlwt, other writers support unicode natively.  \ninf_rep:str, default \u2018inf\u2019\n\n\nRepresentation for infinity (there is no native representation for infinity in Excel).  \nverbose:bool, default True\n\n\nDisplay more information in the error logs.  \nfreeze_panes:tuple of int (length 2), optional\n\n\nSpecifies the one-based bottommost row and rightmost column that is to be frozen.  \nstorage_options:dict, optional\n\n\nExtra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib as header options. For other URLs (e.g. starting with \u201cs3:\/\/\u201d, and \u201cgcs:\/\/\u201d) the key-value pairs are forwarded to fsspec. Please see fsspec and urllib for more details.  New in version 1.2.0.       See also  to_csv\n\nWrite DataFrame to a comma-separated values (csv) file.  ExcelWriter\n\nClass for writing DataFrame objects into excel sheets.  read_excel\n\nRead an Excel file into a pandas DataFrame.  read_csv\n\nRead a comma-separated values (csv) file into DataFrame.    Notes For compatibility with to_csv(), to_excel serializes lists and dicts to strings before writing. Once a workbook has been saved it is not possible to write further data without rewriting the whole workbook. Examples Create, write to and save a workbook: \n>>> df1 = pd.DataFrame([['a', 'b'], ['c', 'd']],\n...                    index=['row 1', 'row 2'],\n...                    columns=['col 1', 'col 2'])\n>>> df1.to_excel(\"output.xlsx\")  \n  To specify the sheet name: \n>>> df1.to_excel(\"output.xlsx\",\n...              sheet_name='Sheet_name_1')  \n  If you wish to write to more than one sheet in the workbook, it is necessary to specify an ExcelWriter object: \n>>> df2 = df1.copy()\n>>> with pd.ExcelWriter('output.xlsx') as writer:  \n...     df1.to_excel(writer, sheet_name='Sheet_name_1')\n...     df2.to_excel(writer, sheet_name='Sheet_name_2')\n  ExcelWriter can also be used to append to an existing Excel file: \n>>> with pd.ExcelWriter('output.xlsx',\n...                     mode='a') as writer:  \n...     df.to_excel(writer, sheet_name='Sheet_name_3')\n  To set the library that is used to write the Excel file, you can pass the engine keyword (the default engine is automatically chosen depending on the file extension): \n>>> df1.to_excel('output1.xlsx', engine='xlsxwriter')","title":"pandas.reference.api.pandas.dataframe.to_excel"}]}
{"task_id":30766151,"prompt":"def f_30766151(s):\n\treturn ","suffix":"","canonical_solution":"re.sub('[^(){}[\\]]', '', s)","test_start":"\nimport re\n\ndef check(candidate):","test":["\n    assert candidate(\"(a(vdwvndw){}]\") == \"((){}]\"\n","\n    assert candidate(\"12345\") == \"\"\n"],"entry_point":"f_30766151","intent":"Extract brackets from string `s`","library":["re"],"docs":[{"text":"get_breaks(filename, lineno)  \nReturn all breakpoints for lineno in filename, or an empty list if none are set.","title":"python.library.bdb#bdb.Bdb.get_breaks"},{"text":"get_file_breaks(filename)  \nReturn all breakpoints in filename, or an empty list if none are set.","title":"python.library.bdb#bdb.Bdb.get_file_breaks"},{"text":"doctest.script_from_examples(s)  \nConvert text with examples to a script. Argument s is a string containing doctest examples. The string is converted to a Python script, where doctest examples in s are converted to regular code, and everything else is converted to Python comments. The generated script is returned as a string. For example, import doctest\nprint(doctest.script_from_examples(r\"\"\"\n    Set x and y to 1 and 2.\n    >>> x, y = 1, 2\n\n    Print their sum:\n    >>> print(x+y)\n    3\n\"\"\"))\n displays: # Set x and y to 1 and 2.\nx, y = 1, 2\n#\n# Print their sum:\nprint(x+y)\n# Expected:\n## 3\n This function is used internally by other functions (see below), but can also be useful when you want to transform an interactive Python session into a Python script.","title":"python.library.doctest#doctest.script_from_examples"},{"text":"token.RSQB  \nToken value for \"]\".","title":"python.library.token#token.RSQB"},{"text":"get_examples(string, name='<string>')  \nExtract all doctest examples from the given string, and return them as a list of Example objects. Line numbers are 0-based. The optional argument name is a name identifying this string, and is only used for error messages.","title":"python.library.doctest#doctest.DocTestParser.get_examples"},{"text":"email.utils.unquote(str)  \nReturn a new string which is an unquoted version of str. If str ends and begins with double quotes, they are stripped off. Likewise if str ends and begins with angle brackets, they are stripped off.","title":"python.library.email.utils#email.utils.unquote"},{"text":"shlex.split(s, comments=False, posix=True)  \nSplit the string s using shell-like syntax. If comments is False (the default), the parsing of comments in the given string will be disabled (setting the commenters attribute of the shlex instance to the empty string). This function operates in POSIX mode by default, but uses non-POSIX mode if the posix argument is false.  Note Since the split() function instantiates a shlex instance, passing None for s will read the string to split from standard input.   Deprecated since version 3.9: Passing None for s will raise an exception in future Python versions.","title":"python.library.shlex#shlex.split"},{"text":"raw_decode(s)  \nDecode a JSON document from s (a str beginning with a JSON document) and return a 2-tuple of the Python representation and the index in s where the document ended. This can be used to decode a JSON document from a string that may have extraneous data at the end.","title":"python.library.json#json.JSONDecoder.raw_decode"},{"text":"matplotlib.backends.backend_ps.quote_ps_string(s)[source]\n \nQuote dangerous characters of S for use in a PostScript string constant.","title":"matplotlib.backend_ps_api#matplotlib.backends.backend_ps.quote_ps_string"},{"text":"parse(string, name='<string>')  \nDivide the given string into examples and intervening text, and return them as a list of alternating Examples and strings. Line numbers for the Examples are 0-based. The optional argument name is a name identifying this string, and is only used for error messages.","title":"python.library.doctest#doctest.DocTestParser.parse"}]}
{"task_id":1143379,"prompt":"def f_1143379(L):\n\treturn ","suffix":"","canonical_solution":"list(dict((x[0], x) for x in L).values())","test_start":"\ndef check(candidate):","test":["\n    L = [['14', '65', 76], ['2', '5', 6], ['7', '12', 33], ['14', '22', 46]]\n    res = [['14', '22', 46], ['2', '5', 6], ['7', '12', 33]]\n    assert(candidate(L) == res)\n","\n    assert candidate([\"a\", \"aa\", \"abc\", \"bac\"]) == [\"abc\", \"bac\"]\n"],"entry_point":"f_1143379","intent":"remove duplicate elements from list 'L'","library":[],"docs":[]}
{"task_id":12330522,"prompt":"def f_12330522(file):\n\treturn ","suffix":"","canonical_solution":"[line.rstrip('\\n') for line in file]","test_start":"\ndef check(candidate):","test":["\n    res = ['1', '2', '3']\n    f = open(\"myfile.txt\", \"a\")\n    f.write(\"1\\n2\\n3\")\n    f.close()\n    f = open(\"myfile.txt\", \"r\")\n    assert candidate(f) == res\n"],"entry_point":"f_12330522","intent":"read a file `file` without newlines","library":[],"docs":[]}
{"task_id":364621,"prompt":"def f_364621(testlist):\n\treturn ","suffix":"","canonical_solution":"[i for (i, x) in enumerate(testlist) if (x == 1)]","test_start":"\ndef check(candidate):","test":["\n    testlist = [1,2,3,5,3,1,2,1,6]\n    assert candidate(testlist) == [0, 5, 7]\n","\n    testlist = [0, -1]\n    assert candidate(testlist) == []\n"],"entry_point":"f_364621","intent":"get the position of item 1 in `testlist`","library":[],"docs":[]}
{"task_id":364621,"prompt":"def f_364621(testlist):\n\treturn ","suffix":"","canonical_solution":"[i for (i, x) in enumerate(testlist) if (x == 1)]","test_start":"\ndef check(candidate):","test":["\n    testlist = [1,2,3,5,3,1,2,1,6]\n    assert candidate(testlist) == [0, 5, 7]\n","\n    testlist = [0, -1]\n    assert candidate(testlist) == []\n"],"entry_point":"f_364621","intent":"get the position of item 1 in `testlist`","library":[],"docs":[]}
{"task_id":364621,"prompt":"def f_364621(testlist, element):\n\treturn ","suffix":"","canonical_solution":"testlist.index(element)","test_start":"\ndef check(candidate):","test":["\n    testlist = [1,2,3,5,3,1,2,1,6]\n    assert candidate(testlist, 1) == 0\n","\n    testlist = [1,2,3,5,3,1,2,1,6]\n    try:\n        candidate(testlist, 14)\n    except:\n        assert True\n"],"entry_point":"f_364621","intent":"get the position of item `element` in list `testlist`","library":[],"docs":[]}
{"task_id":13145368,"prompt":"def f_13145368(lis):\n\treturn ","suffix":"","canonical_solution":"max(lis, key=lambda item: item[1])[0]","test_start":"\ndef check(candidate):","test":["\n    lis = [(101, 153), (255, 827), (361, 961)]\n    assert candidate(lis) == 361\n"],"entry_point":"f_13145368","intent":"find the first element of the tuple with the maximum second element in a list of tuples `lis`","library":[],"docs":[]}
{"task_id":13145368,"prompt":"def f_13145368(lis):\n\treturn ","suffix":"","canonical_solution":"max(lis, key=itemgetter(1))[0]","test_start":"\nfrom operator import itemgetter \n\ndef check(candidate):","test":["\n    lis = [(101, 153), (255, 827), (361, 961)]\n    assert candidate(lis) == 361\n"],"entry_point":"f_13145368","intent":"get the item at index 0 from the tuple that has maximum value at index 1 in list `lis`","library":["operator"],"docs":[{"text":"max(iterable, *[, key, default])  \nmax(arg1, arg2, *args[, key])  \nReturn the largest item in an iterable or the largest of two or more arguments. If one positional argument is provided, it should be an iterable. The largest item in the iterable is returned. If two or more positional arguments are provided, the largest of the positional arguments is returned. There are two optional keyword-only arguments. The key argument specifies a one-argument ordering function like that used for list.sort(). The default argument specifies an object to return if the provided iterable is empty. If the iterable is empty and default is not provided, a ValueError is raised. If multiple items are maximal, the function returns the first one encountered. This is consistent with other sort-stability preserving tools such as sorted(iterable, key=keyfunc, reverse=True)[0] and heapq.nlargest(1, iterable, key=keyfunc).  New in version 3.4: The default keyword-only argument.   Changed in version 3.8: The key can be None.","title":"python.library.functions#max"},{"text":"values_list(*fields, flat=False, named=False)","title":"django.ref.models.querysets#django.db.models.query.QuerySet.values_list"},{"text":"calendar.timegm(tuple)  \nAn unrelated but handy function that takes a time tuple such as returned by the gmtime() function in the time module, and returns the corresponding Unix timestamp value, assuming an epoch of 1970, and the POSIX encoding. In fact, time.gmtime() and timegm() are each others\u2019 inverse.","title":"python.library.calendar#calendar.timegm"},{"text":"update() \n Sets the elements of the color update(r, g, b) -> None update(r, g, b, a=255) -> None update(color_value) -> None  Sets the elements of the color. See parameters for pygame.Color() for the parameters of this function. If the alpha value was not set it will not change.  New in pygame 2.0.1.","title":"pygame.ref.color#pygame.Color.update"},{"text":"types.prepare_class(name, bases=(), kwds=None)  \nCalculates the appropriate metaclass and creates the class namespace. The arguments are the components that make up a class definition header: the class name, the base classes (in order) and the keyword arguments (such as metaclass). The return value is a 3-tuple: metaclass, namespace, kwds metaclass is the appropriate metaclass, namespace is the prepared class namespace and kwds is an updated copy of the passed in kwds argument with any 'metaclass' entry removed. If no kwds argument is passed in, this will be an empty dict.  New in version 3.3.   Changed in version 3.6: The default value for the namespace element of the returned tuple has changed. Now an insertion-order-preserving mapping is used when the metaclass does not have a __prepare__ method.","title":"python.library.types#types.prepare_class"},{"text":"numpy.transpose   numpy.transpose(a, axes=None)[source]\n \nReverse or permute the axes of an array; returns the modified array. For an array a with two axes, transpose(a) gives the matrix transpose. Refer to numpy.ndarray.transpose for full documentation.  Parameters \n \naarray_like\n\n\nInput array.  \naxestuple or list of ints, optional\n\n\nIf specified, it must be a tuple or list which contains a permutation of [0,1,..,N-1] where N is the number of axes of a. The i\u2019th axis of the returned array will correspond to the axis numbered axes[i] of the input. If not specified, defaults to range(a.ndim)[::-1], which reverses the order of the axes.    Returns \n \npndarray\n\n\na with its axes permuted. A view is returned whenever possible.      See also  ndarray.transpose\n\nEquivalent method  moveaxis\nargsort\n  Notes Use transpose(a, argsort(axes)) to invert the transposition of tensors when using the axes keyword argument. Transposing a 1-D array returns an unchanged view of the original array. Examples >>> x = np.arange(4).reshape((2,2))\n>>> x\narray([[0, 1],\n       [2, 3]])\n >>> np.transpose(x)\narray([[0, 2],\n       [1, 3]])\n >>> x = np.ones((1, 2, 3))\n>>> np.transpose(x, (1, 0, 2)).shape\n(2, 1, 3)\n >>> x = np.ones((2, 3, 4, 5))\n>>> np.transpose(x).shape\n(5, 4, 3, 2)","title":"numpy.reference.generated.numpy.transpose"},{"text":"tf.compat.v1.nn.rnn_cell.LSTMStateTuple Tuple used by LSTM Cells for state_size, zero_state, and output state. \ntf.compat.v1.nn.rnn_cell.LSTMStateTuple(\n    c, h\n)\n Stores two elements: (c, h), in that order. Where c is the hidden state and h is the output. Only used when state_is_tuple=True.\n \n\n\n Attributes\n  c  \n \n  h  \n \n  dtype","title":"tensorflow.compat.v1.nn.rnn_cell.lstmstatetuple"},{"text":"numpy.ndarray.transpose method   ndarray.transpose(*axes)\n \nReturns a view of the array with axes transposed. For a 1-D array this has no effect, as a transposed vector is simply the same vector. To convert a 1-D array into a 2D column vector, an additional dimension must be added. np.atleast2d(a).T achieves this, as does a[:, np.newaxis]. For a 2-D array, this is a standard matrix transpose. For an n-D array, if axes are given, their order indicates how the axes are permuted (see Examples). If axes are not provided and a.shape = (i[0], i[1], ... i[n-2], i[n-1]), then a.transpose().shape = (i[n-1], i[n-2], ... i[1], i[0]).  Parameters \n \naxesNone, tuple of ints, or n ints\n\n\n None or no argument: reverses the order of the axes. tuple of ints: i in the j-th place in the tuple means a\u2019s i-th axis becomes a.transpose()\u2019s j-th axis. \nn ints: same as an n-tuple of the same ints (this form is intended simply as a \u201cconvenience\u201d alternative to the tuple form)     Returns \n \noutndarray\n\n\nView of a, with axes suitably permuted.      See also  transpose\n\nEquivalent function  ndarray.T\n\nArray property returning the array transposed.  ndarray.reshape\n\nGive a new shape to an array without changing its data.    Examples >>> a = np.array([[1, 2], [3, 4]])\n>>> a\narray([[1, 2],\n       [3, 4]])\n>>> a.transpose()\narray([[1, 3],\n       [2, 4]])\n>>> a.transpose((1, 0))\narray([[1, 3],\n       [2, 4]])\n>>> a.transpose(1, 0)\narray([[1, 3],\n       [2, 4]])","title":"numpy.reference.generated.numpy.ndarray.transpose"},{"text":"update_lim(axes)[source]","title":"matplotlib._as_gen.mpl_toolkits.axisartist.axislines.gridhelperbase#mpl_toolkits.axisartist.axislines.GridHelperBase.update_lim"},{"text":"numpy.recarray.transpose method   recarray.transpose(*axes)\n \nReturns a view of the array with axes transposed. For a 1-D array this has no effect, as a transposed vector is simply the same vector. To convert a 1-D array into a 2D column vector, an additional dimension must be added. np.atleast2d(a).T achieves this, as does a[:, np.newaxis]. For a 2-D array, this is a standard matrix transpose. For an n-D array, if axes are given, their order indicates how the axes are permuted (see Examples). If axes are not provided and a.shape = (i[0], i[1], ... i[n-2], i[n-1]), then a.transpose().shape = (i[n-1], i[n-2], ... i[1], i[0]).  Parameters \n \naxesNone, tuple of ints, or n ints\n\n\n None or no argument: reverses the order of the axes. tuple of ints: i in the j-th place in the tuple means a\u2019s i-th axis becomes a.transpose()\u2019s j-th axis. \nn ints: same as an n-tuple of the same ints (this form is intended simply as a \u201cconvenience\u201d alternative to the tuple form)     Returns \n \noutndarray\n\n\nView of a, with axes suitably permuted.      See also  transpose\n\nEquivalent function  ndarray.T\n\nArray property returning the array transposed.  ndarray.reshape\n\nGive a new shape to an array without changing its data.    Examples >>> a = np.array([[1, 2], [3, 4]])\n>>> a\narray([[1, 2],\n       [3, 4]])\n>>> a.transpose()\narray([[1, 3],\n       [2, 4]])\n>>> a.transpose((1, 0))\narray([[1, 3],\n       [2, 4]])\n>>> a.transpose(1, 0)\narray([[1, 3],\n       [2, 4]])","title":"numpy.reference.generated.numpy.recarray.transpose"}]}
{"task_id":2689189,"prompt":"def f_2689189():\n\t","suffix":"\n\treturn ","canonical_solution":"time.sleep(1)","test_start":"\nimport time\n\ndef check(candidate):","test":["\n    t1 = time.time()\n    candidate()\n    t2 = time.time()\n    assert t2 - t1 > 1\n"],"entry_point":"f_2689189","intent":"Make a delay of 1 second","library":["time"],"docs":[{"text":"coroutine asyncio.sleep(delay, result=None, *, loop=None)  \nBlock for delay seconds. If result is provided, it is returned to the caller when the coroutine completes. sleep() always suspends the current task, allowing other tasks to run.  Deprecated since version 3.8, will be removed in version 3.10: The loop parameter.  Example of coroutine displaying the current date every second for 5 seconds: import asyncio\nimport datetime\n\nasync def display_date():\n    loop = asyncio.get_running_loop()\n    end_time = loop.time() + 5.0\n    while True:\n        print(datetime.datetime.now())\n        if (loop.time() + 1.0) >= end_time:\n            break\n        await asyncio.sleep(1)\n\nasyncio.run(display_date())","title":"python.library.asyncio-task#asyncio.sleep"},{"text":"curses.delay_output(ms)  \nInsert an ms millisecond pause in output.","title":"python.library.curses#curses.delay_output"},{"text":"turtle.delay(delay=None)  \n Parameters \ndelay \u2013 positive integer   Set or return the drawing delay in milliseconds. (This is approximately the time interval between two consecutive canvas updates.) The longer the drawing delay, the slower the animation. Optional argument: >>> screen.delay()\n10\n>>> screen.delay(5)\n>>> screen.delay()\n5","title":"python.library.turtle#turtle.delay"},{"text":"pygame.time.delay() \n pause the program for an amount of time delay(milliseconds) -> time  Will pause for a given number of milliseconds. This function will use the processor (rather than sleeping) in order to make the delay more accurate than pygame.time.wait(). This returns the actual number of milliseconds used.","title":"pygame.ref.time#pygame.time.delay"},{"text":"time.second  \nIn range(60).","title":"python.library.datetime#datetime.time.second"},{"text":"datetime.second  \nIn range(60).","title":"python.library.datetime#datetime.datetime.second"},{"text":"pandas.tseries.offsets.Second.onOffset   Second.onOffset()","title":"pandas.reference.api.pandas.tseries.offsets.second.onoffset"},{"text":"pandas.tseries.offsets.Second.__call__   Second.__call__(*args, **kwargs)\n \nCall self as a function.","title":"pandas.reference.api.pandas.tseries.offsets.second.__call__"},{"text":"pandas.tseries.offsets.Second   classpandas.tseries.offsets.Second\n \nAttributes       \nbase Returns a copy of the calling offset object with n=1 and all other attributes equal.          \ndelta   \nfreqstr   \nkwds   \nn   \nname   \nnanos   \nnormalize   \nrule_code     Methods       \n__call__(*args, **kwargs) Call self as a function.  \nrollback Roll provided date backward to next offset only if not on offset.  \nrollforward Roll provided date forward to next offset only if not on offset.          \napply   \napply_index   \ncopy   \nisAnchored   \nis_anchored   \nis_month_end   \nis_month_start   \nis_on_offset   \nis_quarter_end   \nis_quarter_start   \nis_year_end   \nis_year_start   \nonOffset","title":"pandas.reference.api.pandas.tseries.offsets.second"},{"text":"rollback()  \nThis method rolls back any changes to the database since the last call to commit().","title":"python.library.sqlite3#sqlite3.Connection.rollback"}]}
{"task_id":12485244,"prompt":"def f_12485244(L):\n\treturn ","suffix":"","canonical_solution":"\"\"\", \"\"\".join('(' + ', '.join(i) + ')' for i in L)","test_start":"\ndef check(candidate):","test":["\n    L = [(\"abc\", \"def\"), (\"hij\", \"klm\")]\n    assert candidate(L) == '(abc, def), (hij, klm)'\n"],"entry_point":"f_12485244","intent":"convert list of tuples `L` to a string","library":[],"docs":[]}
{"task_id":755857,"prompt":"def f_755857():\n\t","suffix":"\n\treturn b","canonical_solution":"b = models.CharField(max_length=7, default='0000000', editable=False)","test_start":"\nfrom django.db import models\n\ndef check(candidate):","test":["\n    assert candidate().get_default() == '0000000'\n"],"entry_point":"f_755857","intent":"Django set default value of field `b` equal to '0000000'","library":["django"],"docs":[{"text":"Field.default","title":"django.ref.models.fields#django.db.models.Field.default"},{"text":"class AddField(model_name, name, field, preserve_default=True)","title":"django.ref.migration-operations#django.db.migrations.operations.AddField"},{"text":"Field.initial","title":"django.ref.forms.fields#django.forms.Field.initial"},{"text":"class AlterField(model_name, name, field, preserve_default=True)","title":"django.ref.migration-operations#django.db.migrations.operations.AlterField"},{"text":"Field.blank","title":"django.ref.models.fields#django.db.models.Field.blank"},{"text":"Field.null","title":"django.ref.models.fields#django.db.models.Field.null"},{"text":"SET_DEFAULT  \nSet the ForeignKey to its default value; a default for the ForeignKey must be set.","title":"django.ref.models.fields#django.db.models.SET_DEFAULT"},{"text":"class BinaryField(max_length=None, **options)","title":"django.ref.models.fields#django.db.models.BinaryField"},{"text":"class BigAutoField(**options)","title":"django.ref.models.fields#django.db.models.BigAutoField"},{"text":"Serializer fields  Each field in a Form class is responsible not only for validating data, but also for \"cleaning\" it \u2014 normalizing it to a consistent format. \u2014 Django documentation  Serializer fields handle converting between primitive values and internal datatypes. They also deal with validating input values, as well as retrieving and setting the values from their parent objects. Note: The serializer fields are declared in fields.py, but by convention you should import them using from rest_framework import serializers and refer to fields as serializers.<FieldName>. Core arguments Each serializer field class constructor takes at least these arguments. Some Field classes take additional, field-specific arguments, but the following should always be accepted: read_only Read-only fields are included in the API output, but should not be included in the input during create or update operations. Any 'read_only' fields that are incorrectly included in the serializer input will be ignored. Set this to True to ensure that the field is used when serializing a representation, but is not used when creating or updating an instance during deserialization. Defaults to False write_only Set this to True to ensure that the field may be used when updating or creating an instance, but is not included when serializing the representation. Defaults to False required Normally an error will be raised if a field is not supplied during deserialization. Set to false if this field is not required to be present during deserialization. Setting this to False also allows the object attribute or dictionary key to be omitted from output when serializing the instance. If the key is not present it will simply not be included in the output representation. Defaults to True. default If set, this gives the default value that will be used for the field if no input value is supplied. If not set the default behaviour is to not populate the attribute at all. The default is not applied during partial update operations. In the partial update case only fields that are provided in the incoming data will have a validated value returned. May be set to a function or other callable, in which case the value will be evaluated each time it is used. When called, it will receive no arguments. If the callable has a requires_context = True attribute, then the serializer field will be passed as an argument. For example: class CurrentUserDefault:\n    \"\"\"\n    May be applied as a `default=...` value on a serializer field.\n    Returns the current user.\n    \"\"\"\n    requires_context = True\n\n    def __call__(self, serializer_field):\n        return serializer_field.context['request'].user\n When serializing the instance, default will be used if the object attribute or dictionary key is not present in the instance. Note that setting a default value implies that the field is not required. Including both the default and required keyword arguments is invalid and will raise an error. allow_null Normally an error will be raised if None is passed to a serializer field. Set this keyword argument to True if None should be considered a valid value. Note that, without an explicit default, setting this argument to True will imply a default value of null for serialization output, but does not imply a default for input deserialization. Defaults to False source The name of the attribute that will be used to populate the field. May be a method that only takes a self argument, such as URLField(source='get_absolute_url'), or may use dotted notation to traverse attributes, such as EmailField(source='user.email'). When serializing fields with dotted notation, it may be necessary to provide a default value if any object is not present or is empty during attribute traversal. The value source='*' has a special meaning, and is used to indicate that the entire object should be passed through to the field. This can be useful for creating nested representations, or for fields which require access to the complete object in order to determine the output representation. Defaults to the name of the field. validators A list of validator functions which should be applied to the incoming field input, and which either raise a validation error or simply return. Validator functions should typically raise serializers.ValidationError, but Django's built-in ValidationError is also supported for compatibility with validators defined in the Django codebase or third party Django packages. error_messages A dictionary of error codes to error messages. label A short text string that may be used as the name of the field in HTML form fields or other descriptive elements. help_text A text string that may be used as a description of the field in HTML form fields or other descriptive elements. initial A value that should be used for pre-populating the value of HTML form fields. You may pass a callable to it, just as you may do with any regular Django Field: import datetime\nfrom rest_framework import serializers\nclass ExampleSerializer(serializers.Serializer):\n    day = serializers.DateField(initial=datetime.date.today)\n style A dictionary of key-value pairs that can be used to control how renderers should render the field. Two examples here are 'input_type' and 'base_template': # Use <input type=\"password\"> for the input.\npassword = serializers.CharField(\n    style={'input_type': 'password'}\n)\n\n# Use a radio input instead of a select input.\ncolor_channel = serializers.ChoiceField(\n    choices=['red', 'green', 'blue'],\n    style={'base_template': 'radio.html'}\n)\n For more details see the HTML & Forms documentation. Boolean fields BooleanField A boolean representation. When using HTML encoded form input be aware that omitting a value will always be treated as setting a field to False, even if it has a default=True option specified. This is because HTML checkbox inputs represent the unchecked state by omitting the value, so REST framework treats omission as if it is an empty checkbox input. Note that Django 2.1 removed the blank kwarg from models.BooleanField. Prior to Django 2.1 models.BooleanField fields were always blank=True. Thus since Django 2.1 default serializers.BooleanField instances will be generated without the required kwarg (i.e. equivalent to required=True) whereas with previous versions of Django, default BooleanField instances will be generated with a required=False option. If you want to control this behaviour manually, explicitly declare the BooleanField on the serializer class, or use the extra_kwargs option to set the required flag. Corresponds to django.db.models.fields.BooleanField. Signature: BooleanField() NullBooleanField A boolean representation that also accepts None as a valid value. Corresponds to django.db.models.fields.NullBooleanField. Signature: NullBooleanField() String fields CharField A text representation. Optionally validates the text to be shorter than max_length and longer than min_length. Corresponds to django.db.models.fields.CharField or django.db.models.fields.TextField. Signature: CharField(max_length=None, min_length=None, allow_blank=False, trim_whitespace=True)  \nmax_length - Validates that the input contains no more than this number of characters. \nmin_length - Validates that the input contains no fewer than this number of characters. \nallow_blank - If set to True then the empty string should be considered a valid value. If set to False then the empty string is considered invalid and will raise a validation error. Defaults to False. \ntrim_whitespace - If set to True then leading and trailing whitespace is trimmed. Defaults to True.  The allow_null option is also available for string fields, although its usage is discouraged in favor of allow_blank. It is valid to set both allow_blank=True and allow_null=True, but doing so means that there will be two differing types of empty value permissible for string representations, which can lead to data inconsistencies and subtle application bugs. EmailField A text representation, validates the text to be a valid e-mail address. Corresponds to django.db.models.fields.EmailField Signature: EmailField(max_length=None, min_length=None, allow_blank=False) RegexField A text representation, that validates the given value matches against a certain regular expression. Corresponds to django.forms.fields.RegexField. Signature: RegexField(regex, max_length=None, min_length=None, allow_blank=False) The mandatory regex argument may either be a string, or a compiled python regular expression object. Uses Django's django.core.validators.RegexValidator for validation. SlugField A RegexField that validates the input against the pattern [a-zA-Z0-9_-]+. Corresponds to django.db.models.fields.SlugField. Signature: SlugField(max_length=50, min_length=None, allow_blank=False) URLField A RegexField that validates the input against a URL matching pattern. Expects fully qualified URLs of the form http:\/\/<host>\/<path>. Corresponds to django.db.models.fields.URLField. Uses Django's django.core.validators.URLValidator for validation. Signature: URLField(max_length=200, min_length=None, allow_blank=False) UUIDField A field that ensures the input is a valid UUID string. The to_internal_value method will return a uuid.UUID instance. On output the field will return a string in the canonical hyphenated format, for example: \"de305d54-75b4-431b-adb2-eb6b9e546013\"\n Signature: UUIDField(format='hex_verbose')  \nformat: Determines the representation format of the uuid value \n'hex_verbose' - The canonical hex representation, including hyphens: \"5ce0e9a5-5ffa-654b-cee0-1238041fb31a\"\n \n'hex' - The compact hex representation of the UUID, not including hyphens: \"5ce0e9a55ffa654bcee01238041fb31a\"\n \n'int' - A 128 bit integer representation of the UUID: \"123456789012312313134124512351145145114\"\n \n'urn' - RFC 4122 URN representation of the UUID: \"urn:uuid:5ce0e9a5-5ffa-654b-cee0-1238041fb31a\" Changing the format parameters only affects representation values. All formats are accepted by to_internal_value\n    FilePathField A field whose choices are limited to the filenames in a certain directory on the filesystem Corresponds to django.forms.fields.FilePathField. Signature: FilePathField(path, match=None, recursive=False, allow_files=True, allow_folders=False, required=None, **kwargs)  \npath - The absolute filesystem path to a directory from which this FilePathField should get its choice. \nmatch - A regular expression, as a string, that FilePathField will use to filter filenames. \nrecursive - Specifies whether all subdirectories of path should be included. Default is False. \nallow_files - Specifies whether files in the specified location should be included. Default is True. Either this or allow_folders must be True. \nallow_folders - Specifies whether folders in the specified location should be included. Default is False. Either this or allow_files must be True.  IPAddressField A field that ensures the input is a valid IPv4 or IPv6 string. Corresponds to django.forms.fields.IPAddressField and django.forms.fields.GenericIPAddressField. Signature: IPAddressField(protocol='both', unpack_ipv4=False, **options)  \nprotocol Limits valid inputs to the specified protocol. Accepted values are 'both' (default), 'IPv4' or 'IPv6'. Matching is case insensitive. \nunpack_ipv4 Unpacks IPv4 mapped addresses like ::ffff:192.0.2.1. If this option is enabled that address would be unpacked to 192.0.2.1. Default is disabled. Can only be used when protocol is set to 'both'.  Numeric fields IntegerField An integer representation. Corresponds to django.db.models.fields.IntegerField, django.db.models.fields.SmallIntegerField, django.db.models.fields.PositiveIntegerField and django.db.models.fields.PositiveSmallIntegerField. Signature: IntegerField(max_value=None, min_value=None)  \nmax_value Validate that the number provided is no greater than this value. \nmin_value Validate that the number provided is no less than this value.  FloatField A floating point representation. Corresponds to django.db.models.fields.FloatField. Signature: FloatField(max_value=None, min_value=None)  \nmax_value Validate that the number provided is no greater than this value. \nmin_value Validate that the number provided is no less than this value.  DecimalField A decimal representation, represented in Python by a Decimal instance. Corresponds to django.db.models.fields.DecimalField. Signature: DecimalField(max_digits, decimal_places, coerce_to_string=None, max_value=None, min_value=None)  \nmax_digits The maximum number of digits allowed in the number. It must be either None or an integer greater than or equal to decimal_places. \ndecimal_places The number of decimal places to store with the number. \ncoerce_to_string Set to True if string values should be returned for the representation, or False if Decimal objects should be returned. Defaults to the same value as the COERCE_DECIMAL_TO_STRING settings key, which will be True unless overridden. If Decimal objects are returned by the serializer, then the final output format will be determined by the renderer. Note that setting localize will force the value to True. \nmax_value Validate that the number provided is no greater than this value. \nmin_value Validate that the number provided is no less than this value. \nlocalize Set to True to enable localization of input and output based on the current locale. This will also force coerce_to_string to True. Defaults to False. Note that data formatting is enabled if you have set USE_L10N=True in your settings file. \nrounding Sets the rounding mode used when quantising to the configured precision. Valid values are decimal module rounding modes. Defaults to None.  Example usage To validate numbers up to 999 with a resolution of 2 decimal places, you would use: serializers.DecimalField(max_digits=5, decimal_places=2)\n And to validate numbers up to anything less than one billion with a resolution of 10 decimal places: serializers.DecimalField(max_digits=19, decimal_places=10)\n This field also takes an optional argument, coerce_to_string. If set to True the representation will be output as a string. If set to False the representation will be left as a Decimal instance and the final representation will be determined by the renderer. If unset, this will default to the same value as the COERCE_DECIMAL_TO_STRING setting, which is True unless set otherwise. Date and time fields DateTimeField A date and time representation. Corresponds to django.db.models.fields.DateTimeField. Signature: DateTimeField(format=api_settings.DATETIME_FORMAT, input_formats=None, default_timezone=None)  \nformat - A string representing the output format. If not specified, this defaults to the same value as the DATETIME_FORMAT settings key, which will be 'iso-8601' unless set. Setting to a format string indicates that to_representation return values should be coerced to string output. Format strings are described below. Setting this value to None indicates that Python datetime objects should be returned by to_representation. In this case the datetime encoding will be determined by the renderer. \ninput_formats - A list of strings representing the input formats which may be used to parse the date. If not specified, the DATETIME_INPUT_FORMATS setting will be used, which defaults to ['iso-8601']. \ndefault_timezone - A pytz.timezone representing the timezone. If not specified and the USE_TZ setting is enabled, this defaults to the current timezone. If USE_TZ is disabled, then datetime objects will be naive.  DateTimeField format strings. Format strings may either be Python strftime formats which explicitly specify the format, or the special string 'iso-8601', which indicates that ISO 8601 style datetimes should be used. (eg '2013-01-29T12:34:56.000000Z') When a value of None is used for the format datetime objects will be returned by to_representation and the final output representation will determined by the renderer class. auto_now and auto_now_add model fields. When using ModelSerializer or HyperlinkedModelSerializer, note that any model fields with auto_now=True or auto_now_add=True will use serializer fields that are read_only=True by default. If you want to override this behavior, you'll need to declare the DateTimeField explicitly on the serializer. For example: class CommentSerializer(serializers.ModelSerializer):\n    created = serializers.DateTimeField()\n\n    class Meta:\n        model = Comment\n DateField A date representation. Corresponds to django.db.models.fields.DateField Signature: DateField(format=api_settings.DATE_FORMAT, input_formats=None)  \nformat - A string representing the output format. If not specified, this defaults to the same value as the DATE_FORMAT settings key, which will be 'iso-8601' unless set. Setting to a format string indicates that to_representation return values should be coerced to string output. Format strings are described below. Setting this value to None indicates that Python date objects should be returned by to_representation. In this case the date encoding will be determined by the renderer. \ninput_formats - A list of strings representing the input formats which may be used to parse the date. If not specified, the DATE_INPUT_FORMATS setting will be used, which defaults to ['iso-8601'].  DateField format strings Format strings may either be Python strftime formats which explicitly specify the format, or the special string 'iso-8601', which indicates that ISO 8601 style dates should be used. (eg '2013-01-29') TimeField A time representation. Corresponds to django.db.models.fields.TimeField Signature: TimeField(format=api_settings.TIME_FORMAT, input_formats=None)  \nformat - A string representing the output format. If not specified, this defaults to the same value as the TIME_FORMAT settings key, which will be 'iso-8601' unless set. Setting to a format string indicates that to_representation return values should be coerced to string output. Format strings are described below. Setting this value to None indicates that Python time objects should be returned by to_representation. In this case the time encoding will be determined by the renderer. \ninput_formats - A list of strings representing the input formats which may be used to parse the date. If not specified, the TIME_INPUT_FORMATS setting will be used, which defaults to ['iso-8601'].  TimeField format strings Format strings may either be Python strftime formats which explicitly specify the format, or the special string 'iso-8601', which indicates that ISO 8601 style times should be used. (eg '12:34:56.000000') DurationField A Duration representation. Corresponds to django.db.models.fields.DurationField The validated_data for these fields will contain a datetime.timedelta instance. The representation is a string following this format '[DD] [HH:[MM:]]ss[.uuuuuu]'. Signature: DurationField(max_value=None, min_value=None)  \nmax_value Validate that the duration provided is no greater than this value. \nmin_value Validate that the duration provided is no less than this value.  Choice selection fields ChoiceField A field that can accept a value out of a limited set of choices. Used by ModelSerializer to automatically generate fields if the corresponding model field includes a choices=\u2026 argument. Signature: ChoiceField(choices)  \nchoices - A list of valid values, or a list of (key, display_name) tuples. \nallow_blank - If set to True then the empty string should be considered a valid value. If set to False then the empty string is considered invalid and will raise a validation error. Defaults to False. \nhtml_cutoff - If set this will be the maximum number of choices that will be displayed by a HTML select drop down. Can be used to ensure that automatically generated ChoiceFields with very large possible selections do not prevent a template from rendering. Defaults to None. \nhtml_cutoff_text - If set this will display a textual indicator if the maximum number of items have been cutoff in an HTML select drop down. Defaults to \"More than {count} items\u2026\"\n  Both the allow_blank and allow_null are valid options on ChoiceField, although it is highly recommended that you only use one and not both. allow_blank should be preferred for textual choices, and allow_null should be preferred for numeric or other non-textual choices. MultipleChoiceField A field that can accept a set of zero, one or many values, chosen from a limited set of choices. Takes a single mandatory argument. to_internal_value returns a set containing the selected values. Signature: MultipleChoiceField(choices)  \nchoices - A list of valid values, or a list of (key, display_name) tuples. \nallow_blank - If set to True then the empty string should be considered a valid value. If set to False then the empty string is considered invalid and will raise a validation error. Defaults to False. \nhtml_cutoff - If set this will be the maximum number of choices that will be displayed by a HTML select drop down. Can be used to ensure that automatically generated ChoiceFields with very large possible selections do not prevent a template from rendering. Defaults to None. \nhtml_cutoff_text - If set this will display a textual indicator if the maximum number of items have been cutoff in an HTML select drop down. Defaults to \"More than {count} items\u2026\"\n  As with ChoiceField, both the allow_blank and allow_null options are valid, although it is highly recommended that you only use one and not both. allow_blank should be preferred for textual choices, and allow_null should be preferred for numeric or other non-textual choices. File upload fields Parsers and file uploads. The FileField and ImageField classes are only suitable for use with MultiPartParser or FileUploadParser. Most parsers, such as e.g. JSON don't support file uploads. Django's regular FILE_UPLOAD_HANDLERS are used for handling uploaded files. FileField A file representation. Performs Django's standard FileField validation. Corresponds to django.forms.fields.FileField. Signature: FileField(max_length=None, allow_empty_file=False, use_url=UPLOADED_FILES_USE_URL)  \nmax_length - Designates the maximum length for the file name. \nallow_empty_file - Designates if empty files are allowed. \nuse_url - If set to True then URL string values will be used for the output representation. If set to False then filename string values will be used for the output representation. Defaults to the value of the UPLOADED_FILES_USE_URL settings key, which is True unless set otherwise.  ImageField An image representation. Validates the uploaded file content as matching a known image format. Corresponds to django.forms.fields.ImageField. Signature: ImageField(max_length=None, allow_empty_file=False, use_url=UPLOADED_FILES_USE_URL)  \nmax_length - Designates the maximum length for the file name. \nallow_empty_file - Designates if empty files are allowed. \nuse_url - If set to True then URL string values will be used for the output representation. If set to False then filename string values will be used for the output representation. Defaults to the value of the UPLOADED_FILES_USE_URL settings key, which is True unless set otherwise.  Requires either the Pillow package or PIL package. The Pillow package is recommended, as PIL is no longer actively maintained. Composite fields ListField A field class that validates a list of objects. Signature: ListField(child=<A_FIELD_INSTANCE>, allow_empty=True, min_length=None, max_length=None)  \nchild - A field instance that should be used for validating the objects in the list. If this argument is not provided then objects in the list will not be validated. \nallow_empty - Designates if empty lists are allowed. \nmin_length - Validates that the list contains no fewer than this number of elements. \nmax_length - Validates that the list contains no more than this number of elements.  For example, to validate a list of integers you might use something like the following: scores = serializers.ListField(\n   child=serializers.IntegerField(min_value=0, max_value=100)\n)\n The ListField class also supports a declarative style that allows you to write reusable list field classes. class StringListField(serializers.ListField):\n    child = serializers.CharField()\n We can now reuse our custom StringListField class throughout our application, without having to provide a child argument to it. DictField A field class that validates a dictionary of objects. The keys in DictField are always assumed to be string values. Signature: DictField(child=<A_FIELD_INSTANCE>, allow_empty=True)  \nchild - A field instance that should be used for validating the values in the dictionary. If this argument is not provided then values in the mapping will not be validated. \nallow_empty - Designates if empty dictionaries are allowed.  For example, to create a field that validates a mapping of strings to strings, you would write something like this: document = DictField(child=CharField())\n You can also use the declarative style, as with ListField. For example: class DocumentField(DictField):\n    child = CharField()\n HStoreField A preconfigured DictField that is compatible with Django's postgres HStoreField. Signature: HStoreField(child=<A_FIELD_INSTANCE>, allow_empty=True)  \nchild - A field instance that is used for validating the values in the dictionary. The default child field accepts both empty strings and null values. \nallow_empty - Designates if empty dictionaries are allowed.  Note that the child field must be an instance of CharField, as the hstore extension stores values as strings. JSONField A field class that validates that the incoming data structure consists of valid JSON primitives. In its alternate binary mode, it will represent and validate JSON-encoded binary strings. Signature: JSONField(binary, encoder)  \nbinary - If set to True then the field will output and validate a JSON encoded string, rather than a primitive data structure. Defaults to False. \nencoder - Use this JSON encoder to serialize input object. Defaults to None.  Miscellaneous fields ReadOnlyField A field class that simply returns the value of the field without modification. This field is used by default with ModelSerializer when including field names that relate to an attribute rather than a model field. Signature: ReadOnlyField() For example, if has_expired was a property on the Account model, then the following serializer would automatically generate it as a ReadOnlyField: class AccountSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = Account\n        fields = ['id', 'account_name', 'has_expired']\n HiddenField A field class that does not take a value based on user input, but instead takes its value from a default value or callable. Signature: HiddenField() For example, to include a field that always provides the current time as part of the serializer validated data, you would use the following: modified = serializers.HiddenField(default=timezone.now)\n The HiddenField class is usually only needed if you have some validation that needs to run based on some pre-provided field values, but you do not want to expose all of those fields to the end user. For further examples on HiddenField see the validators documentation. ModelField A generic field that can be tied to any arbitrary model field. The ModelField class delegates the task of serialization\/deserialization to its associated model field. This field can be used to create serializer fields for custom model fields, without having to create a new custom serializer field. This field is used by ModelSerializer to correspond to custom model field classes. Signature: ModelField(model_field=<Django ModelField instance>) The ModelField class is generally intended for internal use, but can be used by your API if needed. In order to properly instantiate a ModelField, it must be passed a field that is attached to an instantiated model. For example: ModelField(model_field=MyModel()._meta.get_field('custom_field')) SerializerMethodField This is a read-only field. It gets its value by calling a method on the serializer class it is attached to. It can be used to add any sort of data to the serialized representation of your object. Signature: SerializerMethodField(method_name=None)  \nmethod_name - The name of the method on the serializer to be called. If not included this defaults to get_<field_name>.  The serializer method referred to by the method_name argument should accept a single argument (in addition to self), which is the object being serialized. It should return whatever you want to be included in the serialized representation of the object. For example: from django.contrib.auth.models import User\nfrom django.utils.timezone import now\nfrom rest_framework import serializers\n\nclass UserSerializer(serializers.ModelSerializer):\n    days_since_joined = serializers.SerializerMethodField()\n\n    class Meta:\n        model = User\n        fields = '__all__'\n\n    def get_days_since_joined(self, obj):\n        return (now() - obj.date_joined).days\n Custom fields If you want to create a custom field, you'll need to subclass Field and then override either one or both of the .to_representation() and .to_internal_value() methods. These two methods are used to convert between the initial datatype, and a primitive, serializable datatype. Primitive datatypes will typically be any of a number, string, boolean, date\/time\/datetime or None. They may also be any list or dictionary like object that only contains other primitive objects. Other types might be supported, depending on the renderer that you are using. The .to_representation() method is called to convert the initial datatype into a primitive, serializable datatype. The .to_internal_value() method is called to restore a primitive datatype into its internal python representation. This method should raise a serializers.ValidationError if the data is invalid. Examples A Basic Custom Field Let's look at an example of serializing a class that represents an RGB color value: class Color:\n    \"\"\"\n    A color represented in the RGB colorspace.\n    \"\"\"\n    def __init__(self, red, green, blue):\n        assert(red >= 0 and green >= 0 and blue >= 0)\n        assert(red < 256 and green < 256 and blue < 256)\n        self.red, self.green, self.blue = red, green, blue\n\nclass ColorField(serializers.Field):\n    \"\"\"\n    Color objects are serialized into 'rgb(#, #, #)' notation.\n    \"\"\"\n    def to_representation(self, value):\n        return \"rgb(%d, %d, %d)\" % (value.red, value.green, value.blue)\n\n    def to_internal_value(self, data):\n        data = data.strip('rgb(').rstrip(')')\n        red, green, blue = [int(col) for col in data.split(',')]\n        return Color(red, green, blue)\n By default field values are treated as mapping to an attribute on the object. If you need to customize how the field value is accessed and set you need to override .get_attribute() and\/or .get_value(). As an example, let's create a field that can be used to represent the class name of the object being serialized: class ClassNameField(serializers.Field):\n    def get_attribute(self, instance):\n        # We pass the object instance onto `to_representation`,\n        # not just the field attribute.\n        return instance\n\n    def to_representation(self, value):\n        \"\"\"\n        Serialize the value's class name.\n        \"\"\"\n        return value.__class__.__name__\n Raising validation errors Our ColorField class above currently does not perform any data validation. To indicate invalid data, we should raise a serializers.ValidationError, like so: def to_internal_value(self, data):\n    if not isinstance(data, str):\n        msg = 'Incorrect type. Expected a string, but got %s'\n        raise ValidationError(msg % type(data).__name__)\n\n    if not re.match(r'^rgb\\([0-9]+,[0-9]+,[0-9]+\\)$', data):\n        raise ValidationError('Incorrect format. Expected `rgb(#,#,#)`.')\n\n    data = data.strip('rgb(').rstrip(')')\n    red, green, blue = [int(col) for col in data.split(',')]\n\n    if any([col > 255 or col < 0 for col in (red, green, blue)]):\n        raise ValidationError('Value out of range. Must be between 0 and 255.')\n\n    return Color(red, green, blue)\n The .fail() method is a shortcut for raising ValidationError that takes a message string from the error_messages dictionary. For example: default_error_messages = {\n    'incorrect_type': 'Incorrect type. Expected a string, but got {input_type}',\n    'incorrect_format': 'Incorrect format. Expected `rgb(#,#,#)`.',\n    'out_of_range': 'Value out of range. Must be between 0 and 255.'\n}\n\ndef to_internal_value(self, data):\n    if not isinstance(data, str):\n        self.fail('incorrect_type', input_type=type(data).__name__)\n\n    if not re.match(r'^rgb\\([0-9]+,[0-9]+,[0-9]+\\)$', data):\n        self.fail('incorrect_format')\n\n    data = data.strip('rgb(').rstrip(')')\n    red, green, blue = [int(col) for col in data.split(',')]\n\n    if any([col > 255 or col < 0 for col in (red, green, blue)]):\n        self.fail('out_of_range')\n\n    return Color(red, green, blue)\n This style keeps your error messages cleaner and more separated from your code, and should be preferred. Using source='*' Here we'll take an example of a flat DataPoint model with x_coordinate and y_coordinate attributes. class DataPoint(models.Model):\n    label = models.CharField(max_length=50)\n    x_coordinate = models.SmallIntegerField()\n    y_coordinate = models.SmallIntegerField()\n Using a custom field and source='*' we can provide a nested representation of the coordinate pair: class CoordinateField(serializers.Field):\n\n    def to_representation(self, value):\n        ret = {\n            \"x\": value.x_coordinate,\n            \"y\": value.y_coordinate\n        }\n        return ret\n\n    def to_internal_value(self, data):\n        ret = {\n            \"x_coordinate\": data[\"x\"],\n            \"y_coordinate\": data[\"y\"],\n        }\n        return ret\n\n\nclass DataPointSerializer(serializers.ModelSerializer):\n    coordinates = CoordinateField(source='*')\n\n    class Meta:\n        model = DataPoint\n        fields = ['label', 'coordinates']\n Note that this example doesn't handle validation. Partly for that reason, in a real project, the coordinate nesting might be better handled with a nested serializer using source='*', with two IntegerField instances, each with their own source pointing to the relevant field. The key points from the example, though, are:   to_representation is passed the entire DataPoint object and must map from that to the desired output. >>> instance = DataPoint(label='Example', x_coordinate=1, y_coordinate=2)\n>>> out_serializer = DataPointSerializer(instance)\n>>> out_serializer.data\nReturnDict([('label', 'Example'), ('coordinates', {'x': 1, 'y': 2})])\n   Unless our field is to be read-only, to_internal_value must map back to a dict suitable for updating our target object. With source='*', the return from to_internal_value will update the root validated data dictionary, rather than a single key. >>> data = {\n...     \"label\": \"Second Example\",\n...     \"coordinates\": {\n...         \"x\": 3,\n...         \"y\": 4,\n...     }\n... }\n>>> in_serializer = DataPointSerializer(data=data)\n>>> in_serializer.is_valid()\nTrue\n>>> in_serializer.validated_data\nOrderedDict([('label', 'Second Example'),\n             ('y_coordinate', 4),\n             ('x_coordinate', 3)])\n   For completeness lets do the same thing again but with the nested serializer approach suggested above: class NestedCoordinateSerializer(serializers.Serializer):\n    x = serializers.IntegerField(source='x_coordinate')\n    y = serializers.IntegerField(source='y_coordinate')\n\n\nclass DataPointSerializer(serializers.ModelSerializer):\n    coordinates = NestedCoordinateSerializer(source='*')\n\n    class Meta:\n        model = DataPoint\n        fields = ['label', 'coordinates']\n Here the mapping between the target and source attribute pairs (x and x_coordinate, y and y_coordinate) is handled in the IntegerField declarations. It's our NestedCoordinateSerializer that takes source='*'. Our new DataPointSerializer exhibits the same behaviour as the custom field approach. Serializing: >>> out_serializer = DataPointSerializer(instance)\n>>> out_serializer.data\nReturnDict([('label', 'testing'),\n            ('coordinates', OrderedDict([('x', 1), ('y', 2)]))])\n Deserializing: >>> in_serializer = DataPointSerializer(data=data)\n>>> in_serializer.is_valid()\nTrue\n>>> in_serializer.validated_data\nOrderedDict([('label', 'still testing'),\n             ('x_coordinate', 3),\n             ('y_coordinate', 4)])\n But we also get the built-in validation for free: >>> invalid_data = {\n...     \"label\": \"still testing\",\n...     \"coordinates\": {\n...         \"x\": 'a',\n...         \"y\": 'b',\n...     }\n... }\n>>> invalid_serializer = DataPointSerializer(data=invalid_data)\n>>> invalid_serializer.is_valid()\nFalse\n>>> invalid_serializer.errors\nReturnDict([('coordinates',\n             {'x': ['A valid integer is required.'],\n              'y': ['A valid integer is required.']})])\n For this reason, the nested serializer approach would be the first to try. You would use the custom field approach when the nested serializer becomes infeasible or overly complex. Third party packages The following third party packages are also available. DRF Compound Fields The drf-compound-fields package provides \"compound\" serializer fields, such as lists of simple values, which can be described by other fields rather than serializers with the many=True option. Also provided are fields for typed dictionaries and values that can be either a specific type or a list of items of that type. DRF Extra Fields The drf-extra-fields package provides extra serializer fields for REST framework, including Base64ImageField and PointField classes. djangorestframework-recursive the djangorestframework-recursive package provides a RecursiveField for serializing and deserializing recursive structures django-rest-framework-gis The django-rest-framework-gis package provides geographic addons for django rest framework like a GeometryField field and a GeoJSON serializer. django-rest-framework-hstore The django-rest-framework-hstore package provides an HStoreField to support django-hstore DictionaryField model field. fields.py","title":"django_rest_framework.api-guide.fields.index#nullbooleanfield"}]}
{"task_id":16193578,"prompt":"def f_16193578(list5):\n\treturn ","suffix":"","canonical_solution":"sorted(list5, key = lambda x: (degrees(x), x))","test_start":"\nfrom math import degrees\n\ndef check(candidate):","test":["\n    list5 = [4, 1, 2, 3, 9, 5]\n    assert candidate(list5) == [1, 2, 3, 4, 5, 9]\n"],"entry_point":"f_16193578","intent":"Sort lis `list5` in ascending order based on the degrees value of its elements","library":["math"],"docs":[{"text":"deg_mark='^{\\\\circ}'","title":"matplotlib._as_gen.mpl_toolkits.axisartist.angle_helper.formatterdms#mpl_toolkits.axisartist.angle_helper.FormatterDMS.deg_mark"},{"text":"class Degrees(expression, **extra)","title":"django.ref.models.database-functions#django.db.models.functions.Degrees"},{"text":"mpl_toolkits.axisartist.angle_helper.select_step_degree   mpl_toolkits.axisartist.angle_helper.select_step_degree(dv)[source]","title":"matplotlib._as_gen.mpl_toolkits.axisartist.angle_helper.select_step_degree"},{"text":"deg_mark='^\\\\mathrm{h}'","title":"matplotlib._as_gen.mpl_toolkits.axisartist.angle_helper.formatterhms#mpl_toolkits.axisartist.angle_helper.FormatterHMS.deg_mark"},{"text":"tf.raw_ops.ListDiff Computes the difference between two lists of numbers or strings.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.raw_ops.ListDiff  \ntf.raw_ops.ListDiff(\n    x, y, out_idx=tf.dtypes.int32, name=None\n)\n Given a list x and a list y, this operation returns a list out that represents all values that are in x but not in y. The returned list out is sorted in the same order that the numbers appear in x (duplicates are preserved). This operation also returns a list idx that represents the position of each out element in x. In other words: out[i] = x[idx[i]] for i in [0, 1, ..., len(out) - 1] For example, given this input: x = [1, 2, 3, 4, 5, 6]\ny = [1, 3, 5]\n This operation would return: out ==> [2, 4, 6]\nidx ==> [1, 3, 5]\n\n \n\n\n Args\n  x   A Tensor. 1-D. Values to keep.  \n  y   A Tensor. Must have the same type as x. 1-D. Values to remove.  \n  out_idx   An optional tf.DType from: tf.int32, tf.int64. Defaults to tf.int32.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A tuple of Tensor objects (out, idx).     out   A Tensor. Has the same type as x.  \n  idx   A Tensor of type out_idx.","title":"tensorflow.raw_ops.listdiff"},{"text":"classmatplotlib.projections.polar.ThetaFormatter[source]\n \nBases: matplotlib.ticker.Formatter Used to format the theta tick labels. Converts the native unit of radians into degrees and adds a degree symbol.","title":"matplotlib.projections_api#matplotlib.projections.polar.ThetaFormatter"},{"text":"matplotlib.backends.backend_ps.get_bbox_header(lbrt, rotated=False)[source]\n \nReturn a postscript header string for the given bbox lbrt=(l, b, r, t). Optionally, return rotate command.","title":"matplotlib.backend_ps_api#matplotlib.backends.backend_ps.get_bbox_header"},{"text":"classThetaFormatter[source]\n \nBases: matplotlib.ticker.Formatter Used to format the theta tick labels. Converts the native unit of radians into degrees and adds a degree symbol.","title":"matplotlib.projections_api#matplotlib.projections.polar.PolarAxes.ThetaFormatter"},{"text":"fmt_ds='$%d.%s^{\\\\circ}$'","title":"matplotlib._as_gen.mpl_toolkits.axisartist.angle_helper.formatterdms#mpl_toolkits.axisartist.angle_helper.FormatterDMS.fmt_ds"},{"text":"class SlugField(max_length=50, **options)","title":"django.ref.models.fields#django.db.models.SlugField"}]}
{"task_id":16041405,"prompt":"def f_16041405(l):\n\treturn ","suffix":"","canonical_solution":"(n for n in l)","test_start":"\ndef check(candidate):","test":["\n    generator = candidate([1,2,3,5])\n    assert str(type(generator)) == \"<class 'generator'>\"\n    assert [x for x in generator] == [1, 2, 3, 5]\n"],"entry_point":"f_16041405","intent":"convert a list `l` into a generator object","library":[],"docs":[]}
{"task_id":18837607,"prompt":"def f_18837607(oldlist, removelist):\n\treturn ","suffix":"","canonical_solution":"[v for i, v in enumerate(oldlist) if i not in removelist]","test_start":"\ndef check(candidate):","test":["\n    assert candidate([\"asdf\",\"ghjk\",\"qwer\",\"tyui\"], [1,3]) == ['asdf', 'qwer']\n","\n    assert candidate([1,2,3,4,5], [0,4]) == [2,3,4]\n"],"entry_point":"f_18837607","intent":"remove elements from list `oldlist` that have an index number mentioned in list `removelist`","library":[],"docs":[]}
{"task_id":4710067,"prompt":"def f_4710067():\n\treturn ","suffix":"","canonical_solution":"open('yourfile.txt', 'w')","test_start":"\ndef check(candidate):","test":["\n    fw = candidate()\n    assert fw.name == \"yourfile.txt\"\n    assert fw.mode == 'w'\n"],"entry_point":"f_4710067","intent":"Open a file `yourfile.txt` in write mode","library":[],"docs":[]}
{"task_id":7373219,"prompt":"def f_7373219(obj, attr):\n\treturn ","suffix":"","canonical_solution":"getattr(obj, attr)","test_start":"\ndef check(candidate):","test":["\n    class Student:\n        student_id = \"\"\n        student_name = \"\"\n\n        def __init__(self, student_id=101, student_name=\"Adam\"):\n            self.student_id = student_id\n            self.student_name = student_name\n\n    student = Student()\n\n    assert(candidate(student, 'student_name') == \"Adam\")\n    assert(candidate(student, 'student_id') == 101)\n","\n    class Student:\n        student_id = \"\"\n        student_name = \"\"\n\n        def __init__(self, student_id=101, student_name=\"Adam\"):\n            self.student_id = student_id\n            self.student_name = student_name\n\n    student = Student()\n\n    try:\n        value = candidate(student, 'student_none')\n    except: \n        assert True\n"],"entry_point":"f_7373219","intent":"get attribute 'attr' from object `obj`","library":[],"docs":[]}
{"task_id":8171751,"prompt":"def f_8171751():\n\treturn ","suffix":"","canonical_solution":"reduce(lambda a, b: a + b, (('aa',), ('bb',), ('cc',)))","test_start":"\nfrom functools import reduce\n\ndef check(candidate):","test":["\n    assert candidate() == ('aa', 'bb', 'cc')\n"],"entry_point":"f_8171751","intent":"convert tuple of tuples `(('aa',), ('bb',), ('cc',))` to tuple","library":["functools"],"docs":[{"text":"parser.tuple2st(sequence)  \nThis is the same function as sequence2st(). This entry point is maintained for backward compatibility.","title":"python.library.parser#parser.tuple2st"},{"text":"ST.totuple(line_info=False, col_info=False)  \nSame as st2tuple(st, line_info, col_info).","title":"python.library.parser#parser.ST.totuple"},{"text":"class tuple([iterable])  \nTuples may be constructed in a number of ways:  Using a pair of parentheses to denote the empty tuple: ()\n Using a trailing comma for a singleton tuple: a, or (a,)\n Separating items with commas: a, b, c or (a, b, c)\n Using the tuple() built-in: tuple() or tuple(iterable)\n  The constructor builds a tuple whose items are the same and in the same order as iterable\u2019s items. iterable may be either a sequence, a container that supports iteration, or an iterator object. If iterable is already a tuple, it is returned unchanged. For example, tuple('abc') returns ('a', 'b', 'c') and tuple( [1, 2, 3] ) returns (1, 2, 3). If no argument is given, the constructor creates a new empty tuple, (). Note that it is actually the comma which makes a tuple, not the parentheses. The parentheses are optional, except in the empty tuple case, or when they are needed to avoid syntactic ambiguity. For example, f(a, b, c) is a function call with three arguments, while f((a, b, c)) is a function call with a 3-tuple as the sole argument. Tuples implement all of the common sequence operations.","title":"python.library.stdtypes#tuple"},{"text":"numpy.column_stack   numpy.column_stack(tup)[source]\n \nStack 1-D arrays as columns into a 2-D array. Take a sequence of 1-D arrays and stack them as columns to make a single 2-D array. 2-D arrays are stacked as-is, just like with hstack. 1-D arrays are turned into 2-D columns first.  Parameters \n \ntupsequence of 1-D or 2-D arrays.\n\n\nArrays to stack. All of them must have the same first dimension.    Returns \n \nstacked2-D array\n\n\nThe array formed by stacking the given arrays.      See also  \nstack, hstack, vstack, concatenate\n\n  Examples >>> a = np.array((1,2,3))\n>>> b = np.array((2,3,4))\n>>> np.column_stack((a,b))\narray([[1, 2],\n       [2, 3],\n       [3, 4]])","title":"numpy.reference.generated.numpy.column_stack"},{"text":"numpy.row_stack   numpy.row_stack(tup)[source]\n \nStack arrays in sequence vertically (row wise). This is equivalent to concatenation along the first axis after 1-D arrays of shape (N,) have been reshaped to (1,N). Rebuilds arrays divided by vsplit. This function makes most sense for arrays with up to 3 dimensions. For instance, for pixel-data with a height (first axis), width (second axis), and r\/g\/b channels (third axis). The functions concatenate, stack and block provide more general stacking and concatenation operations.  Parameters \n \ntupsequence of ndarrays\n\n\nThe arrays must have the same shape along all but the first axis. 1-D arrays must have the same length.    Returns \n \nstackedndarray\n\n\nThe array formed by stacking the given arrays, will be at least 2-D.      See also  concatenate\n\nJoin a sequence of arrays along an existing axis.  stack\n\nJoin a sequence of arrays along a new axis.  block\n\nAssemble an nd-array from nested lists of blocks.  hstack\n\nStack arrays in sequence horizontally (column wise).  dstack\n\nStack arrays in sequence depth wise (along third axis).  column_stack\n\nStack 1-D arrays as columns into a 2-D array.  vsplit\n\nSplit an array into multiple sub-arrays vertically (row-wise).    Examples >>> a = np.array([1, 2, 3])\n>>> b = np.array([4, 5, 6])\n>>> np.vstack((a,b))\narray([[1, 2, 3],\n       [4, 5, 6]])\n >>> a = np.array([[1], [2], [3]])\n>>> b = np.array([[4], [5], [6]])\n>>> np.vstack((a,b))\narray([[1],\n       [2],\n       [3],\n       [4],\n       [5],\n       [6]])","title":"numpy.reference.generated.numpy.row_stack"},{"text":"parser.st2tuple(st, line_info=False, col_info=False)  \nThis function accepts an ST object from the caller in st and returns a Python tuple representing the equivalent parse tree. Other than returning a tuple instead of a list, this function is identical to st2list(). If line_info is true, line number information will be included for all terminal tokens as a third element of the list representing the token. This information is omitted if the flag is false or omitted.","title":"python.library.parser#parser.st2tuple"},{"text":"dataclasses.astuple(instance, *, tuple_factory=tuple)  \nConverts the dataclass instance to a tuple (by using the factory function tuple_factory). Each dataclass is converted to a tuple of its field values. dataclasses, dicts, lists, and tuples are recursed into. Continuing from the previous example: assert astuple(p) == (10, 20)\nassert astuple(c) == ([(0, 0), (10, 4)],)\n Raises TypeError if instance is not a dataclass instance.","title":"python.library.dataclasses#dataclasses.astuple"},{"text":"numpy.vstack   numpy.vstack(tup)[source]\n \nStack arrays in sequence vertically (row wise). This is equivalent to concatenation along the first axis after 1-D arrays of shape (N,) have been reshaped to (1,N). Rebuilds arrays divided by vsplit. This function makes most sense for arrays with up to 3 dimensions. For instance, for pixel-data with a height (first axis), width (second axis), and r\/g\/b channels (third axis). The functions concatenate, stack and block provide more general stacking and concatenation operations.  Parameters \n \ntupsequence of ndarrays\n\n\nThe arrays must have the same shape along all but the first axis. 1-D arrays must have the same length.    Returns \n \nstackedndarray\n\n\nThe array formed by stacking the given arrays, will be at least 2-D.      See also  concatenate\n\nJoin a sequence of arrays along an existing axis.  stack\n\nJoin a sequence of arrays along a new axis.  block\n\nAssemble an nd-array from nested lists of blocks.  hstack\n\nStack arrays in sequence horizontally (column wise).  dstack\n\nStack arrays in sequence depth wise (along third axis).  column_stack\n\nStack 1-D arrays as columns into a 2-D array.  vsplit\n\nSplit an array into multiple sub-arrays vertically (row-wise).    Examples >>> a = np.array([1, 2, 3])\n>>> b = np.array([4, 5, 6])\n>>> np.vstack((a,b))\narray([[1, 2, 3],\n       [4, 5, 6]])\n >>> a = np.array([[1], [2], [3]])\n>>> b = np.array([[4], [5], [6]])\n>>> np.vstack((a,b))\narray([[1],\n       [2],\n       [3],\n       [4],\n       [5],\n       [6]])","title":"numpy.reference.generated.numpy.vstack"},{"text":"class tuple([iterable])  \nRather than being a function, tuple is actually an immutable sequence type, as documented in Tuples and Sequence Types \u2014 list, tuple, range.","title":"python.library.functions#tuple"},{"text":"content_md5  \nThe Content-MD5 entity-header field, as defined in RFC 1864, is an MD5 digest of the entity-body for the purpose of providing an end-to-end message integrity check (MIC) of the entity-body. (Note: a MIC is good for detecting accidental modification of the entity-body in transit, but is not proof against malicious attacks.)","title":"werkzeug.wrappers.index#werkzeug.wrappers.Response.content_md5"}]}
{"task_id":8171751,"prompt":"def f_8171751():\n\treturn ","suffix":"","canonical_solution":"list(map(lambda a: a[0], (('aa',), ('bb',), ('cc',))))","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == ['aa', 'bb', 'cc']\n"],"entry_point":"f_8171751","intent":"convert tuple of tuples `(('aa',), ('bb',), ('cc',))` to list in one line","library":[],"docs":[]}
{"task_id":28986489,"prompt":"def f_28986489(df):\n\t","suffix":"\n\treturn df","canonical_solution":"df['range'].replace(',', '-', inplace=True)","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    df = pd.DataFrame({'range' : [\",\", \"(50,290)\", \",,,\"]})\n    res = pd.DataFrame({'range' : [\"-\", \"(50,290)\", \",,,\"]})\n    assert candidate(df).equals(res)\n"],"entry_point":"f_28986489","intent":"replace a characters in a column of a dataframe `df`","library":["pandas"],"docs":[{"text":"pandas.DataFrame.replace   DataFrame.replace(to_replace=None, value=NoDefault.no_default, inplace=False, limit=None, regex=False, method=NoDefault.no_default)[source]\n \nReplace values given in to_replace with value. Values of the DataFrame are replaced with other values dynamically. This differs from updating with .loc or .iloc, which require you to specify a location to update with some value.  Parameters \n \nto_replace:str, regex, list, dict, Series, int, float, or None\n\n\nHow to find the values that will be replaced.  \nnumeric, str or regex:  \n numeric: numeric values equal to to_replace will be replaced with value str: string exactly matching to_replace will be replaced with value regex: regexs matching to_replace will be replaced with value  \n  \nlist of str, regex, or numeric:  \n First, if to_replace and value are both lists, they must be the same length. Second, if regex=True then all of the strings in both lists will be interpreted as regexs otherwise they will match directly. This doesn\u2019t matter much for value since there are only a few possible substitution regexes you can use. str, regex and numeric rules apply as above.  \n  \ndict:  \n Dicts can be used to specify different replacement values for different existing values. For example, {'a': 'b', 'y': 'z'} replaces the value \u2018a\u2019 with \u2018b\u2019 and \u2018y\u2019 with \u2018z\u2019. To use a dict in this way the value parameter should be None. For a DataFrame a dict can specify that different values should be replaced in different columns. For example, {'a': 1, 'b': 'z'} looks for the value 1 in column \u2018a\u2019 and the value \u2018z\u2019 in column \u2018b\u2019 and replaces these values with whatever is specified in value. The value parameter should not be None in this case. You can treat this as a special case of passing two lists except that you are specifying the column to search in. For a DataFrame nested dictionaries, e.g., {'a': {'b': np.nan}}, are read as follows: look in column \u2018a\u2019 for the value \u2018b\u2019 and replace it with NaN. The value parameter should be None to use a nested dict in this way. You can nest regular expressions as well. Note that column names (the top-level dictionary keys in a nested dictionary) cannot be regular expressions.  \n  \nNone:  \n This means that the regex argument must be a string, compiled regular expression, or list, dict, ndarray or Series of such elements. If value is also None then this must be a nested dictionary or Series.  \n   See the examples section for examples of each of these.  \nvalue:scalar, dict, list, str, regex, default None\n\n\nValue to replace any values matching to_replace with. For a DataFrame a dict of values can be used to specify which value to use for each column (columns not in the dict will not be filled). Regular expressions, strings and lists or dicts of such objects are also allowed.  \ninplace:bool, default False\n\n\nIf True, performs operation inplace and returns None.  \nlimit:int, default None\n\n\nMaximum size gap to forward or backward fill.  \nregex:bool or same types as to_replace, default False\n\n\nWhether to interpret to_replace and\/or value as regular expressions. If this is True then to_replace must be a string. Alternatively, this could be a regular expression or a list, dict, or array of regular expressions in which case to_replace must be None.  \nmethod:{\u2018pad\u2019, \u2018ffill\u2019, \u2018bfill\u2019, None}\n\n\nThe method to use when for replacement, when to_replace is a scalar, list or tuple and value is None.  Changed in version 0.23.0: Added to DataFrame.     Returns \n DataFrame\n\nObject after replacement.    Raises \n AssertionError\n\n If regex is not a bool and to_replace is not None.   TypeError\n\n If to_replace is not a scalar, array-like, dict, or None If to_replace is a dict and value is not a list, dict, ndarray, or Series If to_replace is None and regex is not compilable into a regular expression or is a list, dict, ndarray, or Series. When replacing multiple bool or datetime64 objects and the arguments to to_replace does not match the type of the value being replaced   ValueError\n\n If a list or an ndarray is passed to to_replace and value but they are not the same length.       See also  DataFrame.fillna\n\nFill NA values.  DataFrame.where\n\nReplace values based on boolean condition.  Series.str.replace\n\nSimple string replacement.    Notes  Regex substitution is performed under the hood with re.sub. The rules for substitution for re.sub are the same. Regular expressions will only substitute on strings, meaning you cannot provide, for example, a regular expression matching floating point numbers and expect the columns in your frame that have a numeric dtype to be matched. However, if those floating point numbers are strings, then you can do this. This method has a lot of options. You are encouraged to experiment and play with this method to gain intuition about how it works. When dict is used as the to_replace value, it is like key(s) in the dict are the to_replace part and value(s) in the dict are the value parameter.  Examples Scalar `to_replace` and `value` \n>>> s = pd.Series([1, 2, 3, 4, 5])\n>>> s.replace(1, 5)\n0    5\n1    2\n2    3\n3    4\n4    5\ndtype: int64\n  \n>>> df = pd.DataFrame({'A': [0, 1, 2, 3, 4],\n...                    'B': [5, 6, 7, 8, 9],\n...                    'C': ['a', 'b', 'c', 'd', 'e']})\n>>> df.replace(0, 5)\n    A  B  C\n0  5  5  a\n1  1  6  b\n2  2  7  c\n3  3  8  d\n4  4  9  e\n  List-like `to_replace` \n>>> df.replace([0, 1, 2, 3], 4)\n    A  B  C\n0  4  5  a\n1  4  6  b\n2  4  7  c\n3  4  8  d\n4  4  9  e\n  \n>>> df.replace([0, 1, 2, 3], [4, 3, 2, 1])\n    A  B  C\n0  4  5  a\n1  3  6  b\n2  2  7  c\n3  1  8  d\n4  4  9  e\n  \n>>> s.replace([1, 2], method='bfill')\n0    3\n1    3\n2    3\n3    4\n4    5\ndtype: int64\n  dict-like `to_replace` \n>>> df.replace({0: 10, 1: 100})\n        A  B  C\n0   10  5  a\n1  100  6  b\n2    2  7  c\n3    3  8  d\n4    4  9  e\n  \n>>> df.replace({'A': 0, 'B': 5}, 100)\n        A    B  C\n0  100  100  a\n1    1    6  b\n2    2    7  c\n3    3    8  d\n4    4    9  e\n  \n>>> df.replace({'A': {0: 100, 4: 400}})\n        A  B  C\n0  100  5  a\n1    1  6  b\n2    2  7  c\n3    3  8  d\n4  400  9  e\n  Regular expression `to_replace` \n>>> df = pd.DataFrame({'A': ['bat', 'foo', 'bait'],\n...                    'B': ['abc', 'bar', 'xyz']})\n>>> df.replace(to_replace=r'^ba.$', value='new', regex=True)\n        A    B\n0   new  abc\n1   foo  new\n2  bait  xyz\n  \n>>> df.replace({'A': r'^ba.$'}, {'A': 'new'}, regex=True)\n        A    B\n0   new  abc\n1   foo  bar\n2  bait  xyz\n  \n>>> df.replace(regex=r'^ba.$', value='new')\n        A    B\n0   new  abc\n1   foo  new\n2  bait  xyz\n  \n>>> df.replace(regex={r'^ba.$': 'new', 'foo': 'xyz'})\n        A    B\n0   new  abc\n1   xyz  new\n2  bait  xyz\n  \n>>> df.replace(regex=[r'^ba.$', 'foo'], value='new')\n        A    B\n0   new  abc\n1   new  new\n2  bait  xyz\n  Compare the behavior of s.replace({'a': None}) and s.replace('a', None) to understand the peculiarities of the to_replace parameter: \n>>> s = pd.Series([10, 'a', 'a', 'b', 'a'])\n  When one uses a dict as the to_replace value, it is like the value(s) in the dict are equal to the value parameter. s.replace({'a': None}) is equivalent to s.replace(to_replace={'a': None}, value=None, method=None): \n>>> s.replace({'a': None})\n0      10\n1    None\n2    None\n3       b\n4    None\ndtype: object\n  When value is not explicitly passed and to_replace is a scalar, list or tuple, replace uses the method parameter (default \u2018pad\u2019) to do the replacement. So this is why the \u2018a\u2019 values are being replaced by 10 in rows 1 and 2 and \u2018b\u2019 in row 4 in this case. \n>>> s.replace('a')\n0    10\n1    10\n2    10\n3     b\n4     b\ndtype: object\n  On the other hand, if None is explicitly passed for value, it will be respected: \n>>> s.replace('a', None)\n0      10\n1    None\n2    None\n3       b\n4    None\ndtype: object\n   \n Changed in version 1.4.0: Previously the explicit None was silently ignored.","title":"pandas.reference.api.pandas.dataframe.replace"},{"text":"pandas.Series.replace   Series.replace(to_replace=None, value=NoDefault.no_default, inplace=False, limit=None, regex=False, method=NoDefault.no_default)[source]\n \nReplace values given in to_replace with value. Values of the Series are replaced with other values dynamically. This differs from updating with .loc or .iloc, which require you to specify a location to update with some value.  Parameters \n \nto_replace:str, regex, list, dict, Series, int, float, or None\n\n\nHow to find the values that will be replaced.  \nnumeric, str or regex:  \n numeric: numeric values equal to to_replace will be replaced with value str: string exactly matching to_replace will be replaced with value regex: regexs matching to_replace will be replaced with value  \n  \nlist of str, regex, or numeric:  \n First, if to_replace and value are both lists, they must be the same length. Second, if regex=True then all of the strings in both lists will be interpreted as regexs otherwise they will match directly. This doesn\u2019t matter much for value since there are only a few possible substitution regexes you can use. str, regex and numeric rules apply as above.  \n  \ndict:  \n Dicts can be used to specify different replacement values for different existing values. For example, {'a': 'b', 'y': 'z'} replaces the value \u2018a\u2019 with \u2018b\u2019 and \u2018y\u2019 with \u2018z\u2019. To use a dict in this way the value parameter should be None. For a DataFrame a dict can specify that different values should be replaced in different columns. For example, {'a': 1, 'b': 'z'} looks for the value 1 in column \u2018a\u2019 and the value \u2018z\u2019 in column \u2018b\u2019 and replaces these values with whatever is specified in value. The value parameter should not be None in this case. You can treat this as a special case of passing two lists except that you are specifying the column to search in. For a DataFrame nested dictionaries, e.g., {'a': {'b': np.nan}}, are read as follows: look in column \u2018a\u2019 for the value \u2018b\u2019 and replace it with NaN. The value parameter should be None to use a nested dict in this way. You can nest regular expressions as well. Note that column names (the top-level dictionary keys in a nested dictionary) cannot be regular expressions.  \n  \nNone:  \n This means that the regex argument must be a string, compiled regular expression, or list, dict, ndarray or Series of such elements. If value is also None then this must be a nested dictionary or Series.  \n   See the examples section for examples of each of these.  \nvalue:scalar, dict, list, str, regex, default None\n\n\nValue to replace any values matching to_replace with. For a DataFrame a dict of values can be used to specify which value to use for each column (columns not in the dict will not be filled). Regular expressions, strings and lists or dicts of such objects are also allowed.  \ninplace:bool, default False\n\n\nIf True, performs operation inplace and returns None.  \nlimit:int, default None\n\n\nMaximum size gap to forward or backward fill.  \nregex:bool or same types as to_replace, default False\n\n\nWhether to interpret to_replace and\/or value as regular expressions. If this is True then to_replace must be a string. Alternatively, this could be a regular expression or a list, dict, or array of regular expressions in which case to_replace must be None.  \nmethod:{\u2018pad\u2019, \u2018ffill\u2019, \u2018bfill\u2019, None}\n\n\nThe method to use when for replacement, when to_replace is a scalar, list or tuple and value is None.  Changed in version 0.23.0: Added to DataFrame.     Returns \n Series\n\nObject after replacement.    Raises \n AssertionError\n\n If regex is not a bool and to_replace is not None.   TypeError\n\n If to_replace is not a scalar, array-like, dict, or None If to_replace is a dict and value is not a list, dict, ndarray, or Series If to_replace is None and regex is not compilable into a regular expression or is a list, dict, ndarray, or Series. When replacing multiple bool or datetime64 objects and the arguments to to_replace does not match the type of the value being replaced   ValueError\n\n If a list or an ndarray is passed to to_replace and value but they are not the same length.       See also  Series.fillna\n\nFill NA values.  Series.where\n\nReplace values based on boolean condition.  Series.str.replace\n\nSimple string replacement.    Notes  Regex substitution is performed under the hood with re.sub. The rules for substitution for re.sub are the same. Regular expressions will only substitute on strings, meaning you cannot provide, for example, a regular expression matching floating point numbers and expect the columns in your frame that have a numeric dtype to be matched. However, if those floating point numbers are strings, then you can do this. This method has a lot of options. You are encouraged to experiment and play with this method to gain intuition about how it works. When dict is used as the to_replace value, it is like key(s) in the dict are the to_replace part and value(s) in the dict are the value parameter.  Examples Scalar `to_replace` and `value` \n>>> s = pd.Series([1, 2, 3, 4, 5])\n>>> s.replace(1, 5)\n0    5\n1    2\n2    3\n3    4\n4    5\ndtype: int64\n  \n>>> df = pd.DataFrame({'A': [0, 1, 2, 3, 4],\n...                    'B': [5, 6, 7, 8, 9],\n...                    'C': ['a', 'b', 'c', 'd', 'e']})\n>>> df.replace(0, 5)\n    A  B  C\n0  5  5  a\n1  1  6  b\n2  2  7  c\n3  3  8  d\n4  4  9  e\n  List-like `to_replace` \n>>> df.replace([0, 1, 2, 3], 4)\n    A  B  C\n0  4  5  a\n1  4  6  b\n2  4  7  c\n3  4  8  d\n4  4  9  e\n  \n>>> df.replace([0, 1, 2, 3], [4, 3, 2, 1])\n    A  B  C\n0  4  5  a\n1  3  6  b\n2  2  7  c\n3  1  8  d\n4  4  9  e\n  \n>>> s.replace([1, 2], method='bfill')\n0    3\n1    3\n2    3\n3    4\n4    5\ndtype: int64\n  dict-like `to_replace` \n>>> df.replace({0: 10, 1: 100})\n        A  B  C\n0   10  5  a\n1  100  6  b\n2    2  7  c\n3    3  8  d\n4    4  9  e\n  \n>>> df.replace({'A': 0, 'B': 5}, 100)\n        A    B  C\n0  100  100  a\n1    1    6  b\n2    2    7  c\n3    3    8  d\n4    4    9  e\n  \n>>> df.replace({'A': {0: 100, 4: 400}})\n        A  B  C\n0  100  5  a\n1    1  6  b\n2    2  7  c\n3    3  8  d\n4  400  9  e\n  Regular expression `to_replace` \n>>> df = pd.DataFrame({'A': ['bat', 'foo', 'bait'],\n...                    'B': ['abc', 'bar', 'xyz']})\n>>> df.replace(to_replace=r'^ba.$', value='new', regex=True)\n        A    B\n0   new  abc\n1   foo  new\n2  bait  xyz\n  \n>>> df.replace({'A': r'^ba.$'}, {'A': 'new'}, regex=True)\n        A    B\n0   new  abc\n1   foo  bar\n2  bait  xyz\n  \n>>> df.replace(regex=r'^ba.$', value='new')\n        A    B\n0   new  abc\n1   foo  new\n2  bait  xyz\n  \n>>> df.replace(regex={r'^ba.$': 'new', 'foo': 'xyz'})\n        A    B\n0   new  abc\n1   xyz  new\n2  bait  xyz\n  \n>>> df.replace(regex=[r'^ba.$', 'foo'], value='new')\n        A    B\n0   new  abc\n1   new  new\n2  bait  xyz\n  Compare the behavior of s.replace({'a': None}) and s.replace('a', None) to understand the peculiarities of the to_replace parameter: \n>>> s = pd.Series([10, 'a', 'a', 'b', 'a'])\n  When one uses a dict as the to_replace value, it is like the value(s) in the dict are equal to the value parameter. s.replace({'a': None}) is equivalent to s.replace(to_replace={'a': None}, value=None, method=None): \n>>> s.replace({'a': None})\n0      10\n1    None\n2    None\n3       b\n4    None\ndtype: object\n  When value is not explicitly passed and to_replace is a scalar, list or tuple, replace uses the method parameter (default \u2018pad\u2019) to do the replacement. So this is why the \u2018a\u2019 values are being replaced by 10 in rows 1 and 2 and \u2018b\u2019 in row 4 in this case. \n>>> s.replace('a')\n0    10\n1    10\n2    10\n3     b\n4     b\ndtype: object\n  On the other hand, if None is explicitly passed for value, it will be respected: \n>>> s.replace('a', None)\n0      10\n1    None\n2    None\n3       b\n4    None\ndtype: object\n   \n Changed in version 1.4.0: Previously the explicit None was silently ignored.","title":"pandas.reference.api.pandas.series.replace"},{"text":"numpy.chararray.replace method   chararray.replace(old, new, count=None)[source]\n \nFor each element in self, return a copy of the string with all occurrences of substring old replaced by new.  See also  char.replace","title":"numpy.reference.generated.numpy.chararray.replace"},{"text":"numpy.char.chararray.replace method   char.chararray.replace(old, new, count=None)[source]\n \nFor each element in self, return a copy of the string with all occurrences of substring old replaced by new.  See also  char.replace","title":"numpy.reference.generated.numpy.char.chararray.replace"},{"text":"pandas.Series.str.replace   Series.str.replace(pat, repl, n=- 1, case=None, flags=0, regex=None)[source]\n \nReplace each occurrence of pattern\/regex in the Series\/Index. Equivalent to str.replace() or re.sub(), depending on the regex value.  Parameters \n \npat:str or compiled regex\n\n\nString can be a character sequence or regular expression.  \nrepl:str or callable\n\n\nReplacement string or a callable. The callable is passed the regex match object and must return a replacement string to be used. See re.sub().  \nn:int, default -1 (all)\n\n\nNumber of replacements to make from start.  \ncase:bool, default None\n\n\nDetermines if replace is case sensitive:  If True, case sensitive (the default if pat is a string) Set to False for case insensitive Cannot be set if pat is a compiled regex.   \nflags:int, default 0 (no flags)\n\n\nRegex module flags, e.g. re.IGNORECASE. Cannot be set if pat is a compiled regex.  \nregex:bool, default True\n\n\nDetermines if the passed-in pattern is a regular expression:  If True, assumes the passed-in pattern is a regular expression. If False, treats the pattern as a literal string Cannot be set to False if pat is a compiled regex or repl is a callable.   New in version 0.23.0.     Returns \n Series or Index of object\n\nA copy of the object with all matching occurrences of pat replaced by repl.    Raises \n ValueError\n\n if regex is False and repl is a callable or pat is a compiled regex if pat is a compiled regex and case or flags is set      Notes When pat is a compiled regex, all flags should be included in the compiled regex. Use of case, flags, or regex=False with a compiled regex will raise an error. Examples When pat is a string and regex is True (the default), the given pat is compiled as a regex. When repl is a string, it replaces matching regex patterns as with re.sub(). NaN value(s) in the Series are left as is: \n>>> pd.Series(['foo', 'fuz', np.nan]).str.replace('f.', 'ba', regex=True)\n0    bao\n1    baz\n2    NaN\ndtype: object\n  When pat is a string and regex is False, every pat is replaced with repl as with str.replace(): \n>>> pd.Series(['f.o', 'fuz', np.nan]).str.replace('f.', 'ba', regex=False)\n0    bao\n1    fuz\n2    NaN\ndtype: object\n  When repl is a callable, it is called on every pat using re.sub(). The callable should expect one positional argument (a regex object) and return a string. To get the idea: \n>>> pd.Series(['foo', 'fuz', np.nan]).str.replace('f', repr, regex=True)\n0    <re.Match object; span=(0, 1), match='f'>oo\n1    <re.Match object; span=(0, 1), match='f'>uz\n2                                            NaN\ndtype: object\n  Reverse every lowercase alphabetic word: \n>>> repl = lambda m: m.group(0)[::-1]\n>>> ser = pd.Series(['foo 123', 'bar baz', np.nan])\n>>> ser.str.replace(r'[a-z]+', repl, regex=True)\n0    oof 123\n1    rab zab\n2        NaN\ndtype: object\n  Using regex groups (extract second group and swap case): \n>>> pat = r\"(?P<one>\\w+) (?P<two>\\w+) (?P<three>\\w+)\"\n>>> repl = lambda m: m.group('two').swapcase()\n>>> ser = pd.Series(['One Two Three', 'Foo Bar Baz'])\n>>> ser.str.replace(pat, repl, regex=True)\n0    tWO\n1    bAR\ndtype: object\n  Using a compiled regex with flags \n>>> import re\n>>> regex_pat = re.compile(r'FUZ', flags=re.IGNORECASE)\n>>> pd.Series(['foo', 'fuz', np.nan]).str.replace(regex_pat, 'bar', regex=True)\n0    foo\n1    bar\n2    NaN\ndtype: object","title":"pandas.reference.api.pandas.series.str.replace"},{"text":"pandas.Series.str.translate   Series.str.translate(table)[source]\n \nMap all characters in the string through the given mapping table. Equivalent to standard str.translate().  Parameters \n \ntable:dict\n\n\nTable is a mapping of Unicode ordinals to Unicode ordinals, strings, or None. Unmapped characters are left untouched. Characters mapped to None are deleted. str.maketrans() is a helper function for making translation tables.    Returns \n Series or Index","title":"pandas.reference.api.pandas.series.str.translate"},{"text":"numpy.char.replace   char.replace(a, old, new, count=None)[source]\n \nFor each element in a, return a copy of the string with all occurrences of substring old replaced by new. Calls str.replace element-wise.  Parameters \n \naarray-like of str or unicode\n\n\nold, newstr or unicode\n\n\ncountint, optional\n\n\nIf the optional argument count is given, only the first count occurrences are replaced.    Returns \n \noutndarray\n\n\nOutput array of str or unicode, depending on input type      See also  str.replace","title":"numpy.reference.generated.numpy.char.replace"},{"text":"pandas.Series.str.slice_replace   Series.str.slice_replace(start=None, stop=None, repl=None)[source]\n \nReplace a positional slice of a string with another value.  Parameters \n \nstart:int, optional\n\n\nLeft index position to use for the slice. If not specified (None), the slice is unbounded on the left, i.e. slice from the start of the string.  \nstop:int, optional\n\n\nRight index position to use for the slice. If not specified (None), the slice is unbounded on the right, i.e. slice until the end of the string.  \nrepl:str, optional\n\n\nString for replacement. If not specified (None), the sliced region is replaced with an empty string.    Returns \n Series or Index\n\nSame type as the original object.      See also  Series.str.slice\n\nJust slicing without replacement.    Examples \n>>> s = pd.Series(['a', 'ab', 'abc', 'abdc', 'abcde'])\n>>> s\n0        a\n1       ab\n2      abc\n3     abdc\n4    abcde\ndtype: object\n  Specify just start, meaning replace start until the end of the string with repl. \n>>> s.str.slice_replace(1, repl='X')\n0    aX\n1    aX\n2    aX\n3    aX\n4    aX\ndtype: object\n  Specify just stop, meaning the start of the string to stop is replaced with repl, and the rest of the string is included. \n>>> s.str.slice_replace(stop=2, repl='X')\n0       X\n1       X\n2      Xc\n3     Xdc\n4    Xcde\ndtype: object\n  Specify start and stop, meaning the slice from start to stop is replaced with repl. Everything before or after start and stop is included as is. \n>>> s.str.slice_replace(start=1, stop=3, repl='X')\n0      aX\n1      aX\n2      aX\n3     aXc\n4    aXde\ndtype: object","title":"pandas.reference.api.pandas.series.str.slice_replace"},{"text":"pandas.Series.str.capitalize   Series.str.capitalize()[source]\n \nConvert strings in the Series\/Index to be capitalized. Equivalent to str.capitalize().  Returns \n Series or Index of object\n    See also  Series.str.lower\n\nConverts all characters to lowercase.  Series.str.upper\n\nConverts all characters to uppercase.  Series.str.title\n\nConverts first character of each word to uppercase and remaining to lowercase.  Series.str.capitalize\n\nConverts first character to uppercase and remaining to lowercase.  Series.str.swapcase\n\nConverts uppercase to lowercase and lowercase to uppercase.  Series.str.casefold\n\nRemoves all case distinctions in the string.    Examples \n>>> s = pd.Series(['lower', 'CAPITALS', 'this is a sentence', 'SwApCaSe'])\n>>> s\n0                 lower\n1              CAPITALS\n2    this is a sentence\n3              SwApCaSe\ndtype: object\n  \n>>> s.str.lower()\n0                 lower\n1              capitals\n2    this is a sentence\n3              swapcase\ndtype: object\n  \n>>> s.str.upper()\n0                 LOWER\n1              CAPITALS\n2    THIS IS A SENTENCE\n3              SWAPCASE\ndtype: object\n  \n>>> s.str.title()\n0                 Lower\n1              Capitals\n2    This Is A Sentence\n3              Swapcase\ndtype: object\n  \n>>> s.str.capitalize()\n0                 Lower\n1              Capitals\n2    This is a sentence\n3              Swapcase\ndtype: object\n  \n>>> s.str.swapcase()\n0                 LOWER\n1              capitals\n2    THIS IS A SENTENCE\n3              sWaPcAsE\ndtype: object","title":"pandas.reference.api.pandas.series.str.capitalize"},{"text":"pandas.Series.str.rjust   Series.str.rjust(width, fillchar=' ')[source]\n \nPad left side of strings in the Series\/Index. Equivalent to str.rjust().  Parameters \n \nwidth:int\n\n\nMinimum width of resulting string; additional characters will be filled with fillchar.  \nfillchar:str\n\n\nAdditional character for filling, default is whitespace.    Returns \n \nfilled:Series\/Index of objects.","title":"pandas.reference.api.pandas.series.str.rjust"}]}
{"task_id":19339,"prompt":"def f_19339():\n\treturn ","suffix":"","canonical_solution":"zip(*[('a', 1), ('b', 2), ('c', 3), ('d', 4)])","test_start":"\ndef check(candidate):","test":["\n    assert [a for a in candidate()] == [('a', 'b', 'c', 'd'), (1, 2, 3, 4)]\n"],"entry_point":"f_19339","intent":"unzip the list `[('a', 1), ('b', 2), ('c', 3), ('d', 4)]`","library":[],"docs":[]}
{"task_id":19339,"prompt":"def f_19339(original):\n\treturn ","suffix":"","canonical_solution":"([a for (a, b) in original], [b for (a, b) in original])","test_start":"\ndef check(candidate):","test":["\n    original = [('a', 1), ('b', 2), ('c', 3), ('d', 4)]\n    assert candidate(original) == (['a', 'b', 'c', 'd'], [1, 2, 3, 4])\n","\n    original2 = [([], 1), ([], 2), (5, 3), (6, 4)]\n    assert candidate(original2) == ([[], [], 5, 6], [1, 2, 3, 4])\n"],"entry_point":"f_19339","intent":"unzip list `original`","library":[],"docs":[]}
{"task_id":19339,"prompt":"def f_19339(original):\n\treturn ","suffix":"","canonical_solution":"((a for (a, b) in original), (b for (a, b) in original))","test_start":"\ndef check(candidate):","test":["\n    original = [('a', 1), ('b', 2), ('c', 3), ('d', 4)]\n    result = candidate(original)\n    assert [a for gen in result for a in gen] == ['a','b','c','d',1,2,3,4]\n","\n    original2 = [([], 1), ([], 2), (5, 3), (6, 4)]\n    result2 = candidate(original2)\n    assert [a for gen in result2 for a in gen] == [[], [], 5, 6, 1, 2, 3, 4]\n"],"entry_point":"f_19339","intent":"unzip list `original` and return a generator","library":[],"docs":[]}
{"task_id":19339,"prompt":"def f_19339():\n\treturn ","suffix":"","canonical_solution":"zip(*[('a', 1), ('b', 2), ('c', 3), ('d', 4), ('e',)])","test_start":"\ndef check(candidate):","test":["\n    assert list(candidate()) == [('a', 'b', 'c', 'd', 'e')]\n"],"entry_point":"f_19339","intent":"unzip list `[('a', 1), ('b', 2), ('c', 3), ('d', 4), ('e', )]`","library":[],"docs":[]}
{"task_id":19339,"prompt":"def f_19339():\n\treturn ","suffix":"","canonical_solution":"list(zip_longest(('a', 1), ('b', 2), ('c', 3), ('d', 4), ('e',)))","test_start":"\nfrom itertools import zip_longest\n\ndef check(candidate):","test":["\n    assert(candidate() == [('a', 'b', 'c', 'd', 'e'), (1, 2, 3, 4, None)])\n"],"entry_point":"f_19339","intent":"unzip list `[('a', 1), ('b', 2), ('c', 3), ('d', 4), ('e', )]` and fill empty results with None","library":["itertools"],"docs":[{"text":"itertools.zip_longest(*iterables, fillvalue=None)  \nMake an iterator that aggregates elements from each of the iterables. If the iterables are of uneven length, missing values are filled-in with fillvalue. Iteration continues until the longest iterable is exhausted. Roughly equivalent to: def zip_longest(*args, fillvalue=None):\n    # zip_longest('ABCD', 'xy', fillvalue='-') --> Ax By C- D-\n    iterators = [iter(it) for it in args]\n    num_active = len(iterators)\n    if not num_active:\n        return\n    while True:\n        values = []\n        for i, it in enumerate(iterators):\n            try:\n                value = next(it)\n            except StopIteration:\n                num_active -= 1\n                if not num_active:\n                    return\n                iterators[i] = repeat(fillvalue)\n                value = fillvalue\n            values.append(value)\n        yield tuple(values)\n If one of the iterables is potentially infinite, then the zip_longest() function should be wrapped with something that limits the number of calls (for example islice() or takewhile()). If not specified, fillvalue defaults to None.","title":"python.library.itertools#itertools.zip_longest"},{"text":"zip(*iterables)  \nMake an iterator that aggregates elements from each of the iterables. Returns an iterator of tuples, where the i-th tuple contains the i-th element from each of the argument sequences or iterables. The iterator stops when the shortest input iterable is exhausted. With a single iterable argument, it returns an iterator of 1-tuples. With no arguments, it returns an empty iterator. Equivalent to: def zip(*iterables):\n    # zip('ABCD', 'xy') --> Ax By\n    sentinel = object()\n    iterators = [iter(it) for it in iterables]\n    while iterators:\n        result = []\n        for it in iterators:\n            elem = next(it, sentinel)\n            if elem is sentinel:\n                return\n            result.append(elem)\n        yield tuple(result)\n The left-to-right evaluation order of the iterables is guaranteed. This makes possible an idiom for clustering a data series into n-length groups using zip(*[iter(s)]*n). This repeats the same iterator n times so that each output tuple has the result of n calls to the iterator. This has the effect of dividing the input into n-length chunks. zip() should only be used with unequal length inputs when you don\u2019t care about trailing, unmatched values from the longer iterables. If those values are important, use itertools.zip_longest() instead. zip() in conjunction with the * operator can be used to unzip a list: >>> x = [1, 2, 3]\n>>> y = [4, 5, 6]\n>>> zipped = zip(x, y)\n>>> list(zipped)\n[(1, 4), (2, 5), (3, 6)]\n>>> x2, y2 = zip(*zip(x, y))\n>>> x == list(x2) and y == list(y2)\nTrue","title":"python.library.functions#zip"},{"text":"tf.raw_ops.OrderedMapUnstageNoKey Op removes and returns the (key, value) element with the smallest  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.raw_ops.OrderedMapUnstageNoKey  \ntf.raw_ops.OrderedMapUnstageNoKey(\n    indices, dtypes, capacity=0, memory_limit=0, container='',\n    shared_name='', name=None\n)\n key from the underlying container. If the underlying container does not contain elements, the op will block until it does.\n \n\n\n Args\n  indices   A Tensor of type int32.  \n  dtypes   A list of tf.DTypes that has length >= 1.  \n  capacity   An optional int that is >= 0. Defaults to 0.  \n  memory_limit   An optional int that is >= 0. Defaults to 0.  \n  container   An optional string. Defaults to \"\".  \n  shared_name   An optional string. Defaults to \"\".  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A tuple of Tensor objects (key, values).     key   A Tensor of type int64.  \n  values   A list of Tensor objects of type dtypes.","title":"tensorflow.raw_ops.orderedmapunstagenokey"},{"text":"Unpacker.unpack_list(unpack_item)  \nUnpacks and returns a list of homogeneous items. The list is unpacked one element at a time by first unpacking an unsigned integer flag. If the flag is 1, then the item is unpacked and appended to the list. A flag of 0 indicates the end of the list. unpack_item is the function that is called to unpack the items.","title":"python.library.xdrlib#xdrlib.Unpacker.unpack_list"},{"text":"parser.tuple2st(sequence)  \nThis is the same function as sequence2st(). This entry point is maintained for backward compatibility.","title":"python.library.parser#parser.tuple2st"},{"text":"tf.raw_ops.MapUnstageNoKey Op removes and returns a random (key, value)  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.raw_ops.MapUnstageNoKey  \ntf.raw_ops.MapUnstageNoKey(\n    indices, dtypes, capacity=0, memory_limit=0, container='',\n    shared_name='', name=None\n)\n from the underlying container. If the underlying container does not contain elements, the op will block until it does.\n \n\n\n Args\n  indices   A Tensor of type int32.  \n  dtypes   A list of tf.DTypes that has length >= 1.  \n  capacity   An optional int that is >= 0. Defaults to 0.  \n  memory_limit   An optional int that is >= 0. Defaults to 0.  \n  container   An optional string. Defaults to \"\".  \n  shared_name   An optional string. Defaults to \"\".  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A tuple of Tensor objects (key, values).     key   A Tensor of type int64.  \n  values   A list of Tensor objects of type dtypes.","title":"tensorflow.raw_ops.mapunstagenokey"},{"text":"curses.ascii.ascii(c)  \nReturn the ASCII value corresponding to the low 7 bits of c.","title":"python.library.curses.ascii#curses.ascii.ascii"},{"text":"curses.ascii.unctrl(c)  \nReturn a string representation of the ASCII character c. If c is printable, this string is the character itself. If the character is a control character (0x00\u20130x1f) the string consists of a caret ('^') followed by the corresponding uppercase letter. If the character is an ASCII delete (0x7f) the string is '^?'. If the character has its meta bit (0x80) set, the meta bit is stripped, the preceding rules applied, and '!' prepended to the result.","title":"python.library.curses.ascii#curses.ascii.unctrl"},{"text":"QueryDict.setlistdefault(key, default_list=None)  \nLike setdefault(), except it takes a list of values instead of a single value.","title":"django.ref.request-response#django.http.QueryDict.setlistdefault"},{"text":"pandas.Series.explode   Series.explode(ignore_index=False)[source]\n \nTransform each element of a list-like to a row.  New in version 0.25.0.   Parameters \n \nignore_index:bool, default False\n\n\nIf True, the resulting index will be labeled 0, 1, \u2026, n - 1.  New in version 1.1.0.     Returns \n Series\n\nExploded lists to rows; index will be duplicated for these rows.      See also  Series.str.split\n\nSplit string values on specified separator.  Series.unstack\n\nUnstack, a.k.a. pivot, Series with MultiIndex to produce DataFrame.  DataFrame.melt\n\nUnpivot a DataFrame from wide format to long format.  DataFrame.explode\n\nExplode a DataFrame from list-like columns to long format.    Notes This routine will explode list-likes including lists, tuples, sets, Series, and np.ndarray. The result dtype of the subset rows will be object. Scalars will be returned unchanged, and empty list-likes will result in a np.nan for that row. In addition, the ordering of elements in the output will be non-deterministic when exploding sets. Examples \n>>> s = pd.Series([[1, 2, 3], 'foo', [], [3, 4]])\n>>> s\n0    [1, 2, 3]\n1          foo\n2           []\n3       [3, 4]\ndtype: object\n  \n>>> s.explode()\n0      1\n0      2\n0      3\n1    foo\n2    NaN\n3      3\n3      4\ndtype: object","title":"pandas.reference.api.pandas.series.explode"}]}
{"task_id":1960516,"prompt":"def f_1960516():\n\treturn ","suffix":"","canonical_solution":"json.dumps('3.9')","test_start":"\nimport json\n\ndef check(candidate):","test":["\n    data = candidate()\n    assert json.loads(data) == '3.9'\n"],"entry_point":"f_1960516","intent":"encode `Decimal('3.9')` to a JSON string","library":["json"],"docs":[{"text":"canonical()  \nReturn the canonical encoding of the argument. Currently, the encoding of a Decimal instance is always canonical, so this operation returns its argument unchanged.","title":"python.library.decimal#decimal.Decimal.canonical"},{"text":"to_eng_string(context=None)  \nConvert to a string, using engineering notation if an exponent is needed. Engineering notation has an exponent which is a multiple of 3. This can leave up to 3 digits to the left of the decimal place and may require the addition of either one or two trailing zeros. For example, this converts Decimal('123E+1') to Decimal('1.23E+3').","title":"python.library.decimal#decimal.Decimal.to_eng_string"},{"text":"copy_decimal(num)  \nReturn a copy of the Decimal instance num.","title":"python.library.decimal#decimal.Context.copy_decimal"},{"text":"decimal.HAVE_CONTEXTVAR  \nThe default value is True. If Python is compiled --without-decimal-contextvar, the C version uses a thread-local rather than a coroutine-local context and the value is False. This is slightly faster in some nested context scenarios.","title":"python.library.decimal#decimal.HAVE_CONTEXTVAR"},{"text":"Serializer fields  Each field in a Form class is responsible not only for validating data, but also for \"cleaning\" it \u2014 normalizing it to a consistent format. \u2014 Django documentation  Serializer fields handle converting between primitive values and internal datatypes. They also deal with validating input values, as well as retrieving and setting the values from their parent objects. Note: The serializer fields are declared in fields.py, but by convention you should import them using from rest_framework import serializers and refer to fields as serializers.<FieldName>. Core arguments Each serializer field class constructor takes at least these arguments. Some Field classes take additional, field-specific arguments, but the following should always be accepted: read_only Read-only fields are included in the API output, but should not be included in the input during create or update operations. Any 'read_only' fields that are incorrectly included in the serializer input will be ignored. Set this to True to ensure that the field is used when serializing a representation, but is not used when creating or updating an instance during deserialization. Defaults to False write_only Set this to True to ensure that the field may be used when updating or creating an instance, but is not included when serializing the representation. Defaults to False required Normally an error will be raised if a field is not supplied during deserialization. Set to false if this field is not required to be present during deserialization. Setting this to False also allows the object attribute or dictionary key to be omitted from output when serializing the instance. If the key is not present it will simply not be included in the output representation. Defaults to True. default If set, this gives the default value that will be used for the field if no input value is supplied. If not set the default behaviour is to not populate the attribute at all. The default is not applied during partial update operations. In the partial update case only fields that are provided in the incoming data will have a validated value returned. May be set to a function or other callable, in which case the value will be evaluated each time it is used. When called, it will receive no arguments. If the callable has a requires_context = True attribute, then the serializer field will be passed as an argument. For example: class CurrentUserDefault:\n    \"\"\"\n    May be applied as a `default=...` value on a serializer field.\n    Returns the current user.\n    \"\"\"\n    requires_context = True\n\n    def __call__(self, serializer_field):\n        return serializer_field.context['request'].user\n When serializing the instance, default will be used if the object attribute or dictionary key is not present in the instance. Note that setting a default value implies that the field is not required. Including both the default and required keyword arguments is invalid and will raise an error. allow_null Normally an error will be raised if None is passed to a serializer field. Set this keyword argument to True if None should be considered a valid value. Note that, without an explicit default, setting this argument to True will imply a default value of null for serialization output, but does not imply a default for input deserialization. Defaults to False source The name of the attribute that will be used to populate the field. May be a method that only takes a self argument, such as URLField(source='get_absolute_url'), or may use dotted notation to traverse attributes, such as EmailField(source='user.email'). When serializing fields with dotted notation, it may be necessary to provide a default value if any object is not present or is empty during attribute traversal. The value source='*' has a special meaning, and is used to indicate that the entire object should be passed through to the field. This can be useful for creating nested representations, or for fields which require access to the complete object in order to determine the output representation. Defaults to the name of the field. validators A list of validator functions which should be applied to the incoming field input, and which either raise a validation error or simply return. Validator functions should typically raise serializers.ValidationError, but Django's built-in ValidationError is also supported for compatibility with validators defined in the Django codebase or third party Django packages. error_messages A dictionary of error codes to error messages. label A short text string that may be used as the name of the field in HTML form fields or other descriptive elements. help_text A text string that may be used as a description of the field in HTML form fields or other descriptive elements. initial A value that should be used for pre-populating the value of HTML form fields. You may pass a callable to it, just as you may do with any regular Django Field: import datetime\nfrom rest_framework import serializers\nclass ExampleSerializer(serializers.Serializer):\n    day = serializers.DateField(initial=datetime.date.today)\n style A dictionary of key-value pairs that can be used to control how renderers should render the field. Two examples here are 'input_type' and 'base_template': # Use <input type=\"password\"> for the input.\npassword = serializers.CharField(\n    style={'input_type': 'password'}\n)\n\n# Use a radio input instead of a select input.\ncolor_channel = serializers.ChoiceField(\n    choices=['red', 'green', 'blue'],\n    style={'base_template': 'radio.html'}\n)\n For more details see the HTML & Forms documentation. Boolean fields BooleanField A boolean representation. When using HTML encoded form input be aware that omitting a value will always be treated as setting a field to False, even if it has a default=True option specified. This is because HTML checkbox inputs represent the unchecked state by omitting the value, so REST framework treats omission as if it is an empty checkbox input. Note that Django 2.1 removed the blank kwarg from models.BooleanField. Prior to Django 2.1 models.BooleanField fields were always blank=True. Thus since Django 2.1 default serializers.BooleanField instances will be generated without the required kwarg (i.e. equivalent to required=True) whereas with previous versions of Django, default BooleanField instances will be generated with a required=False option. If you want to control this behaviour manually, explicitly declare the BooleanField on the serializer class, or use the extra_kwargs option to set the required flag. Corresponds to django.db.models.fields.BooleanField. Signature: BooleanField() NullBooleanField A boolean representation that also accepts None as a valid value. Corresponds to django.db.models.fields.NullBooleanField. Signature: NullBooleanField() String fields CharField A text representation. Optionally validates the text to be shorter than max_length and longer than min_length. Corresponds to django.db.models.fields.CharField or django.db.models.fields.TextField. Signature: CharField(max_length=None, min_length=None, allow_blank=False, trim_whitespace=True)  \nmax_length - Validates that the input contains no more than this number of characters. \nmin_length - Validates that the input contains no fewer than this number of characters. \nallow_blank - If set to True then the empty string should be considered a valid value. If set to False then the empty string is considered invalid and will raise a validation error. Defaults to False. \ntrim_whitespace - If set to True then leading and trailing whitespace is trimmed. Defaults to True.  The allow_null option is also available for string fields, although its usage is discouraged in favor of allow_blank. It is valid to set both allow_blank=True and allow_null=True, but doing so means that there will be two differing types of empty value permissible for string representations, which can lead to data inconsistencies and subtle application bugs. EmailField A text representation, validates the text to be a valid e-mail address. Corresponds to django.db.models.fields.EmailField Signature: EmailField(max_length=None, min_length=None, allow_blank=False) RegexField A text representation, that validates the given value matches against a certain regular expression. Corresponds to django.forms.fields.RegexField. Signature: RegexField(regex, max_length=None, min_length=None, allow_blank=False) The mandatory regex argument may either be a string, or a compiled python regular expression object. Uses Django's django.core.validators.RegexValidator for validation. SlugField A RegexField that validates the input against the pattern [a-zA-Z0-9_-]+. Corresponds to django.db.models.fields.SlugField. Signature: SlugField(max_length=50, min_length=None, allow_blank=False) URLField A RegexField that validates the input against a URL matching pattern. Expects fully qualified URLs of the form http:\/\/<host>\/<path>. Corresponds to django.db.models.fields.URLField. Uses Django's django.core.validators.URLValidator for validation. Signature: URLField(max_length=200, min_length=None, allow_blank=False) UUIDField A field that ensures the input is a valid UUID string. The to_internal_value method will return a uuid.UUID instance. On output the field will return a string in the canonical hyphenated format, for example: \"de305d54-75b4-431b-adb2-eb6b9e546013\"\n Signature: UUIDField(format='hex_verbose')  \nformat: Determines the representation format of the uuid value \n'hex_verbose' - The canonical hex representation, including hyphens: \"5ce0e9a5-5ffa-654b-cee0-1238041fb31a\"\n \n'hex' - The compact hex representation of the UUID, not including hyphens: \"5ce0e9a55ffa654bcee01238041fb31a\"\n \n'int' - A 128 bit integer representation of the UUID: \"123456789012312313134124512351145145114\"\n \n'urn' - RFC 4122 URN representation of the UUID: \"urn:uuid:5ce0e9a5-5ffa-654b-cee0-1238041fb31a\" Changing the format parameters only affects representation values. All formats are accepted by to_internal_value\n    FilePathField A field whose choices are limited to the filenames in a certain directory on the filesystem Corresponds to django.forms.fields.FilePathField. Signature: FilePathField(path, match=None, recursive=False, allow_files=True, allow_folders=False, required=None, **kwargs)  \npath - The absolute filesystem path to a directory from which this FilePathField should get its choice. \nmatch - A regular expression, as a string, that FilePathField will use to filter filenames. \nrecursive - Specifies whether all subdirectories of path should be included. Default is False. \nallow_files - Specifies whether files in the specified location should be included. Default is True. Either this or allow_folders must be True. \nallow_folders - Specifies whether folders in the specified location should be included. Default is False. Either this or allow_files must be True.  IPAddressField A field that ensures the input is a valid IPv4 or IPv6 string. Corresponds to django.forms.fields.IPAddressField and django.forms.fields.GenericIPAddressField. Signature: IPAddressField(protocol='both', unpack_ipv4=False, **options)  \nprotocol Limits valid inputs to the specified protocol. Accepted values are 'both' (default), 'IPv4' or 'IPv6'. Matching is case insensitive. \nunpack_ipv4 Unpacks IPv4 mapped addresses like ::ffff:192.0.2.1. If this option is enabled that address would be unpacked to 192.0.2.1. Default is disabled. Can only be used when protocol is set to 'both'.  Numeric fields IntegerField An integer representation. Corresponds to django.db.models.fields.IntegerField, django.db.models.fields.SmallIntegerField, django.db.models.fields.PositiveIntegerField and django.db.models.fields.PositiveSmallIntegerField. Signature: IntegerField(max_value=None, min_value=None)  \nmax_value Validate that the number provided is no greater than this value. \nmin_value Validate that the number provided is no less than this value.  FloatField A floating point representation. Corresponds to django.db.models.fields.FloatField. Signature: FloatField(max_value=None, min_value=None)  \nmax_value Validate that the number provided is no greater than this value. \nmin_value Validate that the number provided is no less than this value.  DecimalField A decimal representation, represented in Python by a Decimal instance. Corresponds to django.db.models.fields.DecimalField. Signature: DecimalField(max_digits, decimal_places, coerce_to_string=None, max_value=None, min_value=None)  \nmax_digits The maximum number of digits allowed in the number. It must be either None or an integer greater than or equal to decimal_places. \ndecimal_places The number of decimal places to store with the number. \ncoerce_to_string Set to True if string values should be returned for the representation, or False if Decimal objects should be returned. Defaults to the same value as the COERCE_DECIMAL_TO_STRING settings key, which will be True unless overridden. If Decimal objects are returned by the serializer, then the final output format will be determined by the renderer. Note that setting localize will force the value to True. \nmax_value Validate that the number provided is no greater than this value. \nmin_value Validate that the number provided is no less than this value. \nlocalize Set to True to enable localization of input and output based on the current locale. This will also force coerce_to_string to True. Defaults to False. Note that data formatting is enabled if you have set USE_L10N=True in your settings file. \nrounding Sets the rounding mode used when quantising to the configured precision. Valid values are decimal module rounding modes. Defaults to None.  Example usage To validate numbers up to 999 with a resolution of 2 decimal places, you would use: serializers.DecimalField(max_digits=5, decimal_places=2)\n And to validate numbers up to anything less than one billion with a resolution of 10 decimal places: serializers.DecimalField(max_digits=19, decimal_places=10)\n This field also takes an optional argument, coerce_to_string. If set to True the representation will be output as a string. If set to False the representation will be left as a Decimal instance and the final representation will be determined by the renderer. If unset, this will default to the same value as the COERCE_DECIMAL_TO_STRING setting, which is True unless set otherwise. Date and time fields DateTimeField A date and time representation. Corresponds to django.db.models.fields.DateTimeField. Signature: DateTimeField(format=api_settings.DATETIME_FORMAT, input_formats=None, default_timezone=None)  \nformat - A string representing the output format. If not specified, this defaults to the same value as the DATETIME_FORMAT settings key, which will be 'iso-8601' unless set. Setting to a format string indicates that to_representation return values should be coerced to string output. Format strings are described below. Setting this value to None indicates that Python datetime objects should be returned by to_representation. In this case the datetime encoding will be determined by the renderer. \ninput_formats - A list of strings representing the input formats which may be used to parse the date. If not specified, the DATETIME_INPUT_FORMATS setting will be used, which defaults to ['iso-8601']. \ndefault_timezone - A pytz.timezone representing the timezone. If not specified and the USE_TZ setting is enabled, this defaults to the current timezone. If USE_TZ is disabled, then datetime objects will be naive.  DateTimeField format strings. Format strings may either be Python strftime formats which explicitly specify the format, or the special string 'iso-8601', which indicates that ISO 8601 style datetimes should be used. (eg '2013-01-29T12:34:56.000000Z') When a value of None is used for the format datetime objects will be returned by to_representation and the final output representation will determined by the renderer class. auto_now and auto_now_add model fields. When using ModelSerializer or HyperlinkedModelSerializer, note that any model fields with auto_now=True or auto_now_add=True will use serializer fields that are read_only=True by default. If you want to override this behavior, you'll need to declare the DateTimeField explicitly on the serializer. For example: class CommentSerializer(serializers.ModelSerializer):\n    created = serializers.DateTimeField()\n\n    class Meta:\n        model = Comment\n DateField A date representation. Corresponds to django.db.models.fields.DateField Signature: DateField(format=api_settings.DATE_FORMAT, input_formats=None)  \nformat - A string representing the output format. If not specified, this defaults to the same value as the DATE_FORMAT settings key, which will be 'iso-8601' unless set. Setting to a format string indicates that to_representation return values should be coerced to string output. Format strings are described below. Setting this value to None indicates that Python date objects should be returned by to_representation. In this case the date encoding will be determined by the renderer. \ninput_formats - A list of strings representing the input formats which may be used to parse the date. If not specified, the DATE_INPUT_FORMATS setting will be used, which defaults to ['iso-8601'].  DateField format strings Format strings may either be Python strftime formats which explicitly specify the format, or the special string 'iso-8601', which indicates that ISO 8601 style dates should be used. (eg '2013-01-29') TimeField A time representation. Corresponds to django.db.models.fields.TimeField Signature: TimeField(format=api_settings.TIME_FORMAT, input_formats=None)  \nformat - A string representing the output format. If not specified, this defaults to the same value as the TIME_FORMAT settings key, which will be 'iso-8601' unless set. Setting to a format string indicates that to_representation return values should be coerced to string output. Format strings are described below. Setting this value to None indicates that Python time objects should be returned by to_representation. In this case the time encoding will be determined by the renderer. \ninput_formats - A list of strings representing the input formats which may be used to parse the date. If not specified, the TIME_INPUT_FORMATS setting will be used, which defaults to ['iso-8601'].  TimeField format strings Format strings may either be Python strftime formats which explicitly specify the format, or the special string 'iso-8601', which indicates that ISO 8601 style times should be used. (eg '12:34:56.000000') DurationField A Duration representation. Corresponds to django.db.models.fields.DurationField The validated_data for these fields will contain a datetime.timedelta instance. The representation is a string following this format '[DD] [HH:[MM:]]ss[.uuuuuu]'. Signature: DurationField(max_value=None, min_value=None)  \nmax_value Validate that the duration provided is no greater than this value. \nmin_value Validate that the duration provided is no less than this value.  Choice selection fields ChoiceField A field that can accept a value out of a limited set of choices. Used by ModelSerializer to automatically generate fields if the corresponding model field includes a choices=\u2026 argument. Signature: ChoiceField(choices)  \nchoices - A list of valid values, or a list of (key, display_name) tuples. \nallow_blank - If set to True then the empty string should be considered a valid value. If set to False then the empty string is considered invalid and will raise a validation error. Defaults to False. \nhtml_cutoff - If set this will be the maximum number of choices that will be displayed by a HTML select drop down. Can be used to ensure that automatically generated ChoiceFields with very large possible selections do not prevent a template from rendering. Defaults to None. \nhtml_cutoff_text - If set this will display a textual indicator if the maximum number of items have been cutoff in an HTML select drop down. Defaults to \"More than {count} items\u2026\"\n  Both the allow_blank and allow_null are valid options on ChoiceField, although it is highly recommended that you only use one and not both. allow_blank should be preferred for textual choices, and allow_null should be preferred for numeric or other non-textual choices. MultipleChoiceField A field that can accept a set of zero, one or many values, chosen from a limited set of choices. Takes a single mandatory argument. to_internal_value returns a set containing the selected values. Signature: MultipleChoiceField(choices)  \nchoices - A list of valid values, or a list of (key, display_name) tuples. \nallow_blank - If set to True then the empty string should be considered a valid value. If set to False then the empty string is considered invalid and will raise a validation error. Defaults to False. \nhtml_cutoff - If set this will be the maximum number of choices that will be displayed by a HTML select drop down. Can be used to ensure that automatically generated ChoiceFields with very large possible selections do not prevent a template from rendering. Defaults to None. \nhtml_cutoff_text - If set this will display a textual indicator if the maximum number of items have been cutoff in an HTML select drop down. Defaults to \"More than {count} items\u2026\"\n  As with ChoiceField, both the allow_blank and allow_null options are valid, although it is highly recommended that you only use one and not both. allow_blank should be preferred for textual choices, and allow_null should be preferred for numeric or other non-textual choices. File upload fields Parsers and file uploads. The FileField and ImageField classes are only suitable for use with MultiPartParser or FileUploadParser. Most parsers, such as e.g. JSON don't support file uploads. Django's regular FILE_UPLOAD_HANDLERS are used for handling uploaded files. FileField A file representation. Performs Django's standard FileField validation. Corresponds to django.forms.fields.FileField. Signature: FileField(max_length=None, allow_empty_file=False, use_url=UPLOADED_FILES_USE_URL)  \nmax_length - Designates the maximum length for the file name. \nallow_empty_file - Designates if empty files are allowed. \nuse_url - If set to True then URL string values will be used for the output representation. If set to False then filename string values will be used for the output representation. Defaults to the value of the UPLOADED_FILES_USE_URL settings key, which is True unless set otherwise.  ImageField An image representation. Validates the uploaded file content as matching a known image format. Corresponds to django.forms.fields.ImageField. Signature: ImageField(max_length=None, allow_empty_file=False, use_url=UPLOADED_FILES_USE_URL)  \nmax_length - Designates the maximum length for the file name. \nallow_empty_file - Designates if empty files are allowed. \nuse_url - If set to True then URL string values will be used for the output representation. If set to False then filename string values will be used for the output representation. Defaults to the value of the UPLOADED_FILES_USE_URL settings key, which is True unless set otherwise.  Requires either the Pillow package or PIL package. The Pillow package is recommended, as PIL is no longer actively maintained. Composite fields ListField A field class that validates a list of objects. Signature: ListField(child=<A_FIELD_INSTANCE>, allow_empty=True, min_length=None, max_length=None)  \nchild - A field instance that should be used for validating the objects in the list. If this argument is not provided then objects in the list will not be validated. \nallow_empty - Designates if empty lists are allowed. \nmin_length - Validates that the list contains no fewer than this number of elements. \nmax_length - Validates that the list contains no more than this number of elements.  For example, to validate a list of integers you might use something like the following: scores = serializers.ListField(\n   child=serializers.IntegerField(min_value=0, max_value=100)\n)\n The ListField class also supports a declarative style that allows you to write reusable list field classes. class StringListField(serializers.ListField):\n    child = serializers.CharField()\n We can now reuse our custom StringListField class throughout our application, without having to provide a child argument to it. DictField A field class that validates a dictionary of objects. The keys in DictField are always assumed to be string values. Signature: DictField(child=<A_FIELD_INSTANCE>, allow_empty=True)  \nchild - A field instance that should be used for validating the values in the dictionary. If this argument is not provided then values in the mapping will not be validated. \nallow_empty - Designates if empty dictionaries are allowed.  For example, to create a field that validates a mapping of strings to strings, you would write something like this: document = DictField(child=CharField())\n You can also use the declarative style, as with ListField. For example: class DocumentField(DictField):\n    child = CharField()\n HStoreField A preconfigured DictField that is compatible with Django's postgres HStoreField. Signature: HStoreField(child=<A_FIELD_INSTANCE>, allow_empty=True)  \nchild - A field instance that is used for validating the values in the dictionary. The default child field accepts both empty strings and null values. \nallow_empty - Designates if empty dictionaries are allowed.  Note that the child field must be an instance of CharField, as the hstore extension stores values as strings. JSONField A field class that validates that the incoming data structure consists of valid JSON primitives. In its alternate binary mode, it will represent and validate JSON-encoded binary strings. Signature: JSONField(binary, encoder)  \nbinary - If set to True then the field will output and validate a JSON encoded string, rather than a primitive data structure. Defaults to False. \nencoder - Use this JSON encoder to serialize input object. Defaults to None.  Miscellaneous fields ReadOnlyField A field class that simply returns the value of the field without modification. This field is used by default with ModelSerializer when including field names that relate to an attribute rather than a model field. Signature: ReadOnlyField() For example, if has_expired was a property on the Account model, then the following serializer would automatically generate it as a ReadOnlyField: class AccountSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = Account\n        fields = ['id', 'account_name', 'has_expired']\n HiddenField A field class that does not take a value based on user input, but instead takes its value from a default value or callable. Signature: HiddenField() For example, to include a field that always provides the current time as part of the serializer validated data, you would use the following: modified = serializers.HiddenField(default=timezone.now)\n The HiddenField class is usually only needed if you have some validation that needs to run based on some pre-provided field values, but you do not want to expose all of those fields to the end user. For further examples on HiddenField see the validators documentation. ModelField A generic field that can be tied to any arbitrary model field. The ModelField class delegates the task of serialization\/deserialization to its associated model field. This field can be used to create serializer fields for custom model fields, without having to create a new custom serializer field. This field is used by ModelSerializer to correspond to custom model field classes. Signature: ModelField(model_field=<Django ModelField instance>) The ModelField class is generally intended for internal use, but can be used by your API if needed. In order to properly instantiate a ModelField, it must be passed a field that is attached to an instantiated model. For example: ModelField(model_field=MyModel()._meta.get_field('custom_field')) SerializerMethodField This is a read-only field. It gets its value by calling a method on the serializer class it is attached to. It can be used to add any sort of data to the serialized representation of your object. Signature: SerializerMethodField(method_name=None)  \nmethod_name - The name of the method on the serializer to be called. If not included this defaults to get_<field_name>.  The serializer method referred to by the method_name argument should accept a single argument (in addition to self), which is the object being serialized. It should return whatever you want to be included in the serialized representation of the object. For example: from django.contrib.auth.models import User\nfrom django.utils.timezone import now\nfrom rest_framework import serializers\n\nclass UserSerializer(serializers.ModelSerializer):\n    days_since_joined = serializers.SerializerMethodField()\n\n    class Meta:\n        model = User\n        fields = '__all__'\n\n    def get_days_since_joined(self, obj):\n        return (now() - obj.date_joined).days\n Custom fields If you want to create a custom field, you'll need to subclass Field and then override either one or both of the .to_representation() and .to_internal_value() methods. These two methods are used to convert between the initial datatype, and a primitive, serializable datatype. Primitive datatypes will typically be any of a number, string, boolean, date\/time\/datetime or None. They may also be any list or dictionary like object that only contains other primitive objects. Other types might be supported, depending on the renderer that you are using. The .to_representation() method is called to convert the initial datatype into a primitive, serializable datatype. The .to_internal_value() method is called to restore a primitive datatype into its internal python representation. This method should raise a serializers.ValidationError if the data is invalid. Examples A Basic Custom Field Let's look at an example of serializing a class that represents an RGB color value: class Color:\n    \"\"\"\n    A color represented in the RGB colorspace.\n    \"\"\"\n    def __init__(self, red, green, blue):\n        assert(red >= 0 and green >= 0 and blue >= 0)\n        assert(red < 256 and green < 256 and blue < 256)\n        self.red, self.green, self.blue = red, green, blue\n\nclass ColorField(serializers.Field):\n    \"\"\"\n    Color objects are serialized into 'rgb(#, #, #)' notation.\n    \"\"\"\n    def to_representation(self, value):\n        return \"rgb(%d, %d, %d)\" % (value.red, value.green, value.blue)\n\n    def to_internal_value(self, data):\n        data = data.strip('rgb(').rstrip(')')\n        red, green, blue = [int(col) for col in data.split(',')]\n        return Color(red, green, blue)\n By default field values are treated as mapping to an attribute on the object. If you need to customize how the field value is accessed and set you need to override .get_attribute() and\/or .get_value(). As an example, let's create a field that can be used to represent the class name of the object being serialized: class ClassNameField(serializers.Field):\n    def get_attribute(self, instance):\n        # We pass the object instance onto `to_representation`,\n        # not just the field attribute.\n        return instance\n\n    def to_representation(self, value):\n        \"\"\"\n        Serialize the value's class name.\n        \"\"\"\n        return value.__class__.__name__\n Raising validation errors Our ColorField class above currently does not perform any data validation. To indicate invalid data, we should raise a serializers.ValidationError, like so: def to_internal_value(self, data):\n    if not isinstance(data, str):\n        msg = 'Incorrect type. Expected a string, but got %s'\n        raise ValidationError(msg % type(data).__name__)\n\n    if not re.match(r'^rgb\\([0-9]+,[0-9]+,[0-9]+\\)$', data):\n        raise ValidationError('Incorrect format. Expected `rgb(#,#,#)`.')\n\n    data = data.strip('rgb(').rstrip(')')\n    red, green, blue = [int(col) for col in data.split(',')]\n\n    if any([col > 255 or col < 0 for col in (red, green, blue)]):\n        raise ValidationError('Value out of range. Must be between 0 and 255.')\n\n    return Color(red, green, blue)\n The .fail() method is a shortcut for raising ValidationError that takes a message string from the error_messages dictionary. For example: default_error_messages = {\n    'incorrect_type': 'Incorrect type. Expected a string, but got {input_type}',\n    'incorrect_format': 'Incorrect format. Expected `rgb(#,#,#)`.',\n    'out_of_range': 'Value out of range. Must be between 0 and 255.'\n}\n\ndef to_internal_value(self, data):\n    if not isinstance(data, str):\n        self.fail('incorrect_type', input_type=type(data).__name__)\n\n    if not re.match(r'^rgb\\([0-9]+,[0-9]+,[0-9]+\\)$', data):\n        self.fail('incorrect_format')\n\n    data = data.strip('rgb(').rstrip(')')\n    red, green, blue = [int(col) for col in data.split(',')]\n\n    if any([col > 255 or col < 0 for col in (red, green, blue)]):\n        self.fail('out_of_range')\n\n    return Color(red, green, blue)\n This style keeps your error messages cleaner and more separated from your code, and should be preferred. Using source='*' Here we'll take an example of a flat DataPoint model with x_coordinate and y_coordinate attributes. class DataPoint(models.Model):\n    label = models.CharField(max_length=50)\n    x_coordinate = models.SmallIntegerField()\n    y_coordinate = models.SmallIntegerField()\n Using a custom field and source='*' we can provide a nested representation of the coordinate pair: class CoordinateField(serializers.Field):\n\n    def to_representation(self, value):\n        ret = {\n            \"x\": value.x_coordinate,\n            \"y\": value.y_coordinate\n        }\n        return ret\n\n    def to_internal_value(self, data):\n        ret = {\n            \"x_coordinate\": data[\"x\"],\n            \"y_coordinate\": data[\"y\"],\n        }\n        return ret\n\n\nclass DataPointSerializer(serializers.ModelSerializer):\n    coordinates = CoordinateField(source='*')\n\n    class Meta:\n        model = DataPoint\n        fields = ['label', 'coordinates']\n Note that this example doesn't handle validation. Partly for that reason, in a real project, the coordinate nesting might be better handled with a nested serializer using source='*', with two IntegerField instances, each with their own source pointing to the relevant field. The key points from the example, though, are:   to_representation is passed the entire DataPoint object and must map from that to the desired output. >>> instance = DataPoint(label='Example', x_coordinate=1, y_coordinate=2)\n>>> out_serializer = DataPointSerializer(instance)\n>>> out_serializer.data\nReturnDict([('label', 'Example'), ('coordinates', {'x': 1, 'y': 2})])\n   Unless our field is to be read-only, to_internal_value must map back to a dict suitable for updating our target object. With source='*', the return from to_internal_value will update the root validated data dictionary, rather than a single key. >>> data = {\n...     \"label\": \"Second Example\",\n...     \"coordinates\": {\n...         \"x\": 3,\n...         \"y\": 4,\n...     }\n... }\n>>> in_serializer = DataPointSerializer(data=data)\n>>> in_serializer.is_valid()\nTrue\n>>> in_serializer.validated_data\nOrderedDict([('label', 'Second Example'),\n             ('y_coordinate', 4),\n             ('x_coordinate', 3)])\n   For completeness lets do the same thing again but with the nested serializer approach suggested above: class NestedCoordinateSerializer(serializers.Serializer):\n    x = serializers.IntegerField(source='x_coordinate')\n    y = serializers.IntegerField(source='y_coordinate')\n\n\nclass DataPointSerializer(serializers.ModelSerializer):\n    coordinates = NestedCoordinateSerializer(source='*')\n\n    class Meta:\n        model = DataPoint\n        fields = ['label', 'coordinates']\n Here the mapping between the target and source attribute pairs (x and x_coordinate, y and y_coordinate) is handled in the IntegerField declarations. It's our NestedCoordinateSerializer that takes source='*'. Our new DataPointSerializer exhibits the same behaviour as the custom field approach. Serializing: >>> out_serializer = DataPointSerializer(instance)\n>>> out_serializer.data\nReturnDict([('label', 'testing'),\n            ('coordinates', OrderedDict([('x', 1), ('y', 2)]))])\n Deserializing: >>> in_serializer = DataPointSerializer(data=data)\n>>> in_serializer.is_valid()\nTrue\n>>> in_serializer.validated_data\nOrderedDict([('label', 'still testing'),\n             ('x_coordinate', 3),\n             ('y_coordinate', 4)])\n But we also get the built-in validation for free: >>> invalid_data = {\n...     \"label\": \"still testing\",\n...     \"coordinates\": {\n...         \"x\": 'a',\n...         \"y\": 'b',\n...     }\n... }\n>>> invalid_serializer = DataPointSerializer(data=invalid_data)\n>>> invalid_serializer.is_valid()\nFalse\n>>> invalid_serializer.errors\nReturnDict([('coordinates',\n             {'x': ['A valid integer is required.'],\n              'y': ['A valid integer is required.']})])\n For this reason, the nested serializer approach would be the first to try. You would use the custom field approach when the nested serializer becomes infeasible or overly complex. Third party packages The following third party packages are also available. DRF Compound Fields The drf-compound-fields package provides \"compound\" serializer fields, such as lists of simple values, which can be described by other fields rather than serializers with the many=True option. Also provided are fields for typed dictionaries and values that can be either a specific type or a list of items of that type. DRF Extra Fields The drf-extra-fields package provides extra serializer fields for REST framework, including Base64ImageField and PointField classes. djangorestframework-recursive the djangorestframework-recursive package provides a RecursiveField for serializing and deserializing recursive structures django-rest-framework-gis The django-rest-framework-gis package provides geographic addons for django rest framework like a GeometryField field and a GeoJSON serializer. django-rest-framework-hstore The django-rest-framework-hstore package provides an HStoreField to support django-hstore DictionaryField model field. fields.py","title":"django_rest_framework.api-guide.fields.index#decimalfield"},{"text":"decimal.MAX_EMAX","title":"python.library.decimal#decimal.MAX_EMAX"},{"text":"is_canonical()  \nReturn True if the argument is canonical and False otherwise. Currently, a Decimal instance is always canonical, so this operation always returns True.","title":"python.library.decimal#decimal.Decimal.is_canonical"},{"text":"decimal.MAX_PREC","title":"python.library.decimal#decimal.MAX_PREC"},{"text":"canonical(x)  \nReturns the same Decimal object x.","title":"python.library.decimal#decimal.Context.canonical"},{"text":"decimal.MIN_ETINY","title":"python.library.decimal#decimal.MIN_ETINY"}]}
{"task_id":1024847,"prompt":"def f_1024847(d):\n\t","suffix":"\n\treturn d","canonical_solution":"d['mynewkey'] = 'mynewvalue'","test_start":"\ndef check(candidate):","test":["\n    assert candidate({'key': 'value'}) == {'key': 'value', 'mynewkey': 'mynewvalue'}\n"],"entry_point":"f_1024847","intent":"Add key \"mynewkey\" to dictionary `d` with value \"mynewvalue\"","library":[],"docs":[]}
{"task_id":1024847,"prompt":"def f_1024847(data):\n\t","suffix":"\n\treturn data","canonical_solution":"data.update({'a': 1, })","test_start":"\ndef check(candidate):","test":["\n    assert candidate({'key': 'value'}) == {'key': 'value', 'a': 1}\n","\n    assert candidate({'key': 'value', 'a' : 2}) == {'key': 'value', 'a': 1}\n"],"entry_point":"f_1024847","intent":"Add key 'a' to dictionary `data` with value 1","library":[],"docs":[]}
{"task_id":1024847,"prompt":"def f_1024847(data):\n\t","suffix":"\n\treturn data","canonical_solution":"data.update(dict(a=1))","test_start":"\ndef check(candidate):","test":["\n    assert candidate({'key': 'value'}) == {'key': 'value', 'a': 1}\n","\n    assert candidate({'key': 'value', 'a' : 2}) == {'key': 'value', 'a': 1}\n"],"entry_point":"f_1024847","intent":"Add key 'a' to dictionary `data` with value 1","library":[],"docs":[]}
{"task_id":1024847,"prompt":"def f_1024847(data):\n\t","suffix":"\n\treturn data","canonical_solution":"data.update(a=1)","test_start":"\ndef check(candidate):","test":["\n    assert candidate({'key': 'value'}) == {'key': 'value', 'a': 1}\n","\n    assert candidate({'key': 'value', 'a' : 2}) == {'key': 'value', 'a': 1}\n"],"entry_point":"f_1024847","intent":"Add key 'a' to dictionary `data` with value 1","library":[],"docs":[]}
{"task_id":35837346,"prompt":"def f_35837346(matrix):\n\treturn ","suffix":"","canonical_solution":"max([max(i) for i in matrix])","test_start":"\ndef check(candidate):","test":["\n    assert candidate([[1,2,3],[4,5,6],[7,8,9]]) == 9\n","\n    assert candidate([[1.3,2.8],[4.2,10],[7.9,8.1,5]]) == 10\n"],"entry_point":"f_35837346","intent":"find maximal value in matrix `matrix`","library":[],"docs":[]}
{"task_id":20457038,"prompt":"def f_20457038(answer):\n\t","suffix":"\n\treturn answer","canonical_solution":"answer = str(round(answer, 2))","test_start":"\ndef check(candidate):","test":["\n    assert candidate(2.34351) == \"2.34\"\n","\n    assert candidate(99.375) == \"99.38\"\n","\n    assert candidate(4.1) == \"4.1\"\n","\n    assert candidate(3) == \"3\"\n"],"entry_point":"f_20457038","intent":"Round number `answer` to 2 precision after the decimal point","library":[],"docs":[]}
{"task_id":2890896,"prompt":"def f_2890896(s):\n\t","suffix":"\n\treturn ip","canonical_solution":"ip = re.findall('[0-9]+(?:\\\\.[0-9]+){3}', s)","test_start":"\nimport re\n\ndef check(candidate):","test":["\n    assert candidate(\"<html><head><title>Current IP Check<\/title><\/head><body>Current IP Address: 165.91.15.131<\/body><\/html>\") == [\"165.91.15.131\"]\n","\n    assert candidate(\"<html><head><title>Current IP Check<\/title><\/head><body>Current IP Address: 165.91.15.131 and this is not a IP Address: 165.91.15<\/body><\/html>\") == [\"165.91.15.131\"]\n","\n    assert candidate(\"<html><head><title>Current IP Check<\/title><\/head><body>Current IP Address: 192.168.1.1 & this is another IP address: 192.168.1.2<\/body><\/html>\") == [\"192.168.1.1\", \"192.168.1.2\"]\n"],"entry_point":"f_2890896","intent":"extract ip address `ip` from an html string `s`","library":["re"],"docs":[{"text":"socket.inet_aton(ip_string)  \nConvert an IPv4 address from dotted-quad string format (for example, \u2018123.45.67.89\u2019) to 32-bit packed binary format, as a bytes object four characters in length. This is useful when conversing with a program that uses the standard C library and needs objects of type struct in_addr, which is the C type for the 32-bit packed binary this function returns. inet_aton() also accepts strings with less than three dots; see the Unix manual page inet(3) for details. If the IPv4 address string passed to this function is invalid, OSError will be raised. Note that exactly what is valid depends on the underlying C implementation of inet_aton(). inet_aton() does not support IPv6, and inet_pton() should be used instead for IPv4\/v6 dual stack support.","title":"python.library.socket#socket.inet_aton"},{"text":"ip  \nThe address (IPv4Address) without network information. >>> interface = IPv4Interface('192.0.2.5\/24')\n>>> interface.ip\nIPv4Address('192.0.2.5')","title":"python.library.ipaddress#ipaddress.IPv4Interface.ip"},{"text":"raw_decode(s)  \nDecode a JSON document from s (a str beginning with a JSON document) and return a 2-tuple of the Python representation and the index in s where the document ended. This can be used to decode a JSON document from a string that may have extraneous data at the end.","title":"python.library.json#json.JSONDecoder.raw_decode"},{"text":"email.utils.parseaddr(address)  \nParse address \u2013 which should be the value of some address-containing field such as To or Cc \u2013 into its constituent realname and email address parts. Returns a tuple of that information, unless the parse fails, in which case a 2-tuple of ('', '') is returned.","title":"python.library.email.utils#email.utils.parseaddr"},{"text":"aliased_name(s)[source]\n \nReturn 'PROPNAME or alias' if s has an alias, else return 'PROPNAME'. e.g., for the line markerfacecolor property, which has an alias, return 'markerfacecolor or mfc' and for the transform property, which does not, return 'transform'.","title":"matplotlib._as_gen.matplotlib.artist.artistinspector#matplotlib.artist.ArtistInspector.aliased_name"},{"text":"quopri.decodestring(s, header=False)  \nLike decode(), except that it accepts a source bytes and returns the corresponding decoded bytes.","title":"python.library.quopri#quopri.decodestring"},{"text":"aliased_name_rest(s, target)[source]\n \nReturn 'PROPNAME or alias' if s has an alias, else return 'PROPNAME', formatted for reST. e.g., for the line markerfacecolor property, which has an alias, return 'markerfacecolor or mfc' and for the transform property, which does not, return 'transform'.","title":"matplotlib._as_gen.matplotlib.artist.artistinspector#matplotlib.artist.ArtistInspector.aliased_name_rest"},{"text":"ip","title":"python.library.ipaddress#ipaddress.IPv6Interface.ip"},{"text":"unpack_ipv4  \nUnpacks IPv4 mapped addresses like ::ffff:192.0.2.1. If this option is enabled that address would be unpacked to 192.0.2.1. Default is disabled. Can only be used when protocol is set to 'both'.","title":"django.ref.forms.fields#django.forms.GenericIPAddressField.unpack_ipv4"},{"text":"HTMLParser.getpos()  \nReturn current line number and offset.","title":"python.library.html.parser#html.parser.HTMLParser.getpos"}]}
{"task_id":29836836,"prompt":"def f_29836836(df):\n\treturn ","suffix":"","canonical_solution":"df.groupby('A').filter(lambda x: len(x) > 1)","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    assert candidate(pd.DataFrame([[1, 2], [1, 4], [5, 6]], columns=['A', 'B'])).equals(pd.DataFrame([[1, 2], [1, 4]], columns=['A', 'B'])) is True\n","\n    assert candidate(pd.DataFrame([[1, 2], [1, 4], [1, 6]], columns=['A', 'B'])).equals(pd.DataFrame([[1, 2], [1, 4], [1, 6]], columns=['A', 'B'])) is True\n"],"entry_point":"f_29836836","intent":"filter dataframe `df` by values in column `A` that appear more than once","library":["pandas"],"docs":[{"text":"skimage.util.unique_rows(ar) [source]\n \nRemove repeated rows from a 2D array. In particular, if given an array of coordinates of shape (Npoints, Ndim), it will remove repeated points.  Parameters \n \nar2-D ndarray \n\nThe input array.    Returns \n \nar_out2-D ndarray \n\nA copy of the input array with repeated rows removed.    Raises \n \nValueErrorif ar is not two-dimensional. \n   Notes The function will generate a copy of ar if it is not C-contiguous, which will negatively affect performance for large input arrays. Examples >>> ar = np.array([[1, 0, 1],\n...                [0, 1, 0],\n...                [1, 0, 1]], np.uint8)\n>>> unique_rows(ar)\narray([[0, 1, 0],\n       [1, 0, 1]], dtype=uint8)","title":"skimage.api.skimage.util#skimage.util.unique_rows"},{"text":"pandas.DataFrame.drop_duplicates   DataFrame.drop_duplicates(subset=None, keep='first', inplace=False, ignore_index=False)[source]\n \nReturn DataFrame with duplicate rows removed. Considering certain columns is optional. Indexes, including time indexes are ignored.  Parameters \n \nsubset:column label or sequence of labels, optional\n\n\nOnly consider certain columns for identifying duplicates, by default use all of the columns.  \nkeep:{\u2018first\u2019, \u2018last\u2019, False}, default \u2018first\u2019\n\n\nDetermines which duplicates (if any) to keep. - first : Drop duplicates except for the first occurrence. - last : Drop duplicates except for the last occurrence. - False : Drop all duplicates.  \ninplace:bool, default False\n\n\nWhether to drop duplicates in place or to return a copy.  \nignore_index:bool, default False\n\n\nIf True, the resulting axis will be labeled 0, 1, \u2026, n - 1.  New in version 1.0.0.     Returns \n DataFrame or None\n\nDataFrame with duplicates removed or None if inplace=True.      See also  DataFrame.value_counts\n\nCount unique combinations of columns.    Examples Consider dataset containing ramen rating. \n>>> df = pd.DataFrame({\n...     'brand': ['Yum Yum', 'Yum Yum', 'Indomie', 'Indomie', 'Indomie'],\n...     'style': ['cup', 'cup', 'cup', 'pack', 'pack'],\n...     'rating': [4, 4, 3.5, 15, 5]\n... })\n>>> df\n    brand style  rating\n0  Yum Yum   cup     4.0\n1  Yum Yum   cup     4.0\n2  Indomie   cup     3.5\n3  Indomie  pack    15.0\n4  Indomie  pack     5.0\n  By default, it removes duplicate rows based on all columns. \n>>> df.drop_duplicates()\n    brand style  rating\n0  Yum Yum   cup     4.0\n2  Indomie   cup     3.5\n3  Indomie  pack    15.0\n4  Indomie  pack     5.0\n  To remove duplicates on specific column(s), use subset. \n>>> df.drop_duplicates(subset=['brand'])\n    brand style  rating\n0  Yum Yum   cup     4.0\n2  Indomie   cup     3.5\n  To remove duplicates and keep last occurrences, use keep. \n>>> df.drop_duplicates(subset=['brand', 'style'], keep='last')\n    brand style  rating\n1  Yum Yum   cup     4.0\n2  Indomie   cup     3.5\n4  Indomie  pack     5.0","title":"pandas.reference.api.pandas.dataframe.drop_duplicates"},{"text":"numpy.polynomial.legendre.Legendre.has_samecoef method   polynomial.legendre.Legendre.has_samecoef(other)[source]\n \nCheck if coefficients match.  New in version 1.6.0.   Parameters \n \notherclass instance\n\n\nThe other class must have the coef attribute.    Returns \n \nboolboolean\n\n\nTrue if the coefficients are the same, False otherwise.","title":"numpy.reference.generated.numpy.polynomial.legendre.legendre.has_samecoef"},{"text":"Options.unique_together  \n Use UniqueConstraint with the constraints option instead. UniqueConstraint provides more functionality than unique_together. unique_together may be deprecated in the future.  Sets of field names that, taken together, must be unique: unique_together = [['driver', 'restaurant']]\n This is a list of lists that must be unique when considered together. It\u2019s used in the Django admin and is enforced at the database level (i.e., the appropriate UNIQUE statements are included in the CREATE TABLE statement). For convenience, unique_together can be a single list when dealing with a single set of fields: unique_together = ['driver', 'restaurant']\n A ManyToManyField cannot be included in unique_together. (It\u2019s not clear what that would even mean!) If you need to validate uniqueness related to a ManyToManyField, try using a signal or an explicit through model. The ValidationError raised during model validation when the constraint is violated has the unique_together error code.","title":"django.ref.models.options#django.db.models.Options.unique_together"},{"text":"curses.setupterm(term=None, fd=-1)  \nInitialize the terminal. term is a string giving the terminal name, or None; if omitted or None, the value of the TERM environment variable will be used. fd is the file descriptor to which any initialization sequences will be sent; if not supplied or -1, the file descriptor for sys.stdout will be used.","title":"python.library.curses#curses.setupterm"},{"text":"matplotlib.transforms                          Matplotlib includes a framework for arbitrary geometric transformations that is used determine the final position of all elements drawn on the canvas. Transforms are composed into trees of TransformNode objects whose actual value depends on their children. When the contents of children change, their parents are automatically invalidated. The next time an invalidated transform is accessed, it is recomputed to reflect those changes. This invalidation\/caching approach prevents unnecessary recomputations of transforms, and contributes to better interactive performance. For example, here is a graph of the transform tree used to plot data to the graph:  The framework can be used for both affine and non-affine transformations. However, for speed, we want use the backend renderers to perform affine transformations whenever possible. Therefore, it is possible to perform just the affine or non-affine part of a transformation on a set of data. The affine is always assumed to occur after the non-affine. For any transform: full transform == non-affine part + affine part\n The backends are not expected to handle non-affine transformations themselves.   classmatplotlib.transforms.Affine2D(matrix=None, **kwargs)[source]\n \nBases: matplotlib.transforms.Affine2DBase A mutable 2D affine transformation. Initialize an Affine transform from a 3x3 numpy float array: a c e\nb d f\n0 0 1\n If matrix is None, initialize with the identity transform.   __init__(matrix=None, **kwargs)[source]\n \nInitialize an Affine transform from a 3x3 numpy float array: a c e\nb d f\n0 0 1\n If matrix is None, initialize with the identity transform. \n   __module__='matplotlib.transforms'\n\n   __str__()[source]\n \nReturn str(self). \n   clear()[source]\n \nReset the underlying matrix to the identity transform. \n   staticfrom_values(a, b, c, d, e, f)[source]\n \nCreate a new Affine2D instance from the given values: a c e\nb d f\n0 0 1\n . \n   get_matrix()[source]\n \nGet the underlying transformation matrix as a 3x3 numpy array: a c e\nb d f\n0 0 1\n . \n   staticidentity()[source]\n \nReturn a new Affine2D object that is the identity transform. Unless this transform will be mutated later on, consider using the faster IdentityTransform class instead. \n   rotate(theta)[source]\n \nAdd a rotation (in radians) to this transform in place. Returns self, so this method can easily be chained with more calls to rotate(), rotate_deg(), translate() and scale(). \n   rotate_around(x, y, theta)[source]\n \nAdd a rotation (in radians) around the point (x, y) in place. Returns self, so this method can easily be chained with more calls to rotate(), rotate_deg(), translate() and scale(). \n   rotate_deg(degrees)[source]\n \nAdd a rotation (in degrees) to this transform in place. Returns self, so this method can easily be chained with more calls to rotate(), rotate_deg(), translate() and scale(). \n   rotate_deg_around(x, y, degrees)[source]\n \nAdd a rotation (in degrees) around the point (x, y) in place. Returns self, so this method can easily be chained with more calls to rotate(), rotate_deg(), translate() and scale(). \n   scale(sx, sy=None)[source]\n \nAdd a scale in place. If sy is None, the same scale is applied in both the x- and y-directions. Returns self, so this method can easily be chained with more calls to rotate(), rotate_deg(), translate() and scale(). \n   set(other)[source]\n \nSet this transformation from the frozen copy of another Affine2DBase object. \n   set_matrix(mtx)[source]\n \nSet the underlying transformation matrix from a 3x3 numpy array: a c e\nb d f\n0 0 1\n . \n   skew(xShear, yShear)[source]\n \nAdd a skew in place. xShear and yShear are the shear angles along the x- and y-axes, respectively, in radians. Returns self, so this method can easily be chained with more calls to rotate(), rotate_deg(), translate() and scale(). \n   skew_deg(xShear, yShear)[source]\n \nAdd a skew in place. xShear and yShear are the shear angles along the x- and y-axes, respectively, in degrees. Returns self, so this method can easily be chained with more calls to rotate(), rotate_deg(), translate() and scale(). \n   translate(tx, ty)[source]\n \nAdd a translation in place. Returns self, so this method can easily be chained with more calls to rotate(), rotate_deg(), translate() and scale(). \n \n   classmatplotlib.transforms.Affine2DBase(*args, **kwargs)[source]\n \nBases: matplotlib.transforms.AffineBase The base class of all 2D affine transformations. 2D affine transformations are performed using a 3x3 numpy array: a c e\nb d f\n0 0 1\n This class provides the read-only interface. For a mutable 2D affine transformation, use Affine2D. Subclasses of this class will generally only need to override a constructor and get_matrix() that generates a custom 3x3 matrix.  Parameters \n \nshorthand_namestr\n\n\nA string representing the \"name\" of the transform. The name carries no significance other than to improve the readability of str(transform) when DEBUG=True.       __module__='matplotlib.transforms'\n\n   frozen()[source]\n \nReturn a frozen copy of this transform node. The frozen copy will not be updated when its children change. Useful for storing a previously known state of a transform where copy.deepcopy() might normally be used. \n   has_inverse=True\n \nTrue if this transform has a corresponding inverse transform. \n   input_dims=2\n \nThe number of input dimensions of this transform. Must be overridden (with integers) in the subclass. \n   inverted()[source]\n \nReturn the corresponding inverse transformation. It holds x == self.inverted().transform(self.transform(x)). The return value of this method should be treated as temporary. An update to self does not cause a corresponding update to its inverted copy. \n   propertyis_separable\n \nbool(x) -> bool Returns True when the argument x is true, False otherwise. The builtins True and False are the only two instances of the class bool. The class bool is a subclass of the class int, and cannot be subclassed. \n   output_dims=2\n \nThe number of output dimensions of this transform. Must be overridden (with integers) in the subclass. \n   to_values()[source]\n \nReturn the values of the matrix as an (a, b, c, d, e, f) tuple. \n   transform_affine(points)[source]\n \nApply only the affine part of this transformation on the given array of values. transform(values) is always equivalent to transform_affine(transform_non_affine(values)). In non-affine transformations, this is generally a no-op. In affine transformations, this is equivalent to transform(values).  Parameters \n \nvaluesarray\n\n\nThe input values as NumPy array of length input_dims or shape (N x input_dims).    Returns \n array\n\nThe output values as NumPy array of length input_dims or shape (N x output_dims), depending on the input.     \n \n   classmatplotlib.transforms.AffineBase(*args, **kwargs)[source]\n \nBases: matplotlib.transforms.Transform The base class of all affine transformations of any number of dimensions.  Parameters \n \nshorthand_namestr\n\n\nA string representing the \"name\" of the transform. The name carries no significance other than to improve the readability of str(transform) when DEBUG=True.       __array__(*args, **kwargs)[source]\n \nArray interface to get at this Transform's affine matrix. \n   __eq__(other)[source]\n \nReturn self==value. \n   __hash__=None\n\n   __init__(*args, **kwargs)[source]\n \n Parameters \n \nshorthand_namestr\n\n\nA string representing the \"name\" of the transform. The name carries no significance other than to improve the readability of str(transform) when DEBUG=True.     \n   __module__='matplotlib.transforms'\n\n   get_affine()[source]\n \nGet the affine part of this transform. \n   is_affine=True\n\n   transform(values)[source]\n \nApply this transformation on the given array of values.  Parameters \n \nvaluesarray\n\n\nThe input values as NumPy array of length input_dims or shape (N x input_dims).    Returns \n array\n\nThe output values as NumPy array of length input_dims or shape (N x output_dims), depending on the input.     \n   transform_affine(values)[source]\n \nApply only the affine part of this transformation on the given array of values. transform(values) is always equivalent to transform_affine(transform_non_affine(values)). In non-affine transformations, this is generally a no-op. In affine transformations, this is equivalent to transform(values).  Parameters \n \nvaluesarray\n\n\nThe input values as NumPy array of length input_dims or shape (N x input_dims).    Returns \n array\n\nThe output values as NumPy array of length input_dims or shape (N x output_dims), depending on the input.     \n   transform_non_affine(points)[source]\n \nApply only the non-affine part of this transformation. transform(values) is always equivalent to transform_affine(transform_non_affine(values)). In non-affine transformations, this is generally equivalent to transform(values). In affine transformations, this is always a no-op.  Parameters \n \nvaluesarray\n\n\nThe input values as NumPy array of length input_dims or shape (N x input_dims).    Returns \n array\n\nThe output values as NumPy array of length input_dims or shape (N x output_dims), depending on the input.     \n   transform_path(path)[source]\n \nApply the transform to Path path, returning a new Path. In some cases, this transform may insert curves into the path that began as line segments. \n   transform_path_affine(path)[source]\n \nApply the affine part of this transform to Path path, returning a new Path. transform_path(path) is equivalent to transform_path_affine(transform_path_non_affine(values)). \n   transform_path_non_affine(path)[source]\n \nApply the non-affine part of this transform to Path path, returning a new Path. transform_path(path) is equivalent to transform_path_affine(transform_path_non_affine(values)). \n \n   classmatplotlib.transforms.AffineDeltaTransform(transform, **kwargs)[source]\n \nBases: matplotlib.transforms.Affine2DBase A transform wrapper for transforming displacements between pairs of points. This class is intended to be used to transform displacements (\"position deltas\") between pairs of points (e.g., as the offset_transform of Collections): given a transform t such that t =\nAffineDeltaTransform(t) + offset, AffineDeltaTransform satisfies AffineDeltaTransform(a - b) == AffineDeltaTransform(a) -\nAffineDeltaTransform(b). This is implemented by forcing the offset components of the transform matrix to zero. This class is experimental as of 3.3, and the API may change.  Parameters \n \nshorthand_namestr\n\n\nA string representing the \"name\" of the transform. The name carries no significance other than to improve the readability of str(transform) when DEBUG=True.       __init__(transform, **kwargs)[source]\n \n Parameters \n \nshorthand_namestr\n\n\nA string representing the \"name\" of the transform. The name carries no significance other than to improve the readability of str(transform) when DEBUG=True.     \n   __module__='matplotlib.transforms'\n\n   __str__()[source]\n \nReturn str(self). \n   get_matrix()[source]\n \nGet the matrix for the affine part of this transform. \n \n   classmatplotlib.transforms.Bbox(points, **kwargs)[source]\n \nBases: matplotlib.transforms.BboxBase A mutable bounding box. Examples Create from known bounds The default constructor takes the boundary \"points\" [[xmin, ymin],\n[xmax, ymax]]. >>> Bbox([[1, 1], [3, 7]])\nBbox([[1.0, 1.0], [3.0, 7.0]])\n Alternatively, a Bbox can be created from the flattened points array, the so-called \"extents\" (xmin, ymin, xmax, ymax) >>> Bbox.from_extents(1, 1, 3, 7)\nBbox([[1.0, 1.0], [3.0, 7.0]])\n or from the \"bounds\" (xmin, ymin, width, height). >>> Bbox.from_bounds(1, 1, 2, 6)\nBbox([[1.0, 1.0], [3.0, 7.0]])\n Create from collections of points The \"empty\" object for accumulating Bboxs is the null bbox, which is a stand-in for the empty set. >>> Bbox.null()\nBbox([[inf, inf], [-inf, -inf]])\n Adding points to the null bbox will give you the bbox of those points. >>> box = Bbox.null()\n>>> box.update_from_data_xy([[1, 1]])\n>>> box\nBbox([[1.0, 1.0], [1.0, 1.0]])\n>>> box.update_from_data_xy([[2, 3], [3, 2]], ignore=False)\n>>> box\nBbox([[1.0, 1.0], [3.0, 3.0]])\n Setting ignore=True is equivalent to starting over from a null bbox. >>> box.update_from_data_xy([[1, 1]], ignore=True)\n>>> box\nBbox([[1.0, 1.0], [1.0, 1.0]])\n  Warning It is recommended to always specify ignore explicitly. If not, the default value of ignore can be changed at any time by code with access to your Bbox, for example using the method ignore.  Properties of the ``null`` bbox  Note The current behavior of Bbox.null() may be surprising as it does not have all of the properties of the \"empty set\", and as such does not behave like a \"zero\" object in the mathematical sense. We may change that in the future (with a deprecation period).  The null bbox is the identity for intersections >>> Bbox.intersection(Bbox([[1, 1], [3, 7]]), Bbox.null())\nBbox([[1.0, 1.0], [3.0, 7.0]])\n except with itself, where it returns the full space. >>> Bbox.intersection(Bbox.null(), Bbox.null())\nBbox([[-inf, -inf], [inf, inf]])\n A union containing null will always return the full space (not the other set!) >>> Bbox.union([Bbox([[0, 0], [0, 0]]), Bbox.null()])\nBbox([[-inf, -inf], [inf, inf]])\n  Parameters \n \npointsndarray\n\n\nA 2x2 numpy array of the form [[x0, y0], [x1, y1]].       __format__(fmt)[source]\n \nDefault object formatter. \n   __init__(points, **kwargs)[source]\n \n Parameters \n \npointsndarray\n\n\nA 2x2 numpy array of the form [[x0, y0], [x1, y1]].     \n   __module__='matplotlib.transforms'\n\n   __repr__()[source]\n \nReturn repr(self). \n   __str__()[source]\n \nReturn str(self). \n   propertybounds\n \nReturn (x0, y0, width, height). \n   staticfrom_bounds(x0, y0, width, height)[source]\n \nCreate a new Bbox from x0, y0, width and height. width and height may be negative. \n   staticfrom_extents(*args, minpos=None)[source]\n \nCreate a new Bbox from left, bottom, right and top. The y-axis increases upwards.  Parameters \n \nleft, bottom, right, topfloat\n\n\nThe four extents of the bounding box.  \nminposfloat or None\n\n\nIf this is supplied, the Bbox will have a minimum positive value set. This is useful when dealing with logarithmic scales and other scales where negative bounds result in floating point errors.     \n   frozen()[source]\n \nThe base class for anything that participates in the transform tree and needs to invalidate its parents or be invalidated. This includes classes that are not really transforms, such as bounding boxes, since some transforms depend on bounding boxes to compute their values. \n   get_points()[source]\n \nGet the points of the bounding box directly as a numpy array of the form: [[x0, y0], [x1, y1]]. \n   ignore(value)[source]\n \nSet whether the existing bounds of the box should be ignored by subsequent calls to update_from_data_xy().  valuebool\n\n\n When True, subsequent calls to update_from_data_xy() will ignore the existing bounds of the Bbox. When False, subsequent calls to update_from_data_xy() will include the existing bounds of the Bbox.    \n   propertyintervalx\n \nThe pair of x coordinates that define the bounding box. This is not guaranteed to be sorted from left to right. \n   propertyintervaly\n \nThe pair of y coordinates that define the bounding box. This is not guaranteed to be sorted from bottom to top. \n   propertyminpos\n \nThe minimum positive value in both directions within the Bbox. This is useful when dealing with logarithmic scales and other scales where negative bounds result in floating point errors, and will be used as the minimum extent instead of p0. \n   propertyminposx\n \nThe minimum positive value in the x-direction within the Bbox. This is useful when dealing with logarithmic scales and other scales where negative bounds result in floating point errors, and will be used as the minimum x-extent instead of x0. \n   propertyminposy\n \nThe minimum positive value in the y-direction within the Bbox. This is useful when dealing with logarithmic scales and other scales where negative bounds result in floating point errors, and will be used as the minimum y-extent instead of y0. \n   mutated()[source]\n \nReturn whether the bbox has changed since init. \n   mutatedx()[source]\n \nReturn whether the x-limits have changed since init. \n   mutatedy()[source]\n \nReturn whether the y-limits have changed since init. \n   staticnull()[source]\n \nCreate a new null Bbox from (inf, inf) to (-inf, -inf). \n   propertyp0\n \nThe first pair of (x, y) coordinates that define the bounding box. This is not guaranteed to be the bottom-left corner (for that, use min). \n   propertyp1\n \nThe second pair of (x, y) coordinates that define the bounding box. This is not guaranteed to be the top-right corner (for that, use max). \n   set(other)[source]\n \nSet this bounding box from the \"frozen\" bounds of another Bbox. \n   set_points(points)[source]\n \nSet the points of the bounding box directly from a numpy array of the form: [[x0, y0], [x1, y1]]. No error checking is performed, as this method is mainly for internal use. \n   staticunit()[source]\n \nCreate a new unit Bbox from (0, 0) to (1, 1). \n   update_from_data_x(x, ignore=None)[source]\n \nUpdate the x-bounds of the Bbox based on the passed in data. After updating, the bounds will have positive width, and x0 will be the minimal value.  Parameters \n \nxndarray\n\n\nArray of x-values.  \nignorebool, optional\n\n\n When True, ignore the existing bounds of the Bbox. When False, include the existing bounds of the Bbox. When None, use the last value passed to ignore().      \n   update_from_data_xy(xy, ignore=None, updatex=True, updatey=True)[source]\n \nUpdate the bounds of the Bbox based on the passed in data. After updating, the bounds will have positive width and height; x0 and y0 will be the minimal values.  Parameters \n \nxyndarray\n\n\nA numpy array of 2D points.  \nignorebool, optional\n\n\n When True, ignore the existing bounds of the Bbox. When False, include the existing bounds of the Bbox. When None, use the last value passed to ignore().   \nupdatex, updateybool, default: True\n\n\nWhen True, update the x\/y values.     \n   update_from_data_y(y, ignore=None)[source]\n \nUpdate the y-bounds of the Bbox based on the passed in data. After updating, the bounds will have positive height, and y0 will be the minimal value.  Parameters \n \nyndarray\n\n\nArray of y-values.  \nignorebool, optional\n\n\n When True, ignore the existing bounds of the Bbox. When False, include the existing bounds of the Bbox. When None, use the last value passed to ignore().      \n   update_from_path(path, ignore=None, updatex=True, updatey=True)[source]\n \nUpdate the bounds of the Bbox to contain the vertices of the provided path. After updating, the bounds will have positive width and height; x0 and y0 will be the minimal values.  Parameters \n \npathPath\n\n\nignorebool, optional\n\n\n when True, ignore the existing bounds of the Bbox. when False, include the existing bounds of the Bbox. when None, use the last value passed to ignore().   \nupdatex, updateybool, default: True\n\n\nWhen True, update the x\/y values.     \n   propertyx0\n \nThe first of the pair of x coordinates that define the bounding box. This is not guaranteed to be less than x1 (for that, use xmin). \n   propertyx1\n \nThe second of the pair of x coordinates that define the bounding box. This is not guaranteed to be greater than x0 (for that, use xmax). \n   propertyy0\n \nThe first of the pair of y coordinates that define the bounding box. This is not guaranteed to be less than y1 (for that, use ymin). \n   propertyy1\n \nThe second of the pair of y coordinates that define the bounding box. This is not guaranteed to be greater than y0 (for that, use ymax). \n \n   classmatplotlib.transforms.BboxBase(shorthand_name=None)[source]\n \nBases: matplotlib.transforms.TransformNode The base class of all bounding boxes. This class is immutable; Bbox is a mutable subclass. The canonical representation is as two points, with no restrictions on their ordering. Convenience properties are provided to get the left, bottom, right and top edges and width and height, but these are not stored explicitly.  Parameters \n \nshorthand_namestr\n\n\nA string representing the \"name\" of the transform. The name carries no significance other than to improve the readability of str(transform) when DEBUG=True.       __array__(*args, **kwargs)[source]\n\n   __module__='matplotlib.transforms'\n\n   anchored(c, container=None)[source]\n \nReturn a copy of the Bbox anchored to c within container.  Parameters \n \nc(float, float) or {'C', 'SW', 'S', 'SE', 'E', 'NE', ...}\n\n\nEither an (x, y) pair of relative coordinates (0 is left or bottom, 1 is right or top), 'C' (center), or a cardinal direction ('SW', southwest, is bottom left, etc.).  \ncontainerBbox, optional\n\n\nThe box within which the Bbox is positioned; it defaults to the initial Bbox.      See also  Axes.set_anchor\n  \n   propertybounds\n \nReturn (x0, y0, width, height). \n   coefs={'C': (0.5, 0.5), 'E': (1.0, 0.5), 'N': (0.5, 1.0), 'NE': (1.0, 1.0), 'NW': (0, 1.0), 'S': (0.5, 0), 'SE': (1.0, 0), 'SW': (0, 0), 'W': (0, 0.5)}\n\n   contains(x, y)[source]\n \nReturn whether (x, y) is in the bounding box or on its edge. \n   containsx(x)[source]\n \nReturn whether x is in the closed (x0, x1) interval. \n   containsy(y)[source]\n \nReturn whether y is in the closed (y0, y1) interval. \n   corners()[source]\n \nReturn the corners of this rectangle as an array of points. Specifically, this returns the array [[x0, y0], [x0, y1], [x1, y0], [x1, y1]]. \n   count_contains(vertices)[source]\n \nCount the number of vertices contained in the Bbox. Any vertices with a non-finite x or y value are ignored.  Parameters \n \nverticesNx2 Numpy array.\n\n   \n   count_overlaps(bboxes)[source]\n \nCount the number of bounding boxes that overlap this one.  Parameters \n \nbboxessequence of BboxBase\n\n   \n   expanded(sw, sh)[source]\n \nConstruct a Bbox by expanding this one around its center by the factors sw and sh. \n   propertyextents\n \nReturn (x0, y0, x1, y1). \n   frozen()[source]\n \nThe base class for anything that participates in the transform tree and needs to invalidate its parents or be invalidated. This includes classes that are not really transforms, such as bounding boxes, since some transforms depend on bounding boxes to compute their values. \n   fully_contains(x, y)[source]\n \nReturn whether x, y is in the bounding box, but not on its edge. \n   fully_containsx(x)[source]\n \nReturn whether x is in the open (x0, x1) interval. \n   fully_containsy(y)[source]\n \nReturn whether y is in the open (y0, y1) interval. \n   fully_overlaps(other)[source]\n \nReturn whether this bounding box overlaps with the other bounding box, not including the edges.  Parameters \n \notherBboxBase\n\n   \n   get_points()[source]\n\n   propertyheight\n \nThe (signed) height of the bounding box. \n   staticintersection(bbox1, bbox2)[source]\n \nReturn the intersection of bbox1 and bbox2 if they intersect, or None if they don't. \n   propertyintervalx\n \nThe pair of x coordinates that define the bounding box. This is not guaranteed to be sorted from left to right. \n   propertyintervaly\n \nThe pair of y coordinates that define the bounding box. This is not guaranteed to be sorted from bottom to top. \n   is_affine=True\n\n   is_bbox=True\n\n   propertymax\n \nThe top-right corner of the bounding box. \n   propertymin\n \nThe bottom-left corner of the bounding box. \n   overlaps(other)[source]\n \nReturn whether this bounding box overlaps with the other bounding box.  Parameters \n \notherBboxBase\n\n   \n   propertyp0\n \nThe first pair of (x, y) coordinates that define the bounding box. This is not guaranteed to be the bottom-left corner (for that, use min). \n   propertyp1\n \nThe second pair of (x, y) coordinates that define the bounding box. This is not guaranteed to be the top-right corner (for that, use max). \n   padded(p)[source]\n \nConstruct a Bbox by padding this one on all four sides by p. \n   rotated(radians)[source]\n \nReturn the axes-aligned bounding box that bounds the result of rotating this Bbox by an angle of radians. \n   shrunk(mx, my)[source]\n \nReturn a copy of the Bbox, shrunk by the factor mx in the x direction and the factor my in the y direction. The lower left corner of the box remains unchanged. Normally mx and my will be less than 1, but this is not enforced. \n   shrunk_to_aspect(box_aspect, container=None, fig_aspect=1.0)[source]\n \nReturn a copy of the Bbox, shrunk so that it is as large as it can be while having the desired aspect ratio, box_aspect. If the box coordinates are relative (i.e. fractions of a larger box such as a figure) then the physical aspect ratio of that figure is specified with fig_aspect, so that box_aspect can also be given as a ratio of the absolute dimensions, not the relative dimensions. \n   propertysize\n \nThe (signed) width and height of the bounding box. \n   splitx(*args)[source]\n \nReturn a list of new Bbox objects formed by splitting the original one with vertical lines at fractional positions given by args. \n   splity(*args)[source]\n \nReturn a list of new Bbox objects formed by splitting the original one with horizontal lines at fractional positions given by args. \n   transformed(transform)[source]\n \nConstruct a Bbox by statically transforming this one by transform. \n   translated(tx, ty)[source]\n \nConstruct a Bbox by translating this one by tx and ty. \n   staticunion(bboxes)[source]\n \nReturn a Bbox that contains all of the given bboxes. \n   propertywidth\n \nThe (signed) width of the bounding box. \n   propertyx0\n \nThe first of the pair of x coordinates that define the bounding box. This is not guaranteed to be less than x1 (for that, use xmin). \n   propertyx1\n \nThe second of the pair of x coordinates that define the bounding box. This is not guaranteed to be greater than x0 (for that, use xmax). \n   propertyxmax\n \nThe right edge of the bounding box. \n   propertyxmin\n \nThe left edge of the bounding box. \n   propertyy0\n \nThe first of the pair of y coordinates that define the bounding box. This is not guaranteed to be less than y1 (for that, use ymin). \n   propertyy1\n \nThe second of the pair of y coordinates that define the bounding box. This is not guaranteed to be greater than y0 (for that, use ymax). \n   propertyymax\n \nThe top edge of the bounding box. \n   propertyymin\n \nThe bottom edge of the bounding box. \n \n   classmatplotlib.transforms.BboxTransform(boxin, boxout, **kwargs)[source]\n \nBases: matplotlib.transforms.Affine2DBase BboxTransform linearly transforms points from one Bbox to another. Create a new BboxTransform that linearly transforms points from boxin to boxout.   __init__(boxin, boxout, **kwargs)[source]\n \nCreate a new BboxTransform that linearly transforms points from boxin to boxout. \n   __module__='matplotlib.transforms'\n\n   __str__()[source]\n \nReturn str(self). \n   get_matrix()[source]\n \nGet the matrix for the affine part of this transform. \n   is_separable=True\n \nTrue if this transform is separable in the x- and y- dimensions. \n \n   classmatplotlib.transforms.BboxTransformFrom(boxin, **kwargs)[source]\n \nBases: matplotlib.transforms.Affine2DBase BboxTransformFrom linearly transforms points from a given Bbox to the unit bounding box.  Parameters \n \nshorthand_namestr\n\n\nA string representing the \"name\" of the transform. The name carries no significance other than to improve the readability of str(transform) when DEBUG=True.       __init__(boxin, **kwargs)[source]\n \n Parameters \n \nshorthand_namestr\n\n\nA string representing the \"name\" of the transform. The name carries no significance other than to improve the readability of str(transform) when DEBUG=True.     \n   __module__='matplotlib.transforms'\n\n   __str__()[source]\n \nReturn str(self). \n   get_matrix()[source]\n \nGet the matrix for the affine part of this transform. \n   is_separable=True\n \nTrue if this transform is separable in the x- and y- dimensions. \n \n   classmatplotlib.transforms.BboxTransformTo(boxout, **kwargs)[source]\n \nBases: matplotlib.transforms.Affine2DBase BboxTransformTo is a transformation that linearly transforms points from the unit bounding box to a given Bbox. Create a new BboxTransformTo that linearly transforms points from the unit bounding box to boxout.   __init__(boxout, **kwargs)[source]\n \nCreate a new BboxTransformTo that linearly transforms points from the unit bounding box to boxout. \n   __module__='matplotlib.transforms'\n\n   __str__()[source]\n \nReturn str(self). \n   get_matrix()[source]\n \nGet the matrix for the affine part of this transform. \n   is_separable=True\n \nTrue if this transform is separable in the x- and y- dimensions. \n \n   classmatplotlib.transforms.BboxTransformToMaxOnly(boxout, **kwargs)[source]\n \nBases: matplotlib.transforms.BboxTransformTo BboxTransformTo is a transformation that linearly transforms points from the unit bounding box to a given Bbox with a fixed upper left of (0, 0). Create a new BboxTransformTo that linearly transforms points from the unit bounding box to boxout.   __module__='matplotlib.transforms'\n\n   get_matrix()[source]\n \nGet the matrix for the affine part of this transform. \n \n   classmatplotlib.transforms.BlendedAffine2D(x_transform, y_transform, **kwargs)[source]\n \nBases: matplotlib.transforms._BlendedMixin, matplotlib.transforms.Affine2DBase A \"blended\" transform uses one transform for the x-direction, and another transform for the y-direction. This version is an optimization for the case where both child transforms are of type Affine2DBase. Create a new \"blended\" transform using x_transform to transform the x-axis and y_transform to transform the y-axis. Both x_transform and y_transform must be 2D affine transforms. You will generally not call this constructor directly but use the blended_transform_factory function instead, which can determine automatically which kind of blended transform to create.   __init__(x_transform, y_transform, **kwargs)[source]\n \nCreate a new \"blended\" transform using x_transform to transform the x-axis and y_transform to transform the y-axis. Both x_transform and y_transform must be 2D affine transforms. You will generally not call this constructor directly but use the blended_transform_factory function instead, which can determine automatically which kind of blended transform to create. \n   __module__='matplotlib.transforms'\n\n   get_matrix()[source]\n \nGet the matrix for the affine part of this transform. \n   is_separable=True\n \nTrue if this transform is separable in the x- and y- dimensions. \n \n   classmatplotlib.transforms.BlendedGenericTransform(x_transform, y_transform, **kwargs)[source]\n \nBases: matplotlib.transforms._BlendedMixin, matplotlib.transforms.Transform A \"blended\" transform uses one transform for the x-direction, and another transform for the y-direction. This \"generic\" version can handle any given child transform in the x- and y-directions. Create a new \"blended\" transform using x_transform to transform the x-axis and y_transform to transform the y-axis. You will generally not call this constructor directly but use the blended_transform_factory function instead, which can determine automatically which kind of blended transform to create.   __init__(x_transform, y_transform, **kwargs)[source]\n \nCreate a new \"blended\" transform using x_transform to transform the x-axis and y_transform to transform the y-axis. You will generally not call this constructor directly but use the blended_transform_factory function instead, which can determine automatically which kind of blended transform to create. \n   __module__='matplotlib.transforms'\n\n   contains_branch(other)[source]\n \nReturn whether the given transform is a sub-tree of this transform. This routine uses transform equality to identify sub-trees, therefore in many situations it is object id which will be used. For the case where the given transform represents the whole of this transform, returns True. \n   propertydepth\n \nReturn the number of transforms which have been chained together to form this Transform instance.  Note For the special case of a Composite transform, the maximum depth of the two is returned.  \n   frozen()[source]\n \nReturn a frozen copy of this transform node. The frozen copy will not be updated when its children change. Useful for storing a previously known state of a transform where copy.deepcopy() might normally be used. \n   get_affine()[source]\n \nGet the affine part of this transform. \n   propertyhas_inverse\n \nbool(x) -> bool Returns True when the argument x is true, False otherwise. The builtins True and False are the only two instances of the class bool. The class bool is a subclass of the class int, and cannot be subclassed. \n   input_dims=2\n \nThe number of input dimensions of this transform. Must be overridden (with integers) in the subclass. \n   inverted()[source]\n \nReturn the corresponding inverse transformation. It holds x == self.inverted().transform(self.transform(x)). The return value of this method should be treated as temporary. An update to self does not cause a corresponding update to its inverted copy. \n   propertyis_affine\n \nbool(x) -> bool Returns True when the argument x is true, False otherwise. The builtins True and False are the only two instances of the class bool. The class bool is a subclass of the class int, and cannot be subclassed. \n   is_separable=True\n \nTrue if this transform is separable in the x- and y- dimensions. \n   output_dims=2\n \nThe number of output dimensions of this transform. Must be overridden (with integers) in the subclass. \n   pass_through=True\n \nIf pass_through is True, all ancestors will always be invalidated, even if 'self' is already invalid. \n   transform_non_affine(points)[source]\n \nApply only the non-affine part of this transformation. transform(values) is always equivalent to transform_affine(transform_non_affine(values)). In non-affine transformations, this is generally equivalent to transform(values). In affine transformations, this is always a no-op.  Parameters \n \nvaluesarray\n\n\nThe input values as NumPy array of length input_dims or shape (N x input_dims).    Returns \n array\n\nThe output values as NumPy array of length input_dims or shape (N x output_dims), depending on the input.     \n \n   classmatplotlib.transforms.CompositeAffine2D(a, b, **kwargs)[source]\n \nBases: matplotlib.transforms.Affine2DBase A composite transform formed by applying transform a then transform b. This version is an optimization that handles the case where both a and b are 2D affines. Create a new composite transform that is the result of applying Affine2DBase a then Affine2DBase b. You will generally not call this constructor directly but write a +\nb instead, which will automatically choose the best kind of composite transform instance to create.   __init__(a, b, **kwargs)[source]\n \nCreate a new composite transform that is the result of applying Affine2DBase a then Affine2DBase b. You will generally not call this constructor directly but write a +\nb instead, which will automatically choose the best kind of composite transform instance to create. \n   __module__='matplotlib.transforms'\n\n   __str__()[source]\n \nReturn str(self). \n   propertydepth\n \nReturn the number of transforms which have been chained together to form this Transform instance.  Note For the special case of a Composite transform, the maximum depth of the two is returned.  \n   get_matrix()[source]\n \nGet the matrix for the affine part of this transform. \n \n   classmatplotlib.transforms.CompositeGenericTransform(a, b, **kwargs)[source]\n \nBases: matplotlib.transforms.Transform A composite transform formed by applying transform a then transform b. This \"generic\" version can handle any two arbitrary transformations. Create a new composite transform that is the result of applying transform a then transform b. You will generally not call this constructor directly but write a +\nb instead, which will automatically choose the best kind of composite transform instance to create.   __eq__(other)[source]\n \nReturn self==value. \n   __hash__=None\n\n   __init__(a, b, **kwargs)[source]\n \nCreate a new composite transform that is the result of applying transform a then transform b. You will generally not call this constructor directly but write a +\nb instead, which will automatically choose the best kind of composite transform instance to create. \n   __module__='matplotlib.transforms'\n\n   __str__()[source]\n \nReturn str(self). \n   propertydepth\n \nReturn the number of transforms which have been chained together to form this Transform instance.  Note For the special case of a Composite transform, the maximum depth of the two is returned.  \n   frozen()[source]\n \nReturn a frozen copy of this transform node. The frozen copy will not be updated when its children change. Useful for storing a previously known state of a transform where copy.deepcopy() might normally be used. \n   get_affine()[source]\n \nGet the affine part of this transform. \n   propertyhas_inverse\n \nbool(x) -> bool Returns True when the argument x is true, False otherwise. The builtins True and False are the only two instances of the class bool. The class bool is a subclass of the class int, and cannot be subclassed. \n   inverted()[source]\n \nReturn the corresponding inverse transformation. It holds x == self.inverted().transform(self.transform(x)). The return value of this method should be treated as temporary. An update to self does not cause a corresponding update to its inverted copy. \n   propertyis_affine\n \nbool(x) -> bool Returns True when the argument x is true, False otherwise. The builtins True and False are the only two instances of the class bool. The class bool is a subclass of the class int, and cannot be subclassed. \n   propertyis_separable\n \nbool(x) -> bool Returns True when the argument x is true, False otherwise. The builtins True and False are the only two instances of the class bool. The class bool is a subclass of the class int, and cannot be subclassed. \n   pass_through=True\n \nIf pass_through is True, all ancestors will always be invalidated, even if 'self' is already invalid. \n   transform_affine(points)[source]\n \nApply only the affine part of this transformation on the given array of values. transform(values) is always equivalent to transform_affine(transform_non_affine(values)). In non-affine transformations, this is generally a no-op. In affine transformations, this is equivalent to transform(values).  Parameters \n \nvaluesarray\n\n\nThe input values as NumPy array of length input_dims or shape (N x input_dims).    Returns \n array\n\nThe output values as NumPy array of length input_dims or shape (N x output_dims), depending on the input.     \n   transform_non_affine(points)[source]\n \nApply only the non-affine part of this transformation. transform(values) is always equivalent to transform_affine(transform_non_affine(values)). In non-affine transformations, this is generally equivalent to transform(values). In affine transformations, this is always a no-op.  Parameters \n \nvaluesarray\n\n\nThe input values as NumPy array of length input_dims or shape (N x input_dims).    Returns \n array\n\nThe output values as NumPy array of length input_dims or shape (N x output_dims), depending on the input.     \n   transform_path_non_affine(path)[source]\n \nApply the non-affine part of this transform to Path path, returning a new Path. transform_path(path) is equivalent to transform_path_affine(transform_path_non_affine(values)). \n \n   classmatplotlib.transforms.IdentityTransform(*args, **kwargs)[source]\n \nBases: matplotlib.transforms.Affine2DBase A special class that does one thing, the identity transform, in a fast way.  Parameters \n \nshorthand_namestr\n\n\nA string representing the \"name\" of the transform. The name carries no significance other than to improve the readability of str(transform) when DEBUG=True.       __module__='matplotlib.transforms'\n\n   __str__()[source]\n \nReturn str(self). \n   frozen()[source]\n \nReturn a frozen copy of this transform node. The frozen copy will not be updated when its children change. Useful for storing a previously known state of a transform where copy.deepcopy() might normally be used. \n   get_affine()[source]\n \nGet the affine part of this transform. \n   get_matrix()[source]\n \nGet the matrix for the affine part of this transform. \n   inverted()[source]\n \nReturn the corresponding inverse transformation. It holds x == self.inverted().transform(self.transform(x)). The return value of this method should be treated as temporary. An update to self does not cause a corresponding update to its inverted copy. \n   transform(points)[source]\n \nApply this transformation on the given array of values.  Parameters \n \nvaluesarray\n\n\nThe input values as NumPy array of length input_dims or shape (N x input_dims).    Returns \n array\n\nThe output values as NumPy array of length input_dims or shape (N x output_dims), depending on the input.     \n   transform_affine(points)[source]\n \nApply only the affine part of this transformation on the given array of values. transform(values) is always equivalent to transform_affine(transform_non_affine(values)). In non-affine transformations, this is generally a no-op. In affine transformations, this is equivalent to transform(values).  Parameters \n \nvaluesarray\n\n\nThe input values as NumPy array of length input_dims or shape (N x input_dims).    Returns \n array\n\nThe output values as NumPy array of length input_dims or shape (N x output_dims), depending on the input.     \n   transform_non_affine(points)[source]\n \nApply only the non-affine part of this transformation. transform(values) is always equivalent to transform_affine(transform_non_affine(values)). In non-affine transformations, this is generally equivalent to transform(values). In affine transformations, this is always a no-op.  Parameters \n \nvaluesarray\n\n\nThe input values as NumPy array of length input_dims or shape (N x input_dims).    Returns \n array\n\nThe output values as NumPy array of length input_dims or shape (N x output_dims), depending on the input.     \n   transform_path(path)[source]\n \nApply the transform to Path path, returning a new Path. In some cases, this transform may insert curves into the path that began as line segments. \n   transform_path_affine(path)[source]\n \nApply the affine part of this transform to Path path, returning a new Path. transform_path(path) is equivalent to transform_path_affine(transform_path_non_affine(values)). \n   transform_path_non_affine(path)[source]\n \nApply the non-affine part of this transform to Path path, returning a new Path. transform_path(path) is equivalent to transform_path_affine(transform_path_non_affine(values)). \n \n   classmatplotlib.transforms.LockableBbox(bbox, x0=None, y0=None, x1=None, y1=None, **kwargs)[source]\n \nBases: matplotlib.transforms.BboxBase A Bbox where some elements may be locked at certain values. When the child bounding box changes, the bounds of this bbox will update accordingly with the exception of the locked elements.  Parameters \n \nbboxBbox\n\n\nThe child bounding box to wrap.  \nx0float or None\n\n\nThe locked value for x0, or None to leave unlocked.  \ny0float or None\n\n\nThe locked value for y0, or None to leave unlocked.  \nx1float or None\n\n\nThe locked value for x1, or None to leave unlocked.  \ny1float or None\n\n\nThe locked value for y1, or None to leave unlocked.       __init__(bbox, x0=None, y0=None, x1=None, y1=None, **kwargs)[source]\n \n Parameters \n \nbboxBbox\n\n\nThe child bounding box to wrap.  \nx0float or None\n\n\nThe locked value for x0, or None to leave unlocked.  \ny0float or None\n\n\nThe locked value for y0, or None to leave unlocked.  \nx1float or None\n\n\nThe locked value for x1, or None to leave unlocked.  \ny1float or None\n\n\nThe locked value for y1, or None to leave unlocked.     \n   __module__='matplotlib.transforms'\n\n   __str__()[source]\n \nReturn str(self). \n   get_points()[source]\n\n   propertylocked_x0\n \nfloat or None: The value used for the locked x0. \n   propertylocked_x1\n \nfloat or None: The value used for the locked x1. \n   propertylocked_y0\n \nfloat or None: The value used for the locked y0. \n   propertylocked_y1\n \nfloat or None: The value used for the locked y1. \n \n   classmatplotlib.transforms.ScaledTranslation(xt, yt, scale_trans, **kwargs)[source]\n \nBases: matplotlib.transforms.Affine2DBase A transformation that translates by xt and yt, after xt and yt have been transformed by scale_trans.  Parameters \n \nshorthand_namestr\n\n\nA string representing the \"name\" of the transform. The name carries no significance other than to improve the readability of str(transform) when DEBUG=True.       __init__(xt, yt, scale_trans, **kwargs)[source]\n \n Parameters \n \nshorthand_namestr\n\n\nA string representing the \"name\" of the transform. The name carries no significance other than to improve the readability of str(transform) when DEBUG=True.     \n   __module__='matplotlib.transforms'\n\n   __str__()[source]\n \nReturn str(self). \n   get_matrix()[source]\n \nGet the matrix for the affine part of this transform. \n \n   classmatplotlib.transforms.Transform(shorthand_name=None)[source]\n \nBases: matplotlib.transforms.TransformNode The base class of all TransformNode instances that actually perform a transformation. All non-affine transformations should be subclasses of this class. New affine transformations should be subclasses of Affine2D. Subclasses of this class should override the following members (at minimum):  input_dims output_dims transform() \ninverted() (if an inverse exists)  The following attributes may be overridden if the default is unsuitable:  \nis_separable (defaults to True for 1D -> 1D transforms, False otherwise) \nhas_inverse (defaults to True if inverted() is overridden, False otherwise)  If the transform needs to do something non-standard with matplotlib.path.Path objects, such as adding curves where there were once line segments, it should override:  transform_path()   Parameters \n \nshorthand_namestr\n\n\nA string representing the \"name\" of the transform. The name carries no significance other than to improve the readability of str(transform) when DEBUG=True.       __add__(other)[source]\n \nCompose two transforms together so that self is followed by other. A + B returns a transform C so that C.transform(x) == B.transform(A.transform(x)). \n   __array__(*args, **kwargs)[source]\n \nArray interface to get at this Transform's affine matrix. \n   classmethod__init_subclass__()[source]\n \nThis method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. \n   __module__='matplotlib.transforms'\n\n   __sub__(other)[source]\n \nCompose self with the inverse of other, cancelling identical terms if any: # In general:\nA - B == A + B.inverted()\n# (but see note regarding frozen transforms below).\n\n# If A \"ends with\" B (i.e. A == A' + B for some A') we can cancel\n# out B:\n(A' + B) - B == A'\n\n# Likewise, if B \"starts with\" A (B = A + B'), we can cancel out A:\nA - (A + B') == B'.inverted() == B'^-1\n Cancellation (rather than naively returning A + B.inverted()) is important for multiple reasons:  It avoids floating-point inaccuracies when computing the inverse of B: B - B is guaranteed to cancel out exactly (resulting in the identity transform), whereas B + B.inverted() may differ by a small epsilon. \nB.inverted() always returns a frozen transform: if one computes A + B + B.inverted() and later mutates B, then B.inverted() won't be updated and the last two terms won't cancel out anymore; on the other hand, A + B - B will always be equal to A even if B is mutated.  \n   contains_branch(other)[source]\n \nReturn whether the given transform is a sub-tree of this transform. This routine uses transform equality to identify sub-trees, therefore in many situations it is object id which will be used. For the case where the given transform represents the whole of this transform, returns True. \n   contains_branch_seperately(other_transform)[source]\n \nReturn whether the given branch is a sub-tree of this transform on each separate dimension. A common use for this method is to identify if a transform is a blended transform containing an axes' data transform. e.g.: x_isdata, y_isdata = trans.contains_branch_seperately(ax.transData)\n \n   propertydepth\n \nReturn the number of transforms which have been chained together to form this Transform instance.  Note For the special case of a Composite transform, the maximum depth of the two is returned.  \n   get_affine()[source]\n \nGet the affine part of this transform. \n   get_matrix()[source]\n \nGet the matrix for the affine part of this transform. \n   has_inverse=False\n \nTrue if this transform has a corresponding inverse transform. \n   input_dims=None\n \nThe number of input dimensions of this transform. Must be overridden (with integers) in the subclass. \n   inverted()[source]\n \nReturn the corresponding inverse transformation. It holds x == self.inverted().transform(self.transform(x)). The return value of this method should be treated as temporary. An update to self does not cause a corresponding update to its inverted copy. \n   is_separable=False\n \nTrue if this transform is separable in the x- and y- dimensions. \n   output_dims=None\n \nThe number of output dimensions of this transform. Must be overridden (with integers) in the subclass. \n   transform(values)[source]\n \nApply this transformation on the given array of values.  Parameters \n \nvaluesarray\n\n\nThe input values as NumPy array of length input_dims or shape (N x input_dims).    Returns \n array\n\nThe output values as NumPy array of length input_dims or shape (N x output_dims), depending on the input.     \n   transform_affine(values)[source]\n \nApply only the affine part of this transformation on the given array of values. transform(values) is always equivalent to transform_affine(transform_non_affine(values)). In non-affine transformations, this is generally a no-op. In affine transformations, this is equivalent to transform(values).  Parameters \n \nvaluesarray\n\n\nThe input values as NumPy array of length input_dims or shape (N x input_dims).    Returns \n array\n\nThe output values as NumPy array of length input_dims or shape (N x output_dims), depending on the input.     \n   transform_angles(angles, pts, radians=False, pushoff=1e-05)[source]\n \nTransform a set of angles anchored at specific locations.  Parameters \n \nangles(N,) array-like\n\n\nThe angles to transform.  \npts(N, 2) array-like\n\n\nThe points where the angles are anchored.  \nradiansbool, default: False\n\n\nWhether angles are radians or degrees.  \npushofffloat\n\n\nFor each point in pts and angle in angles, the transformed angle is computed by transforming a segment of length pushoff starting at that point and making that angle relative to the horizontal axis, and measuring the angle between the horizontal axis and the transformed segment.    Returns \n (N,) array\n   \n   transform_bbox(bbox)[source]\n \nTransform the given bounding box. For smarter transforms including caching (a common requirement in Matplotlib), see TransformedBbox. \n   transform_non_affine(values)[source]\n \nApply only the non-affine part of this transformation. transform(values) is always equivalent to transform_affine(transform_non_affine(values)). In non-affine transformations, this is generally equivalent to transform(values). In affine transformations, this is always a no-op.  Parameters \n \nvaluesarray\n\n\nThe input values as NumPy array of length input_dims or shape (N x input_dims).    Returns \n array\n\nThe output values as NumPy array of length input_dims or shape (N x output_dims), depending on the input.     \n   transform_path(path)[source]\n \nApply the transform to Path path, returning a new Path. In some cases, this transform may insert curves into the path that began as line segments. \n   transform_path_affine(path)[source]\n \nApply the affine part of this transform to Path path, returning a new Path. transform_path(path) is equivalent to transform_path_affine(transform_path_non_affine(values)). \n   transform_path_non_affine(path)[source]\n \nApply the non-affine part of this transform to Path path, returning a new Path. transform_path(path) is equivalent to transform_path_affine(transform_path_non_affine(values)). \n   transform_point(point)[source]\n \nReturn a transformed point. This function is only kept for backcompatibility; the more general transform method is capable of transforming both a list of points and a single point. The point is given as a sequence of length input_dims. The transformed point is returned as a sequence of length output_dims. \n \n   classmatplotlib.transforms.TransformNode(shorthand_name=None)[source]\n \nBases: object The base class for anything that participates in the transform tree and needs to invalidate its parents or be invalidated. This includes classes that are not really transforms, such as bounding boxes, since some transforms depend on bounding boxes to compute their values.  Parameters \n \nshorthand_namestr\n\n\nA string representing the \"name\" of the transform. The name carries no significance other than to improve the readability of str(transform) when DEBUG=True.       INVALID=3\n\n   INVALID_AFFINE=2\n\n   INVALID_NON_AFFINE=1\n\n   __copy__()[source]\n\n   __deepcopy__(memo)[source]\n\n   __dict__=mappingproxy({'__module__': 'matplotlib.transforms', '__doc__': '\\n The base class for anything that participates in the transform tree\\n and needs to invalidate its parents or be invalidated. This includes\\n classes that are not really transforms, such as bounding boxes, since some\\n transforms depend on bounding boxes to compute their values.\\n ', 'INVALID_NON_AFFINE': 1, 'INVALID_AFFINE': 2, 'INVALID': 3, 'is_affine': False, 'is_bbox': False, 'pass_through': False, '__init__': <function TransformNode.__init__>, '__getstate__': <function TransformNode.__getstate__>, '__setstate__': <function TransformNode.__setstate__>, '__copy__': <function TransformNode.__copy__>, '__deepcopy__': <function TransformNode.__deepcopy__>, 'invalidate': <function TransformNode.invalidate>, '_invalidate_internal': <function TransformNode._invalidate_internal>, 'set_children': <function TransformNode.set_children>, 'frozen': <function TransformNode.frozen>, '__dict__': <attribute '__dict__' of 'TransformNode' objects>, '__weakref__': <attribute '__weakref__' of 'TransformNode' objects>, '__annotations__': {}})\n\n   __getstate__()[source]\n\n   __init__(shorthand_name=None)[source]\n \n Parameters \n \nshorthand_namestr\n\n\nA string representing the \"name\" of the transform. The name carries no significance other than to improve the readability of str(transform) when DEBUG=True.     \n   __module__='matplotlib.transforms'\n\n   __setstate__(data_dict)[source]\n\n   __weakref__\n \nlist of weak references to the object (if defined) \n   frozen()[source]\n \nReturn a frozen copy of this transform node. The frozen copy will not be updated when its children change. Useful for storing a previously known state of a transform where copy.deepcopy() might normally be used. \n   invalidate()[source]\n \nInvalidate this TransformNode and triggers an invalidation of its ancestors. Should be called any time the transform changes. \n   is_affine=False\n\n   is_bbox=False\n\n   pass_through=False\n \nIf pass_through is True, all ancestors will always be invalidated, even if 'self' is already invalid. \n   set_children(*children)[source]\n \nSet the children of the transform, to let the invalidation system know which transforms can invalidate this transform. Should be called from the constructor of any transforms that depend on other transforms. \n \n   classmatplotlib.transforms.TransformWrapper(child)[source]\n \nBases: matplotlib.transforms.Transform A helper class that holds a single child transform and acts equivalently to it. This is useful if a node of the transform tree must be replaced at run time with a transform of a different type. This class allows that replacement to correctly trigger invalidation. TransformWrapper instances must have the same input and output dimensions during their entire lifetime, so the child transform may only be replaced with another child transform of the same dimensions. child: A Transform instance. This child may later be replaced with set().   __eq__(other)[source]\n \nReturn self==value. \n   __hash__=None\n\n   __init__(child)[source]\n \nchild: A Transform instance. This child may later be replaced with set(). \n   __module__='matplotlib.transforms'\n\n   __str__()[source]\n \nReturn str(self). \n   frozen()[source]\n \nReturn a frozen copy of this transform node. The frozen copy will not be updated when its children change. Useful for storing a previously known state of a transform where copy.deepcopy() might normally be used. \n   propertyhas_inverse\n \nbool(x) -> bool Returns True when the argument x is true, False otherwise. The builtins True and False are the only two instances of the class bool. The class bool is a subclass of the class int, and cannot be subclassed. \n   propertyis_affine\n \nbool(x) -> bool Returns True when the argument x is true, False otherwise. The builtins True and False are the only two instances of the class bool. The class bool is a subclass of the class int, and cannot be subclassed. \n   propertyis_separable\n \nbool(x) -> bool Returns True when the argument x is true, False otherwise. The builtins True and False are the only two instances of the class bool. The class bool is a subclass of the class int, and cannot be subclassed. \n   pass_through=True\n \nIf pass_through is True, all ancestors will always be invalidated, even if 'self' is already invalid. \n   set(child)[source]\n \nReplace the current child of this transform with another one. The new child must have the same number of input and output dimensions as the current child. \n \n   classmatplotlib.transforms.TransformedBbox(bbox, transform, **kwargs)[source]\n \nBases: matplotlib.transforms.BboxBase A Bbox that is automatically transformed by a given transform. When either the child bounding box or transform changes, the bounds of this bbox will update accordingly.  Parameters \n \nbboxBbox\n\n\ntransformTransform\n\n     __init__(bbox, transform, **kwargs)[source]\n \n Parameters \n \nbboxBbox\n\n\ntransformTransform\n\n   \n   __module__='matplotlib.transforms'\n\n   __str__()[source]\n \nReturn str(self). \n   get_points()[source]\n\n \n   classmatplotlib.transforms.TransformedPatchPath(patch)[source]\n \nBases: matplotlib.transforms.TransformedPath A TransformedPatchPath caches a non-affine transformed copy of the Patch. This cached copy is automatically updated when the non-affine part of the transform or the patch changes.  Parameters \n \npatchPatch\n\n     __init__(patch)[source]\n \n Parameters \n \npatchPatch\n\n   \n   __module__='matplotlib.transforms'\n\n \n   classmatplotlib.transforms.TransformedPath(path, transform)[source]\n \nBases: matplotlib.transforms.TransformNode A TransformedPath caches a non-affine transformed copy of the Path. This cached copy is automatically updated when the non-affine part of the transform changes.  Note Paths are considered immutable by this class. Any update to the path's vertices\/codes will not trigger a transform recomputation.   Parameters \n \npathPath\n\n\ntransformTransform\n\n     __init__(path, transform)[source]\n \n Parameters \n \npathPath\n\n\ntransformTransform\n\n   \n   __module__='matplotlib.transforms'\n\n   get_affine()[source]\n\n   get_fully_transformed_path()[source]\n \nReturn a fully-transformed copy of the child path. \n   get_transformed_path_and_affine()[source]\n \nReturn a copy of the child path, with the non-affine part of the transform already applied, along with the affine part of the path necessary to complete the transformation. \n   get_transformed_points_and_affine()[source]\n \nReturn a copy of the child path, with the non-affine part of the transform already applied, along with the affine part of the path necessary to complete the transformation. Unlike get_transformed_path_and_affine(), no interpolation will be performed. \n \n   matplotlib.transforms.blended_transform_factory(x_transform, y_transform)[source]\n \nCreate a new \"blended\" transform using x_transform to transform the x-axis and y_transform to transform the y-axis. A faster version of the blended transform is returned for the case where both child transforms are affine. \n   matplotlib.transforms.composite_transform_factory(a, b)[source]\n \nCreate a new composite transform that is the result of applying transform a then transform b. Shortcut versions of the blended transform are provided for the case where both child transforms are affine, or one or the other is the identity transform. Composite transforms may also be created using the '+' operator, e.g.: c = a + b\n \n   matplotlib.transforms.interval_contains(interval, val)[source]\n \nCheck, inclusively, whether an interval includes a given value.  Parameters \n \ninterval(float, float)\n\n\nThe endpoints of the interval.  \nvalfloat\n\n\nValue to check is within interval.    Returns \n bool\n\nWhether val is within the interval.     \n   matplotlib.transforms.interval_contains_open(interval, val)[source]\n \nCheck, excluding endpoints, whether an interval includes a given value.  Parameters \n \ninterval(float, float)\n\n\nThe endpoints of the interval.  \nvalfloat\n\n\nValue to check is within interval.    Returns \n bool\n\nWhether val is within the interval.     \n   matplotlib.transforms.nonsingular(vmin, vmax, expander=0.001, tiny=1e-15, increasing=True)[source]\n \nModify the endpoints of a range as needed to avoid singularities.  Parameters \n \nvmin, vmaxfloat\n\n\nThe initial endpoints.  \nexpanderfloat, default: 0.001\n\n\nFractional amount by which vmin and vmax are expanded if the original interval is too small, based on tiny.  \ntinyfloat, default: 1e-15\n\n\nThreshold for the ratio of the interval to the maximum absolute value of its endpoints. If the interval is smaller than this, it will be expanded. This value should be around 1e-15 or larger; otherwise the interval will be approaching the double precision resolution limit.  \nincreasingbool, default: True\n\n\nIf True, swap vmin, vmax if vmin > vmax.    Returns \n \nvmin, vmaxfloat\n\n\nEndpoints, expanded and\/or swapped if necessary. If either input is inf or NaN, or if both inputs are 0 or very close to zero, it returns -expander, expander.     \n   matplotlib.transforms.offset_copy(trans, fig=None, x=0.0, y=0.0, units='inches')[source]\n \nReturn a new transform with an added offset.  Parameters \n \ntransTransform subclass\n\n\nAny transform, to which offset will be applied.  \nfigFigure, default: None\n\n\nCurrent figure. It can be None if units are 'dots'.  \nx, yfloat, default: 0.0\n\n\nThe offset to apply.  \nunits{'inches', 'points', 'dots'}, default: 'inches'\n\n\nUnits of the offset.    Returns \n \nTransform subclass\n\nTransform with applied offset.","title":"matplotlib.transformations"},{"text":"classCurveA(head_length=0.4, head_width=0.2, widthA=1.0, widthB=1.0, lengthA=0.2, lengthB=0.2, angleA=0, angleB=0, scaleA=None, scaleB=None)[source]\n \nBases: matplotlib.patches.ArrowStyle._Curve An arrow with a head at its begin point.  Parameters \n \nhead_lengthfloat, default: 0.4\n\n\nLength of the arrow head, relative to mutation_scale.  \nhead_widthfloat, default: 0.2\n\n\nWidth of the arrow head, relative to mutation_scale.  \nwidthAfloat, default: 1.0\n\n\nWidth of the bracket at the beginning of the arrow  \nwidthBfloat, default: 1.0\n\n\nWidth of the bracket at the end of the arrow  \nlengthAfloat, default: 0.2\n\n\nLength of the bracket at the beginning of the arrow  \nlengthBfloat, default: 0.2\n\n\nLength of the bracket at the end of the arrow  \nangleAfloat, default 0\n\n\nOrientation of the bracket at the beginning, as a counterclockwise angle. 0 degrees means perpendicular to the line.  \nangleBfloat, default 0\n\n\nOrientation of the bracket at the beginning, as a counterclockwise angle. 0 degrees means perpendicular to the line.  \nscaleAfloat, default mutation_size\n\n\nThe mutation_size for the beginning bracket  \nscaleBfloat, default mutation_size\n\n\nThe mutation_size for the end bracket       arrow='<-'","title":"matplotlib._as_gen.matplotlib.patches.arrowstyle#matplotlib.patches.ArrowStyle.CurveA"},{"text":"get_paused() \n true if the drive is paused get_paused() -> bool  Returns True if the drive is currently paused.","title":"pygame.ref.cdrom#pygame.cdrom.CD.get_paused"},{"text":"staticfrom_values(a, b, c, d, e, f)[source]\n \nCreate a new Affine2D instance from the given values: a c e\nb d f\n0 0 1\n .","title":"matplotlib.transformations#matplotlib.transforms.Affine2D.from_values"},{"text":"matplotlib.markers Functions to handle markers; used by the marker functionality of plot, scatter, and errorbar. All possible markers are defined here:   \nmarker symbol description   \n\".\"  point  \n\",\"  pixel  \n\"o\"  circle  \n\"v\"  triangle_down  \n\"^\"  triangle_up  \n\"<\"  triangle_left  \n\">\"  triangle_right  \n\"1\"  tri_down  \n\"2\"  tri_up  \n\"3\"  tri_left  \n\"4\"  tri_right  \n\"8\"  octagon  \n\"s\"  square  \n\"p\"  pentagon  \n\"P\"  plus (filled)  \n\"*\"  star  \n\"h\"  hexagon1  \n\"H\"  hexagon2  \n\"+\"  plus  \n\"x\"  x  \n\"X\"  x (filled)  \n\"D\"  diamond  \n\"d\"  thin_diamond  \n\"|\"  vline  \n\"_\"  hline  \n0 (TICKLEFT)  tickleft  \n1 (TICKRIGHT)  tickright  \n2 (TICKUP)  tickup  \n3 (TICKDOWN)  tickdown  \n4 (CARETLEFT)  caretleft  \n5 (CARETRIGHT)  caretright  \n6 (CARETUP)  caretup  \n7 (CARETDOWN)  caretdown  \n8 (CARETLEFTBASE)  caretleft (centered at base)  \n9 (CARETRIGHTBASE)  caretright (centered at base)  \n10 (CARETUPBASE)  caretup (centered at base)  \n11 (CARETDOWNBASE)  caretdown (centered at base)  \n\"None\", \" \" or \"\"  nothing  \n'$...$'  Render the string using mathtext. E.g \"$f$\" for marker showing the letter f.  \nverts  A list of (x, y) pairs used for Path vertices. The center of the marker is located at (0, 0) and the size is normalized, such that the created path is encapsulated inside the unit cell.  \npath  A Path instance.  \n(numsides, 0, angle)  A regular polygon with numsides sides, rotated by angle.  \n(numsides, 1, angle)  A star-like symbol with numsides sides, rotated by angle.  \n(numsides, 2, angle)  An asterisk with numsides sides, rotated by angle.   None is the default which means 'nothing', however this table is referred to from other docs for the valid inputs from marker inputs and in those cases None still means 'default'. Note that special symbols can be defined via the STIX math font, e.g. \"$\\u266B$\". For an overview over the STIX font symbols refer to the STIX font table. Also see the STIX Fonts. Integer numbers from 0 to 11 create lines and triangles. Those are equally accessible via capitalized variables, like CARETDOWNBASE. Hence the following are equivalent: plt.plot([1, 2, 3], marker=11)\nplt.plot([1, 2, 3], marker=matplotlib.markers.CARETDOWNBASE)\n Examples showing the use of markers:  Marker reference Marker examples   Classes  \nMarkerStyle([marker, fillstyle]) A class representing marker types.","title":"matplotlib.markers_api"}]}
{"task_id":2545397,"prompt":"def f_2545397(myfile):\n\treturn ","suffix":"","canonical_solution":"[x for x in myfile if x != '']","test_start":"\ndef check(candidate):","test":["\n    with open('.\/tmp.txt', 'w') as fw: \n        for s in [\"hello\", \"world\", \"!!!\"]:\n            fw.write(f\"{s}\\n\")\n\n    with open('.\/tmp.txt', 'r') as myfile:\n        lines = candidate(myfile)\n        assert isinstance(lines, list)\n        assert len(lines) == 3\n        assert lines[0].strip() == \"hello\"\n"],"entry_point":"f_2545397","intent":"append each line in file `myfile` into a list","library":[],"docs":[]}
{"task_id":2545397,"prompt":"def f_2545397():\n\t","suffix":"\n\treturn lst","canonical_solution":"lst = list(map(int, open('filename.txt').readlines()))","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    with open('.\/filename.txt', 'w') as fw: \n        for s in [\"1\", \"2\", \"100\"]:\n            fw.write(f\"{s}\\n\")\n\n    assert candidate() == [1, 2, 100]\n"],"entry_point":"f_2545397","intent":"Get a list of integers `lst` from a file `filename.txt`","library":["pandas"],"docs":[]}
{"task_id":35420052,"prompt":"def f_35420052(plt, mappable, ax3):\n\t","suffix":"\n\treturn plt","canonical_solution":"plt.colorbar(mappable=mappable, cax=ax3)","test_start":"\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom obspy.core.trace import Trace\nfrom obspy.imaging.spectrogram import spectrogram\n\ndef check(candidate):","test":["\n    spl1 = Trace(data=np.arange(0, 10))\n    fig = plt.figure()\n    ax1 = fig.add_axes([0.1, 0.75, 0.7, 0.2]) #[left bottom width height]\n    ax2 = fig.add_axes([0.1, 0.1, 0.7, 0.60], sharex=ax1)\n    ax3 = fig.add_axes([0.83, 0.1, 0.03, 0.6])\n\n    #make time vector\n    t = np.arange(spl1.stats.npts) \/ spl1.stats.sampling_rate\n\n    #plot waveform (top subfigure)    \n    ax1.plot(t, spl1.data, 'k')\n\n    #plot spectrogram (bottom subfigure)\n    spl2 = spl1\n    fig = spl2.spectrogram(show=False, axes=ax2, wlen=10)\n    mappable = ax2.images[0]\n    candidate(plt, mappable, ax3)\n    \n    im=ax2.images\n    assert im[-1].colorbar is not None\n"],"entry_point":"f_35420052","intent":"add color bar with image `mappable` to plot `plt`","library":["matplotlib","numpy","obspy"],"docs":[{"text":"colorbar(mappable, *, ticks=None, **kwargs)[source]","title":"matplotlib._as_gen.mpl_toolkits.axes_grid1.axes_grid.cbaraxesbase#mpl_toolkits.axes_grid1.axes_grid.CbarAxesBase.colorbar"},{"text":"colorbar(mappable, cax=None, ax=None, use_gridspec=True, **kw)[source]\n \nAdd a colorbar to a plot.  Parameters \n mappable\n\nThe matplotlib.cm.ScalarMappable (i.e., AxesImage, ContourSet, etc.) described by this colorbar. This argument is mandatory for the Figure.colorbar method but optional for the pyplot.colorbar function, which sets the default to the current image. Note that one can create a ScalarMappable \"on-the-fly\" to generate colorbars not attached to a previously drawn artist, e.g. fig.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap), ax=ax)\n  \ncaxAxes, optional\n\n\nAxes into which the colorbar will be drawn.  \naxAxes, list of Axes, optional\n\n\nOne or more parent axes from which space for a new colorbar axes will be stolen, if cax is None. This has no effect if cax is set.  \nuse_gridspecbool, optional\n\n\nIf cax is None, a new cax is created as an instance of Axes. If ax is an instance of Subplot and use_gridspec is True, cax is created as an instance of Subplot using the gridspec module.    Returns \n \ncolorbarColorbar\n\n   Notes Additional keyword arguments are of two kinds: axes properties:  locationNone or {'left', 'right', 'top', 'bottom'}\n\n\nThe location, relative to the parent axes, where the colorbar axes is created. It also determines the orientation of the colorbar (colorbars on the left and right are vertical, colorbars at the top and bottom are horizontal). If None, the location will come from the orientation if it is set (vertical colorbars on the right, horizontal ones at the bottom), or default to 'right' if orientation is unset.  orientationNone or {'vertical', 'horizontal'}\n\n\nThe orientation of the colorbar. It is preferable to set the location of the colorbar, as that also determines the orientation; passing incompatible values for location and orientation raises an exception.  fractionfloat, default: 0.15\n\n\nFraction of original axes to use for colorbar.  shrinkfloat, default: 1.0\n\n\nFraction by which to multiply the size of the colorbar.  aspectfloat, default: 20\n\n\nRatio of long to short dimensions.  padfloat, default: 0.05 if vertical, 0.15 if horizontal\n\n\nFraction of original axes between colorbar and new image axes.  anchor(float, float), optional\n\n\nThe anchor point of the colorbar axes. Defaults to (0.0, 0.5) if vertical; (0.5, 1.0) if horizontal.  panchor(float, float), or False, optional\n\n\nThe anchor point of the colorbar parent axes. If False, the parent axes' anchor will be unchanged. Defaults to (1.0, 0.5) if vertical; (0.5, 0.0) if horizontal.   colorbar properties:   \nProperty Description   \nextend {'neither', 'both', 'min', 'max'} If not 'neither', make pointed end(s) for out-of- range values. These are set for a given colormap using the colormap set_under and set_over methods.  \nextendfrac {None, 'auto', length, lengths} If set to None, both the minimum and maximum triangular colorbar extensions with have a length of 5% of the interior colorbar length (this is the default setting). If set to 'auto', makes the triangular colorbar extensions the same lengths as the interior boxes (when spacing is set to 'uniform') or the same lengths as the respective adjacent interior boxes (when spacing is set to 'proportional'). If a scalar, indicates the length of both the minimum and maximum triangular colorbar extensions as a fraction of the interior colorbar length. A two-element sequence of fractions may also be given, indicating the lengths of the minimum and maximum colorbar extensions respectively as a fraction of the interior colorbar length.  \nextendrect bool If False the minimum and maximum colorbar extensions will be triangular (the default). If True the extensions will be rectangular.  \nspacing {'uniform', 'proportional'} Uniform spacing gives each discrete color the same space; proportional makes the space proportional to the data interval.  \nticks None or list of ticks or Locator If None, ticks are determined automatically from the input.  \nformat None or str or Formatter If None, ScalarFormatter is used. If a format string is given, e.g., '%.3f', that is used. An alternative Formatter may be given instead.  \ndrawedges bool Whether to draw lines at color boundaries.  \nlabel str The label on the colorbar's long axis.   The following will probably be useful only in the context of indexed colors (that is, when the mappable has norm=NoNorm()), or other unusual circumstances.   \nProperty Description   \nboundaries None or a sequence  \nvalues None or a sequence which must be of length 1 less than the sequence of boundaries. For each region delimited by adjacent entries in boundaries, the colormapped to the corresponding value in values will be used.   If mappable is a ContourSet, its extend kwarg is included automatically. The shrink kwarg provides a simple way to scale the colorbar with respect to the axes. Note that if cax is specified, it determines the size of the colorbar and shrink and aspect kwargs are ignored. For more precise control, you can manually specify the positions of the axes objects in which the mappable and the colorbar are drawn. In this case, do not use any of the axes properties kwargs. It is known that some vector graphics viewers (svg and pdf) renders white gaps between segments of the colorbar. This is due to bugs in the viewers, not Matplotlib. As a workaround, the colorbar can be rendered with overlapping segments: cbar = colorbar()\ncbar.solids.set_edgecolor(\"face\")\ndraw()\n However this has negative consequences in other circumstances, e.g. with semi-transparent images (alpha < 1) and colorbar extensions; therefore, this workaround is not used by default (see issue #1188).","title":"matplotlib.figure_api#matplotlib.figure.FigureBase.colorbar"},{"text":"colorbar(mappable, cax=None, ax=None, use_gridspec=True, **kw)[source]\n \nAdd a colorbar to a plot.  Parameters \n mappable\n\nThe matplotlib.cm.ScalarMappable (i.e., AxesImage, ContourSet, etc.) described by this colorbar. This argument is mandatory for the Figure.colorbar method but optional for the pyplot.colorbar function, which sets the default to the current image. Note that one can create a ScalarMappable \"on-the-fly\" to generate colorbars not attached to a previously drawn artist, e.g. fig.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap), ax=ax)\n  \ncaxAxes, optional\n\n\nAxes into which the colorbar will be drawn.  \naxAxes, list of Axes, optional\n\n\nOne or more parent axes from which space for a new colorbar axes will be stolen, if cax is None. This has no effect if cax is set.  \nuse_gridspecbool, optional\n\n\nIf cax is None, a new cax is created as an instance of Axes. If ax is an instance of Subplot and use_gridspec is True, cax is created as an instance of Subplot using the gridspec module.    Returns \n \ncolorbarColorbar\n\n   Notes Additional keyword arguments are of two kinds: axes properties:  locationNone or {'left', 'right', 'top', 'bottom'}\n\n\nThe location, relative to the parent axes, where the colorbar axes is created. It also determines the orientation of the colorbar (colorbars on the left and right are vertical, colorbars at the top and bottom are horizontal). If None, the location will come from the orientation if it is set (vertical colorbars on the right, horizontal ones at the bottom), or default to 'right' if orientation is unset.  orientationNone or {'vertical', 'horizontal'}\n\n\nThe orientation of the colorbar. It is preferable to set the location of the colorbar, as that also determines the orientation; passing incompatible values for location and orientation raises an exception.  fractionfloat, default: 0.15\n\n\nFraction of original axes to use for colorbar.  shrinkfloat, default: 1.0\n\n\nFraction by which to multiply the size of the colorbar.  aspectfloat, default: 20\n\n\nRatio of long to short dimensions.  padfloat, default: 0.05 if vertical, 0.15 if horizontal\n\n\nFraction of original axes between colorbar and new image axes.  anchor(float, float), optional\n\n\nThe anchor point of the colorbar axes. Defaults to (0.0, 0.5) if vertical; (0.5, 1.0) if horizontal.  panchor(float, float), or False, optional\n\n\nThe anchor point of the colorbar parent axes. If False, the parent axes' anchor will be unchanged. Defaults to (1.0, 0.5) if vertical; (0.5, 0.0) if horizontal.   colorbar properties:   \nProperty Description   \nextend {'neither', 'both', 'min', 'max'} If not 'neither', make pointed end(s) for out-of- range values. These are set for a given colormap using the colormap set_under and set_over methods.  \nextendfrac {None, 'auto', length, lengths} If set to None, both the minimum and maximum triangular colorbar extensions with have a length of 5% of the interior colorbar length (this is the default setting). If set to 'auto', makes the triangular colorbar extensions the same lengths as the interior boxes (when spacing is set to 'uniform') or the same lengths as the respective adjacent interior boxes (when spacing is set to 'proportional'). If a scalar, indicates the length of both the minimum and maximum triangular colorbar extensions as a fraction of the interior colorbar length. A two-element sequence of fractions may also be given, indicating the lengths of the minimum and maximum colorbar extensions respectively as a fraction of the interior colorbar length.  \nextendrect bool If False the minimum and maximum colorbar extensions will be triangular (the default). If True the extensions will be rectangular.  \nspacing {'uniform', 'proportional'} Uniform spacing gives each discrete color the same space; proportional makes the space proportional to the data interval.  \nticks None or list of ticks or Locator If None, ticks are determined automatically from the input.  \nformat None or str or Formatter If None, ScalarFormatter is used. If a format string is given, e.g., '%.3f', that is used. An alternative Formatter may be given instead.  \ndrawedges bool Whether to draw lines at color boundaries.  \nlabel str The label on the colorbar's long axis.   The following will probably be useful only in the context of indexed colors (that is, when the mappable has norm=NoNorm()), or other unusual circumstances.   \nProperty Description   \nboundaries None or a sequence  \nvalues None or a sequence which must be of length 1 less than the sequence of boundaries. For each region delimited by adjacent entries in boundaries, the colormapped to the corresponding value in values will be used.   If mappable is a ContourSet, its extend kwarg is included automatically. The shrink kwarg provides a simple way to scale the colorbar with respect to the axes. Note that if cax is specified, it determines the size of the colorbar and shrink and aspect kwargs are ignored. For more precise control, you can manually specify the positions of the axes objects in which the mappable and the colorbar are drawn. In this case, do not use any of the axes properties kwargs. It is known that some vector graphics viewers (svg and pdf) renders white gaps between segments of the colorbar. This is due to bugs in the viewers, not Matplotlib. As a workaround, the colorbar can be rendered with overlapping segments: cbar = colorbar()\ncbar.solids.set_edgecolor(\"face\")\ndraw()\n However this has negative consequences in other circumstances, e.g. with semi-transparent images (alpha < 1) and colorbar extensions; therefore, this workaround is not used by default (see issue #1188).","title":"matplotlib.figure_api#matplotlib.figure.Figure.colorbar"},{"text":"colorbar(mappable, cax=None, ax=None, use_gridspec=True, **kw)[source]\n \nAdd a colorbar to a plot.  Parameters \n mappable\n\nThe matplotlib.cm.ScalarMappable (i.e., AxesImage, ContourSet, etc.) described by this colorbar. This argument is mandatory for the Figure.colorbar method but optional for the pyplot.colorbar function, which sets the default to the current image. Note that one can create a ScalarMappable \"on-the-fly\" to generate colorbars not attached to a previously drawn artist, e.g. fig.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap), ax=ax)\n  \ncaxAxes, optional\n\n\nAxes into which the colorbar will be drawn.  \naxAxes, list of Axes, optional\n\n\nOne or more parent axes from which space for a new colorbar axes will be stolen, if cax is None. This has no effect if cax is set.  \nuse_gridspecbool, optional\n\n\nIf cax is None, a new cax is created as an instance of Axes. If ax is an instance of Subplot and use_gridspec is True, cax is created as an instance of Subplot using the gridspec module.    Returns \n \ncolorbarColorbar\n\n   Notes Additional keyword arguments are of two kinds: axes properties:  locationNone or {'left', 'right', 'top', 'bottom'}\n\n\nThe location, relative to the parent axes, where the colorbar axes is created. It also determines the orientation of the colorbar (colorbars on the left and right are vertical, colorbars at the top and bottom are horizontal). If None, the location will come from the orientation if it is set (vertical colorbars on the right, horizontal ones at the bottom), or default to 'right' if orientation is unset.  orientationNone or {'vertical', 'horizontal'}\n\n\nThe orientation of the colorbar. It is preferable to set the location of the colorbar, as that also determines the orientation; passing incompatible values for location and orientation raises an exception.  fractionfloat, default: 0.15\n\n\nFraction of original axes to use for colorbar.  shrinkfloat, default: 1.0\n\n\nFraction by which to multiply the size of the colorbar.  aspectfloat, default: 20\n\n\nRatio of long to short dimensions.  padfloat, default: 0.05 if vertical, 0.15 if horizontal\n\n\nFraction of original axes between colorbar and new image axes.  anchor(float, float), optional\n\n\nThe anchor point of the colorbar axes. Defaults to (0.0, 0.5) if vertical; (0.5, 1.0) if horizontal.  panchor(float, float), or False, optional\n\n\nThe anchor point of the colorbar parent axes. If False, the parent axes' anchor will be unchanged. Defaults to (1.0, 0.5) if vertical; (0.5, 0.0) if horizontal.   colorbar properties:   \nProperty Description   \nextend {'neither', 'both', 'min', 'max'} If not 'neither', make pointed end(s) for out-of- range values. These are set for a given colormap using the colormap set_under and set_over methods.  \nextendfrac {None, 'auto', length, lengths} If set to None, both the minimum and maximum triangular colorbar extensions with have a length of 5% of the interior colorbar length (this is the default setting). If set to 'auto', makes the triangular colorbar extensions the same lengths as the interior boxes (when spacing is set to 'uniform') or the same lengths as the respective adjacent interior boxes (when spacing is set to 'proportional'). If a scalar, indicates the length of both the minimum and maximum triangular colorbar extensions as a fraction of the interior colorbar length. A two-element sequence of fractions may also be given, indicating the lengths of the minimum and maximum colorbar extensions respectively as a fraction of the interior colorbar length.  \nextendrect bool If False the minimum and maximum colorbar extensions will be triangular (the default). If True the extensions will be rectangular.  \nspacing {'uniform', 'proportional'} Uniform spacing gives each discrete color the same space; proportional makes the space proportional to the data interval.  \nticks None or list of ticks or Locator If None, ticks are determined automatically from the input.  \nformat None or str or Formatter If None, ScalarFormatter is used. If a format string is given, e.g., '%.3f', that is used. An alternative Formatter may be given instead.  \ndrawedges bool Whether to draw lines at color boundaries.  \nlabel str The label on the colorbar's long axis.   The following will probably be useful only in the context of indexed colors (that is, when the mappable has norm=NoNorm()), or other unusual circumstances.   \nProperty Description   \nboundaries None or a sequence  \nvalues None or a sequence which must be of length 1 less than the sequence of boundaries. For each region delimited by adjacent entries in boundaries, the colormapped to the corresponding value in values will be used.   If mappable is a ContourSet, its extend kwarg is included automatically. The shrink kwarg provides a simple way to scale the colorbar with respect to the axes. Note that if cax is specified, it determines the size of the colorbar and shrink and aspect kwargs are ignored. For more precise control, you can manually specify the positions of the axes objects in which the mappable and the colorbar are drawn. In this case, do not use any of the axes properties kwargs. It is known that some vector graphics viewers (svg and pdf) renders white gaps between segments of the colorbar. This is due to bugs in the viewers, not Matplotlib. As a workaround, the colorbar can be rendered with overlapping segments: cbar = colorbar()\ncbar.solids.set_edgecolor(\"face\")\ndraw()\n However this has negative consequences in other circumstances, e.g. with semi-transparent images (alpha < 1) and colorbar extensions; therefore, this workaround is not used by default (see issue #1188).","title":"matplotlib.figure_api#matplotlib.figure.SubFigure.colorbar"},{"text":"matplotlib.pyplot.colorbar   matplotlib.pyplot.colorbar(mappable=None, cax=None, ax=None, **kw)[source]\n \nAdd a colorbar to a plot.  Parameters \n mappable\n\nThe matplotlib.cm.ScalarMappable (i.e., AxesImage, ContourSet, etc.) described by this colorbar. This argument is mandatory for the Figure.colorbar method but optional for the pyplot.colorbar function, which sets the default to the current image. Note that one can create a ScalarMappable \"on-the-fly\" to generate colorbars not attached to a previously drawn artist, e.g. fig.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap), ax=ax)\n  \ncaxAxes, optional\n\n\nAxes into which the colorbar will be drawn.  \naxAxes, list of Axes, optional\n\n\nOne or more parent axes from which space for a new colorbar axes will be stolen, if cax is None. This has no effect if cax is set.  \nuse_gridspecbool, optional\n\n\nIf cax is None, a new cax is created as an instance of Axes. If ax is an instance of Subplot and use_gridspec is True, cax is created as an instance of Subplot using the gridspec module.    Returns \n \ncolorbarColorbar\n\n   Notes Additional keyword arguments are of two kinds: axes properties:  locationNone or {'left', 'right', 'top', 'bottom'}\n\n\nThe location, relative to the parent axes, where the colorbar axes is created. It also determines the orientation of the colorbar (colorbars on the left and right are vertical, colorbars at the top and bottom are horizontal). If None, the location will come from the orientation if it is set (vertical colorbars on the right, horizontal ones at the bottom), or default to 'right' if orientation is unset.  orientationNone or {'vertical', 'horizontal'}\n\n\nThe orientation of the colorbar. It is preferable to set the location of the colorbar, as that also determines the orientation; passing incompatible values for location and orientation raises an exception.  fractionfloat, default: 0.15\n\n\nFraction of original axes to use for colorbar.  shrinkfloat, default: 1.0\n\n\nFraction by which to multiply the size of the colorbar.  aspectfloat, default: 20\n\n\nRatio of long to short dimensions.  padfloat, default: 0.05 if vertical, 0.15 if horizontal\n\n\nFraction of original axes between colorbar and new image axes.  anchor(float, float), optional\n\n\nThe anchor point of the colorbar axes. Defaults to (0.0, 0.5) if vertical; (0.5, 1.0) if horizontal.  panchor(float, float), or False, optional\n\n\nThe anchor point of the colorbar parent axes. If False, the parent axes' anchor will be unchanged. Defaults to (1.0, 0.5) if vertical; (0.5, 0.0) if horizontal.   colorbar properties:   \nProperty Description   \nextend {'neither', 'both', 'min', 'max'} If not 'neither', make pointed end(s) for out-of- range values. These are set for a given colormap using the colormap set_under and set_over methods.  \nextendfrac {None, 'auto', length, lengths} If set to None, both the minimum and maximum triangular colorbar extensions with have a length of 5% of the interior colorbar length (this is the default setting). If set to 'auto', makes the triangular colorbar extensions the same lengths as the interior boxes (when spacing is set to 'uniform') or the same lengths as the respective adjacent interior boxes (when spacing is set to 'proportional'). If a scalar, indicates the length of both the minimum and maximum triangular colorbar extensions as a fraction of the interior colorbar length. A two-element sequence of fractions may also be given, indicating the lengths of the minimum and maximum colorbar extensions respectively as a fraction of the interior colorbar length.  \nextendrect bool If False the minimum and maximum colorbar extensions will be triangular (the default). If True the extensions will be rectangular.  \nspacing {'uniform', 'proportional'} Uniform spacing gives each discrete color the same space; proportional makes the space proportional to the data interval.  \nticks None or list of ticks or Locator If None, ticks are determined automatically from the input.  \nformat None or str or Formatter If None, ScalarFormatter is used. If a format string is given, e.g., '%.3f', that is used. An alternative Formatter may be given instead.  \ndrawedges bool Whether to draw lines at color boundaries.  \nlabel str The label on the colorbar's long axis.   The following will probably be useful only in the context of indexed colors (that is, when the mappable has norm=NoNorm()), or other unusual circumstances.   \nProperty Description   \nboundaries None or a sequence  \nvalues None or a sequence which must be of length 1 less than the sequence of boundaries. For each region delimited by adjacent entries in boundaries, the colormapped to the corresponding value in values will be used.   If mappable is a ContourSet, its extend kwarg is included automatically. The shrink kwarg provides a simple way to scale the colorbar with respect to the axes. Note that if cax is specified, it determines the size of the colorbar and shrink and aspect kwargs are ignored. For more precise control, you can manually specify the positions of the axes objects in which the mappable and the colorbar are drawn. In this case, do not use any of the axes properties kwargs. It is known that some vector graphics viewers (svg and pdf) renders white gaps between segments of the colorbar. This is due to bugs in the viewers, not Matplotlib. As a workaround, the colorbar can be rendered with overlapping segments: cbar = colorbar()\ncbar.solids.set_edgecolor(\"face\")\ndraw()\n However this has negative consequences in other circumstances, e.g. with semi-transparent images (alpha < 1) and colorbar extensions; therefore, this workaround is not used by default (see issue #1188). \n  Examples using matplotlib.pyplot.colorbar\n \n   Subplots spacings and margins   \n\n   Ellipse Collection   \n\n   Axes Divider   \n\n   Simple Colorbar   \n\n   Image tutorial   \n\n   Tight Layout guide","title":"matplotlib._as_gen.matplotlib.pyplot.colorbar"},{"text":"matplotlib.colorbar.colorbar_factory(cax, mappable, **kwargs)[source]\n \n[Deprecated] Create a colorbar on the given axes for the given mappable.  Note This is a low-level function to turn an existing axes into a colorbar axes. Typically, you'll want to use colorbar instead, which automatically handles creation and placement of a suitable axes as well.   Parameters \n \ncaxAxes\n\n\nThe Axes to turn into a colorbar.  \nmappableScalarMappable\n\n\nThe mappable to be described by the colorbar.  **kwargs\n\nKeyword arguments are passed to the respective colorbar class.    Returns \n Colorbar\n\nThe created colorbar instance.     Notes  Deprecated since version 3.4.","title":"matplotlib.colorbar_api#matplotlib.colorbar.colorbar_factory"},{"text":"classmatplotlib.colorbar.Colorbar(ax, mappable=None, *, cmap=None, norm=None, alpha=None, values=None, boundaries=None, orientation='vertical', ticklocation='auto', extend=None, spacing='uniform', ticks=None, format=None, drawedges=False, filled=True, extendfrac=None, extendrect=False, label='')[source]\n \nBases: object Draw a colorbar in an existing axes. Typically, colorbars are created using Figure.colorbar or pyplot.colorbar and associated with ScalarMappables (such as an AxesImage generated via imshow). In order to draw a colorbar not associated with other elements in the figure, e.g. when showing a colormap by itself, one can create an empty ScalarMappable, or directly pass cmap and norm instead of mappable to Colorbar. Useful public methods are set_label() and add_lines().  Parameters \n \naxAxes\n\n\nThe Axes instance in which the colorbar is drawn.  \nmappableScalarMappable\n\n\nThe mappable whose colormap and norm will be used. To show the under- and over- value colors, the mappable's norm should be specified as norm = colors.Normalize(clip=False)\n To show the colors versus index instead of on a 0-1 scale, use: norm=colors.NoNorm()\n  \ncmapColormap, default: rcParams[\"image.cmap\"] (default: 'viridis')\n\n\nThe colormap to use. This parameter is ignored, unless mappable is None.  \nnormNormalize\n\n\nThe normalization to use. This parameter is ignored, unless mappable is None.  \nalphafloat\n\n\nThe colorbar transparency between 0 (transparent) and 1 (opaque).  values, boundaries\n\nIf unset, the colormap will be displayed on a 0-1 scale.  \norientation{'vertical', 'horizontal'}\n\n\nticklocation{'auto', 'left', 'right', 'top', 'bottom'}\n\n\nextend{'neither', 'both', 'min', 'max'}\n\n\nspacing{'uniform', 'proportional'}\n\n\nticksLocator or array-like of float\n\n\nformatstr or Formatter\n\n\ndrawedgesbool\n\n\nfilledbool\n\nextendfrac\nextendrec\n\nlabelstr\n\n  Attributes \n \naxAxes\n\n\nThe Axes instance in which the colorbar is drawn.  \nlineslist\n\n\nA list of LineCollection (empty if no lines were drawn).  \ndividersLineCollection\n\n\nA LineCollection (empty if drawedges is False).       add_lines(*args, **kwargs)[source]\n \nDraw lines on the colorbar. The lines are appended to the list lines.  Parameters \n \nlevelsarray-like\n\n\nThe positions of the lines.  \ncolorscolor or list of colors\n\n\nEither a single color applying to all lines or one color value for each line.  \nlinewidthsfloat or array-like\n\n\nEither a single linewidth applying to all lines or one linewidth for each line.  \nerasebool, default: True\n\n\nWhether to remove any previously added lines.     Notes Alternatively, this method can also be called with the signature colorbar.add_lines(contour_set, erase=True), in which case levels, colors, and linewidths are taken from contour_set. \n   drag_pan(button, key, x, y)[source]\n\n   draw_all()[source]\n \nCalculate any free parameters based on the current cmap and norm, and do all the drawing. \n   get_ticks(minor=False)[source]\n \nReturn the ticks as a list of locations.  Parameters \n \nminorboolean, default: False\n\n\nif True return the minor ticks.     \n   minorticks_off()[source]\n \nTurn the minor ticks of the colorbar off. \n   minorticks_on()[source]\n \nTurn on colorbar minor ticks. \n   n_rasterize=50\n\n   propertypatch[source]\n\n   remove()[source]\n \nRemove this colorbar from the figure. If the colorbar was created with use_gridspec=True the previous gridspec is restored. \n   set_alpha(alpha)[source]\n \nSet the transparency between 0 (transparent) and 1 (opaque). If an array is provided, alpha will be set to None to use the transparency values associated with the colormap. \n   set_label(label, *, loc=None, **kwargs)[source]\n \nAdd a label to the long axis of the colorbar.  Parameters \n \nlabelstr\n\n\nThe label text.  \nlocstr, optional\n\n\nThe location of the label.  For horizontal orientation one of {'left', 'center', 'right'} For vertical orientation one of {'bottom', 'center', 'top'}  Defaults to rcParams[\"xaxis.labellocation\"] (default: 'center') or rcParams[\"yaxis.labellocation\"] (default: 'center') depending on the orientation.  **kwargs\n\nKeyword arguments are passed to set_xlabel \/ set_ylabel. Supported keywords are labelpad and Text properties.     \n   set_ticklabels(ticklabels, update_ticks=<deprecated parameter>, *, minor=False, **kwargs)[source]\n \nSet tick labels.  Discouraged The use of this method is discouraged, because of the dependency on tick positions. In most cases, you'll want to use set_ticks(positions, labels=labels) instead. If you are using this method, you should always fix the tick positions before, e.g. by using Colorbar.set_ticks or by explicitly setting a FixedLocator on the long axis of the colorbar. Otherwise, ticks are free to move and the labels may end up in unexpected positions.   Parameters \n \nticklabelssequence of str or of Text\n\n\nTexts for labeling each tick location in the sequence set by Colorbar.set_ticks; the number of labels must match the number of locations.  \nupdate_ticksbool, default: True\n\n This keyword argument is ignored and will be be removed. Deprecated  minorbool\n\n\nIf True, set minor ticks instead of major ticks.    **kwargs\n\nText properties for the labels.     \n   set_ticks(ticks, update_ticks=<deprecated parameter>, labels=None, *, minor=False, **kwargs)[source]\n \nSet tick locations.  Parameters \n \ntickslist of floats\n\n\nList of tick locations.  \nlabelslist of str, optional\n\n\nList of tick labels. If not set, the labels show the data value.  \nminorbool, default: False\n\n\nIf False, set the major ticks; if True, the minor ticks.  **kwargs\n\nText properties for the labels. These take effect only if you pass labels. In other cases, please use tick_params.     \n   update_normal(mappable)[source]\n \nUpdate solid patches, lines, etc. This is meant to be called when the norm of the image or contour plot to which this colorbar belongs changes. If the norm on the mappable is different than before, this resets the locator and formatter for the axis, so if these have been customized, they will need to be customized again. However, if the norm only changes values of vmin, vmax or cmap then the old formatter and locator will be preserved. \n   update_ticks()[source]\n \nSetup the ticks and ticklabels. This should not be needed by users.","title":"matplotlib.colorbar_api#matplotlib.colorbar.Colorbar"},{"text":"mpl_toolkits.axes_grid1.axes_grid.CbarAxesBase   classmpl_toolkits.axes_grid1.axes_grid.CbarAxesBase(*args, orientation, **kwargs)[source]\n \nBases: object   cla()[source]\n\n   colorbar(mappable, *, ticks=None, **kwargs)[source]\n\n   toggle_label(b)[source]","title":"matplotlib._as_gen.mpl_toolkits.axes_grid1.axes_grid.cbaraxesbase"},{"text":"matplotlib.colorbar Colorbars are a visualization of the mapping from scalar values to colors. In Matplotlib they are drawn into a dedicated Axes.  Note Colorbars are typically created through Figure.colorbar or its pyplot wrapper pyplot.colorbar, which internally use Colorbar together with make_axes_gridspec (for GridSpec-positioned axes) or make_axes (for non-GridSpec-positioned axes). End-users most likely won't need to directly use this module's API.    classmatplotlib.colorbar.Colorbar(ax, mappable=None, *, cmap=None, norm=None, alpha=None, values=None, boundaries=None, orientation='vertical', ticklocation='auto', extend=None, spacing='uniform', ticks=None, format=None, drawedges=False, filled=True, extendfrac=None, extendrect=False, label='')[source]\n \nBases: object Draw a colorbar in an existing axes. Typically, colorbars are created using Figure.colorbar or pyplot.colorbar and associated with ScalarMappables (such as an AxesImage generated via imshow). In order to draw a colorbar not associated with other elements in the figure, e.g. when showing a colormap by itself, one can create an empty ScalarMappable, or directly pass cmap and norm instead of mappable to Colorbar. Useful public methods are set_label() and add_lines().  Parameters \n \naxAxes\n\n\nThe Axes instance in which the colorbar is drawn.  \nmappableScalarMappable\n\n\nThe mappable whose colormap and norm will be used. To show the under- and over- value colors, the mappable's norm should be specified as norm = colors.Normalize(clip=False)\n To show the colors versus index instead of on a 0-1 scale, use: norm=colors.NoNorm()\n  \ncmapColormap, default: rcParams[\"image.cmap\"] (default: 'viridis')\n\n\nThe colormap to use. This parameter is ignored, unless mappable is None.  \nnormNormalize\n\n\nThe normalization to use. This parameter is ignored, unless mappable is None.  \nalphafloat\n\n\nThe colorbar transparency between 0 (transparent) and 1 (opaque).  values, boundaries\n\nIf unset, the colormap will be displayed on a 0-1 scale.  \norientation{'vertical', 'horizontal'}\n\n\nticklocation{'auto', 'left', 'right', 'top', 'bottom'}\n\n\nextend{'neither', 'both', 'min', 'max'}\n\n\nspacing{'uniform', 'proportional'}\n\n\nticksLocator or array-like of float\n\n\nformatstr or Formatter\n\n\ndrawedgesbool\n\n\nfilledbool\n\nextendfrac\nextendrec\n\nlabelstr\n\n  Attributes \n \naxAxes\n\n\nThe Axes instance in which the colorbar is drawn.  \nlineslist\n\n\nA list of LineCollection (empty if no lines were drawn).  \ndividersLineCollection\n\n\nA LineCollection (empty if drawedges is False).       add_lines(*args, **kwargs)[source]\n \nDraw lines on the colorbar. The lines are appended to the list lines.  Parameters \n \nlevelsarray-like\n\n\nThe positions of the lines.  \ncolorscolor or list of colors\n\n\nEither a single color applying to all lines or one color value for each line.  \nlinewidthsfloat or array-like\n\n\nEither a single linewidth applying to all lines or one linewidth for each line.  \nerasebool, default: True\n\n\nWhether to remove any previously added lines.     Notes Alternatively, this method can also be called with the signature colorbar.add_lines(contour_set, erase=True), in which case levels, colors, and linewidths are taken from contour_set. \n   drag_pan(button, key, x, y)[source]\n\n   draw_all()[source]\n \nCalculate any free parameters based on the current cmap and norm, and do all the drawing. \n   get_ticks(minor=False)[source]\n \nReturn the ticks as a list of locations.  Parameters \n \nminorboolean, default: False\n\n\nif True return the minor ticks.     \n   minorticks_off()[source]\n \nTurn the minor ticks of the colorbar off. \n   minorticks_on()[source]\n \nTurn on colorbar minor ticks. \n   n_rasterize=50\n\n   propertypatch[source]\n\n   remove()[source]\n \nRemove this colorbar from the figure. If the colorbar was created with use_gridspec=True the previous gridspec is restored. \n   set_alpha(alpha)[source]\n \nSet the transparency between 0 (transparent) and 1 (opaque). If an array is provided, alpha will be set to None to use the transparency values associated with the colormap. \n   set_label(label, *, loc=None, **kwargs)[source]\n \nAdd a label to the long axis of the colorbar.  Parameters \n \nlabelstr\n\n\nThe label text.  \nlocstr, optional\n\n\nThe location of the label.  For horizontal orientation one of {'left', 'center', 'right'} For vertical orientation one of {'bottom', 'center', 'top'}  Defaults to rcParams[\"xaxis.labellocation\"] (default: 'center') or rcParams[\"yaxis.labellocation\"] (default: 'center') depending on the orientation.  **kwargs\n\nKeyword arguments are passed to set_xlabel \/ set_ylabel. Supported keywords are labelpad and Text properties.     \n   set_ticklabels(ticklabels, update_ticks=<deprecated parameter>, *, minor=False, **kwargs)[source]\n \nSet tick labels.  Discouraged The use of this method is discouraged, because of the dependency on tick positions. In most cases, you'll want to use set_ticks(positions, labels=labels) instead. If you are using this method, you should always fix the tick positions before, e.g. by using Colorbar.set_ticks or by explicitly setting a FixedLocator on the long axis of the colorbar. Otherwise, ticks are free to move and the labels may end up in unexpected positions.   Parameters \n \nticklabelssequence of str or of Text\n\n\nTexts for labeling each tick location in the sequence set by Colorbar.set_ticks; the number of labels must match the number of locations.  \nupdate_ticksbool, default: True\n\n This keyword argument is ignored and will be be removed. Deprecated  minorbool\n\n\nIf True, set minor ticks instead of major ticks.    **kwargs\n\nText properties for the labels.     \n   set_ticks(ticks, update_ticks=<deprecated parameter>, labels=None, *, minor=False, **kwargs)[source]\n \nSet tick locations.  Parameters \n \ntickslist of floats\n\n\nList of tick locations.  \nlabelslist of str, optional\n\n\nList of tick labels. If not set, the labels show the data value.  \nminorbool, default: False\n\n\nIf False, set the major ticks; if True, the minor ticks.  **kwargs\n\nText properties for the labels. These take effect only if you pass labels. In other cases, please use tick_params.     \n   update_normal(mappable)[source]\n \nUpdate solid patches, lines, etc. This is meant to be called when the norm of the image or contour plot to which this colorbar belongs changes. If the norm on the mappable is different than before, this resets the locator and formatter for the axis, so if these have been customized, they will need to be customized again. However, if the norm only changes values of vmin, vmax or cmap then the old formatter and locator will be preserved. \n   update_ticks()[source]\n \nSetup the ticks and ticklabels. This should not be needed by users. \n \n   matplotlib.colorbar.ColorbarBase[source]\n \nalias of matplotlib.colorbar.Colorbar \n   classmatplotlib.colorbar.ColorbarPatch(ax, mappable=None, *, cmap=None, norm=None, alpha=None, values=None, boundaries=None, orientation='vertical', ticklocation='auto', extend=None, spacing='uniform', ticks=None, format=None, drawedges=False, filled=True, extendfrac=None, extendrect=False, label='')[source]\n \nBases: matplotlib.colorbar.Colorbar [Deprecated] Notes  Deprecated since version 3.4:   \n   matplotlib.colorbar.colorbar_factory(cax, mappable, **kwargs)[source]\n \n[Deprecated] Create a colorbar on the given axes for the given mappable.  Note This is a low-level function to turn an existing axes into a colorbar axes. Typically, you'll want to use colorbar instead, which automatically handles creation and placement of a suitable axes as well.   Parameters \n \ncaxAxes\n\n\nThe Axes to turn into a colorbar.  \nmappableScalarMappable\n\n\nThe mappable to be described by the colorbar.  **kwargs\n\nKeyword arguments are passed to the respective colorbar class.    Returns \n Colorbar\n\nThe created colorbar instance.     Notes  Deprecated since version 3.4.  \n   matplotlib.colorbar.make_axes(parents, location=None, orientation=None, fraction=0.15, shrink=1.0, aspect=20, **kw)[source]\n \nCreate an Axes suitable for a colorbar. The axes is placed in the figure of the parents axes, by resizing and repositioning parents.  Parameters \n \nparentsAxes or list of Axes\n\n\nThe Axes to use as parents for placing the colorbar.  \nlocationNone or {'left', 'right', 'top', 'bottom'}\n\n\nThe location, relative to the parent axes, where the colorbar axes is created. It also determines the orientation of the colorbar (colorbars on the left and right are vertical, colorbars at the top and bottom are horizontal). If None, the location will come from the orientation if it is set (vertical colorbars on the right, horizontal ones at the bottom), or default to 'right' if orientation is unset.  \norientationNone or {'vertical', 'horizontal'}\n\n\nThe orientation of the colorbar. It is preferable to set the location of the colorbar, as that also determines the orientation; passing incompatible values for location and orientation raises an exception.  \nfractionfloat, default: 0.15\n\n\nFraction of original axes to use for colorbar.  \nshrinkfloat, default: 1.0\n\n\nFraction by which to multiply the size of the colorbar.  \naspectfloat, default: 20\n\n\nRatio of long to short dimensions.    Returns \n \ncaxAxes\n\n\nThe child axes.  \nkwdict\n\n\nThe reduced keyword dictionary to be passed when creating the colorbar instance.    Other Parameters \n \npadfloat, default: 0.05 if vertical, 0.15 if horizontal\n\n\nFraction of original axes between colorbar and new image axes.  \nanchor(float, float), optional\n\n\nThe anchor point of the colorbar axes. Defaults to (0.0, 0.5) if vertical; (0.5, 1.0) if horizontal.  \npanchor(float, float), or False, optional\n\n\nThe anchor point of the colorbar parent axes. If False, the parent axes' anchor will be unchanged. Defaults to (1.0, 0.5) if vertical; (0.5, 0.0) if horizontal.     \n   matplotlib.colorbar.make_axes_gridspec(parent, *, location=None, orientation=None, fraction=0.15, shrink=1.0, aspect=20, **kw)[source]\n \nCreate a SubplotBase suitable for a colorbar. The axes is placed in the figure of the parent axes, by resizing and repositioning parent. This function is similar to make_axes. Primary differences are  \nmake_axes_gridspec should only be used with a SubplotBase parent. \nmake_axes creates an Axes; make_axes_gridspec creates a SubplotBase. \nmake_axes updates the position of the parent. make_axes_gridspec replaces the grid_spec attribute of the parent with a new one.  While this function is meant to be compatible with make_axes, there could be some minor differences.  Parameters \n \nparentAxes\n\n\nThe Axes to use as parent for placing the colorbar.  \nlocationNone or {'left', 'right', 'top', 'bottom'}\n\n\nThe location, relative to the parent axes, where the colorbar axes is created. It also determines the orientation of the colorbar (colorbars on the left and right are vertical, colorbars at the top and bottom are horizontal). If None, the location will come from the orientation if it is set (vertical colorbars on the right, horizontal ones at the bottom), or default to 'right' if orientation is unset.  \norientationNone or {'vertical', 'horizontal'}\n\n\nThe orientation of the colorbar. It is preferable to set the location of the colorbar, as that also determines the orientation; passing incompatible values for location and orientation raises an exception.  \nfractionfloat, default: 0.15\n\n\nFraction of original axes to use for colorbar.  \nshrinkfloat, default: 1.0\n\n\nFraction by which to multiply the size of the colorbar.  \naspectfloat, default: 20\n\n\nRatio of long to short dimensions.    Returns \n \ncaxSubplotBase\n\n\nThe child axes.  \nkwdict\n\n\nThe reduced keyword dictionary to be passed when creating the colorbar instance.    Other Parameters \n \npadfloat, default: 0.05 if vertical, 0.15 if horizontal\n\n\nFraction of original axes between colorbar and new image axes.  \nanchor(float, float), optional\n\n\nThe anchor point of the colorbar axes. Defaults to (0.0, 0.5) if vertical; (0.5, 1.0) if horizontal.  \npanchor(float, float), or False, optional\n\n\nThe anchor point of the colorbar parent axes. If False, the parent axes' anchor will be unchanged. Defaults to (1.0, 0.5) if vertical; (0.5, 0.0) if horizontal.","title":"matplotlib.colorbar_api"},{"text":"classmatplotlib.colorbar.ColorbarPatch(ax, mappable=None, *, cmap=None, norm=None, alpha=None, values=None, boundaries=None, orientation='vertical', ticklocation='auto', extend=None, spacing='uniform', ticks=None, format=None, drawedges=False, filled=True, extendfrac=None, extendrect=False, label='')[source]\n \nBases: matplotlib.colorbar.Colorbar [Deprecated] Notes  Deprecated since version 3.4:","title":"matplotlib.colorbar_api#matplotlib.colorbar.ColorbarPatch"}]}
{"task_id":29903025,"prompt":"def f_29903025(df):\n\treturn ","suffix":"","canonical_solution":"Counter(' '.join(df['text']).split()).most_common(100)","test_start":"\nimport pandas as pd\nfrom collections import Counter\n \ndef check(candidate):","test":["\n    df = pd.DataFrame({\"text\": [\n      'Python is a high-level, general-purpose programming language.', \n      'Its design philosophy emphasizes code readability with the use of significant indentation. Python is dynamically-typed and garbage-collected.'\n    ]})\n    assert candidate(df) == [('Python', 2),('is', 2),('a', 1),('high-level,', 1),('general-purpose', 1),\n        ('programming', 1),('language.', 1),('Its', 1),('design', 1),('philosophy', 1),('emphasizes', 1),\n        ('code', 1),('readability', 1),('with', 1), ('the', 1),('use', 1),('of', 1),('significant', 1),\n        ('indentation.', 1),('dynamically-typed', 1),('and', 1),('garbage-collected.', 1)]\n"],"entry_point":"f_29903025","intent":"count most frequent 100 words in column 'text' of dataframe `df`","library":["collections","pandas"],"docs":[{"text":"locale.ALT_DIGITS  \nGet a representation of up to 100 values used to represent the values 0 to 99.","title":"python.library.locale#locale.ALT_DIGITS"},{"text":"numpy.random.RandomState.standard_t method   random.RandomState.standard_t(df, size=None)\n \nDraw samples from a standard Student\u2019s t distribution with df degrees of freedom. A special case of the hyperbolic distribution. As df gets large, the result resembles that of the standard normal distribution (standard_normal).  Note New code should use the standard_t method of a default_rng() instance instead; please see the Quick Start.   Parameters \n \ndffloat or array_like of floats\n\n\nDegrees of freedom, must be > 0.  \nsizeint or tuple of ints, optional\n\n\nOutput shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if df is a scalar. Otherwise, np.array(df).size samples are drawn.    Returns \n \noutndarray or scalar\n\n\nDrawn samples from the parameterized standard Student\u2019s t distribution.      See also  Generator.standard_t\n\nwhich should be used for new code.    Notes The probability density function for the t distribution is  \\[P(x, df) = \\frac{\\Gamma(\\frac{df+1}{2})}{\\sqrt{\\pi df} \\Gamma(\\frac{df}{2})}\\Bigl( 1+\\frac{x^2}{df} \\Bigr)^{-(df+1)\/2}\\] The t test is based on an assumption that the data come from a Normal distribution. The t test provides a way to test whether the sample mean (that is the mean calculated from the data) is a good estimate of the true mean. The derivation of the t-distribution was first published in 1908 by William Gosset while working for the Guinness Brewery in Dublin. Due to proprietary issues, he had to publish under a pseudonym, and so he used the name Student. References  1 \nDalgaard, Peter, \u201cIntroductory Statistics With R\u201d, Springer, 2002.  2 \nWikipedia, \u201cStudent\u2019s t-distribution\u201d https:\/\/en.wikipedia.org\/wiki\/Student\u2019s_t-distribution   Examples From Dalgaard page 83 [1], suppose the daily energy intake for 11 women in kilojoules (kJ) is: >>> intake = np.array([5260., 5470, 5640, 6180, 6390, 6515, 6805, 7515, \\\n...                    7515, 8230, 8770])\n Does their energy intake deviate systematically from the recommended value of 7725 kJ? Our null hypothesis will be the absence of deviation, and the alternate hypothesis will be the presence of an effect that could be either positive or negative, hence making our test 2-tailed. Because we are estimating the mean and we have N=11 values in our sample, we have N-1=10 degrees of freedom. We set our significance level to 95% and compute the t statistic using the empirical mean and empirical standard deviation of our intake. We use a ddof of 1 to base the computation of our empirical standard deviation on an unbiased estimate of the variance (note: the final estimate is not unbiased due to the concave nature of the square root). >>> np.mean(intake)\n6753.636363636364\n>>> intake.std(ddof=1)\n1142.1232221373727\n>>> t = (np.mean(intake)-7725)\/(intake.std(ddof=1)\/np.sqrt(len(intake)))\n>>> t\n-2.8207540608310198\n We draw 1000000 samples from Student\u2019s t distribution with the adequate degrees of freedom. >>> import matplotlib.pyplot as plt\n>>> s = np.random.standard_t(10, size=1000000)\n>>> h = plt.hist(s, bins=100, density=True)\n Does our t statistic land in one of the two critical regions found at both tails of the distribution? >>> np.sum(np.abs(t) < np.abs(s)) \/ float(len(s))\n0.018318  #random < 0.05, statistic is in critical region\n The probability value for this 2-tailed test is about 1.83%, which is lower than the 5% pre-determined significance threshold. Therefore, the probability of observing values as extreme as our intake conditionally on the null hypothesis being true is too low, and we reject the null hypothesis of no deviation.","title":"numpy.reference.random.generated.numpy.random.randomstate.standard_t"},{"text":"numpy.random.standard_t   random.standard_t(df, size=None)\n \nDraw samples from a standard Student\u2019s t distribution with df degrees of freedom. A special case of the hyperbolic distribution. As df gets large, the result resembles that of the standard normal distribution (standard_normal).  Note New code should use the standard_t method of a default_rng() instance instead; please see the Quick Start.   Parameters \n \ndffloat or array_like of floats\n\n\nDegrees of freedom, must be > 0.  \nsizeint or tuple of ints, optional\n\n\nOutput shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if df is a scalar. Otherwise, np.array(df).size samples are drawn.    Returns \n \noutndarray or scalar\n\n\nDrawn samples from the parameterized standard Student\u2019s t distribution.      See also  Generator.standard_t\n\nwhich should be used for new code.    Notes The probability density function for the t distribution is  \\[P(x, df) = \\frac{\\Gamma(\\frac{df+1}{2})}{\\sqrt{\\pi df} \\Gamma(\\frac{df}{2})}\\Bigl( 1+\\frac{x^2}{df} \\Bigr)^{-(df+1)\/2}\\] The t test is based on an assumption that the data come from a Normal distribution. The t test provides a way to test whether the sample mean (that is the mean calculated from the data) is a good estimate of the true mean. The derivation of the t-distribution was first published in 1908 by William Gosset while working for the Guinness Brewery in Dublin. Due to proprietary issues, he had to publish under a pseudonym, and so he used the name Student. References  1 \nDalgaard, Peter, \u201cIntroductory Statistics With R\u201d, Springer, 2002.  2 \nWikipedia, \u201cStudent\u2019s t-distribution\u201d https:\/\/en.wikipedia.org\/wiki\/Student\u2019s_t-distribution   Examples From Dalgaard page 83 [1], suppose the daily energy intake for 11 women in kilojoules (kJ) is: >>> intake = np.array([5260., 5470, 5640, 6180, 6390, 6515, 6805, 7515, \\\n...                    7515, 8230, 8770])\n Does their energy intake deviate systematically from the recommended value of 7725 kJ? Our null hypothesis will be the absence of deviation, and the alternate hypothesis will be the presence of an effect that could be either positive or negative, hence making our test 2-tailed. Because we are estimating the mean and we have N=11 values in our sample, we have N-1=10 degrees of freedom. We set our significance level to 95% and compute the t statistic using the empirical mean and empirical standard deviation of our intake. We use a ddof of 1 to base the computation of our empirical standard deviation on an unbiased estimate of the variance (note: the final estimate is not unbiased due to the concave nature of the square root). >>> np.mean(intake)\n6753.636363636364\n>>> intake.std(ddof=1)\n1142.1232221373727\n>>> t = (np.mean(intake)-7725)\/(intake.std(ddof=1)\/np.sqrt(len(intake)))\n>>> t\n-2.8207540608310198\n We draw 1000000 samples from Student\u2019s t distribution with the adequate degrees of freedom. >>> import matplotlib.pyplot as plt\n>>> s = np.random.standard_t(10, size=1000000)\n>>> h = plt.hist(s, bins=100, density=True)\n Does our t statistic land in one of the two critical regions found at both tails of the distribution? >>> np.sum(np.abs(t) < np.abs(s)) \/ float(len(s))\n0.018318  #random < 0.05, statistic is in critical region\n The probability value for this 2-tailed test is about 1.83%, which is lower than the 5% pre-determined significance threshold. Therefore, the probability of observing values as extreme as our intake conditionally on the null hypothesis being true is too low, and we reject the null hypothesis of no deviation.","title":"numpy.reference.random.generated.numpy.random.standard_t"},{"text":"tf.summary.text     View source on GitHub    Write a text summary. \ntf.summary.text(\n    name, data, step=None, description=None\n)\n\n \n\n\n Arguments\n  name   A name for this summary. The summary tag used for TensorBoard will be this name prefixed by any active name scopes.  \n  data   A UTF-8 string tensor value.  \n  step   Explicit int64-castable monotonic step value for this summary. If omitted, this defaults to tf.summary.experimental.get_step(), which must not be None.  \n  description   Optional long-form description for this summary, as a constant str. Markdown is supported. Defaults to empty.   \n \n\n\n Returns   True on success, or false if no summary was emitted because no default summary writer was available.  \n\n \n\n\n Raises\n  ValueError   if a default writer exists, but no step was provided and tf.summary.experimental.get_step() is None.","title":"tensorflow.summary.text"},{"text":"numpy.random.Generator.standard_t method   random.Generator.standard_t(df, size=None)\n \nDraw samples from a standard Student\u2019s t distribution with df degrees of freedom. A special case of the hyperbolic distribution. As df gets large, the result resembles that of the standard normal distribution (standard_normal).  Parameters \n \ndffloat or array_like of floats\n\n\nDegrees of freedom, must be > 0.  \nsizeint or tuple of ints, optional\n\n\nOutput shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if df is a scalar. Otherwise, np.array(df).size samples are drawn.    Returns \n \noutndarray or scalar\n\n\nDrawn samples from the parameterized standard Student\u2019s t distribution.     Notes The probability density function for the t distribution is  \\[P(x, df) = \\frac{\\Gamma(\\frac{df+1}{2})}{\\sqrt{\\pi df} \\Gamma(\\frac{df}{2})}\\Bigl( 1+\\frac{x^2}{df} \\Bigr)^{-(df+1)\/2}\\] The t test is based on an assumption that the data come from a Normal distribution. The t test provides a way to test whether the sample mean (that is the mean calculated from the data) is a good estimate of the true mean. The derivation of the t-distribution was first published in 1908 by William Gosset while working for the Guinness Brewery in Dublin. Due to proprietary issues, he had to publish under a pseudonym, and so he used the name Student. References  1 \nDalgaard, Peter, \u201cIntroductory Statistics With R\u201d, Springer, 2002.  2 \nWikipedia, \u201cStudent\u2019s t-distribution\u201d https:\/\/en.wikipedia.org\/wiki\/Student\u2019s_t-distribution   Examples From Dalgaard page 83 [1], suppose the daily energy intake for 11 women in kilojoules (kJ) is: >>> intake = np.array([5260., 5470, 5640, 6180, 6390, 6515, 6805, 7515, \\\n...                    7515, 8230, 8770])\n Does their energy intake deviate systematically from the recommended value of 7725 kJ? Our null hypothesis will be the absence of deviation, and the alternate hypothesis will be the presence of an effect that could be either positive or negative, hence making our test 2-tailed. Because we are estimating the mean and we have N=11 values in our sample, we have N-1=10 degrees of freedom. We set our significance level to 95% and compute the t statistic using the empirical mean and empirical standard deviation of our intake. We use a ddof of 1 to base the computation of our empirical standard deviation on an unbiased estimate of the variance (note: the final estimate is not unbiased due to the concave nature of the square root). >>> np.mean(intake)\n6753.636363636364\n>>> intake.std(ddof=1)\n1142.1232221373727\n>>> t = (np.mean(intake)-7725)\/(intake.std(ddof=1)\/np.sqrt(len(intake)))\n>>> t\n-2.8207540608310198\n We draw 1000000 samples from Student\u2019s t distribution with the adequate degrees of freedom. >>> import matplotlib.pyplot as plt\n>>> s = np.random.default_rng().standard_t(10, size=1000000)\n>>> h = plt.hist(s, bins=100, density=True)\n Does our t statistic land in one of the two critical regions found at both tails of the distribution? >>> np.sum(np.abs(t) < np.abs(s)) \/ float(len(s))\n0.018318  #random < 0.05, statistic is in critical region\n The probability value for this 2-tailed test is about 1.83%, which is lower than the 5% pre-determined significance threshold. Therefore, the probability of observing values as extreme as our intake conditionally on the null hypothesis being true is too low, and we reject the null hypothesis of no deviation.","title":"numpy.reference.random.generated.numpy.random.generator.standard_t"},{"text":"ModelAdmin.search_help_text  \n New in Django 4.0.  Set search_help_text to specify a descriptive text for the search box which will be displayed below it.","title":"django.ref.contrib.admin.index#django.contrib.admin.ModelAdmin.search_help_text"},{"text":"pandas.DatetimeIndex.inferred_freq   DatetimeIndex.inferred_freq\n \nTries to return a string representing a frequency guess, generated by infer_freq. Returns None if it can\u2019t autodetect the frequency.","title":"pandas.reference.api.pandas.datetimeindex.inferred_freq"},{"text":"class torch.distributions.studentT.StudentT(df, loc=0.0, scale=1.0, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a Student\u2019s t-distribution parameterized by degree of freedom df, mean loc and scale scale. Example: >>> m = StudentT(torch.tensor([2.0]))\n>>> m.sample()  # Student's t-distributed with degrees of freedom=2\ntensor([ 0.1046])\n  Parameters \n \ndf (float or Tensor) \u2013 degrees of freedom \nloc (float or Tensor) \u2013 mean of the distribution \nscale (float or Tensor) \u2013 scale of the distribution     \narg_constraints = {'df': GreaterThan(lower_bound=0.0), 'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} \n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = Real() \n  \nproperty variance","title":"torch.distributions#torch.distributions.studentT.StudentT"},{"text":"numpy.random.RandomState.f method   random.RandomState.f(dfnum, dfden, size=None)\n \nDraw samples from an F distribution. Samples are drawn from an F distribution with specified parameters, dfnum (degrees of freedom in numerator) and dfden (degrees of freedom in denominator), where both parameters must be greater than zero. The random variate of the F distribution (also known as the Fisher distribution) is a continuous probability distribution that arises in ANOVA tests, and is the ratio of two chi-square variates.  Note New code should use the f method of a default_rng() instance instead; please see the Quick Start.   Parameters \n \ndfnumfloat or array_like of floats\n\n\nDegrees of freedom in numerator, must be > 0.  \ndfdenfloat or array_like of float\n\n\nDegrees of freedom in denominator, must be > 0.  \nsizeint or tuple of ints, optional\n\n\nOutput shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if dfnum and dfden are both scalars. Otherwise, np.broadcast(dfnum, dfden).size samples are drawn.    Returns \n \noutndarray or scalar\n\n\nDrawn samples from the parameterized Fisher distribution.      See also  scipy.stats.f\n\nprobability density function, distribution or cumulative density function, etc.  Generator.f\n\nwhich should be used for new code.    Notes The F statistic is used to compare in-group variances to between-group variances. Calculating the distribution depends on the sampling, and so it is a function of the respective degrees of freedom in the problem. The variable dfnum is the number of samples minus one, the between-groups degrees of freedom, while dfden is the within-groups degrees of freedom, the sum of the number of samples in each group minus the number of groups. References  1 \nGlantz, Stanton A. \u201cPrimer of Biostatistics.\u201d, McGraw-Hill, Fifth Edition, 2002.  2 \nWikipedia, \u201cF-distribution\u201d, https:\/\/en.wikipedia.org\/wiki\/F-distribution   Examples An example from Glantz[1], pp 47-40: Two groups, children of diabetics (25 people) and children from people without diabetes (25 controls). Fasting blood glucose was measured, case group had a mean value of 86.1, controls had a mean value of 82.2. Standard deviations were 2.09 and 2.49 respectively. Are these data consistent with the null hypothesis that the parents diabetic status does not affect their children\u2019s blood glucose levels? Calculating the F statistic from the data gives a value of 36.01. Draw samples from the distribution: >>> dfnum = 1. # between group degrees of freedom\n>>> dfden = 48. # within groups degrees of freedom\n>>> s = np.random.f(dfnum, dfden, 1000)\n The lower bound for the top 1% of the samples is : >>> np.sort(s)[-10]\n7.61988120985 # random\n So there is about a 1% chance that the F statistic will exceed 7.62, the measured value is 36, so the null hypothesis is rejected at the 1% level.","title":"numpy.reference.random.generated.numpy.random.randomstate.f"},{"text":"pandas.infer_freq   pandas.infer_freq(index, warn=True)[source]\n \nInfer the most likely frequency given the input index. If the frequency is uncertain, a warning will be printed.  Parameters \n \nindex:DatetimeIndex or TimedeltaIndex\n\n\nIf passed a Series will use the values of the series (NOT THE INDEX).  \nwarn:bool, default True\n\n  Returns \n str or None\n\nNone if no discernible frequency.    Raises \n TypeError\n\nIf the index is not datetime-like.  ValueError\n\nIf there are fewer than three values.     Examples \n>>> idx = pd.date_range(start='2020\/12\/01', end='2020\/12\/30', periods=30)\n>>> pd.infer_freq(idx)\n'D'","title":"pandas.reference.api.pandas.infer_freq"}]}
{"task_id":7378180,"prompt":"def f_7378180():\n\treturn ","suffix":"","canonical_solution":"list(itertools.combinations((1, 2, 3), 2))","test_start":"\nimport itertools\n\ndef check(candidate):","test":["\n    assert candidate() == [(1, 2), (1, 3), (2, 3)]\n"],"entry_point":"f_7378180","intent":"generate all 2-element subsets of tuple `(1, 2, 3)`","library":["itertools"],"docs":[{"text":"torch.combinations(input, r=2, with_replacement=False) \u2192 seq  \nCompute combinations of length rr  of the given tensor. The behavior is similar to python\u2019s itertools.combinations when with_replacement is set to False, and itertools.combinations_with_replacement when with_replacement is set to True.  Parameters \n \ninput (Tensor) \u2013 1D vector. \nr (int, optional) \u2013 number of elements to combine \nwith_replacement (boolean, optional) \u2013 whether to allow duplication in combination   Returns \nA tensor equivalent to converting all the input tensors into lists, do itertools.combinations or itertools.combinations_with_replacement on these lists, and finally convert the resulting list into tensor.  Return type \nTensor   Example: >>> a = [1, 2, 3]\n>>> list(itertools.combinations(a, r=2))\n[(1, 2), (1, 3), (2, 3)]\n>>> list(itertools.combinations(a, r=3))\n[(1, 2, 3)]\n>>> list(itertools.combinations_with_replacement(a, r=2))\n[(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)]\n>>> tensor_a = torch.tensor(a)\n>>> torch.combinations(tensor_a)\ntensor([[1, 2],\n        [1, 3],\n        [2, 3]])\n>>> torch.combinations(tensor_a, r=3)\ntensor([[1, 2, 3]])\n>>> torch.combinations(tensor_a, with_replacement=True)\ntensor([[1, 1],\n        [1, 2],\n        [1, 3],\n        [2, 2],\n        [2, 3],\n        [3, 3]])","title":"torch.generated.torch.combinations#torch.combinations"},{"text":"itertools.combinations(iterable, r)  \nReturn r length subsequences of elements from the input iterable. The combination tuples are emitted in lexicographic ordering according to the order of the input iterable. So, if the input iterable is sorted, the combination tuples will be produced in sorted order. Elements are treated as unique based on their position, not on their value. So if the input elements are unique, there will be no repeat values in each combination. Roughly equivalent to: def combinations(iterable, r):\n    # combinations('ABCD', 2) --> AB AC AD BC BD CD\n    # combinations(range(4), 3) --> 012 013 023 123\n    pool = tuple(iterable)\n    n = len(pool)\n    if r > n:\n        return\n    indices = list(range(r))\n    yield tuple(pool[i] for i in indices)\n    while True:\n        for i in reversed(range(r)):\n            if indices[i] != i + n - r:\n                break\n        else:\n            return\n        indices[i] += 1\n        for j in range(i+1, r):\n            indices[j] = indices[j-1] + 1\n        yield tuple(pool[i] for i in indices)\n The code for combinations() can be also expressed as a subsequence of permutations() after filtering entries where the elements are not in sorted order (according to their position in the input pool): def combinations(iterable, r):\n    pool = tuple(iterable)\n    n = len(pool)\n    for indices in permutations(range(n), r):\n        if sorted(indices) == list(indices):\n            yield tuple(pool[i] for i in indices)\n The number of items returned is n! \/ r! \/ (n-r)! when 0 <= r <= n or zero when r > n.","title":"python.library.itertools#itertools.combinations"},{"text":"itertools.permutations(iterable, r=None)  \nReturn successive r length permutations of elements in the iterable. If r is not specified or is None, then r defaults to the length of the iterable and all possible full-length permutations are generated. The permutation tuples are emitted in lexicographic ordering according to the order of the input iterable. So, if the input iterable is sorted, the combination tuples will be produced in sorted order. Elements are treated as unique based on their position, not on their value. So if the input elements are unique, there will be no repeat values in each permutation. Roughly equivalent to: def permutations(iterable, r=None):\n    # permutations('ABCD', 2) --> AB AC AD BA BC BD CA CB CD DA DB DC\n    # permutations(range(3)) --> 012 021 102 120 201 210\n    pool = tuple(iterable)\n    n = len(pool)\n    r = n if r is None else r\n    if r > n:\n        return\n    indices = list(range(n))\n    cycles = list(range(n, n-r, -1))\n    yield tuple(pool[i] for i in indices[:r])\n    while n:\n        for i in reversed(range(r)):\n            cycles[i] -= 1\n            if cycles[i] == 0:\n                indices[i:] = indices[i+1:] + indices[i:i+1]\n                cycles[i] = n - i\n            else:\n                j = cycles[i]\n                indices[i], indices[-j] = indices[-j], indices[i]\n                yield tuple(pool[i] for i in indices[:r])\n                break\n        else:\n            return\n The code for permutations() can be also expressed as a subsequence of product(), filtered to exclude entries with repeated elements (those from the same position in the input pool): def permutations(iterable, r=None):\n    pool = tuple(iterable)\n    n = len(pool)\n    r = n if r is None else r\n    for indices in product(range(n), repeat=r):\n        if len(set(indices)) == r:\n            yield tuple(pool[i] for i in indices)\n The number of items returned is n! \/ (n-r)! when 0 <= r <= n or zero when r > n.","title":"python.library.itertools#itertools.permutations"},{"text":"itertools.combinations_with_replacement(iterable, r)  \nReturn r length subsequences of elements from the input iterable allowing individual elements to be repeated more than once. The combination tuples are emitted in lexicographic ordering according to the order of the input iterable. So, if the input iterable is sorted, the combination tuples will be produced in sorted order. Elements are treated as unique based on their position, not on their value. So if the input elements are unique, the generated combinations will also be unique. Roughly equivalent to: def combinations_with_replacement(iterable, r):\n    # combinations_with_replacement('ABC', 2) --> AA AB AC BB BC CC\n    pool = tuple(iterable)\n    n = len(pool)\n    if not n and r:\n        return\n    indices = [0] * r\n    yield tuple(pool[i] for i in indices)\n    while True:\n        for i in reversed(range(r)):\n            if indices[i] != n - 1:\n                break\n        else:\n            return\n        indices[i:] = [indices[i] + 1] * (r - i)\n        yield tuple(pool[i] for i in indices)\n The code for combinations_with_replacement() can be also expressed as a subsequence of product() after filtering entries where the elements are not in sorted order (according to their position in the input pool): def combinations_with_replacement(iterable, r):\n    pool = tuple(iterable)\n    n = len(pool)\n    for indices in product(range(n), repeat=r):\n        if sorted(indices) == list(indices):\n            yield tuple(pool[i] for i in indices)\n The number of items returned is (n+r-1)! \/ r! \/ (n-1)! when n > 0.  New in version 3.1.","title":"python.library.itertools#itertools.combinations_with_replacement"},{"text":"torch.utils.data.random_split(dataset, lengths, generator=<torch._C.Generator object>) [source]\n \nRandomly split a dataset into non-overlapping new datasets of given lengths. Optionally fix the generator for reproducible results, e.g.: >>> random_split(range(10), [3, 7], generator=torch.Generator().manual_seed(42))\n  Parameters \n \ndataset (Dataset) \u2013 Dataset to be split \nlengths (sequence) \u2013 lengths of splits to be produced \ngenerator (Generator) \u2013 Generator used for the random permutation.","title":"torch.data#torch.utils.data.random_split"},{"text":"itertools.product(*iterables, repeat=1)  \nCartesian product of input iterables. Roughly equivalent to nested for-loops in a generator expression. For example, product(A, B) returns the same as ((x,y) for x in A for y in B). The nested loops cycle like an odometer with the rightmost element advancing on every iteration. This pattern creates a lexicographic ordering so that if the input\u2019s iterables are sorted, the product tuples are emitted in sorted order. To compute the product of an iterable with itself, specify the number of repetitions with the optional repeat keyword argument. For example, product(A, repeat=4) means the same as product(A, A, A, A). This function is roughly equivalent to the following code, except that the actual implementation does not build up intermediate results in memory: def product(*args, repeat=1):\n    # product('ABCD', 'xy') --> Ax Ay Bx By Cx Cy Dx Dy\n    # product(range(2), repeat=3) --> 000 001 010 011 100 101 110 111\n    pools = [tuple(pool) for pool in args] * repeat\n    result = [[]]\n    for pool in pools:\n        result = [x+[y] for x in result for y in pool]\n    for prod in result:\n        yield tuple(prod)\n Before product() runs, it completely consumes the input iterables, keeping pools of values in memory to generate the products. Accordingly, it is only useful with finite inputs.","title":"python.library.itertools#itertools.product"},{"text":"ModelAdmin.get_fieldsets(request, obj=None)  \nThe get_fieldsets method is given the HttpRequest and the obj being edited (or None on an add form) and is expected to return a list of two-tuples, in which each two-tuple represents a <fieldset> on the admin form page, as described above in the ModelAdmin.fieldsets section.","title":"django.ref.contrib.admin.index#django.contrib.admin.ModelAdmin.get_fieldsets"},{"text":"zip(*iterables)  \nMake an iterator that aggregates elements from each of the iterables. Returns an iterator of tuples, where the i-th tuple contains the i-th element from each of the argument sequences or iterables. The iterator stops when the shortest input iterable is exhausted. With a single iterable argument, it returns an iterator of 1-tuples. With no arguments, it returns an empty iterator. Equivalent to: def zip(*iterables):\n    # zip('ABCD', 'xy') --> Ax By\n    sentinel = object()\n    iterators = [iter(it) for it in iterables]\n    while iterators:\n        result = []\n        for it in iterators:\n            elem = next(it, sentinel)\n            if elem is sentinel:\n                return\n            result.append(elem)\n        yield tuple(result)\n The left-to-right evaluation order of the iterables is guaranteed. This makes possible an idiom for clustering a data series into n-length groups using zip(*[iter(s)]*n). This repeats the same iterator n times so that each output tuple has the result of n calls to the iterator. This has the effect of dividing the input into n-length chunks. zip() should only be used with unequal length inputs when you don\u2019t care about trailing, unmatched values from the longer iterables. If those values are important, use itertools.zip_longest() instead. zip() in conjunction with the * operator can be used to unzip a list: >>> x = [1, 2, 3]\n>>> y = [4, 5, 6]\n>>> zipped = zip(x, y)\n>>> list(zipped)\n[(1, 4), (2, 5), (3, 6)]\n>>> x2, y2 = zip(*zip(x, y))\n>>> x == list(x2) and y == list(y2)\nTrue","title":"python.library.functions#zip"},{"text":"tf.raw_ops.GeneratorDataset Creates a dataset that invokes a function to generate elements.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.raw_ops.GeneratorDataset  \ntf.raw_ops.GeneratorDataset(\n    init_func_other_args, next_func_other_args, finalize_func_other_args, init_func,\n    next_func, finalize_func, output_types, output_shapes, name=None\n)\n\n \n\n\n Args\n  init_func_other_args   A list of Tensor objects.  \n  next_func_other_args   A list of Tensor objects.  \n  finalize_func_other_args   A list of Tensor objects.  \n  init_func   A function decorated with @Defun.  \n  next_func   A function decorated with @Defun.  \n  finalize_func   A function decorated with @Defun.  \n  output_types   A list of tf.DTypes that has length >= 1.  \n  output_shapes   A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor of type variant.","title":"tensorflow.raw_ops.generatordataset"},{"text":"get_matching_blocks()  \nReturn list of triples describing non-overlapping matching subsequences. Each triple is of the form (i, j, n), and means that a[i:i+n] == b[j:j+n]. The triples are monotonically increasing in i and j. The last triple is a dummy, and has the value (len(a), len(b), 0). It is the only triple with n == 0. If (i, j, n) and (i', j', n') are adjacent triples in the list, and the second is not the last triple in the list, then i+n < i' or j+n < j'; in other words, adjacent triples always describe non-adjacent equal blocks. >>> s = SequenceMatcher(None, \"abxcd\", \"abcd\")\n>>> s.get_matching_blocks()\n[Match(a=0, b=0, size=2), Match(a=3, b=2, size=2), Match(a=5, b=4, size=0)]","title":"python.library.difflib#difflib.SequenceMatcher.get_matching_blocks"}]}
{"task_id":4530069,"prompt":"def f_4530069():\n\treturn ","suffix":"","canonical_solution":"datetime.now(pytz.utc)","test_start":"\nimport pytz\nimport time\nfrom datetime import datetime, timezone\n\ndef check(candidate):","test":["\n    assert (candidate() - datetime(1970, 1, 1).replace(tzinfo=timezone.utc)).total_seconds() - time.time() <= 1\n"],"entry_point":"f_4530069","intent":"get a value of datetime.today() in the UTC time zone","library":["datetime","pytz","time"],"docs":[{"text":"classmethod datetime.today()  \nReturn the current local datetime, with tzinfo None. Equivalent to: datetime.fromtimestamp(time.time())\n See also now(), fromtimestamp(). This method is functionally equivalent to now(), but without a tz parameter.","title":"python.library.datetime#datetime.datetime.today"},{"text":"classmethod datetime.utcnow()  \nReturn the current UTC date and time, with tzinfo None. This is like now(), but returns the current UTC date and time, as a naive datetime object. An aware current UTC datetime can be obtained by calling datetime.now(timezone.utc). See also now().  Warning Because naive datetime objects are treated by many datetime methods as local times, it is preferred to use aware datetimes to represent times in UTC. As such, the recommended way to create an object representing the current time in UTC is by calling datetime.now(timezone.utc).","title":"python.library.datetime#datetime.datetime.utcnow"},{"text":"pandas.Timestamp.today   classmethodTimestamp.today(cls, tz=None)\n \nReturn the current time in the local timezone. This differs from datetime.today() in that it can be localized to a passed timezone.  Parameters \n \ntz:str or timezone object, default None\n\n\nTimezone to localize to.     Examples \n>>> pd.Timestamp.today()    \nTimestamp('2020-11-16 22:37:39.969883')\n  Analogous for pd.NaT: \n>>> pd.NaT.today()\nNaT","title":"pandas.reference.api.pandas.timestamp.today"},{"text":"classmethod date.today()  \nReturn the current local date. This is equivalent to date.fromtimestamp(time.time()).","title":"python.library.datetime#datetime.date.today"},{"text":"classmethod datetime.now(tz=None)  \nReturn the current local date and time. If optional argument tz is None or not specified, this is like today(), but, if possible, supplies more precision than can be gotten from going through a time.time() timestamp (for example, this may be possible on platforms supplying the C gettimeofday() function). If tz is not None, it must be an instance of a tzinfo subclass, and the current date and time are converted to tz\u2019s time zone. This function is preferred over today() and utcnow().","title":"python.library.datetime#datetime.datetime.now"},{"text":"pandas.Timestamp.utcnow   classmethodTimestamp.utcnow()\n \nReturn a new Timestamp representing UTC day and time. Examples \n>>> pd.Timestamp.utcnow()   \nTimestamp('2020-11-16 22:50:18.092888+0000', tz='UTC')","title":"pandas.reference.api.pandas.timestamp.utcnow"},{"text":"timezone.utc  \nThe UTC timezone, timezone(timedelta(0)).","title":"python.library.datetime#datetime.timezone.utc"},{"text":"utc  \ntzinfo instance that represents UTC.","title":"django.ref.utils#django.utils.timezone.utc"},{"text":"pandas.Timestamp.now   classmethodTimestamp.now(tz=None)\n \nReturn new Timestamp object representing current time local to tz.  Parameters \n \ntz:str or timezone object, default None\n\n\nTimezone to localize to.     Examples \n>>> pd.Timestamp.now()  \nTimestamp('2020-11-16 22:06:16.378782')\n  Analogous for pd.NaT: \n>>> pd.NaT.now()\nNaT","title":"pandas.reference.api.pandas.timestamp.now"},{"text":"now()  \nReturns a datetime that represents the current point in time. Exactly what\u2019s returned depends on the value of USE_TZ:  If USE_TZ is False, this will be a naive datetime (i.e. a datetime without an associated timezone) that represents the current time in the system\u2019s local timezone. If USE_TZ is True, this will be an aware datetime representing the current time in UTC. Note that now() will always return times in UTC regardless of the value of TIME_ZONE; you can use localtime() to get the time in the current time zone.","title":"django.ref.utils#django.utils.timezone.now"}]}
{"task_id":4842956,"prompt":"def f_4842956(list1):\n\t","suffix":"\n\treturn list2","canonical_solution":"list2 = [x for x in list1 if x != []]","test_start":"\ndef check(candidate):","test":["\n    assert candidate([[\"a\"], [], [\"b\"]]) == [[\"a\"], [\"b\"]]\n","\n    assert candidate([[], [1,2,3], [], [\"b\"]]) == [[1,2,3], [\"b\"]]\n"],"entry_point":"f_4842956","intent":"Get a new list `list2`by removing empty list from a list of lists `list1`","library":[],"docs":[]}
{"task_id":4842956,"prompt":"def f_4842956(list1):\n\t","suffix":"\n\treturn list2","canonical_solution":"list2 = [x for x in list1 if x]","test_start":"\ndef check(candidate):","test":["\n    assert candidate([[\"a\"], [], [\"b\"]]) == [[\"a\"], [\"b\"]]\n","\n    assert candidate([[], [1,2,3], [], [\"b\"]]) == [[1,2,3], [\"b\"]]\n"],"entry_point":"f_4842956","intent":"Create `list2` to contain the lists from list `list1` excluding the empty lists from `list1`","library":[],"docs":[]}
{"task_id":9262278,"prompt":"def f_9262278(data):\n\treturn ","suffix":"","canonical_solution":"HttpResponse(data, content_type='application\/json')","test_start":"\nimport os\nimport json\nfrom django.http import HttpResponse\nfrom django.conf import settings\nif not settings.configured:\n    settings.configure(DEBUG=True)\n\ndef check(candidate):","test":["\n    if settings.DEBUG:\n        assert candidate(json.dumps({\"Sample-Key\": \"Sample-Value\"})).content == b'{\"Sample-Key\": \"Sample-Value\"}'\n","\n    if settings.DEBUG:\n        assert candidate(json.dumps({\"Sample-Key\": \"Sample-Value\"}))['content-type'] == 'application\/json'\n"],"entry_point":"f_9262278","intent":"Django response with JSON `data`","library":["django","json","os"],"docs":[{"text":"class JsonResponse(data, encoder=DjangoJSONEncoder, safe=True, json_dumps_params=None, **kwargs)  \nAn HttpResponse subclass that helps to create a JSON-encoded response. It inherits most behavior from its superclass with a couple differences: Its default Content-Type header is set to application\/json. The first parameter, data, should be a dict instance. If the safe parameter is set to False (see below) it can be any JSON-serializable object. The encoder, which defaults to django.core.serializers.json.DjangoJSONEncoder, will be used to serialize the data. See JSON serialization for more details about this serializer. The safe boolean parameter defaults to True. If it\u2019s set to False, any object can be passed for serialization (otherwise only dict instances are allowed). If safe is True and a non-dict object is passed as the first argument, a TypeError will be raised. The json_dumps_params parameter is a dictionary of keyword arguments to pass to the json.dumps() call used to generate the response.","title":"django.ref.request-response#django.http.JsonResponse"},{"text":"Responses  Unlike basic HttpResponse objects, TemplateResponse objects retain the details of the context that was provided by the view to compute the response. The final output of the response is not computed until it is needed, later in the response process. \u2014 Django documentation  REST framework supports HTTP content negotiation by providing a Response class which allows you to return content that can be rendered into multiple content types, depending on the client request. The Response class subclasses Django's SimpleTemplateResponse. Response objects are initialised with data, which should consist of native Python primitives. REST framework then uses standard HTTP content negotiation to determine how it should render the final response content. There's no requirement for you to use the Response class, you can also return regular HttpResponse or StreamingHttpResponse objects from your views if required. Using the Response class simply provides a nicer interface for returning content-negotiated Web API responses, that can be rendered to multiple formats. Unless you want to heavily customize REST framework for some reason, you should always use an APIView class or @api_view function for views that return Response objects. Doing so ensures that the view can perform content negotiation and select the appropriate renderer for the response, before it is returned from the view. Creating responses Response() Signature: Response(data, status=None, template_name=None, headers=None, content_type=None) Unlike regular HttpResponse objects, you do not instantiate Response objects with rendered content. Instead you pass in unrendered data, which may consist of any Python primitives. The renderers used by the Response class cannot natively handle complex datatypes such as Django model instances, so you need to serialize the data into primitive datatypes before creating the Response object. You can use REST framework's Serializer classes to perform this data serialization, or use your own custom serialization. Arguments:  \ndata: The serialized data for the response. \nstatus: A status code for the response. Defaults to 200. See also status codes. \ntemplate_name: A template name to use if HTMLRenderer is selected. \nheaders: A dictionary of HTTP headers to use in the response. \ncontent_type: The content type of the response. Typically, this will be set automatically by the renderer as determined by content negotiation, but there may be some cases where you need to specify the content type explicitly.  Attributes .data The unrendered, serialized data of the response. .status_code The numeric status code of the HTTP response. .content The rendered content of the response. The .render() method must have been called before .content can be accessed. .template_name The template_name, if supplied. Only required if HTMLRenderer or some other custom template renderer is the accepted renderer for the response. .accepted_renderer The renderer instance that will be used to render the response. Set automatically by the APIView or @api_view immediately before the response is returned from the view. .accepted_media_type The media type that was selected by the content negotiation stage. Set automatically by the APIView or @api_view immediately before the response is returned from the view. .renderer_context A dictionary of additional context information that will be passed to the renderer's .render() method. Set automatically by the APIView or @api_view immediately before the response is returned from the view. Standard HttpResponse attributes The Response class extends SimpleTemplateResponse, and all the usual attributes and methods are also available on the response. For example you can set headers on the response in the standard way: response = Response()\nresponse['Cache-Control'] = 'no-cache'\n .render() Signature: .render() As with any other TemplateResponse, this method is called to render the serialized data of the response into the final response content. When .render() is called, the response content will be set to the result of calling the .render(data, accepted_media_type, renderer_context) method on the accepted_renderer instance. You won't typically need to call .render() yourself, as it's handled by Django's standard response cycle. response.py","title":"django_rest_framework.api-guide.responses.index"},{"text":"flask.json.jsonify(*args, **kwargs)  \nSerialize data to JSON and wrap it in a Response with the application\/json mimetype. Uses dumps() to serialize the data, but args and kwargs are treated as data rather than arguments to json.dumps().  Single argument: Treated as a single value. Multiple arguments: Treated as a list of values. jsonify(1, 2, 3) is the same as jsonify([1, 2, 3]). Keyword arguments: Treated as a dict of values. jsonify(data=data, errors=errors) is the same as jsonify({\"data\": data, \"errors\": errors}). Passing both arguments and keyword arguments is not allowed as it\u2019s not clear what should happen.  from flask import jsonify\n\n@app.route(\"\/users\/me\")\ndef get_current_user():\n    return jsonify(\n        username=g.user.username,\n        email=g.user.email,\n        id=g.user.id,\n    )\n Will return a JSON response like this: {\n  \"username\": \"admin\",\n  \"email\": \"admin@localhost\",\n  \"id\": 42\n}\n The default output omits indents and spaces after separators. In debug mode or if JSONIFY_PRETTYPRINT_REGULAR is True, the output will be formatted to be easier to read.  Changelog Changed in version 0.11: Added support for serializing top-level arrays. This introduces a security risk in ancient browsers. See JSON Security.   New in version 0.2.   Parameters \n \nargs (Any) \u2013  \nkwargs (Any) \u2013    Return type \nResponse","title":"flask.api.index#flask.json.jsonify"},{"text":"set_data(value)  \nSets a new string as response. The value must be a string or bytes. If a string is set it\u2019s encoded to the charset of the response (utf-8 by default).  Changelog New in version 0.9.   Parameters \nvalue (Union[bytes, str]) \u2013   Return type \nNone","title":"flask.api.index#flask.Response.set_data"},{"text":"set_data(value)  \nSets a new string as response. The value must be a string or bytes. If a string is set it\u2019s encoded to the charset of the response (utf-8 by default).  Changelog New in version 0.9.   Parameters \nvalue (Union[bytes, str]) \u2013   Return type \nNone","title":"werkzeug.wrappers.index#werkzeug.wrappers.Response.set_data"},{"text":"parse_duration(value)  \nParses a string and returns a datetime.timedelta. Expects data in the format \"DD HH:MM:SS.uuuuuu\", \"DD HH:MM:SS,uuuuuu\", or as specified by ISO 8601 (e.g. P4DT1H15M20S which is equivalent to 4 1:15:20) or PostgreSQL\u2019s day-time interval format (e.g. 3 days 04:05:06).","title":"django.ref.utils#django.utils.dateparse.parse_duration"},{"text":"sklearn.datasets.get_data_home(data_home=None) \u2192 str[source]\n \nReturn the path of the scikit-learn data dir. This folder is used by some large dataset loaders to avoid downloading the data several times. By default the data dir is set to a folder named \u2018scikit_learn_data\u2019 in the user home folder. Alternatively, it can be set by the \u2018SCIKIT_LEARN_DATA\u2019 environment variable or programmatically by giving an explicit folder path. The \u2018~\u2019 symbol is expanded to the user home folder. If the folder does not already exist, it is automatically created.  Parameters \n \ndata_homestr, default=None \n\nThe path to scikit-learn data directory. If None, the default path is ~\/sklearn_learn_data.","title":"sklearn.modules.generated.sklearn.datasets.get_data_home#sklearn.datasets.get_data_home"},{"text":"create_superuser(username, email=None, password=None, **extra_fields)  \nSame as create_user(), but sets is_staff and is_superuser to True.","title":"django.ref.contrib.auth#django.contrib.auth.models.UserManager.create_superuser"},{"text":"send_mail(subject, message, from_email, recipient_list, fail_silently=False, auth_user=None, auth_password=None, connection=None, html_message=None)","title":"django.topics.email#django.core.mail.send_mail"},{"text":"pandas.get_dummies   pandas.get_dummies(data, prefix=None, prefix_sep='_', dummy_na=False, columns=None, sparse=False, drop_first=False, dtype=None)[source]\n \nConvert categorical variable into dummy\/indicator variables.  Parameters \n \ndata:array-like, Series, or DataFrame\n\n\nData of which to get dummy indicators.  \nprefix:str, list of str, or dict of str, default None\n\n\nString to append DataFrame column names. Pass a list with length equal to the number of columns when calling get_dummies on a DataFrame. Alternatively, prefix can be a dictionary mapping column names to prefixes.  \nprefix_sep:str, default \u2018_\u2019\n\n\nIf appending prefix, separator\/delimiter to use. Or pass a list or dictionary as with prefix.  \ndummy_na:bool, default False\n\n\nAdd a column to indicate NaNs, if False NaNs are ignored.  \ncolumns:list-like, default None\n\n\nColumn names in the DataFrame to be encoded. If columns is None then all the columns with object or category dtype will be converted.  \nsparse:bool, default False\n\n\nWhether the dummy-encoded columns should be backed by a SparseArray (True) or a regular NumPy array (False).  \ndrop_first:bool, default False\n\n\nWhether to get k-1 dummies out of k categorical levels by removing the first level.  \ndtype:dtype, default np.uint8\n\n\nData type for new columns. Only a single dtype is allowed.    Returns \n DataFrame\n\nDummy-coded data.      See also  Series.str.get_dummies\n\nConvert Series to dummy codes.    Examples \n>>> s = pd.Series(list('abca'))\n  \n>>> pd.get_dummies(s)\n   a  b  c\n0  1  0  0\n1  0  1  0\n2  0  0  1\n3  1  0  0\n  \n>>> s1 = ['a', 'b', np.nan]\n  \n>>> pd.get_dummies(s1)\n   a  b\n0  1  0\n1  0  1\n2  0  0\n  \n>>> pd.get_dummies(s1, dummy_na=True)\n   a  b  NaN\n0  1  0    0\n1  0  1    0\n2  0  0    1\n  \n>>> df = pd.DataFrame({'A': ['a', 'b', 'a'], 'B': ['b', 'a', 'c'],\n...                    'C': [1, 2, 3]})\n  \n>>> pd.get_dummies(df, prefix=['col1', 'col2'])\n   C  col1_a  col1_b  col2_a  col2_b  col2_c\n0  1       1       0       0       1       0\n1  2       0       1       1       0       0\n2  3       1       0       0       0       1\n  \n>>> pd.get_dummies(pd.Series(list('abcaa')))\n   a  b  c\n0  1  0  0\n1  0  1  0\n2  0  0  1\n3  1  0  0\n4  1  0  0\n  \n>>> pd.get_dummies(pd.Series(list('abcaa')), drop_first=True)\n   b  c\n0  0  0\n1  1  0\n2  0  1\n3  0  0\n4  0  0\n  \n>>> pd.get_dummies(pd.Series(list('abc')), dtype=float)\n     a    b    c\n0  1.0  0.0  0.0\n1  0.0  1.0  0.0\n2  0.0  0.0  1.0","title":"pandas.reference.api.pandas.get_dummies"}]}
{"task_id":17284947,"prompt":"def f_17284947(example_str):\n\treturn ","suffix":"","canonical_solution":"re.findall('(.*?)\\\\[.*?\\\\]', example_str)","test_start":"\nimport re \n\ndef check(candidate):  ","test":["\n    list_elems = candidate(\"Josie Smith [3996 COLLEGE AVENUE, SOMETOWN, MD 21003]Mugsy Dog Smith [2560 OAK ST, GLENMEADE, WI 14098]\")\n    assert \"\".join(list_elems).strip() == 'Josie Smith Mugsy Dog Smith'\n"],"entry_point":"f_17284947","intent":"get all text that is not enclosed within square brackets in string `example_str`","library":["re"],"docs":[{"text":"parse(string, name='<string>')  \nDivide the given string into examples and intervening text, and return them as a list of alternating Examples and strings. Line numbers for the Examples are 0-based. The optional argument name is a name identifying this string, and is only used for error messages.","title":"python.library.doctest#doctest.DocTestParser.parse"},{"text":"get_examples(string, name='<string>')  \nExtract all doctest examples from the given string, and return them as a list of Example objects. Line numbers are 0-based. The optional argument name is a name identifying this string, and is only used for error messages.","title":"python.library.doctest#doctest.DocTestParser.get_examples"},{"text":"email.utils.unquote(str)  \nReturn a new string which is an unquoted version of str. If str ends and begins with double quotes, they are stripped off. Likewise if str ends and begins with angle brackets, they are stripped off.","title":"python.library.email.utils#email.utils.unquote"},{"text":"get_doctest(string, globs, name, filename, lineno)  \nExtract all doctest examples from the given string, and collect them into a DocTest object. globs, name, filename, and lineno are attributes for the new DocTest object. See the documentation for DocTest for more information.","title":"python.library.doctest#doctest.DocTestParser.get_doctest"},{"text":"DocTestFailure.example  \nThe Example that failed.","title":"python.library.doctest#doctest.DocTestFailure.example"},{"text":"class doctest.DocTestParser  \nA processing class used to extract interactive examples from a string, and use them to create a DocTest object. DocTestParser defines the following methods:  \nget_doctest(string, globs, name, filename, lineno)  \nExtract all doctest examples from the given string, and collect them into a DocTest object. globs, name, filename, and lineno are attributes for the new DocTest object. See the documentation for DocTest for more information. \n  \nget_examples(string, name='<string>')  \nExtract all doctest examples from the given string, and return them as a list of Example objects. Line numbers are 0-based. The optional argument name is a name identifying this string, and is only used for error messages. \n  \nparse(string, name='<string>')  \nDivide the given string into examples and intervening text, and return them as a list of alternating Examples and strings. Line numbers for the Examples are 0-based. The optional argument name is a name identifying this string, and is only used for error messages.","title":"python.library.doctest#doctest.DocTestParser"},{"text":"report_success(out, test, example, got)  \nReport that the given example ran successfully. This method is provided to allow subclasses of DocTestRunner to customize their output; it should not be called directly. example is the example about to be processed. got is the actual output from the example. test is the test containing example. out is the output function that was passed to DocTestRunner.run().","title":"python.library.doctest#doctest.DocTestRunner.report_success"},{"text":"doctest.script_from_examples(s)  \nConvert text with examples to a script. Argument s is a string containing doctest examples. The string is converted to a Python script, where doctest examples in s are converted to regular code, and everything else is converted to Python comments. The generated script is returned as a string. For example, import doctest\nprint(doctest.script_from_examples(r\"\"\"\n    Set x and y to 1 and 2.\n    >>> x, y = 1, 2\n\n    Print their sum:\n    >>> print(x+y)\n    3\n\"\"\"))\n displays: # Set x and y to 1 and 2.\nx, y = 1, 2\n#\n# Print their sum:\nprint(x+y)\n# Expected:\n## 3\n This function is used internally by other functions (see below), but can also be useful when you want to transform an interactive Python session into a Python script.","title":"python.library.doctest#doctest.script_from_examples"},{"text":"report_failure(out, test, example, got)  \nReport that the given example failed. This method is provided to allow subclasses of DocTestRunner to customize their output; it should not be called directly. example is the example about to be processed. got is the actual output from the example. test is the test containing example. out is the output function that was passed to DocTestRunner.run().","title":"python.library.doctest#doctest.DocTestRunner.report_failure"},{"text":"UnexpectedException.example  \nThe Example that failed.","title":"python.library.doctest#doctest.UnexpectedException.example"}]}
{"task_id":17284947,"prompt":"def f_17284947(example_str):\n\treturn ","suffix":"","canonical_solution":"re.findall('(.*?)(?:\\\\[.*?\\\\]|$)', example_str)","test_start":"\nimport re \n\ndef check(candidate): ","test":["\n    list_elems = candidate(\"Josie Smith [3996 COLLEGE AVENUE, SOMETOWN, MD 21003]Mugsy Dog Smith [2560 OAK ST, GLENMEADE, WI 14098]\")\n    assert \"\".join(list_elems).strip() == 'Josie Smith Mugsy Dog Smith'\n"],"entry_point":"f_17284947","intent":"Use a regex to get all text in a string `example_str` that is not surrounded by square brackets","library":["re"],"docs":[{"text":"parse(string, name='<string>')  \nDivide the given string into examples and intervening text, and return them as a list of alternating Examples and strings. Line numbers for the Examples are 0-based. The optional argument name is a name identifying this string, and is only used for error messages.","title":"python.library.doctest#doctest.DocTestParser.parse"},{"text":"get_examples(string, name='<string>')  \nExtract all doctest examples from the given string, and return them as a list of Example objects. Line numbers are 0-based. The optional argument name is a name identifying this string, and is only used for error messages.","title":"python.library.doctest#doctest.DocTestParser.get_examples"},{"text":"email.utils.unquote(str)  \nReturn a new string which is an unquoted version of str. If str ends and begins with double quotes, they are stripped off. Likewise if str ends and begins with angle brackets, they are stripped off.","title":"python.library.email.utils#email.utils.unquote"},{"text":"get_doctest(string, globs, name, filename, lineno)  \nExtract all doctest examples from the given string, and collect them into a DocTest object. globs, name, filename, and lineno are attributes for the new DocTest object. See the documentation for DocTest for more information.","title":"python.library.doctest#doctest.DocTestParser.get_doctest"},{"text":"DocTestFailure.example  \nThe Example that failed.","title":"python.library.doctest#doctest.DocTestFailure.example"},{"text":"class doctest.DocTestParser  \nA processing class used to extract interactive examples from a string, and use them to create a DocTest object. DocTestParser defines the following methods:  \nget_doctest(string, globs, name, filename, lineno)  \nExtract all doctest examples from the given string, and collect them into a DocTest object. globs, name, filename, and lineno are attributes for the new DocTest object. See the documentation for DocTest for more information. \n  \nget_examples(string, name='<string>')  \nExtract all doctest examples from the given string, and return them as a list of Example objects. Line numbers are 0-based. The optional argument name is a name identifying this string, and is only used for error messages. \n  \nparse(string, name='<string>')  \nDivide the given string into examples and intervening text, and return them as a list of alternating Examples and strings. Line numbers for the Examples are 0-based. The optional argument name is a name identifying this string, and is only used for error messages.","title":"python.library.doctest#doctest.DocTestParser"},{"text":"report_success(out, test, example, got)  \nReport that the given example ran successfully. This method is provided to allow subclasses of DocTestRunner to customize their output; it should not be called directly. example is the example about to be processed. got is the actual output from the example. test is the test containing example. out is the output function that was passed to DocTestRunner.run().","title":"python.library.doctest#doctest.DocTestRunner.report_success"},{"text":"report_failure(out, test, example, got)  \nReport that the given example failed. This method is provided to allow subclasses of DocTestRunner to customize their output; it should not be called directly. example is the example about to be processed. got is the actual output from the example. test is the test containing example. out is the output function that was passed to DocTestRunner.run().","title":"python.library.doctest#doctest.DocTestRunner.report_failure"},{"text":"UnexpectedException.example  \nThe Example that failed.","title":"python.library.doctest#doctest.UnexpectedException.example"},{"text":"examples  \nA list of Example objects encoding the individual interactive Python examples that should be run by this test.","title":"python.library.doctest#doctest.DocTest.examples"}]}
{"task_id":14182339,"prompt":"def f_14182339():\n\treturn ","suffix":"","canonical_solution":"re.findall('\\\\(.+?\\\\)|\\\\w', '(zyx)bc')","test_start":"\nimport re \n\ndef check(candidate):    ","test":["\n    assert candidate() == ['(zyx)', 'b', 'c']\n"],"entry_point":"f_14182339","intent":"get whatever is between parentheses as a single match, and any char outside as an individual match in string '(zyx)bc'","library":["re"],"docs":[{"text":"Match.__getitem__(g)  \nThis is identical to m.group(g). This allows easier access to an individual group from a match: >>> m = re.match(r\"(\\w+) (\\w+)\", \"Isaac Newton, physicist\")\n>>> m[0]       # The entire match\n'Isaac Newton'\n>>> m[1]       # The first parenthesized subgroup.\n'Isaac'\n>>> m[2]       # The second parenthesized subgroup.\n'Newton'\n  New in version 3.6.","title":"python.library.re#re.Match.__getitem__"},{"text":"Match.span([group])  \nFor a match m, return the 2-tuple (m.start(group), m.end(group)). Note that if group did not contribute to the match, this is (-1, -1). group defaults to zero, the entire match.","title":"python.library.re#re.Match.span"},{"text":"email.utils.unquote(str)  \nReturn a new string which is an unquoted version of str. If str ends and begins with double quotes, they are stripped off. Likewise if str ends and begins with angle brackets, they are stripped off.","title":"python.library.email.utils#email.utils.unquote"},{"text":"Match.groups(default=None)  \nReturn a tuple containing all the subgroups of the match, from 1 up to however many groups are in the pattern. The default argument is used for groups that did not participate in the match; it defaults to None. For example: >>> m = re.match(r\"(\\d+)\\.(\\d+)\", \"24.1632\")\n>>> m.groups()\n('24', '1632')\n If we make the decimal place and everything after it optional, not all groups might participate in the match. These groups will default to None unless the default argument is given: >>> m = re.match(r\"(\\d+)\\.?(\\d+)?\", \"24\")\n>>> m.groups()      # Second group defaults to None.\n('24', None)\n>>> m.groups('0')   # Now, the second group defaults to '0'.\n('24', '0')","title":"python.library.re#re.Match.groups"},{"text":"statichexify(match)[source]","title":"matplotlib.backend_pdf_api#matplotlib.backends.backend_pdf.Name.hexify"},{"text":"get_topmost_subplotspec()[source]\n \nReturn the topmost SubplotSpec instance associated with the subplot.","title":"matplotlib._as_gen.matplotlib.gridspec.gridspecfromsubplotspec#matplotlib.gridspec.GridSpecFromSubplotSpec.get_topmost_subplotspec"},{"text":"raw_decode(s)  \nDecode a JSON document from s (a str beginning with a JSON document) and return a 2-tuple of the Python representation and the index in s where the document ended. This can be used to decode a JSON document from a string that may have extraneous data at the end.","title":"python.library.json#json.JSONDecoder.raw_decode"},{"text":"get_topmost_subplotspec()[source]\n \nReturn the topmost SubplotSpec instance associated with the subplot.","title":"matplotlib._as_gen.matplotlib.gridspec.subplotspec#matplotlib.gridspec.SubplotSpec.get_topmost_subplotspec"},{"text":"tf.RaggedTensorSpec     View source on GitHub    Type specification for a tf.RaggedTensor. Inherits From: TypeSpec  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.RaggedTensorSpec  \ntf.RaggedTensorSpec(\n    shape=None, dtype=tf.dtypes.float32, ragged_rank=None,\n    row_splits_dtype=tf.dtypes.int64, flat_values_spec=None\n)\n\n \n\n\n Args\n  shape   The shape of the RaggedTensor, or None to allow any shape. If a shape is specified, then all ragged dimensions must have size None.  \n  dtype   tf.DType of values in the RaggedTensor.  \n  ragged_rank   Python integer, the number of times the RaggedTensor's flat_values is partitioned. Defaults to shape.ndims - 1.  \n  row_splits_dtype   dtype for the RaggedTensor's row_splits tensor. One of tf.int32 or tf.int64.  \n  flat_values_spec   TypeSpec for flat_value of the RaggedTensor. It shall be provided when the flat_values is a CompositeTensor rather then Tensor. If both dtype and flat_values_spec and are provided, dtype must be the same as flat_values_spec.dtype. (experimental)   \n \n\n\n Attributes\n  dtype   The tf.dtypes.DType specified by this type for the RaggedTensor. \nrt = tf.ragged.constant([[\"a\"], [\"b\", \"c\"]], dtype=tf.string)\ntf.type_spec_from_value(rt).dtype\ntf.string\n\n \n  flat_values_spec   The TypeSpec of the flat_values of RaggedTensor.  \n  ragged_rank   The number of times the RaggedTensor's flat_values is partitioned. Defaults to shape.ndims - 1. \nvalues = tf.ragged.constant([[1, 2, 3], [4], [5, 6], [7, 8, 9, 10]])\ntf.type_spec_from_value(values).ragged_rank\n1\n \nrt1 = tf.RaggedTensor.from_uniform_row_length(values, 2)\ntf.type_spec_from_value(rt1).ragged_rank\n2\n\n \n  row_splits_dtype   The tf.dtypes.DType of the RaggedTensor's row_splits. \nrt = tf.ragged.constant([[1, 2, 3], [4]], row_splits_dtype=tf.int64)\ntf.type_spec_from_value(rt).row_splits_dtype\ntf.int64\n\n \n  shape   The statically known shape of the RaggedTensor. \nrt = tf.ragged.constant([[0], [1, 2]])\ntf.type_spec_from_value(rt).shape\nTensorShape([2, None])\n \nrt = tf.ragged.constant([[[0, 1]], [[1, 2], [3, 4]]], ragged_rank=1)\ntf.type_spec_from_value(rt).shape\nTensorShape([2, None, 2])\n\n \n  value_type   The Python type for values that are compatible with this TypeSpec. In particular, all values that are compatible with this TypeSpec must be an instance of this type. \n   Methods from_value View source \n@classmethod\nfrom_value(\n    value\n)\n is_compatible_with View source \nis_compatible_with(\n    spec_or_value\n)\n Returns true if spec_or_value is compatible with this TypeSpec. most_specific_compatible_type View source \nmost_specific_compatible_type(\n    other\n)\n Returns the most specific TypeSpec compatible with self and other.\n \n\n\n Args\n  other   A TypeSpec.   \n \n\n\n Raises\n  ValueError   If there is no TypeSpec that is compatible with both self and other.    __eq__ View source \n__eq__(\n    other\n)\n Return self==value. __ne__ View source \n__ne__(\n    other\n)\n Return self!=value.","title":"tensorflow.raggedtensorspec"},{"text":"difflib.restore(sequence, which)  \nReturn one of the two sequences that generated a delta. Given a sequence produced by Differ.compare() or ndiff(), extract lines originating from file 1 or 2 (parameter which), stripping off line prefixes. Example: >>> diff = ndiff('one\\ntwo\\nthree\\n'.splitlines(keepends=True),\n...              'ore\\ntree\\nemu\\n'.splitlines(keepends=True))\n>>> diff = list(diff) # materialize the generated delta into a list\n>>> print(''.join(restore(diff, 1)), end=\"\")\none\ntwo\nthree\n>>> print(''.join(restore(diff, 2)), end=\"\")\nore\ntree\nemu","title":"python.library.difflib#difflib.restore"}]}
{"task_id":14182339,"prompt":"def f_14182339():\n\treturn ","suffix":"","canonical_solution":"re.findall('\\\\((.*?)\\\\)|(\\\\w)', '(zyx)bc')","test_start":"\nimport re \n\ndef check(candidate): ","test":["\n    assert candidate() == [('zyx', ''), ('', 'b'), ('', 'c')]\n"],"entry_point":"f_14182339","intent":"match regex '\\\\((.*?)\\\\)|(\\\\w)' with string '(zyx)bc'","library":["re"],"docs":[{"text":"re.M  \nre.MULTILINE  \nWhen specified, the pattern character '^' matches at the beginning of the string and at the beginning of each line (immediately following each newline); and the pattern character '$' matches at the end of the string and at the end of each line (immediately preceding each newline). By default, '^' matches only at the beginning of the string, and '$' only at the end of the string and immediately before the newline (if any) at the end of the string. Corresponds to the inline flag (?m).","title":"python.library.re#re.MULTILINE"},{"text":"re.M  \nre.MULTILINE  \nWhen specified, the pattern character '^' matches at the beginning of the string and at the beginning of each line (immediately following each newline); and the pattern character '$' matches at the end of the string and at the end of each line (immediately preceding each newline). By default, '^' matches only at the beginning of the string, and '$' only at the end of the string and immediately before the newline (if any) at the end of the string. Corresponds to the inline flag (?m).","title":"python.library.re#re.M"},{"text":"numpy.polynomial.chebyshev.chebgrid3d   polynomial.chebyshev.chebgrid3d(x, y, z, c)[source]\n \nEvaluate a 3-D Chebyshev series on the Cartesian product of x, y, and z. This function returns the values:  \\[p(a,b,c) = \\sum_{i,j,k} c_{i,j,k} * T_i(a) * T_j(b) * T_k(c)\\] where the points (a, b, c) consist of all triples formed by taking a from x, b from y, and c from z. The resulting points form a grid with x in the first dimension, y in the second, and z in the third. The parameters x, y, and z are converted to arrays only if they are tuples or a lists, otherwise they are treated as a scalars. In either case, either x, y, and z or their elements must support multiplication and addition both with themselves and with the elements of c. If c has fewer than three dimensions, ones are implicitly appended to its shape to make it 3-D. The shape of the result will be c.shape[3:] + x.shape + y.shape + z.shape.  Parameters \n \nx, y, zarray_like, compatible objects\n\n\nThe three dimensional series is evaluated at the points in the Cartesian product of x, y, and z. If x,`y`, or z is a list or tuple, it is first converted to an ndarray, otherwise it is left unchanged and, if it isn\u2019t an ndarray, it is treated as a scalar.  \ncarray_like\n\n\nArray of coefficients ordered so that the coefficients for terms of degree i,j are contained in c[i,j]. If c has dimension greater than two the remaining indices enumerate multiple sets of coefficients.    Returns \n \nvaluesndarray, compatible object\n\n\nThe values of the two dimensional polynomial at points in the Cartesian product of x and y.      See also  \nchebval, chebval2d, chebgrid2d, chebval3d\n\n  Notes  New in version 1.7.0.","title":"numpy.reference.generated.numpy.polynomial.chebyshev.chebgrid3d"},{"text":"locale.YESEXPR  \nGet a regular expression that can be used with the regex function to recognize a positive response to a yes\/no question.  Note The expression is in the syntax suitable for the regex() function from the C library, which might differ from the syntax used in re.","title":"python.library.locale#locale.YESEXPR"},{"text":"pandas.Series.str.extract   Series.str.extract(pat, flags=0, expand=True)[source]\n \nExtract capture groups in the regex pat as columns in a DataFrame. For each subject string in the Series, extract groups from the first match of regular expression pat.  Parameters \n \npat:str\n\n\nRegular expression pattern with capturing groups.  \nflags:int, default 0 (no flags)\n\n\nFlags from the re module, e.g. re.IGNORECASE, that modify regular expression matching for things like case, spaces, etc. For more details, see re.  \nexpand:bool, default True\n\n\nIf True, return DataFrame with one column per capture group. If False, return a Series\/Index if there is one capture group or DataFrame if there are multiple capture groups.    Returns \n DataFrame or Series or Index\n\nA DataFrame with one row for each subject string, and one column for each group. Any capture group names in regular expression pat will be used for column names; otherwise capture group numbers will be used. The dtype of each result column is always object, even when no match is found. If expand=False and pat has only one capture group, then return a Series (if subject is a Series) or Index (if subject is an Index).      See also  extractall\n\nReturns all matches (not just the first match).    Examples A pattern with two groups will return a DataFrame with two columns. Non-matches will be NaN. \n>>> s = pd.Series(['a1', 'b2', 'c3'])\n>>> s.str.extract(r'([ab])(\\d)')\n    0    1\n0    a    1\n1    b    2\n2  NaN  NaN\n  A pattern may contain optional groups. \n>>> s.str.extract(r'([ab])?(\\d)')\n    0  1\n0    a  1\n1    b  2\n2  NaN  3\n  Named groups will become column names in the result. \n>>> s.str.extract(r'(?P<letter>[ab])(?P<digit>\\d)')\nletter digit\n0      a     1\n1      b     2\n2    NaN   NaN\n  A pattern with one group will return a DataFrame with one column if expand=True. \n>>> s.str.extract(r'[ab](\\d)', expand=True)\n    0\n0    1\n1    2\n2  NaN\n  A pattern with one group will return a Series if expand=False. \n>>> s.str.extract(r'[ab](\\d)', expand=False)\n0      1\n1      2\n2    NaN\ndtype: object","title":"pandas.reference.api.pandas.series.str.extract"},{"text":"class typing.Pattern  \nclass typing.Match  \nThese type aliases correspond to the return types from re.compile() and re.match(). These types (and the corresponding functions) are generic in AnyStr and can be made specific by writing Pattern[str], Pattern[bytes], Match[str], or Match[bytes]. These types are also in the typing.re namespace.  Deprecated since version 3.9: Classes Pattern and Match from re now support []. See PEP 585 and Generic Alias Type.","title":"python.library.typing#typing.Pattern"},{"text":"tf.bitwise.bitwise_xor Elementwise computes the bitwise XOR of x and y.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.bitwise.bitwise_xor  \ntf.bitwise.bitwise_xor(\n    x, y, name=None\n)\n The result will have those bits set, that are different in x and y. The computation is performed on the underlying representations of x and y. For example: import tensorflow as tf\nfrom tensorflow.python.ops import bitwise_ops\ndtype_list = [tf.int8, tf.int16, tf.int32, tf.int64,\n              tf.uint8, tf.uint16, tf.uint32, tf.uint64]\n\nfor dtype in dtype_list:\n  lhs = tf.constant([0, 5, 3, 14], dtype=dtype)\n  rhs = tf.constant([5, 0, 7, 11], dtype=dtype)\n  exp = tf.constant([5, 5, 4, 5],  dtype=tf.float32)\n\n  res = bitwise_ops.bitwise_xor(lhs, rhs)\n  tf.assert_equal(tf.cast(res, tf.float32), exp) # TRUE\n\n \n\n\n Args\n  x   A Tensor. Must be one of the following types: int8, int16, int32, int64, uint8, uint16, uint32, uint64.  \n  y   A Tensor. Must have the same type as x.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor. Has the same type as x.","title":"tensorflow.bitwise.bitwise_xor"},{"text":"class typing.Pattern  \nclass typing.Match  \nThese type aliases correspond to the return types from re.compile() and re.match(). These types (and the corresponding functions) are generic in AnyStr and can be made specific by writing Pattern[str], Pattern[bytes], Match[str], or Match[bytes]. These types are also in the typing.re namespace.  Deprecated since version 3.9: Classes Pattern and Match from re now support []. See PEP 585 and Generic Alias Type.","title":"python.library.typing#typing.Match"},{"text":"Match.span([group])  \nFor a match m, return the 2-tuple (m.start(group), m.end(group)). Note that if group did not contribute to the match, this is (-1, -1). group defaults to zero, the entire match.","title":"python.library.re#re.Match.span"},{"text":"re.S  \nre.DOTALL  \nMake the '.' special character match any character at all, including a newline; without this flag, '.' will match anything except a newline. Corresponds to the inline flag (?s).","title":"python.library.re#re.S"}]}
{"task_id":14182339,"prompt":"def f_14182339():\n\treturn ","suffix":"","canonical_solution":"re.findall('\\\\(.*?\\\\)|\\\\w', '(zyx)bc')","test_start":"\nimport re \n\ndef check(candidate): ","test":["\n    assert candidate() == ['(zyx)', 'b', 'c']\n"],"entry_point":"f_14182339","intent":"match multiple regex patterns with the alternation operator `|` in a string `(zyx)bc`","library":["re"],"docs":[{"text":"re.findall(pattern, string, flags=0)  \nReturn all non-overlapping matches of pattern in string, as a list of strings. The string is scanned left-to-right, and matches are returned in the order found. If one or more groups are present in the pattern, return a list of groups; this will be a list of tuples if the pattern has more than one group. Empty matches are included in the result.  Changed in version 3.7: Non-empty matches can now start just after a previous empty match.","title":"python.library.re#re.findall"},{"text":"pandas.Series.str.findall   Series.str.findall(pat, flags=0)[source]\n \nFind all occurrences of pattern or regular expression in the Series\/Index. Equivalent to applying re.findall() to all the elements in the Series\/Index.  Parameters \n \npat:str\n\n\nPattern or regular expression.  \nflags:int, default 0\n\n\nFlags from re module, e.g. re.IGNORECASE (default is 0, which means no flags).    Returns \n Series\/Index of lists of strings\n\nAll non-overlapping matches of pattern or regular expression in each string of this Series\/Index.      See also  count\n\nCount occurrences of pattern or regular expression in each string of the Series\/Index.  extractall\n\nFor each string in the Series, extract groups from all matches of regular expression and return a DataFrame with one row for each match and one column for each group.  re.findall\n\nThe equivalent re function to all non-overlapping matches of pattern or regular expression in string, as a list of strings.    Examples \n>>> s = pd.Series(['Lion', 'Monkey', 'Rabbit'])\n  The search for the pattern \u2018Monkey\u2019 returns one match: \n>>> s.str.findall('Monkey')\n0          []\n1    [Monkey]\n2          []\ndtype: object\n  On the other hand, the search for the pattern \u2018MONKEY\u2019 doesn\u2019t return any match: \n>>> s.str.findall('MONKEY')\n0    []\n1    []\n2    []\ndtype: object\n  Flags can be added to the pattern or regular expression. For instance, to find the pattern \u2018MONKEY\u2019 ignoring the case: \n>>> import re\n>>> s.str.findall('MONKEY', flags=re.IGNORECASE)\n0          []\n1    [Monkey]\n2          []\ndtype: object\n  When the pattern matches more than one string in the Series, all matches are returned: \n>>> s.str.findall('on')\n0    [on]\n1    [on]\n2      []\ndtype: object\n  Regular expressions are supported too. For instance, the search for all the strings ending with the word \u2018on\u2019 is shown next: \n>>> s.str.findall('on$')\n0    [on]\n1      []\n2      []\ndtype: object\n  If the pattern is found more than once in the same string, then a list of multiple strings is returned: \n>>> s.str.findall('b')\n0        []\n1        []\n2    [b, b]\ndtype: object","title":"pandas.reference.api.pandas.series.str.findall"},{"text":"Pattern.findall(string[, pos[, endpos]])  \nSimilar to the findall() function, using the compiled pattern, but also accepts optional pos and endpos parameters that limit the search region like for search().","title":"python.library.re#re.Pattern.findall"},{"text":"re.M  \nre.MULTILINE  \nWhen specified, the pattern character '^' matches at the beginning of the string and at the beginning of each line (immediately following each newline); and the pattern character '$' matches at the end of the string and at the end of each line (immediately preceding each newline). By default, '^' matches only at the beginning of the string, and '$' only at the end of the string and immediately before the newline (if any) at the end of the string. Corresponds to the inline flag (?m).","title":"python.library.re#re.MULTILINE"},{"text":"tf.strings.regex_full_match       View source on GitHub    Check if the input matches the regex pattern.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.strings.regex_full_match  \ntf.strings.regex_full_match(\n    input, pattern, name=None\n)\n The input is a string tensor of any shape. The pattern is a scalar string tensor which is applied to every element of the input tensor. The boolean values (True or False) of the output tensor indicate if the input matches the regex pattern provided. The pattern follows the re2 syntax (https:\/\/github.com\/google\/re2\/wiki\/Syntax) Examples: \ntf.strings.regex_full_match([\"TF lib\", \"lib TF\"], \".*lib$\")\n<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True, False])>\ntf.strings.regex_full_match([\"TF lib\", \"lib TF\"], \".*TF$\")\n<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>\n\n \n\n\n Args\n  input   A Tensor of type string. A string tensor of the text to be processed.  \n  pattern   A Tensor of type string. A scalar string tensor containing the regular expression to match the input.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor of type bool.","title":"tensorflow.strings.regex_full_match"},{"text":"pandas.Series.str.extractall   Series.str.extractall(pat, flags=0)[source]\n \nExtract capture groups in the regex pat as columns in DataFrame. For each subject string in the Series, extract groups from all matches of regular expression pat. When each subject string in the Series has exactly one match, extractall(pat).xs(0, level=\u2019match\u2019) is the same as extract(pat).  Parameters \n \npat:str\n\n\nRegular expression pattern with capturing groups.  \nflags:int, default 0 (no flags)\n\n\nA re module flag, for example re.IGNORECASE. These allow to modify regular expression matching for things like case, spaces, etc. Multiple flags can be combined with the bitwise OR operator, for example re.IGNORECASE | re.MULTILINE.    Returns \n DataFrame\n\nA DataFrame with one row for each match, and one column for each group. Its rows have a MultiIndex with first levels that come from the subject Series. The last level is named \u2018match\u2019 and indexes the matches in each item of the Series. Any capture group names in regular expression pat will be used for column names; otherwise capture group numbers will be used.      See also  extract\n\nReturns first match only (not all matches).    Examples A pattern with one group will return a DataFrame with one column. Indices with no matches will not appear in the result. \n>>> s = pd.Series([\"a1a2\", \"b1\", \"c1\"], index=[\"A\", \"B\", \"C\"])\n>>> s.str.extractall(r\"[ab](\\d)\")\n        0\nmatch\nA 0      1\n  1      2\nB 0      1\n  Capture group names are used for column names of the result. \n>>> s.str.extractall(r\"[ab](?P<digit>\\d)\")\n        digit\nmatch\nA 0         1\n  1         2\nB 0         1\n  A pattern with two groups will return a DataFrame with two columns. \n>>> s.str.extractall(r\"(?P<letter>[ab])(?P<digit>\\d)\")\n        letter digit\nmatch\nA 0          a     1\n  1          a     2\nB 0          b     1\n  Optional groups that do not match are NaN in the result. \n>>> s.str.extractall(r\"(?P<letter>[ab])?(?P<digit>\\d)\")\n        letter digit\nmatch\nA 0          a     1\n  1          a     2\nB 0          b     1\nC 0        NaN     1","title":"pandas.reference.api.pandas.series.str.extractall"},{"text":"tf.raw_ops.RegexFullMatch   Check if the input matches the regex pattern.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.raw_ops.RegexFullMatch  \ntf.raw_ops.RegexFullMatch(\n    input, pattern, name=None\n)\n The input is a string tensor of any shape. The pattern is a scalar string tensor which is applied to every element of the input tensor. The boolean values (True or False) of the output tensor indicate if the input matches the regex pattern provided. The pattern follows the re2 syntax (https:\/\/github.com\/google\/re2\/wiki\/Syntax) Examples: \ntf.strings.regex_full_match([\"TF lib\", \"lib TF\"], \".*lib$\")\n<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True, False])>\ntf.strings.regex_full_match([\"TF lib\", \"lib TF\"], \".*TF$\")\n<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>\n\n \n\n\n Args\n  input   A Tensor of type string. A string tensor of the text to be processed.  \n  pattern   A Tensor of type string. A scalar string tensor containing the regular expression to match the input.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor of type bool.","title":"tensorflow.raw_ops.regexfullmatch"},{"text":"test.support.set_match_tests(patterns)  \nDefine match test with regular expression patterns.","title":"python.library.test#test.support.set_match_tests"},{"text":"re.M  \nre.MULTILINE  \nWhen specified, the pattern character '^' matches at the beginning of the string and at the beginning of each line (immediately following each newline); and the pattern character '$' matches at the end of the string and at the end of each line (immediately preceding each newline). By default, '^' matches only at the beginning of the string, and '$' only at the end of the string and immediately before the newline (if any) at the end of the string. Corresponds to the inline flag (?m).","title":"python.library.re#re.M"},{"text":"i18n_patterns(*urls, prefix_default_language=True)","title":"django.topics.i18n.translation#django.conf.urls.i18n.i18n_patterns"}]}
{"task_id":7126916,"prompt":"def f_7126916(elements):\n\t","suffix":"\n\treturn elements","canonical_solution":"elements = ['%{0}%'.format(element) for element in elements]","test_start":"\ndef check(candidate): ","test":["\n    elements = ['abc', 'def', 'ijk', 'mno']\n    assert candidate(elements) == ['%abc%', '%def%', '%ijk%', '%mno%']\n","\n    elements = [1, 2, 3, 4, 500]\n    assert candidate(elements) == ['%1%', '%2%', '%3%', '%4%', '%500%']\n"],"entry_point":"f_7126916","intent":"formate each string cin list `elements` into pattern '%{0}%'","library":[],"docs":[]}
{"task_id":3595685,"prompt":"def f_3595685():\n\treturn ","suffix":"","canonical_solution":"subprocess.Popen(['background-process', 'arguments'])","test_start":"\nimport subprocess\nfrom unittest.mock import Mock\n\ndef check(candidate):","test":["\n    subprocess.Popen = Mock(return_value = 0)\n    assert candidate() == 0\n"],"entry_point":"f_3595685","intent":"Open a background process 'background-process' with arguments 'arguments'","library":["subprocess"],"docs":[{"text":"coroutine asyncio.create_subprocess_shell(cmd, stdin=None, stdout=None, stderr=None, loop=None, limit=None, **kwds)  \nRun the cmd shell command. The limit argument sets the buffer limit for StreamReader wrappers for Process.stdout and Process.stderr (if subprocess.PIPE is passed to stdout and stderr arguments). Return a Process instance. See the documentation of loop.subprocess_shell() for other parameters.  Important It is the application\u2019s responsibility to ensure that all whitespace and special characters are quoted appropriately to avoid shell injection vulnerabilities. The shlex.quote() function can be used to properly escape whitespace and special shell characters in strings that are going to be used to construct shell commands.   Deprecated since version 3.8, will be removed in version 3.10: The loop parameter.","title":"python.library.asyncio-subprocess#asyncio.create_subprocess_shell"},{"text":"coroutine asyncio.create_subprocess_exec(program, *args, stdin=None, stdout=None, stderr=None, loop=None, limit=None, **kwds)  \nCreate a subprocess. The limit argument sets the buffer limit for StreamReader wrappers for Process.stdout and Process.stderr (if subprocess.PIPE is passed to stdout and stderr arguments). Return a Process instance. See the documentation of loop.subprocess_exec() for other parameters.  Deprecated since version 3.8, will be removed in version 3.10: The loop parameter.","title":"python.library.asyncio-subprocess#asyncio.create_subprocess_exec"},{"text":"test.support.script_helper.spawn_python(*args, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, **kw)  \nRun a Python subprocess with the given arguments. kw is extra keyword args to pass to subprocess.Popen(). Returns a subprocess.Popen object.","title":"python.library.test#test.support.script_helper.spawn_python"},{"text":"start([initializer[, initargs]])  \nStart a subprocess to start the manager. If initializer is not None then the subprocess will call initializer(*initargs) when it starts.","title":"python.library.multiprocessing#multiprocessing.managers.BaseManager.start"},{"text":"Subprocesses Source code: Lib\/asyncio\/subprocess.py, Lib\/asyncio\/base_subprocess.py This section describes high-level async\/await asyncio APIs to create and manage subprocesses. Here\u2019s an example of how asyncio can run a shell command and obtain its result: import asyncio\n\nasync def run(cmd):\n    proc = await asyncio.create_subprocess_shell(\n        cmd,\n        stdout=asyncio.subprocess.PIPE,\n        stderr=asyncio.subprocess.PIPE)\n\n    stdout, stderr = await proc.communicate()\n\n    print(f'[{cmd!r} exited with {proc.returncode}]')\n    if stdout:\n        print(f'[stdout]\\n{stdout.decode()}')\n    if stderr:\n        print(f'[stderr]\\n{stderr.decode()}')\n\nasyncio.run(run('ls \/zzz'))\n will print: ['ls \/zzz' exited with 1]\n[stderr]\nls: \/zzz: No such file or directory\n Because all asyncio subprocess functions are asynchronous and asyncio provides many tools to work with such functions, it is easy to execute and monitor multiple subprocesses in parallel. It is indeed trivial to modify the above example to run several commands simultaneously: async def main():\n    await asyncio.gather(\n        run('ls \/zzz'),\n        run('sleep 1; echo \"hello\"'))\n\nasyncio.run(main())\n See also the Examples subsection. Creating Subprocesses  \ncoroutine asyncio.create_subprocess_exec(program, *args, stdin=None, stdout=None, stderr=None, loop=None, limit=None, **kwds)  \nCreate a subprocess. The limit argument sets the buffer limit for StreamReader wrappers for Process.stdout and Process.stderr (if subprocess.PIPE is passed to stdout and stderr arguments). Return a Process instance. See the documentation of loop.subprocess_exec() for other parameters.  Deprecated since version 3.8, will be removed in version 3.10: The loop parameter.  \n  \ncoroutine asyncio.create_subprocess_shell(cmd, stdin=None, stdout=None, stderr=None, loop=None, limit=None, **kwds)  \nRun the cmd shell command. The limit argument sets the buffer limit for StreamReader wrappers for Process.stdout and Process.stderr (if subprocess.PIPE is passed to stdout and stderr arguments). Return a Process instance. See the documentation of loop.subprocess_shell() for other parameters.  Important It is the application\u2019s responsibility to ensure that all whitespace and special characters are quoted appropriately to avoid shell injection vulnerabilities. The shlex.quote() function can be used to properly escape whitespace and special shell characters in strings that are going to be used to construct shell commands.   Deprecated since version 3.8, will be removed in version 3.10: The loop parameter.  \n  Note Subprocesses are available for Windows if a ProactorEventLoop is used. See Subprocess Support on Windows for details.   See also asyncio also has the following low-level APIs to work with subprocesses: loop.subprocess_exec(), loop.subprocess_shell(), loop.connect_read_pipe(), loop.connect_write_pipe(), as well as the Subprocess Transports and Subprocess Protocols.  Constants  \nasyncio.subprocess.PIPE  \nCan be passed to the stdin, stdout or stderr parameters. If PIPE is passed to stdin argument, the Process.stdin attribute will point to a StreamWriter instance. If PIPE is passed to stdout or stderr arguments, the Process.stdout and Process.stderr attributes will point to StreamReader instances. \n  \nasyncio.subprocess.STDOUT  \nSpecial value that can be used as the stderr argument and indicates that standard error should be redirected into standard output. \n  \nasyncio.subprocess.DEVNULL  \nSpecial value that can be used as the stdin, stdout or stderr argument to process creation functions. It indicates that the special file os.devnull will be used for the corresponding subprocess stream. \n Interacting with Subprocesses Both create_subprocess_exec() and create_subprocess_shell() functions return instances of the Process class. Process is a high-level wrapper that allows communicating with subprocesses and watching for their completion.  \nclass asyncio.subprocess.Process  \nAn object that wraps OS processes created by the create_subprocess_exec() and create_subprocess_shell() functions. This class is designed to have a similar API to the subprocess.Popen class, but there are some notable differences:  unlike Popen, Process instances do not have an equivalent to the poll() method; the communicate() and wait() methods don\u2019t have a timeout parameter: use the wait_for() function; the Process.wait() method is asynchronous, whereas subprocess.Popen.wait() method is implemented as a blocking busy loop; the universal_newlines parameter is not supported.  This class is not thread safe. See also the Subprocess and Threads section.  \ncoroutine wait()  \nWait for the child process to terminate. Set and return the returncode attribute.  Note This method can deadlock when using stdout=PIPE or stderr=PIPE and the child process generates so much output that it blocks waiting for the OS pipe buffer to accept more data. Use the communicate() method when using pipes to avoid this condition.  \n  \ncoroutine communicate(input=None)  \nInteract with process:  send data to stdin (if input is not None); read data from stdout and stderr, until EOF is reached; wait for process to terminate.  The optional input argument is the data (bytes object) that will be sent to the child process. Return a tuple (stdout_data, stderr_data). If either BrokenPipeError or ConnectionResetError exception is raised when writing input into stdin, the exception is ignored. This condition occurs when the process exits before all data are written into stdin. If it is desired to send data to the process\u2019 stdin, the process needs to be created with stdin=PIPE. Similarly, to get anything other than None in the result tuple, the process has to be created with stdout=PIPE and\/or stderr=PIPE arguments. Note, that the data read is buffered in memory, so do not use this method if the data size is large or unlimited. \n  \nsend_signal(signal)  \nSends the signal signal to the child process.  Note On Windows, SIGTERM is an alias for terminate(). CTRL_C_EVENT and CTRL_BREAK_EVENT can be sent to processes started with a creationflags parameter which includes CREATE_NEW_PROCESS_GROUP.  \n  \nterminate()  \nStop the child process. On POSIX systems this method sends signal.SIGTERM to the child process. On Windows the Win32 API function TerminateProcess() is called to stop the child process. \n  \nkill()  \nKill the child process. On POSIX systems this method sends SIGKILL to the child process. On Windows this method is an alias for terminate(). \n  \nstdin  \nStandard input stream (StreamWriter) or None if the process was created with stdin=None. \n  \nstdout  \nStandard output stream (StreamReader) or None if the process was created with stdout=None. \n  \nstderr  \nStandard error stream (StreamReader) or None if the process was created with stderr=None. \n  Warning Use the communicate() method rather than process.stdin.write(), await process.stdout.read() or await process.stderr.read. This avoids deadlocks due to streams pausing reading or writing and blocking the child process.   \npid  \nProcess identification number (PID). Note that for processes created by the create_subprocess_shell() function, this attribute is the PID of the spawned shell. \n  \nreturncode  \nReturn code of the process when it exits. A None value indicates that the process has not terminated yet. A negative value -N indicates that the child was terminated by signal N (POSIX only). \n \n Subprocess and Threads Standard asyncio event loop supports running subprocesses from different threads by default. On Windows subprocesses are provided by ProactorEventLoop only (default), SelectorEventLoop has no subprocess support. On UNIX child watchers are used for subprocess finish waiting, see Process Watchers for more info.  Changed in version 3.8: UNIX switched to use ThreadedChildWatcher for spawning subprocesses from different threads without any limitation. Spawning a subprocess with inactive current child watcher raises RuntimeError.  Note that alternative event loop implementations might have own limitations; please refer to their documentation.  See also The Concurrency and multithreading in asyncio section.  Examples An example using the Process class to control a subprocess and the StreamReader class to read from its standard output. The subprocess is created by the create_subprocess_exec() function: import asyncio\nimport sys\n\nasync def get_date():\n    code = 'import datetime; print(datetime.datetime.now())'\n\n    # Create the subprocess; redirect the standard output\n    # into a pipe.\n    proc = await asyncio.create_subprocess_exec(\n        sys.executable, '-c', code,\n        stdout=asyncio.subprocess.PIPE)\n\n    # Read one line of output.\n    data = await proc.stdout.readline()\n    line = data.decode('ascii').rstrip()\n\n    # Wait for the subprocess exit.\n    await proc.wait()\n    return line\n\ndate = asyncio.run(get_date())\nprint(f\"Current date: {date}\")\n See also the same example written using low-level APIs.","title":"python.library.asyncio-subprocess"},{"text":"subprocess \u2014 Subprocess management Source code: Lib\/subprocess.py The subprocess module allows you to spawn new processes, connect to their input\/output\/error pipes, and obtain their return codes. This module intends to replace several older modules and functions: os.system\nos.spawn*\n Information about how the subprocess module can be used to replace these modules and functions can be found in the following sections.  See also PEP 324 \u2013 PEP proposing the subprocess module  Using the subprocess Module The recommended approach to invoking subprocesses is to use the run() function for all use cases it can handle. For more advanced use cases, the underlying Popen interface can be used directly. The run() function was added in Python 3.5; if you need to retain compatibility with older versions, see the Older high-level API section.  \nsubprocess.run(args, *, stdin=None, input=None, stdout=None, stderr=None, capture_output=False, shell=False, cwd=None, timeout=None, check=False, encoding=None, errors=None, text=None, env=None, universal_newlines=None, **other_popen_kwargs)  \nRun the command described by args. Wait for command to complete, then return a CompletedProcess instance. The arguments shown above are merely the most common ones, described below in Frequently Used Arguments (hence the use of keyword-only notation in the abbreviated signature). The full function signature is largely the same as that of the Popen constructor - most of the arguments to this function are passed through to that interface. (timeout, input, check, and capture_output are not.) If capture_output is true, stdout and stderr will be captured. When used, the internal Popen object is automatically created with stdout=PIPE and stderr=PIPE. The stdout and stderr arguments may not be supplied at the same time as capture_output. If you wish to capture and combine both streams into one, use stdout=PIPE and stderr=STDOUT instead of capture_output. The timeout argument is passed to Popen.communicate(). If the timeout expires, the child process will be killed and waited for. The TimeoutExpired exception will be re-raised after the child process has terminated. The input argument is passed to Popen.communicate() and thus to the subprocess\u2019s stdin. If used it must be a byte sequence, or a string if encoding or errors is specified or text is true. When used, the internal Popen object is automatically created with stdin=PIPE, and the stdin argument may not be used as well. If check is true, and the process exits with a non-zero exit code, a CalledProcessError exception will be raised. Attributes of that exception hold the arguments, the exit code, and stdout and stderr if they were captured. If encoding or errors are specified, or text is true, file objects for stdin, stdout and stderr are opened in text mode using the specified encoding and errors or the io.TextIOWrapper default. The universal_newlines argument is equivalent to text and is provided for backwards compatibility. By default, file objects are opened in binary mode. If env is not None, it must be a mapping that defines the environment variables for the new process; these are used instead of the default behavior of inheriting the current process\u2019 environment. It is passed directly to Popen. Examples: >>> subprocess.run([\"ls\", \"-l\"])  # doesn't capture output\nCompletedProcess(args=['ls', '-l'], returncode=0)\n\n>>> subprocess.run(\"exit 1\", shell=True, check=True)\nTraceback (most recent call last):\n  ...\nsubprocess.CalledProcessError: Command 'exit 1' returned non-zero exit status 1\n\n>>> subprocess.run([\"ls\", \"-l\", \"\/dev\/null\"], capture_output=True)\nCompletedProcess(args=['ls', '-l', '\/dev\/null'], returncode=0,\nstdout=b'crw-rw-rw- 1 root root 1, 3 Jan 23 16:23 \/dev\/null\\n', stderr=b'')\n  New in version 3.5.   Changed in version 3.6: Added encoding and errors parameters   Changed in version 3.7: Added the text parameter, as a more understandable alias of universal_newlines. Added the capture_output parameter.  \n  \nclass subprocess.CompletedProcess  \nThe return value from run(), representing a process that has finished.  \nargs  \nThe arguments used to launch the process. This may be a list or a string. \n  \nreturncode  \nExit status of the child process. Typically, an exit status of 0 indicates that it ran successfully. A negative value -N indicates that the child was terminated by signal N (POSIX only). \n  \nstdout  \nCaptured stdout from the child process. A bytes sequence, or a string if run() was called with an encoding, errors, or text=True. None if stdout was not captured. If you ran the process with stderr=subprocess.STDOUT, stdout and stderr will be combined in this attribute, and stderr will be None. \n  \nstderr  \nCaptured stderr from the child process. A bytes sequence, or a string if run() was called with an encoding, errors, or text=True. None if stderr was not captured. \n  \ncheck_returncode()  \nIf returncode is non-zero, raise a CalledProcessError. \n  New in version 3.5.  \n  \nsubprocess.DEVNULL  \nSpecial value that can be used as the stdin, stdout or stderr argument to Popen and indicates that the special file os.devnull will be used.  New in version 3.3.  \n  \nsubprocess.PIPE  \nSpecial value that can be used as the stdin, stdout or stderr argument to Popen and indicates that a pipe to the standard stream should be opened. Most useful with Popen.communicate(). \n  \nsubprocess.STDOUT  \nSpecial value that can be used as the stderr argument to Popen and indicates that standard error should go into the same handle as standard output. \n  \nexception subprocess.SubprocessError  \nBase class for all other exceptions from this module.  New in version 3.3.  \n  \nexception subprocess.TimeoutExpired  \nSubclass of SubprocessError, raised when a timeout expires while waiting for a child process.  \ncmd  \nCommand that was used to spawn the child process. \n  \ntimeout  \nTimeout in seconds. \n  \noutput  \nOutput of the child process if it was captured by run() or check_output(). Otherwise, None. \n  \nstdout  \nAlias for output, for symmetry with stderr. \n  \nstderr  \nStderr output of the child process if it was captured by run(). Otherwise, None. \n  New in version 3.3.   Changed in version 3.5: stdout and stderr attributes added  \n  \nexception subprocess.CalledProcessError  \nSubclass of SubprocessError, raised when a process run by check_call() or check_output() returns a non-zero exit status.  \nreturncode  \nExit status of the child process. If the process exited due to a signal, this will be the negative signal number. \n  \ncmd  \nCommand that was used to spawn the child process. \n  \noutput  \nOutput of the child process if it was captured by run() or check_output(). Otherwise, None. \n  \nstdout  \nAlias for output, for symmetry with stderr. \n  \nstderr  \nStderr output of the child process if it was captured by run(). Otherwise, None. \n  Changed in version 3.5: stdout and stderr attributes added  \n Frequently Used Arguments To support a wide variety of use cases, the Popen constructor (and the convenience functions) accept a large number of optional arguments. For most typical use cases, many of these arguments can be safely left at their default values. The arguments that are most commonly needed are: args is required for all calls and should be a string, or a sequence of program arguments. Providing a sequence of arguments is generally preferred, as it allows the module to take care of any required escaping and quoting of arguments (e.g. to permit spaces in file names). If passing a single string, either shell must be True (see below) or else the string must simply name the program to be executed without specifying any arguments. stdin, stdout and stderr specify the executed program\u2019s standard input, standard output and standard error file handles, respectively. Valid values are PIPE, DEVNULL, an existing file descriptor (a positive integer), an existing file object, and None. PIPE indicates that a new pipe to the child should be created. DEVNULL indicates that the special file os.devnull will be used. With the default settings of None, no redirection will occur; the child\u2019s file handles will be inherited from the parent. Additionally, stderr can be STDOUT, which indicates that the stderr data from the child process should be captured into the same file handle as for stdout. If encoding or errors are specified, or text (also known as universal_newlines) is true, the file objects stdin, stdout and stderr will be opened in text mode using the encoding and errors specified in the call or the defaults for io.TextIOWrapper. For stdin, line ending characters '\\n' in the input will be converted to the default line separator os.linesep. For stdout and stderr, all line endings in the output will be converted to '\\n'. For more information see the documentation of the io.TextIOWrapper class when the newline argument to its constructor is None. If text mode is not used, stdin, stdout and stderr will be opened as binary streams. No encoding or line ending conversion is performed.  New in version 3.6: Added encoding and errors parameters.   New in version 3.7: Added the text parameter as an alias for universal_newlines.   Note The newlines attribute of the file objects Popen.stdin, Popen.stdout and Popen.stderr are not updated by the Popen.communicate() method.  If shell is True, the specified command will be executed through the shell. This can be useful if you are using Python primarily for the enhanced control flow it offers over most system shells and still want convenient access to other shell features such as shell pipes, filename wildcards, environment variable expansion, and expansion of ~ to a user\u2019s home directory. However, note that Python itself offers implementations of many shell-like features (in particular, glob, fnmatch, os.walk(), os.path.expandvars(), os.path.expanduser(), and shutil).  Changed in version 3.3: When universal_newlines is True, the class uses the encoding locale.getpreferredencoding(False) instead of locale.getpreferredencoding(). See the io.TextIOWrapper class for more information on this change.   Note Read the Security Considerations section before using shell=True.  These options, along with all of the other options, are described in more detail in the Popen constructor documentation. Popen Constructor The underlying process creation and management in this module is handled by the Popen class. It offers a lot of flexibility so that developers are able to handle the less common cases not covered by the convenience functions.  \nclass subprocess.Popen(args, bufsize=-1, executable=None, stdin=None, stdout=None, stderr=None, preexec_fn=None, close_fds=True, shell=False, cwd=None, env=None, universal_newlines=None, startupinfo=None, creationflags=0, restore_signals=True, start_new_session=False, pass_fds=(), *, group=None, extra_groups=None, user=None, umask=-1, encoding=None, errors=None, text=None)  \nExecute a child program in a new process. On POSIX, the class uses os.execvp()-like behavior to execute the child program. On Windows, the class uses the Windows CreateProcess() function. The arguments to Popen are as follows. args should be a sequence of program arguments or else a single string or path-like object. By default, the program to execute is the first item in args if args is a sequence. If args is a string, the interpretation is platform-dependent and described below. See the shell and executable arguments for additional differences from the default behavior. Unless otherwise stated, it is recommended to pass args as a sequence. An example of passing some arguments to an external program as a sequence is: Popen([\"\/usr\/bin\/git\", \"commit\", \"-m\", \"Fixes a bug.\"])\n On POSIX, if args is a string, the string is interpreted as the name or path of the program to execute. However, this can only be done if not passing arguments to the program.  Note It may not be obvious how to break a shell command into a sequence of arguments, especially in complex cases. shlex.split() can illustrate how to determine the correct tokenization for args: >>> import shlex, subprocess\n>>> command_line = input()\n\/bin\/vikings -input eggs.txt -output \"spam spam.txt\" -cmd \"echo '$MONEY'\"\n>>> args = shlex.split(command_line)\n>>> print(args)\n['\/bin\/vikings', '-input', 'eggs.txt', '-output', 'spam spam.txt', '-cmd', \"echo '$MONEY'\"]\n>>> p = subprocess.Popen(args) # Success!\n Note in particular that options (such as -input) and arguments (such as eggs.txt) that are separated by whitespace in the shell go in separate list elements, while arguments that need quoting or backslash escaping when used in the shell (such as filenames containing spaces or the echo command shown above) are single list elements.  On Windows, if args is a sequence, it will be converted to a string in a manner described in Converting an argument sequence to a string on Windows. This is because the underlying CreateProcess() operates on strings.  Changed in version 3.6: args parameter accepts a path-like object if shell is False and a sequence containing path-like objects on POSIX.   Changed in version 3.8: args parameter accepts a path-like object if shell is False and a sequence containing bytes and path-like objects on Windows.  The shell argument (which defaults to False) specifies whether to use the shell as the program to execute. If shell is True, it is recommended to pass args as a string rather than as a sequence. On POSIX with shell=True, the shell defaults to \/bin\/sh. If args is a string, the string specifies the command to execute through the shell. This means that the string must be formatted exactly as it would be when typed at the shell prompt. This includes, for example, quoting or backslash escaping filenames with spaces in them. If args is a sequence, the first item specifies the command string, and any additional items will be treated as additional arguments to the shell itself. That is to say, Popen does the equivalent of: Popen(['\/bin\/sh', '-c', args[0], args[1], ...])\n On Windows with shell=True, the COMSPEC environment variable specifies the default shell. The only time you need to specify shell=True on Windows is when the command you wish to execute is built into the shell (e.g. dir or copy). You do not need shell=True to run a batch file or console-based executable.  Note Read the Security Considerations section before using shell=True.  bufsize will be supplied as the corresponding argument to the open() function when creating the stdin\/stdout\/stderr pipe file objects:  \n0 means unbuffered (read and write are one system call and can return short) \n1 means line buffered (only usable if universal_newlines=True i.e., in a text mode) any other positive value means use a buffer of approximately that size negative bufsize (the default) means the system default of io.DEFAULT_BUFFER_SIZE will be used.   Changed in version 3.3.1: bufsize now defaults to -1 to enable buffering by default to match the behavior that most code expects. In versions prior to Python 3.2.4 and 3.3.1 it incorrectly defaulted to 0 which was unbuffered and allowed short reads. This was unintentional and did not match the behavior of Python 2 as most code expected.  The executable argument specifies a replacement program to execute. It is very seldom needed. When shell=False, executable replaces the program to execute specified by args. However, the original args is still passed to the program. Most programs treat the program specified by args as the command name, which can then be different from the program actually executed. On POSIX, the args name becomes the display name for the executable in utilities such as ps. If shell=True, on POSIX the executable argument specifies a replacement shell for the default \/bin\/sh.  Changed in version 3.6: executable parameter accepts a path-like object on POSIX.   Changed in version 3.8: executable parameter accepts a bytes and path-like object on Windows.  stdin, stdout and stderr specify the executed program\u2019s standard input, standard output and standard error file handles, respectively. Valid values are PIPE, DEVNULL, an existing file descriptor (a positive integer), an existing file object, and None. PIPE indicates that a new pipe to the child should be created. DEVNULL indicates that the special file os.devnull will be used. With the default settings of None, no redirection will occur; the child\u2019s file handles will be inherited from the parent. Additionally, stderr can be STDOUT, which indicates that the stderr data from the applications should be captured into the same file handle as for stdout. If preexec_fn is set to a callable object, this object will be called in the child process just before the child is executed. (POSIX only)  Warning The preexec_fn parameter is not safe to use in the presence of threads in your application. The child process could deadlock before exec is called. If you must use it, keep it trivial! Minimize the number of libraries you call into.   Note If you need to modify the environment for the child use the env parameter rather than doing it in a preexec_fn. The start_new_session parameter can take the place of a previously common use of preexec_fn to call os.setsid() in the child.   Changed in version 3.8: The preexec_fn parameter is no longer supported in subinterpreters. The use of the parameter in a subinterpreter raises RuntimeError. The new restriction may affect applications that are deployed in mod_wsgi, uWSGI, and other embedded environments.  If close_fds is true, all file descriptors except 0, 1 and 2 will be closed before the child process is executed. Otherwise when close_fds is false, file descriptors obey their inheritable flag as described in Inheritance of File Descriptors. On Windows, if close_fds is true then no handles will be inherited by the child process unless explicitly passed in the handle_list element of STARTUPINFO.lpAttributeList, or by standard handle redirection.  Changed in version 3.2: The default for close_fds was changed from False to what is described above.   Changed in version 3.7: On Windows the default for close_fds was changed from False to True when redirecting the standard handles. It\u2019s now possible to set close_fds to True when redirecting the standard handles.  pass_fds is an optional sequence of file descriptors to keep open between the parent and child. Providing any pass_fds forces close_fds to be True. (POSIX only)  Changed in version 3.2: The pass_fds parameter was added.  If cwd is not None, the function changes the working directory to cwd before executing the child. cwd can be a string, bytes or path-like object. In particular, the function looks for executable (or for the first item in args) relative to cwd if the executable path is a relative path.  Changed in version 3.6: cwd parameter accepts a path-like object on POSIX.   Changed in version 3.7: cwd parameter accepts a path-like object on Windows.   Changed in version 3.8: cwd parameter accepts a bytes object on Windows.  If restore_signals is true (the default) all signals that Python has set to SIG_IGN are restored to SIG_DFL in the child process before the exec. Currently this includes the SIGPIPE, SIGXFZ and SIGXFSZ signals. (POSIX only)  Changed in version 3.2: restore_signals was added.  If start_new_session is true the setsid() system call will be made in the child process prior to the execution of the subprocess. (POSIX only)  Changed in version 3.2: start_new_session was added.  If group is not None, the setregid() system call will be made in the child process prior to the execution of the subprocess. If the provided value is a string, it will be looked up via grp.getgrnam() and the value in gr_gid will be used. If the value is an integer, it will be passed verbatim. (POSIX only) Availability: POSIX  New in version 3.9.  If extra_groups is not None, the setgroups() system call will be made in the child process prior to the execution of the subprocess. Strings provided in extra_groups will be looked up via grp.getgrnam() and the values in gr_gid will be used. Integer values will be passed verbatim. (POSIX only) Availability: POSIX  New in version 3.9.  If user is not None, the setreuid() system call will be made in the child process prior to the execution of the subprocess. If the provided value is a string, it will be looked up via pwd.getpwnam() and the value in pw_uid will be used. If the value is an integer, it will be passed verbatim. (POSIX only) Availability: POSIX  New in version 3.9.  If umask is not negative, the umask() system call will be made in the child process prior to the execution of the subprocess. Availability: POSIX  New in version 3.9.  If env is not None, it must be a mapping that defines the environment variables for the new process; these are used instead of the default behavior of inheriting the current process\u2019 environment.  Note If specified, env must provide any variables required for the program to execute. On Windows, in order to run a side-by-side assembly the specified env must include a valid SystemRoot.  If encoding or errors are specified, or text is true, the file objects stdin, stdout and stderr are opened in text mode with the specified encoding and errors, as described above in Frequently Used Arguments. The universal_newlines argument is equivalent to text and is provided for backwards compatibility. By default, file objects are opened in binary mode.  New in version 3.6: encoding and errors were added.   New in version 3.7: text was added as a more readable alias for universal_newlines.  If given, startupinfo will be a STARTUPINFO object, which is passed to the underlying CreateProcess function. creationflags, if given, can be one or more of the following flags:  CREATE_NEW_CONSOLE CREATE_NEW_PROCESS_GROUP ABOVE_NORMAL_PRIORITY_CLASS BELOW_NORMAL_PRIORITY_CLASS HIGH_PRIORITY_CLASS IDLE_PRIORITY_CLASS NORMAL_PRIORITY_CLASS REALTIME_PRIORITY_CLASS CREATE_NO_WINDOW DETACHED_PROCESS CREATE_DEFAULT_ERROR_MODE CREATE_BREAKAWAY_FROM_JOB  Popen objects are supported as context managers via the with statement: on exit, standard file descriptors are closed, and the process is waited for. with Popen([\"ifconfig\"], stdout=PIPE) as proc:\n    log.write(proc.stdout.read())\n\nPopen and the other functions in this module that use it raise an auditing event subprocess.Popen with arguments executable, args, cwd, and env. The value for args may be a single string or a list of strings, depending on platform.  Changed in version 3.2: Added context manager support.   Changed in version 3.6: Popen destructor now emits a ResourceWarning warning if the child process is still running.   Changed in version 3.8: Popen can use os.posix_spawn() in some cases for better performance. On Windows Subsystem for Linux and QEMU User Emulation, Popen constructor using os.posix_spawn() no longer raise an exception on errors like missing program, but the child process fails with a non-zero returncode.  \n Exceptions Exceptions raised in the child process, before the new program has started to execute, will be re-raised in the parent. The most common exception raised is OSError. This occurs, for example, when trying to execute a non-existent file. Applications should prepare for OSError exceptions. A ValueError will be raised if Popen is called with invalid arguments. check_call() and check_output() will raise CalledProcessError if the called process returns a non-zero return code. All of the functions and methods that accept a timeout parameter, such as call() and Popen.communicate() will raise TimeoutExpired if the timeout expires before the process exits. Exceptions defined in this module all inherit from SubprocessError.  New in version 3.3: The SubprocessError base class was added.  Security Considerations Unlike some other popen functions, this implementation will never implicitly call a system shell. This means that all characters, including shell metacharacters, can safely be passed to child processes. If the shell is invoked explicitly, via shell=True, it is the application\u2019s responsibility to ensure that all whitespace and metacharacters are quoted appropriately to avoid shell injection vulnerabilities. When using shell=True, the shlex.quote() function can be used to properly escape whitespace and shell metacharacters in strings that are going to be used to construct shell commands. Popen Objects Instances of the Popen class have the following methods:  \nPopen.poll()  \nCheck if child process has terminated. Set and return returncode attribute. Otherwise, returns None. \n  \nPopen.wait(timeout=None)  \nWait for child process to terminate. Set and return returncode attribute. If the process does not terminate after timeout seconds, raise a TimeoutExpired exception. It is safe to catch this exception and retry the wait.  Note This will deadlock when using stdout=PIPE or stderr=PIPE and the child process generates enough output to a pipe such that it blocks waiting for the OS pipe buffer to accept more data. Use Popen.communicate() when using pipes to avoid that.   Note The function is implemented using a busy loop (non-blocking call and short sleeps). Use the asyncio module for an asynchronous wait: see asyncio.create_subprocess_exec.   Changed in version 3.3: timeout was added.  \n  \nPopen.communicate(input=None, timeout=None)  \nInteract with process: Send data to stdin. Read data from stdout and stderr, until end-of-file is reached. Wait for process to terminate and set the returncode attribute. The optional input argument should be data to be sent to the child process, or None, if no data should be sent to the child. If streams were opened in text mode, input must be a string. Otherwise, it must be bytes. communicate() returns a tuple (stdout_data, stderr_data). The data will be strings if streams were opened in text mode; otherwise, bytes. Note that if you want to send data to the process\u2019s stdin, you need to create the Popen object with stdin=PIPE. Similarly, to get anything other than None in the result tuple, you need to give stdout=PIPE and\/or stderr=PIPE too. If the process does not terminate after timeout seconds, a TimeoutExpired exception will be raised. Catching this exception and retrying communication will not lose any output. The child process is not killed if the timeout expires, so in order to cleanup properly a well-behaved application should kill the child process and finish communication: proc = subprocess.Popen(...)\ntry:\n    outs, errs = proc.communicate(timeout=15)\nexcept TimeoutExpired:\n    proc.kill()\n    outs, errs = proc.communicate()\n  Note The data read is buffered in memory, so do not use this method if the data size is large or unlimited.   Changed in version 3.3: timeout was added.  \n  \nPopen.send_signal(signal)  \nSends the signal signal to the child. Do nothing if the process completed.  Note On Windows, SIGTERM is an alias for terminate(). CTRL_C_EVENT and CTRL_BREAK_EVENT can be sent to processes started with a creationflags parameter which includes CREATE_NEW_PROCESS_GROUP.  \n  \nPopen.terminate()  \nStop the child. On POSIX OSs the method sends SIGTERM to the child. On Windows the Win32 API function TerminateProcess() is called to stop the child. \n  \nPopen.kill()  \nKills the child. On POSIX OSs the function sends SIGKILL to the child. On Windows kill() is an alias for terminate(). \n The following attributes are also available:  \nPopen.args  \nThe args argument as it was passed to Popen \u2013 a sequence of program arguments or else a single string.  New in version 3.3.  \n  \nPopen.stdin  \nIf the stdin argument was PIPE, this attribute is a writeable stream object as returned by open(). If the encoding or errors arguments were specified or the universal_newlines argument was True, the stream is a text stream, otherwise it is a byte stream. If the stdin argument was not PIPE, this attribute is None. \n  \nPopen.stdout  \nIf the stdout argument was PIPE, this attribute is a readable stream object as returned by open(). Reading from the stream provides output from the child process. If the encoding or errors arguments were specified or the universal_newlines argument was True, the stream is a text stream, otherwise it is a byte stream. If the stdout argument was not PIPE, this attribute is None. \n  \nPopen.stderr  \nIf the stderr argument was PIPE, this attribute is a readable stream object as returned by open(). Reading from the stream provides error output from the child process. If the encoding or errors arguments were specified or the universal_newlines argument was True, the stream is a text stream, otherwise it is a byte stream. If the stderr argument was not PIPE, this attribute is None. \n  Warning Use communicate() rather than .stdin.write, .stdout.read or .stderr.read to avoid deadlocks due to any of the other OS pipe buffers filling up and blocking the child process.   \nPopen.pid  \nThe process ID of the child process. Note that if you set the shell argument to True, this is the process ID of the spawned shell. \n  \nPopen.returncode  \nThe child return code, set by poll() and wait() (and indirectly by communicate()). A None value indicates that the process hasn\u2019t terminated yet. A negative value -N indicates that the child was terminated by signal N (POSIX only). \n Windows Popen Helpers The STARTUPINFO class and following constants are only available on Windows.  \nclass subprocess.STARTUPINFO(*, dwFlags=0, hStdInput=None, hStdOutput=None, hStdError=None, wShowWindow=0, lpAttributeList=None)  \nPartial support of the Windows STARTUPINFO structure is used for Popen creation. The following attributes can be set by passing them as keyword-only arguments.  Changed in version 3.7: Keyword-only argument support was added.   \ndwFlags  \nA bit field that determines whether certain STARTUPINFO attributes are used when the process creates a window. si = subprocess.STARTUPINFO()\nsi.dwFlags = subprocess.STARTF_USESTDHANDLES | subprocess.STARTF_USESHOWWINDOW\n \n  \nhStdInput  \nIf dwFlags specifies STARTF_USESTDHANDLES, this attribute is the standard input handle for the process. If STARTF_USESTDHANDLES is not specified, the default for standard input is the keyboard buffer. \n  \nhStdOutput  \nIf dwFlags specifies STARTF_USESTDHANDLES, this attribute is the standard output handle for the process. Otherwise, this attribute is ignored and the default for standard output is the console window\u2019s buffer. \n  \nhStdError  \nIf dwFlags specifies STARTF_USESTDHANDLES, this attribute is the standard error handle for the process. Otherwise, this attribute is ignored and the default for standard error is the console window\u2019s buffer. \n  \nwShowWindow  \nIf dwFlags specifies STARTF_USESHOWWINDOW, this attribute can be any of the values that can be specified in the nCmdShow parameter for the ShowWindow function, except for SW_SHOWDEFAULT. Otherwise, this attribute is ignored. SW_HIDE is provided for this attribute. It is used when Popen is called with shell=True. \n  \nlpAttributeList  \nA dictionary of additional attributes for process creation as given in STARTUPINFOEX, see UpdateProcThreadAttribute. Supported attributes:  handle_list\n\nSequence of handles that will be inherited. close_fds must be true if non-empty. The handles must be temporarily made inheritable by os.set_handle_inheritable() when passed to the Popen constructor, else OSError will be raised with Windows error ERROR_INVALID_PARAMETER (87).  Warning In a multithreaded process, use caution to avoid leaking handles that are marked inheritable when combining this feature with concurrent calls to other process creation functions that inherit all handles such as os.system(). This also applies to standard handle redirection, which temporarily creates inheritable handles.     New in version 3.7.  \n \n Windows Constants The subprocess module exposes the following constants.  \nsubprocess.STD_INPUT_HANDLE  \nThe standard input device. Initially, this is the console input buffer, CONIN$. \n  \nsubprocess.STD_OUTPUT_HANDLE  \nThe standard output device. Initially, this is the active console screen buffer, CONOUT$. \n  \nsubprocess.STD_ERROR_HANDLE  \nThe standard error device. Initially, this is the active console screen buffer, CONOUT$. \n  \nsubprocess.SW_HIDE  \nHides the window. Another window will be activated. \n  \nsubprocess.STARTF_USESTDHANDLES  \nSpecifies that the STARTUPINFO.hStdInput, STARTUPINFO.hStdOutput, and STARTUPINFO.hStdError attributes contain additional information. \n  \nsubprocess.STARTF_USESHOWWINDOW  \nSpecifies that the STARTUPINFO.wShowWindow attribute contains additional information. \n  \nsubprocess.CREATE_NEW_CONSOLE  \nThe new process has a new console, instead of inheriting its parent\u2019s console (the default). \n  \nsubprocess.CREATE_NEW_PROCESS_GROUP  \nA Popen creationflags parameter to specify that a new process group will be created. This flag is necessary for using os.kill() on the subprocess. This flag is ignored if CREATE_NEW_CONSOLE is specified. \n  \nsubprocess.ABOVE_NORMAL_PRIORITY_CLASS  \nA Popen creationflags parameter to specify that a new process will have an above average priority.  New in version 3.7.  \n  \nsubprocess.BELOW_NORMAL_PRIORITY_CLASS  \nA Popen creationflags parameter to specify that a new process will have a below average priority.  New in version 3.7.  \n  \nsubprocess.HIGH_PRIORITY_CLASS  \nA Popen creationflags parameter to specify that a new process will have a high priority.  New in version 3.7.  \n  \nsubprocess.IDLE_PRIORITY_CLASS  \nA Popen creationflags parameter to specify that a new process will have an idle (lowest) priority.  New in version 3.7.  \n  \nsubprocess.NORMAL_PRIORITY_CLASS  \nA Popen creationflags parameter to specify that a new process will have an normal priority. (default)  New in version 3.7.  \n  \nsubprocess.REALTIME_PRIORITY_CLASS  \nA Popen creationflags parameter to specify that a new process will have realtime priority. You should almost never use REALTIME_PRIORITY_CLASS, because this interrupts system threads that manage mouse input, keyboard input, and background disk flushing. This class can be appropriate for applications that \u201ctalk\u201d directly to hardware or that perform brief tasks that should have limited interruptions.  New in version 3.7.  \n  \nsubprocess.CREATE_NO_WINDOW  \nA Popen creationflags parameter to specify that a new process will not create a window.  New in version 3.7.  \n  \nsubprocess.DETACHED_PROCESS  \nA Popen creationflags parameter to specify that a new process will not inherit its parent\u2019s console. This value cannot be used with CREATE_NEW_CONSOLE.  New in version 3.7.  \n  \nsubprocess.CREATE_DEFAULT_ERROR_MODE  \nA Popen creationflags parameter to specify that a new process does not inherit the error mode of the calling process. Instead, the new process gets the default error mode. This feature is particularly useful for multithreaded shell applications that run with hard errors disabled.  New in version 3.7.  \n  \nsubprocess.CREATE_BREAKAWAY_FROM_JOB  \nA Popen creationflags parameter to specify that a new process is not associated with the job.  New in version 3.7.  \n Older high-level API Prior to Python 3.5, these three functions comprised the high level API to subprocess. You can now use run() in many cases, but lots of existing code calls these functions.  \nsubprocess.call(args, *, stdin=None, stdout=None, stderr=None, shell=False, cwd=None, timeout=None, **other_popen_kwargs)  \nRun the command described by args. Wait for command to complete, then return the returncode attribute. Code needing to capture stdout or stderr should use run() instead: run(...).returncode\n To suppress stdout or stderr, supply a value of DEVNULL. The arguments shown above are merely some common ones. The full function signature is the same as that of the Popen constructor - this function passes all supplied arguments other than timeout directly through to that interface.  Note Do not use stdout=PIPE or stderr=PIPE with this function. The child process will block if it generates enough output to a pipe to fill up the OS pipe buffer as the pipes are not being read from.   Changed in version 3.3: timeout was added.  \n  \nsubprocess.check_call(args, *, stdin=None, stdout=None, stderr=None, shell=False, cwd=None, timeout=None, **other_popen_kwargs)  \nRun command with arguments. Wait for command to complete. If the return code was zero then return, otherwise raise CalledProcessError. The CalledProcessError object will have the return code in the returncode attribute. Code needing to capture stdout or stderr should use run() instead: run(..., check=True)\n To suppress stdout or stderr, supply a value of DEVNULL. The arguments shown above are merely some common ones. The full function signature is the same as that of the Popen constructor - this function passes all supplied arguments other than timeout directly through to that interface.  Note Do not use stdout=PIPE or stderr=PIPE with this function. The child process will block if it generates enough output to a pipe to fill up the OS pipe buffer as the pipes are not being read from.   Changed in version 3.3: timeout was added.  \n  \nsubprocess.check_output(args, *, stdin=None, stderr=None, shell=False, cwd=None, encoding=None, errors=None, universal_newlines=None, timeout=None, text=None, **other_popen_kwargs)  \nRun command with arguments and return its output. If the return code was non-zero it raises a CalledProcessError. The CalledProcessError object will have the return code in the returncode attribute and any output in the output attribute. This is equivalent to: run(..., check=True, stdout=PIPE).stdout\n The arguments shown above are merely some common ones. The full function signature is largely the same as that of run() - most arguments are passed directly through to that interface. One API deviation from run() behavior exists: passing input=None will behave the same as input=b'' (or input='', depending on other arguments) rather than using the parent\u2019s standard input file handle. By default, this function will return the data as encoded bytes. The actual encoding of the output data may depend on the command being invoked, so the decoding to text will often need to be handled at the application level. This behaviour may be overridden by setting text, encoding, errors, or universal_newlines to True as described in Frequently Used Arguments and run(). To also capture standard error in the result, use stderr=subprocess.STDOUT: >>> subprocess.check_output(\n...     \"ls non_existent_file; exit 0\",\n...     stderr=subprocess.STDOUT,\n...     shell=True)\n'ls: non_existent_file: No such file or directory\\n'\n  New in version 3.1.   Changed in version 3.3: timeout was added.   Changed in version 3.4: Support for the input keyword argument was added.   Changed in version 3.6: encoding and errors were added. See run() for details.   New in version 3.7: text was added as a more readable alias for universal_newlines.  \n Replacing Older Functions with the subprocess Module In this section, \u201ca becomes b\u201d means that b can be used as a replacement for a.  Note All \u201ca\u201d functions in this section fail (more or less) silently if the executed program cannot be found; the \u201cb\u201d replacements raise OSError instead. In addition, the replacements using check_output() will fail with a CalledProcessError if the requested operation produces a non-zero return code. The output is still available as the output attribute of the raised exception.  In the following examples, we assume that the relevant functions have already been imported from the subprocess module. Replacing \/bin\/sh shell command substitution output=$(mycmd myarg)\n becomes: output = check_output([\"mycmd\", \"myarg\"])\n Replacing shell pipeline output=$(dmesg | grep hda)\n becomes: p1 = Popen([\"dmesg\"], stdout=PIPE)\np2 = Popen([\"grep\", \"hda\"], stdin=p1.stdout, stdout=PIPE)\np1.stdout.close()  # Allow p1 to receive a SIGPIPE if p2 exits.\noutput = p2.communicate()[0]\n The p1.stdout.close() call after starting the p2 is important in order for p1 to receive a SIGPIPE if p2 exits before p1. Alternatively, for trusted input, the shell\u2019s own pipeline support may still be used directly: output=$(dmesg | grep hda)\n becomes: output=check_output(\"dmesg | grep hda\", shell=True)\n Replacing os.system()\n sts = os.system(\"mycmd\" + \" myarg\")\n# becomes\nsts = call(\"mycmd\" + \" myarg\", shell=True)\n Notes:  Calling the program through the shell is usually not required.  A more realistic example would look like this: try:\n    retcode = call(\"mycmd\" + \" myarg\", shell=True)\n    if retcode < 0:\n        print(\"Child was terminated by signal\", -retcode, file=sys.stderr)\n    else:\n        print(\"Child returned\", retcode, file=sys.stderr)\nexcept OSError as e:\n    print(\"Execution failed:\", e, file=sys.stderr)\n Replacing the os.spawn family P_NOWAIT example: pid = os.spawnlp(os.P_NOWAIT, \"\/bin\/mycmd\", \"mycmd\", \"myarg\")\n==>\npid = Popen([\"\/bin\/mycmd\", \"myarg\"]).pid\n P_WAIT example: retcode = os.spawnlp(os.P_WAIT, \"\/bin\/mycmd\", \"mycmd\", \"myarg\")\n==>\nretcode = call([\"\/bin\/mycmd\", \"myarg\"])\n Vector example: os.spawnvp(os.P_NOWAIT, path, args)\n==>\nPopen([path] + args[1:])\n Environment example: os.spawnlpe(os.P_NOWAIT, \"\/bin\/mycmd\", \"mycmd\", \"myarg\", env)\n==>\nPopen([\"\/bin\/mycmd\", \"myarg\"], env={\"PATH\": \"\/usr\/bin\"})\n Replacing os.popen(), os.popen2(), os.popen3()\n (child_stdin, child_stdout) = os.popen2(cmd, mode, bufsize)\n==>\np = Popen(cmd, shell=True, bufsize=bufsize,\n          stdin=PIPE, stdout=PIPE, close_fds=True)\n(child_stdin, child_stdout) = (p.stdin, p.stdout)\n (child_stdin,\n child_stdout,\n child_stderr) = os.popen3(cmd, mode, bufsize)\n==>\np = Popen(cmd, shell=True, bufsize=bufsize,\n          stdin=PIPE, stdout=PIPE, stderr=PIPE, close_fds=True)\n(child_stdin,\n child_stdout,\n child_stderr) = (p.stdin, p.stdout, p.stderr)\n (child_stdin, child_stdout_and_stderr) = os.popen4(cmd, mode, bufsize)\n==>\np = Popen(cmd, shell=True, bufsize=bufsize,\n          stdin=PIPE, stdout=PIPE, stderr=STDOUT, close_fds=True)\n(child_stdin, child_stdout_and_stderr) = (p.stdin, p.stdout)\n Return code handling translates as follows: pipe = os.popen(cmd, 'w')\n...\nrc = pipe.close()\nif rc is not None and rc >> 8:\n    print(\"There were some errors\")\n==>\nprocess = Popen(cmd, stdin=PIPE)\n...\nprocess.stdin.close()\nif process.wait() != 0:\n    print(\"There were some errors\")\n Replacing functions from the popen2 module  Note If the cmd argument to popen2 functions is a string, the command is executed through \/bin\/sh. If it is a list, the command is directly executed.  (child_stdout, child_stdin) = popen2.popen2(\"somestring\", bufsize, mode)\n==>\np = Popen(\"somestring\", shell=True, bufsize=bufsize,\n          stdin=PIPE, stdout=PIPE, close_fds=True)\n(child_stdout, child_stdin) = (p.stdout, p.stdin)\n (child_stdout, child_stdin) = popen2.popen2([\"mycmd\", \"myarg\"], bufsize, mode)\n==>\np = Popen([\"mycmd\", \"myarg\"], bufsize=bufsize,\n          stdin=PIPE, stdout=PIPE, close_fds=True)\n(child_stdout, child_stdin) = (p.stdout, p.stdin)\n popen2.Popen3 and popen2.Popen4 basically work as subprocess.Popen, except that:  \nPopen raises an exception if the execution fails. The capturestderr argument is replaced with the stderr argument. \nstdin=PIPE and stdout=PIPE must be specified. popen2 closes all file descriptors by default, but you have to specify close_fds=True with Popen to guarantee this behavior on all platforms or past Python versions.  Legacy Shell Invocation Functions This module also provides the following legacy functions from the 2.x commands module. These operations implicitly invoke the system shell and none of the guarantees described above regarding security and exception handling consistency are valid for these functions.  \nsubprocess.getstatusoutput(cmd)  \nReturn (exitcode, output) of executing cmd in a shell. Execute the string cmd in a shell with Popen.check_output() and return a 2-tuple (exitcode, output). The locale encoding is used; see the notes on Frequently Used Arguments for more details. A trailing newline is stripped from the output. The exit code for the command can be interpreted as the return code of subprocess. Example: >>> subprocess.getstatusoutput('ls \/bin\/ls')\n(0, '\/bin\/ls')\n>>> subprocess.getstatusoutput('cat \/bin\/junk')\n(1, 'cat: \/bin\/junk: No such file or directory')\n>>> subprocess.getstatusoutput('\/bin\/junk')\n(127, 'sh: \/bin\/junk: not found')\n>>> subprocess.getstatusoutput('\/bin\/kill $$')\n(-15, '')\n Availability: POSIX & Windows.  Changed in version 3.3.4: Windows support was added. The function now returns (exitcode, output) instead of (status, output) as it did in Python 3.3.3 and earlier. exitcode has the same value as returncode.  \n  \nsubprocess.getoutput(cmd)  \nReturn output (stdout and stderr) of executing cmd in a shell. Like getstatusoutput(), except the exit code is ignored and the return value is a string containing the command\u2019s output. Example: >>> subprocess.getoutput('ls \/bin\/ls')\n'\/bin\/ls'\n Availability: POSIX & Windows.  Changed in version 3.3.4: Windows support added  \n Notes Converting an argument sequence to a string on Windows On Windows, an args sequence is converted to a string that can be parsed using the following rules (which correspond to the rules used by the MS C runtime):  Arguments are delimited by white space, which is either a space or a tab. A string surrounded by double quotation marks is interpreted as a single argument, regardless of white space contained within. A quoted string can be embedded in an argument. A double quotation mark preceded by a backslash is interpreted as a literal double quotation mark. Backslashes are interpreted literally, unless they immediately precede a double quotation mark. If backslashes immediately precede a double quotation mark, every pair of backslashes is interpreted as a literal backslash. If the number of backslashes is odd, the last backslash escapes the next double quotation mark as described in rule 3.   See also  \nshlex\n\n\nModule which provides function to parse and escape command lines.","title":"python.library.subprocess"},{"text":"subprocess.run(args, *, stdin=None, input=None, stdout=None, stderr=None, capture_output=False, shell=False, cwd=None, timeout=None, check=False, encoding=None, errors=None, text=None, env=None, universal_newlines=None, **other_popen_kwargs)  \nRun the command described by args. Wait for command to complete, then return a CompletedProcess instance. The arguments shown above are merely the most common ones, described below in Frequently Used Arguments (hence the use of keyword-only notation in the abbreviated signature). The full function signature is largely the same as that of the Popen constructor - most of the arguments to this function are passed through to that interface. (timeout, input, check, and capture_output are not.) If capture_output is true, stdout and stderr will be captured. When used, the internal Popen object is automatically created with stdout=PIPE and stderr=PIPE. The stdout and stderr arguments may not be supplied at the same time as capture_output. If you wish to capture and combine both streams into one, use stdout=PIPE and stderr=STDOUT instead of capture_output. The timeout argument is passed to Popen.communicate(). If the timeout expires, the child process will be killed and waited for. The TimeoutExpired exception will be re-raised after the child process has terminated. The input argument is passed to Popen.communicate() and thus to the subprocess\u2019s stdin. If used it must be a byte sequence, or a string if encoding or errors is specified or text is true. When used, the internal Popen object is automatically created with stdin=PIPE, and the stdin argument may not be used as well. If check is true, and the process exits with a non-zero exit code, a CalledProcessError exception will be raised. Attributes of that exception hold the arguments, the exit code, and stdout and stderr if they were captured. If encoding or errors are specified, or text is true, file objects for stdin, stdout and stderr are opened in text mode using the specified encoding and errors or the io.TextIOWrapper default. The universal_newlines argument is equivalent to text and is provided for backwards compatibility. By default, file objects are opened in binary mode. If env is not None, it must be a mapping that defines the environment variables for the new process; these are used instead of the default behavior of inheriting the current process\u2019 environment. It is passed directly to Popen. Examples: >>> subprocess.run([\"ls\", \"-l\"])  # doesn't capture output\nCompletedProcess(args=['ls', '-l'], returncode=0)\n\n>>> subprocess.run(\"exit 1\", shell=True, check=True)\nTraceback (most recent call last):\n  ...\nsubprocess.CalledProcessError: Command 'exit 1' returned non-zero exit status 1\n\n>>> subprocess.run([\"ls\", \"-l\", \"\/dev\/null\"], capture_output=True)\nCompletedProcess(args=['ls', '-l', '\/dev\/null'], returncode=0,\nstdout=b'crw-rw-rw- 1 root root 1, 3 Jan 23 16:23 \/dev\/null\\n', stderr=b'')\n  New in version 3.5.   Changed in version 3.6: Added encoding and errors parameters   Changed in version 3.7: Added the text parameter, as a more understandable alias of universal_newlines. Added the capture_output parameter.","title":"python.library.subprocess#subprocess.run"},{"text":"asyncio.subprocess.PIPE  \nCan be passed to the stdin, stdout or stderr parameters. If PIPE is passed to stdin argument, the Process.stdin attribute will point to a StreamWriter instance. If PIPE is passed to stdout or stderr arguments, the Process.stdout and Process.stderr attributes will point to StreamReader instances.","title":"python.library.asyncio-subprocess#asyncio.asyncio.subprocess.PIPE"},{"text":"subprocess.CREATE_NO_WINDOW  \nA Popen creationflags parameter to specify that a new process will not create a window.  New in version 3.7.","title":"python.library.subprocess#subprocess.CREATE_NO_WINDOW"},{"text":"coroutine loop.subprocess_exec(protocol_factory, *args, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, **kwargs)  \nCreate a subprocess from one or more string arguments specified by args. args must be a list of strings represented by:  \nstr; or bytes, encoded to the filesystem encoding.  The first string specifies the program executable, and the remaining strings specify the arguments. Together, string arguments form the argv of the program. This is similar to the standard library subprocess.Popen class called with shell=False and the list of strings passed as the first argument; however, where Popen takes a single argument which is list of strings, subprocess_exec takes multiple string arguments. The protocol_factory must be a callable returning a subclass of the asyncio.SubprocessProtocol class. Other parameters:  \nstdin can be any of these:  a file-like object representing a pipe to be connected to the subprocess\u2019s standard input stream using connect_write_pipe()\n the subprocess.PIPE constant (default) which will create a new pipe and connect it, the value None which will make the subprocess inherit the file descriptor from this process the subprocess.DEVNULL constant which indicates that the special os.devnull file will be used   \nstdout can be any of these:  a file-like object representing a pipe to be connected to the subprocess\u2019s standard output stream using connect_write_pipe()\n the subprocess.PIPE constant (default) which will create a new pipe and connect it, the value None which will make the subprocess inherit the file descriptor from this process the subprocess.DEVNULL constant which indicates that the special os.devnull file will be used   \nstderr can be any of these:  a file-like object representing a pipe to be connected to the subprocess\u2019s standard error stream using connect_write_pipe()\n the subprocess.PIPE constant (default) which will create a new pipe and connect it, the value None which will make the subprocess inherit the file descriptor from this process the subprocess.DEVNULL constant which indicates that the special os.devnull file will be used the subprocess.STDOUT constant which will connect the standard error stream to the process\u2019 standard output stream   \nAll other keyword arguments are passed to subprocess.Popen without interpretation, except for bufsize, universal_newlines, shell, text, encoding and errors, which should not be specified at all. The asyncio subprocess API does not support decoding the streams as text. bytes.decode() can be used to convert the bytes returned from the stream to text.   See the constructor of the subprocess.Popen class for documentation on other arguments. Returns a pair of (transport, protocol), where transport conforms to the asyncio.SubprocessTransport base class and protocol is an object instantiated by the protocol_factory.","title":"python.library.asyncio-eventloop#asyncio.loop.subprocess_exec"}]}
{"task_id":18453566,"prompt":"def f_18453566(mydict, mykeys):\n\treturn ","suffix":"","canonical_solution":"[mydict[x] for x in mykeys]","test_start":"\ndef check(candidate):","test":["\n    mydict = {'one': 1, 'two': 2, 'three': 3}\n    mykeys = ['three', 'one']\n    assert candidate(mydict, mykeys) == [3, 1]\n","\n    mydict = {'one': 1.0, 'two': 2.0, 'three': 3.0}\n    mykeys = ['one']\n    assert candidate(mydict, mykeys) == [1.0]\n"],"entry_point":"f_18453566","intent":"get list of values from dictionary 'mydict' w.r.t. list of keys 'mykeys'","library":[],"docs":[]}
{"task_id":12692135,"prompt":"def f_12692135():\n\treturn ","suffix":"","canonical_solution":"dict([('Name', 'Joe'), ('Age', 22)])","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == {'Name': 'Joe', 'Age': 22}\n"],"entry_point":"f_12692135","intent":"convert list `[('Name', 'Joe'), ('Age', 22)]` into a dictionary","library":[],"docs":[]}
{"task_id":14401047,"prompt":"def f_14401047(data):\n\treturn ","suffix":"","canonical_solution":"data.mean(axis=1).reshape(data.shape[0], -1)","test_start":"\nimport numpy as np\n\ndef check(candidate):","test":["\n    data = np.array([[1, 2, 3, 4, 4, 3, 7, 1], [6, 2, 3, 4, 3, 4, 4, 1]])\n    expected_res = np.array([[3.125], [3.375]])\n    assert np.array_equal(candidate(data), expected_res)\n"],"entry_point":"f_14401047","intent":"average each two columns of array `data`","library":["numpy"],"docs":[{"text":"statistics.mean(data)  \nReturn the sample arithmetic mean of data which can be a sequence or iterable. The arithmetic mean is the sum of the data divided by the number of data points. It is commonly called \u201cthe average\u201d, although it is only one of many different mathematical averages. It is a measure of the central location of the data. If data is empty, StatisticsError will be raised. Some examples of use: >>> mean([1, 2, 3, 4, 4])\n2.8\n>>> mean([-1.0, 2.5, 3.25, 5.75])\n2.625\n\n>>> from fractions import Fraction as F\n>>> mean([F(3, 7), F(1, 21), F(5, 3), F(1, 3)])\nFraction(13, 21)\n\n>>> from decimal import Decimal as D\n>>> mean([D(\"0.5\"), D(\"0.75\"), D(\"0.625\"), D(\"0.375\")])\nDecimal('0.5625')\n  Note The mean is strongly affected by outliers and is not a robust estimator for central location: the mean is not necessarily a typical example of the data points. For more robust measures of central location, see median() and mode(). The sample mean gives an unbiased estimate of the true population mean, so that when taken on average over all the possible samples, mean(sample) converges on the true mean of the entire population. If data represents the entire population rather than a sample, then mean(data) is equivalent to calculating the true population mean \u03bc.","title":"python.library.statistics#statistics.mean"},{"text":"class RegrAvgX(y, x, filter=None, default=None)  \nReturns the average of the independent variable (sum(x)\/N) as a float, or default if there aren\u2019t any matching rows.","title":"django.ref.contrib.postgres.aggregates#django.contrib.postgres.aggregates.RegrAvgX"},{"text":"class RegrAvgY(y, x, filter=None, default=None)  \nReturns the average of the dependent variable (sum(y)\/N) as a float, or default if there aren\u2019t any matching rows.","title":"django.ref.contrib.postgres.aggregates#django.contrib.postgres.aggregates.RegrAvgY"},{"text":"statistics.fmean(data)  \nConvert data to floats and compute the arithmetic mean. This runs faster than the mean() function and it always returns a float. The data may be a sequence or iterable. If the input dataset is empty, raises a StatisticsError. >>> fmean([3.5, 4.0, 5.25])\n4.25\n  New in version 3.8.","title":"python.library.statistics#statistics.fmean"},{"text":"statistics.median(data)  \nReturn the median (middle value) of numeric data, using the common \u201cmean of middle two\u201d method. If data is empty, StatisticsError is raised. data can be a sequence or iterable. The median is a robust measure of central location and is less affected by the presence of outliers. When the number of data points is odd, the middle data point is returned: >>> median([1, 3, 5])\n3\n When the number of data points is even, the median is interpolated by taking the average of the two middle values: >>> median([1, 3, 5, 7])\n4.0\n This is suited for when your data is discrete, and you don\u2019t mind that the median may not be an actual data point. If the data is ordinal (supports order operations) but not numeric (doesn\u2019t support addition), consider using median_low() or median_high() instead.","title":"python.library.statistics#statistics.median"},{"text":"numpy.matrix.mean method   matrix.mean(axis=None, dtype=None, out=None)[source]\n \nReturns the average of the matrix elements along the given axis. Refer to numpy.mean for full documentation.  See also  numpy.mean\n  Notes Same as ndarray.mean except that, where that returns an ndarray, this returns a matrix object. Examples >>> x = np.matrix(np.arange(12).reshape((3, 4)))\n>>> x\nmatrix([[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]])\n>>> x.mean()\n5.5\n>>> x.mean(0)\nmatrix([[4., 5., 6., 7.]])\n>>> x.mean(1)\nmatrix([[ 1.5],\n        [ 5.5],\n        [ 9.5]])","title":"numpy.reference.generated.numpy.matrix.mean"},{"text":"class RegrSXY(y, x, filter=None, default=None)  \nReturns sum(x*y) - sum(x) * sum(y)\/N (\u201csum of products\u201d of independent times dependent variable) as a float, or default if there aren\u2019t any matching rows.","title":"django.ref.contrib.postgres.aggregates#django.contrib.postgres.aggregates.RegrSXY"},{"text":"class Avg(expression, output_field=None, distinct=False, filter=None, default=None, **extra)  \nReturns the mean value of the given expression, which must be numeric unless you specify a different output_field.  Default alias: <field>__avg\n Return type: float if input is int, otherwise same as input field, or output_field if supplied  Has one optional argument:  \ndistinct  \nIf distinct=True, Avg returns the mean value of unique values. This is the SQL equivalent of AVG(DISTINCT <field>). The default value is False.","title":"django.ref.models.querysets#django.db.models.Avg"},{"text":"tf.keras.layers.AveragePooling2D     View source on GitHub    Average pooling operation for spatial data. Inherits From: Layer, Module  View aliases  Main aliases \ntf.keras.layers.AvgPool2D Compat aliases for migration See Migration guide for more details. tf.compat.v1.keras.layers.AveragePooling2D, tf.compat.v1.keras.layers.AvgPool2D  \ntf.keras.layers.AveragePooling2D(\n    pool_size=(2, 2), strides=None, padding='valid', data_format=None,\n    **kwargs\n)\n\n \n\n\n Arguments\n  pool_size   integer or tuple of 2 integers, factors by which to downscale (vertical, horizontal). (2, 2) will halve the input in both spatial dimension. If only one integer is specified, the same window length will be used for both dimensions.  \n  strides   Integer, tuple of 2 integers, or None. Strides values. If None, it will default to pool_size.  \n  padding   One of \"valid\" or \"same\" (case-insensitive). \"valid\" means no padding. \"same\" results in padding evenly to the left\/right or up\/down of the input such that output has the same height\/width dimension as the input.  \n  data_format   A string, one of channels_last (default) or channels_first. The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width). It defaults to the image_data_format value found in your Keras config file at ~\/.keras\/keras.json. If you never set it, then it will be \"channels_last\".    Input shape:  If data_format='channels_last': 4D tensor with shape (batch_size, rows, cols, channels). If data_format='channels_first': 4D tensor with shape (batch_size, channels, rows, cols).  Output shape:  If data_format='channels_last': 4D tensor with shape (batch_size, pooled_rows, pooled_cols, channels). If data_format='channels_first': 4D tensor with shape (batch_size, channels, pooled_rows, pooled_cols).","title":"tensorflow.keras.layers.averagepooling2d"},{"text":"tf.math.segment_mean   Computes the mean along segments of a tensor.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.math.segment_mean, tf.compat.v1.segment_mean  \ntf.math.segment_mean(\n    data, segment_ids, name=None\n)\n Read the section on segmentation for an explanation of segments. Computes a tensor such that \\(output_i = \\frac{\\sum_j data_j}{N}\\) where mean is over j such that segment_ids[j] == i and N is the total number of values summed. If the mean is empty for a given segment ID i, output[i] = 0.    For example: c = tf.constant([[1.0,2,3,4], [4, 3, 2, 1], [5,6,7,8]])\ntf.segment_mean(c, tf.constant([0, 0, 1]))\n# ==> [[2.5, 2.5, 2.5, 2.5],\n#      [5, 6, 7, 8]]\n\n \n\n\n Args\n  data   A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.  \n  segment_ids   A Tensor. Must be one of the following types: int32, int64. A 1-D tensor whose size is equal to the size of data's first dimension. Values should be sorted and can be repeated.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor. Has the same type as data.","title":"tensorflow.math.segment_mean"}]}
{"task_id":18886596,"prompt":"def f_18886596(s):\n\treturn ","suffix":"","canonical_solution":"s.replace('\"', '\\\"')","test_start":"\ndef check(candidate):","test":["\n    s = 'This sentence has some \"quotes\" in it'\n    assert candidate(s) == 'This sentence has some \\\"quotes\\\" in it'\n"],"entry_point":"f_18886596","intent":"double backslash escape all double quotes in string `s`","library":[],"docs":[]}
{"task_id":5932059,"prompt":"def f_5932059(s):\n\treturn ","suffix":"","canonical_solution":"re.split('(\\\\W+)', s)","test_start":"\nimport re \n\ndef check(candidate):","test":["\n    s = \"this is  a\\nsentence\"\n    assert candidate(s) == ['this', ' ', 'is', '  ', 'a', '\\n', 'sentence']\n"],"entry_point":"f_5932059","intent":"split a string `s` into a list of words and whitespace","library":["re"],"docs":[{"text":"shlex.split(s, comments=False, posix=True)  \nSplit the string s using shell-like syntax. If comments is False (the default), the parsing of comments in the given string will be disabled (setting the commenters attribute of the shlex instance to the empty string). This function operates in POSIX mode by default, but uses non-POSIX mode if the posix argument is false.  Note Since the split() function instantiates a shlex instance, passing None for s will read the string to split from standard input.   Deprecated since version 3.9: Passing None for s will raise an exception in future Python versions.","title":"python.library.shlex#shlex.split"},{"text":"str.split(sep=None, maxsplit=-1)  \nReturn a list of the words in the string, using sep as the delimiter string. If maxsplit is given, at most maxsplit splits are done (thus, the list will have at most maxsplit+1 elements). If maxsplit is not specified or -1, then there is no limit on the number of splits (all possible splits are made). If sep is given, consecutive delimiters are not grouped together and are deemed to delimit empty strings (for example, '1,,2'.split(',') returns ['1', '', '2']). The sep argument may consist of multiple characters (for example, '1<>2<>3'.split('<>') returns ['1', '2', '3']). Splitting an empty string with a specified separator returns ['']. For example: >>> '1,2,3'.split(',')\n['1', '2', '3']\n>>> '1,2,3'.split(',', maxsplit=1)\n['1', '2,3']\n>>> '1,2,,3,'.split(',')\n['1', '2', '', '3', '']\n If sep is not specified or is None, a different splitting algorithm is applied: runs of consecutive whitespace are regarded as a single separator, and the result will contain no empty strings at the start or end if the string has leading or trailing whitespace. Consequently, splitting an empty string or a string consisting of just whitespace with a None separator returns []. For example: >>> '1 2 3'.split()\n['1', '2', '3']\n>>> '1 2 3'.split(maxsplit=1)\n['1', '2 3']\n>>> '   1   2   3   '.split()\n['1', '2', '3']","title":"python.library.stdtypes#str.split"},{"text":"HttpRequest.META  \nA dictionary containing all available HTTP headers. Available headers depend on the client and server, but here are some examples:  \nCONTENT_LENGTH \u2013 The length of the request body (as a string). \nCONTENT_TYPE \u2013 The MIME type of the request body. \nHTTP_ACCEPT \u2013 Acceptable content types for the response. \nHTTP_ACCEPT_ENCODING \u2013 Acceptable encodings for the response. \nHTTP_ACCEPT_LANGUAGE \u2013 Acceptable languages for the response. \nHTTP_HOST \u2013 The HTTP Host header sent by the client. \nHTTP_REFERER \u2013 The referring page, if any. \nHTTP_USER_AGENT \u2013 The client\u2019s user-agent string. \nQUERY_STRING \u2013 The query string, as a single (unparsed) string. \nREMOTE_ADDR \u2013 The IP address of the client. \nREMOTE_HOST \u2013 The hostname of the client. \nREMOTE_USER \u2013 The user authenticated by the web server, if any. \nREQUEST_METHOD \u2013 A string such as \"GET\" or \"POST\". \nSERVER_NAME \u2013 The hostname of the server. \nSERVER_PORT \u2013 The port of the server (as a string).  With the exception of CONTENT_LENGTH and CONTENT_TYPE, as given above, any HTTP headers in the request are converted to META keys by converting all characters to uppercase, replacing any hyphens with underscores and adding an HTTP_ prefix to the name. So, for example, a header called X-Bender would be mapped to the META key HTTP_X_BENDER. Note that runserver strips all headers with underscores in the name, so you won\u2019t see them in META. This prevents header-spoofing based on ambiguity between underscores and dashes both being normalizing to underscores in WSGI environment variables. It matches the behavior of web servers like Nginx and Apache 2.4+. HttpRequest.headers is a simpler way to access all HTTP-prefixed headers, plus CONTENT_LENGTH and CONTENT_TYPE.","title":"django.ref.request-response#django.http.HttpRequest.META"},{"text":"flask.escape()  \nReplace the characters &, <, >, ', and \" in the string with HTML-safe sequences. Use this if you need to display text that might contain such characters in HTML. If the object has an __html__ method, it is called and the return value is assumed to already be safe for HTML.  Parameters \ns \u2013 An object to be converted to a string and escaped.  Returns \nA Markup string with the escaped text.","title":"flask.api.index#flask.escape"},{"text":"str.rsplit(sep=None, maxsplit=-1)  \nReturn a list of the words in the string, using sep as the delimiter string. If maxsplit is given, at most maxsplit splits are done, the rightmost ones. If sep is not specified or None, any whitespace string is a separator. Except for splitting from the right, rsplit() behaves like split() which is described in detail below.","title":"python.library.stdtypes#str.rsplit"},{"text":"point_at_t(t)[source]\n \nEvaluate the curve at a single point, returning a tuple of d floats.","title":"matplotlib.bezier_api#matplotlib.bezier.BezierSegment.point_at_t"},{"text":"werkzeug.utils.escape(s)  \nReplace &, <, >, \", and ' with HTML-safe sequences. None is escaped to an empty string.  Deprecated since version 2.0: Will be removed in Werkzeug 2.1. Use MarkupSafe instead.   Parameters \ns (Any) \u2013   Return type \nstr","title":"werkzeug.utils.index#werkzeug.utils.escape"},{"text":"shlex.whitespace  \nCharacters that will be considered whitespace and skipped. Whitespace bounds tokens. By default, includes space, tab, linefeed and carriage-return.","title":"python.library.shlex#shlex.shlex.whitespace"},{"text":"str.splitlines([keepends])  \nReturn a list of the lines in the string, breaking at line boundaries. Line breaks are not included in the resulting list unless keepends is given and true. This method splits on the following line boundaries. In particular, the boundaries are a superset of universal newlines.   \nRepresentation Description   \n\\n Line Feed  \n\\r Carriage Return  \n\\r\\n Carriage Return + Line Feed  \n\\v or \\x0b Line Tabulation  \n\\f or \\x0c Form Feed  \n\\x1c File Separator  \n\\x1d Group Separator  \n\\x1e Record Separator  \n\\x85 Next Line (C1 Control Code)  \n\\u2028 Line Separator  \n\\u2029 Paragraph Separator    Changed in version 3.2: \\v and \\f added to list of line boundaries.  For example: >>> 'ab c\\n\\nde fg\\rkl\\r\\n'.splitlines()\n['ab c', '', 'de fg', 'kl']\n>>> 'ab c\\n\\nde fg\\rkl\\r\\n'.splitlines(keepends=True)\n['ab c\\n', '\\n', 'de fg\\r', 'kl\\r\\n']\n Unlike split() when a delimiter string sep is given, this method returns an empty list for the empty string, and a terminal line break does not result in an extra line: >>> \"\".splitlines()\n[]\n>>> \"One line\\n\".splitlines()\n['One line']\n For comparison, split('\\n') gives: >>> ''.split('\\n')\n['']\n>>> 'Two lines\\n'.split('\\n')\n['Two lines', '']","title":"python.library.stdtypes#str.splitlines"},{"text":"shlex.whitespace_split  \nIf True, tokens will only be split in whitespaces. This is useful, for example, for parsing command lines with shlex, getting tokens in a similar way to shell arguments. When used in combination with punctuation_chars, tokens will be split on whitespace in addition to those characters.  Changed in version 3.8: The punctuation_chars attribute was made compatible with the whitespace_split attribute.","title":"python.library.shlex#shlex.shlex.whitespace_split"}]}
{"task_id":9938130,"prompt":"def f_9938130(df):\n\treturn ","suffix":"","canonical_solution":"df.plot(kind='barh', stacked=True)","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    data = [[1, 3], [2, 5], [4, 8]]\n    df = pd.DataFrame(data, columns = ['a', 'b'])\n    assert str(candidate(df)).split('(')[0] == 'AxesSubplot'\n"],"entry_point":"f_9938130","intent":"plotting stacked barplots on a panda data frame `df`","library":["pandas"],"docs":[{"text":"matplotlib.axes.Axes.stackplot   Axes.stackplot(x, *args, labels=(), colors=None, baseline='zero', data=None, **kwargs)[source]\n \nDraw a stacked area plot.  Parameters \n \nx(N,) array-like\n\n\ny(M, N) array-like\n\n\nThe data is assumed to be unstacked. Each of the following calls is legal: stackplot(x, y)           # where y has shape (M, N)\nstackplot(x, y1, y2, y3)  # where y1, y2, y3, y4 have length N\n  \nbaseline{'zero', 'sym', 'wiggle', 'weighted_wiggle'}\n\n\nMethod used to calculate the baseline:  \n'zero': Constant zero baseline, i.e. a simple stacked plot. \n'sym': Symmetric around zero and is sometimes called 'ThemeRiver'. \n'wiggle': Minimizes the sum of the squared slopes. \n'weighted_wiggle': Does the same but weights to account for size of each layer. It is also called 'Streamgraph'-layout. More details can be found at http:\/\/leebyron.com\/streamgraph\/.   \nlabelslist of str, optional\n\n\nA sequence of labels to assign to each data series. If unspecified, then no labels will be applied to artists.  \ncolorslist of color, optional\n\n\nA sequence of colors to be cycled through and used to color the stacked areas. The sequence need not be exactly the same length as the number of provided y, in which case the colors will repeat from the beginning. If not specified, the colors from the Axes property cycle will be used.  \ndataindexable object, optional\n\n\nIf given, all parameters also accept a string s, which is interpreted as data[s] (unless this raises an exception).  **kwargs\n\nAll other keyword arguments are passed to Axes.fill_between.    Returns \n list of PolyCollection\n\n\nA list of PolyCollection instances, one for each element in the stacked area plot.     \n  Examples using matplotlib.axes.Axes.stackplot\n \n   Stackplots and streamgraphs","title":"matplotlib._as_gen.matplotlib.axes.axes.stackplot"},{"text":"matplotlib.pyplot.stackplot   matplotlib.pyplot.stackplot(x, *args, labels=(), colors=None, baseline='zero', data=None, **kwargs)[source]\n \nDraw a stacked area plot.  Parameters \n \nx(N,) array-like\n\n\ny(M, N) array-like\n\n\nThe data is assumed to be unstacked. Each of the following calls is legal: stackplot(x, y)           # where y has shape (M, N)\nstackplot(x, y1, y2, y3)  # where y1, y2, y3, y4 have length N\n  \nbaseline{'zero', 'sym', 'wiggle', 'weighted_wiggle'}\n\n\nMethod used to calculate the baseline:  \n'zero': Constant zero baseline, i.e. a simple stacked plot. \n'sym': Symmetric around zero and is sometimes called 'ThemeRiver'. \n'wiggle': Minimizes the sum of the squared slopes. \n'weighted_wiggle': Does the same but weights to account for size of each layer. It is also called 'Streamgraph'-layout. More details can be found at http:\/\/leebyron.com\/streamgraph\/.   \nlabelslist of str, optional\n\n\nA sequence of labels to assign to each data series. If unspecified, then no labels will be applied to artists.  \ncolorslist of color, optional\n\n\nA sequence of colors to be cycled through and used to color the stacked areas. The sequence need not be exactly the same length as the number of provided y, in which case the colors will repeat from the beginning. If not specified, the colors from the Axes property cycle will be used.  \ndataindexable object, optional\n\n\nIf given, all parameters also accept a string s, which is interpreted as data[s] (unless this raises an exception).  **kwargs\n\nAll other keyword arguments are passed to Axes.fill_between.    Returns \n list of PolyCollection\n\n\nA list of PolyCollection instances, one for each element in the stacked area plot.","title":"matplotlib._as_gen.matplotlib.pyplot.stackplot"},{"text":"pandas.DataFrame.plot.bar   DataFrame.plot.bar(x=None, y=None, **kwargs)[source]\n \nVertical bar plot. A bar plot is a plot that presents categorical data with rectangular bars with lengths proportional to the values that they represent. A bar plot shows comparisons among discrete categories. One axis of the plot shows the specific categories being compared, and the other axis represents a measured value.  Parameters \n \nx:label or position, optional\n\n\nAllows plotting of one column versus another. If not specified, the index of the DataFrame is used.  \ny:label or position, optional\n\n\nAllows plotting of one column versus another. If not specified, all numerical columns are used.  \ncolor:str, array-like, or dict, optional\n\n\nThe color for each of the DataFrame\u2019s columns. Possible values are:  \n A single color string referred to by name, RGB or RGBA code,\n\nfor instance \u2018red\u2019 or \u2018#a98d19\u2019.    \n A sequence of color strings referred to by name, RGB or RGBA\n\ncode, which will be used for each column recursively. For instance [\u2018green\u2019,\u2019yellow\u2019] each column\u2019s bar will be filled in green or yellow, alternatively. If there is only a single column to be plotted, then only the first color from the color list will be used.    \n A dict of the form {column name:color}, so that each column will be\n\n\ncolored accordingly. For example, if your columns are called a and b, then passing {\u2018a\u2019: \u2018green\u2019, \u2018b\u2019: \u2018red\u2019} will color bars for column a in green and bars for column b in red.      New in version 1.1.0.   **kwargs\n\nAdditional keyword arguments are documented in DataFrame.plot().    Returns \n matplotlib.axes.Axes or np.ndarray of them\n\nAn ndarray is returned with one matplotlib.axes.Axes per column when subplots=True.      See also  DataFrame.plot.barh\n\nHorizontal bar plot.  DataFrame.plot\n\nMake plots of a DataFrame.  matplotlib.pyplot.bar\n\nMake a bar plot with matplotlib.    Examples Basic plot. \n>>> df = pd.DataFrame({'lab':['A', 'B', 'C'], 'val':[10, 30, 20]})\n>>> ax = df.plot.bar(x='lab', y='val', rot=0)\n     Plot a whole dataframe to a bar plot. Each column is assigned a distinct color, and each row is nested in a group along the horizontal axis. \n>>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n>>> index = ['snail', 'pig', 'elephant',\n...          'rabbit', 'giraffe', 'coyote', 'horse']\n>>> df = pd.DataFrame({'speed': speed,\n...                    'lifespan': lifespan}, index=index)\n>>> ax = df.plot.bar(rot=0)\n     Plot stacked bar charts for the DataFrame \n>>> ax = df.plot.bar(stacked=True)\n     Instead of nesting, the figure can be split by column with subplots=True. In this case, a numpy.ndarray of matplotlib.axes.Axes are returned. \n>>> axes = df.plot.bar(rot=0, subplots=True)\n>>> axes[1].legend(loc=2)  \n     If you don\u2019t like the default colours, you can specify how you\u2019d like each column to be colored. \n>>> axes = df.plot.bar(\n...     rot=0, subplots=True, color={\"speed\": \"red\", \"lifespan\": \"green\"}\n... )\n>>> axes[1].legend(loc=2)  \n     Plot a single column. \n>>> ax = df.plot.bar(y='speed', rot=0)\n     Plot only selected categories for the DataFrame. \n>>> ax = df.plot.bar(x='lifespan', rot=0)","title":"pandas.reference.api.pandas.dataframe.plot.bar"},{"text":"pandas.DataFrame.plot.barh   DataFrame.plot.barh(x=None, y=None, **kwargs)[source]\n \nMake a horizontal bar plot. A horizontal bar plot is a plot that presents quantitative data with rectangular bars with lengths proportional to the values that they represent. A bar plot shows comparisons among discrete categories. One axis of the plot shows the specific categories being compared, and the other axis represents a measured value.  Parameters \n \nx:label or position, optional\n\n\nAllows plotting of one column versus another. If not specified, the index of the DataFrame is used.  \ny:label or position, optional\n\n\nAllows plotting of one column versus another. If not specified, all numerical columns are used.  \ncolor:str, array-like, or dict, optional\n\n\nThe color for each of the DataFrame\u2019s columns. Possible values are:  \n A single color string referred to by name, RGB or RGBA code,\n\nfor instance \u2018red\u2019 or \u2018#a98d19\u2019.    \n A sequence of color strings referred to by name, RGB or RGBA\n\ncode, which will be used for each column recursively. For instance [\u2018green\u2019,\u2019yellow\u2019] each column\u2019s bar will be filled in green or yellow, alternatively. If there is only a single column to be plotted, then only the first color from the color list will be used.    \n A dict of the form {column name:color}, so that each column will be\n\n\ncolored accordingly. For example, if your columns are called a and b, then passing {\u2018a\u2019: \u2018green\u2019, \u2018b\u2019: \u2018red\u2019} will color bars for column a in green and bars for column b in red.      New in version 1.1.0.   **kwargs\n\nAdditional keyword arguments are documented in DataFrame.plot().    Returns \n matplotlib.axes.Axes or np.ndarray of them\n\nAn ndarray is returned with one matplotlib.axes.Axes per column when subplots=True.      See also  DataFrame.plot.bar\n\nVertical bar plot.  DataFrame.plot\n\nMake plots of DataFrame using matplotlib.  matplotlib.axes.Axes.bar\n\nPlot a vertical bar plot using matplotlib.    Examples Basic example \n>>> df = pd.DataFrame({'lab': ['A', 'B', 'C'], 'val': [10, 30, 20]})\n>>> ax = df.plot.barh(x='lab', y='val')\n     Plot a whole DataFrame to a horizontal bar plot \n>>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n>>> index = ['snail', 'pig', 'elephant',\n...          'rabbit', 'giraffe', 'coyote', 'horse']\n>>> df = pd.DataFrame({'speed': speed,\n...                    'lifespan': lifespan}, index=index)\n>>> ax = df.plot.barh()\n     Plot stacked barh charts for the DataFrame \n>>> ax = df.plot.barh(stacked=True)\n     We can specify colors for each column \n>>> ax = df.plot.barh(color={\"speed\": \"red\", \"lifespan\": \"green\"})\n     Plot a column of the DataFrame to a horizontal bar plot \n>>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n>>> index = ['snail', 'pig', 'elephant',\n...          'rabbit', 'giraffe', 'coyote', 'horse']\n>>> df = pd.DataFrame({'speed': speed,\n...                    'lifespan': lifespan}, index=index)\n>>> ax = df.plot.barh(y='speed')\n     Plot DataFrame versus the desired column \n>>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n>>> index = ['snail', 'pig', 'elephant',\n...          'rabbit', 'giraffe', 'coyote', 'horse']\n>>> df = pd.DataFrame({'speed': speed,\n...                    'lifespan': lifespan}, index=index)\n>>> ax = df.plot.barh(x='lifespan')","title":"pandas.reference.api.pandas.dataframe.plot.barh"},{"text":"pandas.Series.plot.bar   Series.plot.bar(x=None, y=None, **kwargs)[source]\n \nVertical bar plot. A bar plot is a plot that presents categorical data with rectangular bars with lengths proportional to the values that they represent. A bar plot shows comparisons among discrete categories. One axis of the plot shows the specific categories being compared, and the other axis represents a measured value.  Parameters \n \nx:label or position, optional\n\n\nAllows plotting of one column versus another. If not specified, the index of the DataFrame is used.  \ny:label or position, optional\n\n\nAllows plotting of one column versus another. If not specified, all numerical columns are used.  \ncolor:str, array-like, or dict, optional\n\n\nThe color for each of the DataFrame\u2019s columns. Possible values are:  \n A single color string referred to by name, RGB or RGBA code,\n\nfor instance \u2018red\u2019 or \u2018#a98d19\u2019.    \n A sequence of color strings referred to by name, RGB or RGBA\n\ncode, which will be used for each column recursively. For instance [\u2018green\u2019,\u2019yellow\u2019] each column\u2019s bar will be filled in green or yellow, alternatively. If there is only a single column to be plotted, then only the first color from the color list will be used.    \n A dict of the form {column name:color}, so that each column will be\n\n\ncolored accordingly. For example, if your columns are called a and b, then passing {\u2018a\u2019: \u2018green\u2019, \u2018b\u2019: \u2018red\u2019} will color bars for column a in green and bars for column b in red.      New in version 1.1.0.   **kwargs\n\nAdditional keyword arguments are documented in DataFrame.plot().    Returns \n matplotlib.axes.Axes or np.ndarray of them\n\nAn ndarray is returned with one matplotlib.axes.Axes per column when subplots=True.      See also  DataFrame.plot.barh\n\nHorizontal bar plot.  DataFrame.plot\n\nMake plots of a DataFrame.  matplotlib.pyplot.bar\n\nMake a bar plot with matplotlib.    Examples Basic plot. \n>>> df = pd.DataFrame({'lab':['A', 'B', 'C'], 'val':[10, 30, 20]})\n>>> ax = df.plot.bar(x='lab', y='val', rot=0)\n     Plot a whole dataframe to a bar plot. Each column is assigned a distinct color, and each row is nested in a group along the horizontal axis. \n>>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n>>> index = ['snail', 'pig', 'elephant',\n...          'rabbit', 'giraffe', 'coyote', 'horse']\n>>> df = pd.DataFrame({'speed': speed,\n...                    'lifespan': lifespan}, index=index)\n>>> ax = df.plot.bar(rot=0)\n     Plot stacked bar charts for the DataFrame \n>>> ax = df.plot.bar(stacked=True)\n     Instead of nesting, the figure can be split by column with subplots=True. In this case, a numpy.ndarray of matplotlib.axes.Axes are returned. \n>>> axes = df.plot.bar(rot=0, subplots=True)\n>>> axes[1].legend(loc=2)  \n     If you don\u2019t like the default colours, you can specify how you\u2019d like each column to be colored. \n>>> axes = df.plot.bar(\n...     rot=0, subplots=True, color={\"speed\": \"red\", \"lifespan\": \"green\"}\n... )\n>>> axes[1].legend(loc=2)  \n     Plot a single column. \n>>> ax = df.plot.bar(y='speed', rot=0)\n     Plot only selected categories for the DataFrame. \n>>> ax = df.plot.bar(x='lifespan', rot=0)","title":"pandas.reference.api.pandas.series.plot.bar"},{"text":"pandas.Series.plot.barh   Series.plot.barh(x=None, y=None, **kwargs)[source]\n \nMake a horizontal bar plot. A horizontal bar plot is a plot that presents quantitative data with rectangular bars with lengths proportional to the values that they represent. A bar plot shows comparisons among discrete categories. One axis of the plot shows the specific categories being compared, and the other axis represents a measured value.  Parameters \n \nx:label or position, optional\n\n\nAllows plotting of one column versus another. If not specified, the index of the DataFrame is used.  \ny:label or position, optional\n\n\nAllows plotting of one column versus another. If not specified, all numerical columns are used.  \ncolor:str, array-like, or dict, optional\n\n\nThe color for each of the DataFrame\u2019s columns. Possible values are:  \n A single color string referred to by name, RGB or RGBA code,\n\nfor instance \u2018red\u2019 or \u2018#a98d19\u2019.    \n A sequence of color strings referred to by name, RGB or RGBA\n\ncode, which will be used for each column recursively. For instance [\u2018green\u2019,\u2019yellow\u2019] each column\u2019s bar will be filled in green or yellow, alternatively. If there is only a single column to be plotted, then only the first color from the color list will be used.    \n A dict of the form {column name:color}, so that each column will be\n\n\ncolored accordingly. For example, if your columns are called a and b, then passing {\u2018a\u2019: \u2018green\u2019, \u2018b\u2019: \u2018red\u2019} will color bars for column a in green and bars for column b in red.      New in version 1.1.0.   **kwargs\n\nAdditional keyword arguments are documented in DataFrame.plot().    Returns \n matplotlib.axes.Axes or np.ndarray of them\n\nAn ndarray is returned with one matplotlib.axes.Axes per column when subplots=True.      See also  DataFrame.plot.bar\n\nVertical bar plot.  DataFrame.plot\n\nMake plots of DataFrame using matplotlib.  matplotlib.axes.Axes.bar\n\nPlot a vertical bar plot using matplotlib.    Examples Basic example \n>>> df = pd.DataFrame({'lab': ['A', 'B', 'C'], 'val': [10, 30, 20]})\n>>> ax = df.plot.barh(x='lab', y='val')\n     Plot a whole DataFrame to a horizontal bar plot \n>>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n>>> index = ['snail', 'pig', 'elephant',\n...          'rabbit', 'giraffe', 'coyote', 'horse']\n>>> df = pd.DataFrame({'speed': speed,\n...                    'lifespan': lifespan}, index=index)\n>>> ax = df.plot.barh()\n     Plot stacked barh charts for the DataFrame \n>>> ax = df.plot.barh(stacked=True)\n     We can specify colors for each column \n>>> ax = df.plot.barh(color={\"speed\": \"red\", \"lifespan\": \"green\"})\n     Plot a column of the DataFrame to a horizontal bar plot \n>>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n>>> index = ['snail', 'pig', 'elephant',\n...          'rabbit', 'giraffe', 'coyote', 'horse']\n>>> df = pd.DataFrame({'speed': speed,\n...                    'lifespan': lifespan}, index=index)\n>>> ax = df.plot.barh(y='speed')\n     Plot DataFrame versus the desired column \n>>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n>>> index = ['snail', 'pig', 'elephant',\n...          'rabbit', 'giraffe', 'coyote', 'horse']\n>>> df = pd.DataFrame({'speed': speed,\n...                    'lifespan': lifespan}, index=index)\n>>> ax = df.plot.barh(x='lifespan')","title":"pandas.reference.api.pandas.series.plot.barh"},{"text":"class sklearn.ensemble.StackingRegressor(estimators, final_estimator=None, *, cv=None, n_jobs=None, passthrough=False, verbose=0) [source]\n \nStack of estimators with a final regressor. Stacked generalization consists in stacking the output of individual estimator and use a regressor to compute the final prediction. Stacking allows to use the strength of each individual estimator by using their output as input of a final estimator. Note that estimators_ are fitted on the full X while final_estimator_ is trained using cross-validated predictions of the base estimators using cross_val_predict. Read more in the User Guide.  New in version 0.22.   Parameters \n \nestimatorslist of (str, estimator) \n\nBase estimators which will be stacked together. Each element of the list is defined as a tuple of string (i.e. name) and an estimator instance. An estimator can be set to \u2018drop\u2019 using set_params.  \nfinal_estimatorestimator, default=None \n\nA regressor which will be used to combine the base estimators. The default regressor is a RidgeCV.  \ncvint, cross-validation generator or an iterable, default=None \n\nDetermines the cross-validation splitting strategy used in cross_val_predict to train final_estimator. Possible inputs for cv are:  None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified) KFold, An object to be used as a cross-validation generator, An iterable yielding train, test splits.  For integer\/None inputs, if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used. In all other cases, KFold is used. Refer User Guide for the various cross-validation strategies that can be used here.  Note A larger number of split will provide no benefits if the number of training samples is large enough. Indeed, the training time will increase. cv is not used for model evaluation but for prediction.   \nn_jobsint, default=None \n\nThe number of jobs to run in parallel for fit of all estimators. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details.  \npassthroughbool, default=False \n\nWhen False, only the predictions of estimators will be used as training data for final_estimator. When True, the final_estimator is trained on the predictions as well as the original training data.  \nverboseint, default=0 \n\nVerbosity level.    Attributes \n \nestimators_list of estimator \n\nThe elements of the estimators parameter, having been fitted on the training data. If an estimator has been set to 'drop', it will not appear in estimators_.  \nnamed_estimators_Bunch \n\nAttribute to access any fitted sub-estimators by name.  \nfinal_estimator_estimator \n\nThe regressor to stacked the base estimators fitted.     References  \n1  \nWolpert, David H. \u201cStacked generalization.\u201d Neural networks 5.2 (1992): 241-259.   Examples >>> from sklearn.datasets import load_diabetes\n>>> from sklearn.linear_model import RidgeCV\n>>> from sklearn.svm import LinearSVR\n>>> from sklearn.ensemble import RandomForestRegressor\n>>> from sklearn.ensemble import StackingRegressor\n>>> X, y = load_diabetes(return_X_y=True)\n>>> estimators = [\n...     ('lr', RidgeCV()),\n...     ('svr', LinearSVR(random_state=42))\n... ]\n>>> reg = StackingRegressor(\n...     estimators=estimators,\n...     final_estimator=RandomForestRegressor(n_estimators=10,\n...                                           random_state=42)\n... )\n>>> from sklearn.model_selection import train_test_split\n>>> X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, random_state=42\n... )\n>>> reg.fit(X_train, y_train).score(X_test, y_test)\n0.3...\n Methods  \nfit(X, y[, sample_weight]) Fit the estimators.  \nfit_transform(X[, y]) Fit to data, then transform it.  \nget_params([deep]) Get the parameters of an estimator from the ensemble.  \npredict(X, **predict_params) Predict target for X.  \nscore(X, y[, sample_weight]) Return the coefficient of determination \\(R^2\\) of the prediction.  \nset_params(**params) Set the parameters of an estimator from the ensemble.  \ntransform(X) Return the predictions for X for each estimator.    \nfit(X, y, sample_weight=None) [source]\n \nFit the estimators.  Parameters \n \nX{array-like, sparse matrix} of shape (n_samples, n_features) \n\nTraining vectors, where n_samples is the number of samples and n_features is the number of features.  \nyarray-like of shape (n_samples,) \n\nTarget values.  \nsample_weightarray-like of shape (n_samples,), default=None \n\nSample weights. If None, then samples are equally weighted. Note that this is supported only if all underlying estimators support sample weights.    Returns \n \nselfobject \n   \n  \nfit_transform(X, y=None, **fit_params) [source]\n \nFit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.  Parameters \n \nXarray-like of shape (n_samples, n_features) \n\nInput samples.  \nyarray-like of shape (n_samples,) or (n_samples, n_outputs), default=None \n\nTarget values (None for unsupervised transformations).  \n**fit_paramsdict \n\nAdditional fit parameters.    Returns \n \nX_newndarray array of shape (n_samples, n_features_new) \n\nTransformed array.     \n  \nget_params(deep=True) [source]\n \nGet the parameters of an estimator from the ensemble. Returns the parameters given in the constructor as well as the estimators contained within the estimators parameter.  Parameters \n \ndeepbool, default=True \n\nSetting it to True gets the various estimators and the parameters of the estimators as well.     \n  \nproperty n_features_in_  \nNumber of features seen during fit. \n  \npredict(X, **predict_params) [source]\n \nPredict target for X.  Parameters \n \nX{array-like, sparse matrix} of shape (n_samples, n_features) \n\nTraining vectors, where n_samples is the number of samples and n_features is the number of features.  \n**predict_paramsdict of str -> obj \n\nParameters to the predict called by the final_estimator. Note that this may be used to return uncertainties from some estimators with return_std or return_cov. Be aware that it will only accounts for uncertainty in the final estimator.    Returns \n \ny_predndarray of shape (n_samples,) or (n_samples, n_output) \n\nPredicted targets.     \n  \nscore(X, y, sample_weight=None) [source]\n \nReturn the coefficient of determination \\(R^2\\) of the prediction. The coefficient \\(R^2\\) is defined as \\((1 - \\frac{u}{v})\\), where \\(u\\) is the residual sum of squares ((y_true - y_pred)\n** 2).sum() and \\(v\\) is the total sum of squares ((y_true -\ny_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a \\(R^2\\) score of 0.0.  Parameters \n \nXarray-like of shape (n_samples, n_features) \n\nTest samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead with shape (n_samples, n_samples_fitted), where n_samples_fitted is the number of samples used in the fitting for the estimator.  \nyarray-like of shape (n_samples,) or (n_samples, n_outputs) \n\nTrue values for X.  \nsample_weightarray-like of shape (n_samples,), default=None \n\nSample weights.    Returns \n \nscorefloat \n\n\\(R^2\\) of self.predict(X) wrt. y.     Notes The \\(R^2\\) score used when calling score on a regressor uses multioutput='uniform_average' from version 0.23 to keep consistent with default value of r2_score. This influences the score method of all the multioutput regressors (except for MultiOutputRegressor). \n  \nset_params(**params) [source]\n \nSet the parameters of an estimator from the ensemble. Valid parameter keys can be listed with get_params(). Note that you can directly set the parameters of the estimators contained in estimators.  Parameters \n \n**paramskeyword arguments \n\nSpecific parameters using e.g. set_params(parameter_name=new_value). In addition, to setting the parameters of the estimator, the individual estimator of the estimators can also be set, or can be removed by setting them to \u2018drop\u2019.     \n  \ntransform(X) [source]\n \nReturn the predictions for X for each estimator.  Parameters \n \nX{array-like, sparse matrix} of shape (n_samples, n_features) \n\nTraining vectors, where n_samples is the number of samples and n_features is the number of features.    Returns \n \ny_predsndarray of shape (n_samples, n_estimators) \n\nPrediction outputs for each estimator.","title":"sklearn.modules.generated.sklearn.ensemble.stackingregressor#sklearn.ensemble.StackingRegressor"},{"text":"sklearn.ensemble.StackingRegressor  \nclass sklearn.ensemble.StackingRegressor(estimators, final_estimator=None, *, cv=None, n_jobs=None, passthrough=False, verbose=0) [source]\n \nStack of estimators with a final regressor. Stacked generalization consists in stacking the output of individual estimator and use a regressor to compute the final prediction. Stacking allows to use the strength of each individual estimator by using their output as input of a final estimator. Note that estimators_ are fitted on the full X while final_estimator_ is trained using cross-validated predictions of the base estimators using cross_val_predict. Read more in the User Guide.  New in version 0.22.   Parameters \n \nestimatorslist of (str, estimator) \n\nBase estimators which will be stacked together. Each element of the list is defined as a tuple of string (i.e. name) and an estimator instance. An estimator can be set to \u2018drop\u2019 using set_params.  \nfinal_estimatorestimator, default=None \n\nA regressor which will be used to combine the base estimators. The default regressor is a RidgeCV.  \ncvint, cross-validation generator or an iterable, default=None \n\nDetermines the cross-validation splitting strategy used in cross_val_predict to train final_estimator. Possible inputs for cv are:  None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified) KFold, An object to be used as a cross-validation generator, An iterable yielding train, test splits.  For integer\/None inputs, if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used. In all other cases, KFold is used. Refer User Guide for the various cross-validation strategies that can be used here.  Note A larger number of split will provide no benefits if the number of training samples is large enough. Indeed, the training time will increase. cv is not used for model evaluation but for prediction.   \nn_jobsint, default=None \n\nThe number of jobs to run in parallel for fit of all estimators. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details.  \npassthroughbool, default=False \n\nWhen False, only the predictions of estimators will be used as training data for final_estimator. When True, the final_estimator is trained on the predictions as well as the original training data.  \nverboseint, default=0 \n\nVerbosity level.    Attributes \n \nestimators_list of estimator \n\nThe elements of the estimators parameter, having been fitted on the training data. If an estimator has been set to 'drop', it will not appear in estimators_.  \nnamed_estimators_Bunch \n\nAttribute to access any fitted sub-estimators by name.  \nfinal_estimator_estimator \n\nThe regressor to stacked the base estimators fitted.     References  \n1  \nWolpert, David H. \u201cStacked generalization.\u201d Neural networks 5.2 (1992): 241-259.   Examples >>> from sklearn.datasets import load_diabetes\n>>> from sklearn.linear_model import RidgeCV\n>>> from sklearn.svm import LinearSVR\n>>> from sklearn.ensemble import RandomForestRegressor\n>>> from sklearn.ensemble import StackingRegressor\n>>> X, y = load_diabetes(return_X_y=True)\n>>> estimators = [\n...     ('lr', RidgeCV()),\n...     ('svr', LinearSVR(random_state=42))\n... ]\n>>> reg = StackingRegressor(\n...     estimators=estimators,\n...     final_estimator=RandomForestRegressor(n_estimators=10,\n...                                           random_state=42)\n... )\n>>> from sklearn.model_selection import train_test_split\n>>> X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, random_state=42\n... )\n>>> reg.fit(X_train, y_train).score(X_test, y_test)\n0.3...\n Methods  \nfit(X, y[, sample_weight]) Fit the estimators.  \nfit_transform(X[, y]) Fit to data, then transform it.  \nget_params([deep]) Get the parameters of an estimator from the ensemble.  \npredict(X, **predict_params) Predict target for X.  \nscore(X, y[, sample_weight]) Return the coefficient of determination \\(R^2\\) of the prediction.  \nset_params(**params) Set the parameters of an estimator from the ensemble.  \ntransform(X) Return the predictions for X for each estimator.    \nfit(X, y, sample_weight=None) [source]\n \nFit the estimators.  Parameters \n \nX{array-like, sparse matrix} of shape (n_samples, n_features) \n\nTraining vectors, where n_samples is the number of samples and n_features is the number of features.  \nyarray-like of shape (n_samples,) \n\nTarget values.  \nsample_weightarray-like of shape (n_samples,), default=None \n\nSample weights. If None, then samples are equally weighted. Note that this is supported only if all underlying estimators support sample weights.    Returns \n \nselfobject \n   \n  \nfit_transform(X, y=None, **fit_params) [source]\n \nFit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.  Parameters \n \nXarray-like of shape (n_samples, n_features) \n\nInput samples.  \nyarray-like of shape (n_samples,) or (n_samples, n_outputs), default=None \n\nTarget values (None for unsupervised transformations).  \n**fit_paramsdict \n\nAdditional fit parameters.    Returns \n \nX_newndarray array of shape (n_samples, n_features_new) \n\nTransformed array.     \n  \nget_params(deep=True) [source]\n \nGet the parameters of an estimator from the ensemble. Returns the parameters given in the constructor as well as the estimators contained within the estimators parameter.  Parameters \n \ndeepbool, default=True \n\nSetting it to True gets the various estimators and the parameters of the estimators as well.     \n  \nproperty n_features_in_  \nNumber of features seen during fit. \n  \npredict(X, **predict_params) [source]\n \nPredict target for X.  Parameters \n \nX{array-like, sparse matrix} of shape (n_samples, n_features) \n\nTraining vectors, where n_samples is the number of samples and n_features is the number of features.  \n**predict_paramsdict of str -> obj \n\nParameters to the predict called by the final_estimator. Note that this may be used to return uncertainties from some estimators with return_std or return_cov. Be aware that it will only accounts for uncertainty in the final estimator.    Returns \n \ny_predndarray of shape (n_samples,) or (n_samples, n_output) \n\nPredicted targets.     \n  \nscore(X, y, sample_weight=None) [source]\n \nReturn the coefficient of determination \\(R^2\\) of the prediction. The coefficient \\(R^2\\) is defined as \\((1 - \\frac{u}{v})\\), where \\(u\\) is the residual sum of squares ((y_true - y_pred)\n** 2).sum() and \\(v\\) is the total sum of squares ((y_true -\ny_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a \\(R^2\\) score of 0.0.  Parameters \n \nXarray-like of shape (n_samples, n_features) \n\nTest samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead with shape (n_samples, n_samples_fitted), where n_samples_fitted is the number of samples used in the fitting for the estimator.  \nyarray-like of shape (n_samples,) or (n_samples, n_outputs) \n\nTrue values for X.  \nsample_weightarray-like of shape (n_samples,), default=None \n\nSample weights.    Returns \n \nscorefloat \n\n\\(R^2\\) of self.predict(X) wrt. y.     Notes The \\(R^2\\) score used when calling score on a regressor uses multioutput='uniform_average' from version 0.23 to keep consistent with default value of r2_score. This influences the score method of all the multioutput regressors (except for MultiOutputRegressor). \n  \nset_params(**params) [source]\n \nSet the parameters of an estimator from the ensemble. Valid parameter keys can be listed with get_params(). Note that you can directly set the parameters of the estimators contained in estimators.  Parameters \n \n**paramskeyword arguments \n\nSpecific parameters using e.g. set_params(parameter_name=new_value). In addition, to setting the parameters of the estimator, the individual estimator of the estimators can also be set, or can be removed by setting them to \u2018drop\u2019.     \n  \ntransform(X) [source]\n \nReturn the predictions for X for each estimator.  Parameters \n \nX{array-like, sparse matrix} of shape (n_samples, n_features) \n\nTraining vectors, where n_samples is the number of samples and n_features is the number of features.    Returns \n \ny_predsndarray of shape (n_samples, n_estimators) \n\nPrediction outputs for each estimator.     \n \n Examples using sklearn.ensemble.StackingRegressor\n \n  Combine predictors using stacking","title":"sklearn.modules.generated.sklearn.ensemble.stackingregressor"},{"text":"pandas.core.groupby.DataFrameGroupBy.boxplot   DataFrameGroupBy.boxplot(subplots=True, column=None, fontsize=None, rot=0, grid=True, ax=None, figsize=None, layout=None, sharex=False, sharey=True, backend=None, **kwargs)[source]\n \nMake box plots from DataFrameGroupBy data.  Parameters \n \ngrouped:Grouped DataFrame\n\n\nsubplots:bool\n\n\n False - no subplots will be used True - create a subplot for each group.   \ncolumn:column name or list of names, or vector\n\n\nCan be any valid input to groupby.  \nfontsize:int or str\n\n\nrot:label rotation angle\n\n\ngrid:Setting this to True will show the grid\n\n\nax:Matplotlib axis object, default None\n\n\nfigsize:A tuple (width, height) in inches\n\n\nlayout:tuple (optional)\n\n\nThe layout of the plot: (rows, columns).  \nsharex:bool, default False\n\n\nWhether x-axes will be shared among subplots.  \nsharey:bool, default True\n\n\nWhether y-axes will be shared among subplots.  \nbackend:str, default None\n\n\nBackend to use instead of the backend specified in the option plotting.backend. For instance, \u2018matplotlib\u2019. Alternatively, to specify the plotting.backend for the whole session, set pd.options.plotting.backend.  New in version 1.0.0.   **kwargs\n\nAll other plotting keyword arguments to be passed to matplotlib\u2019s boxplot function.    Returns \n dict of key\/value = group key\/DataFrame.boxplot return value\nor DataFrame.boxplot return value in case subplots=figures=False\n   Examples You can create boxplots for grouped data and show them as separate subplots: \n>>> import itertools\n>>> tuples = [t for t in itertools.product(range(1000), range(4))]\n>>> index = pd.MultiIndex.from_tuples(tuples, names=['lvl0', 'lvl1'])\n>>> data = np.random.randn(len(index),4)\n>>> df = pd.DataFrame(data, columns=list('ABCD'), index=index)\n>>> grouped = df.groupby(level='lvl1')\n>>> grouped.boxplot(rot=45, fontsize=12, figsize=(8,10))  \n     The subplots=False option shows the boxplots in a single figure. \n>>> grouped.boxplot(subplots=False, rot=45, fontsize=12)","title":"pandas.reference.api.pandas.core.groupby.dataframegroupby.boxplot"},{"text":"predict(X, **predict_params) [source]\n \nPredict target for X.  Parameters \n \nX{array-like, sparse matrix} of shape (n_samples, n_features) \n\nTraining vectors, where n_samples is the number of samples and n_features is the number of features.  \n**predict_paramsdict of str -> obj \n\nParameters to the predict called by the final_estimator. Note that this may be used to return uncertainties from some estimators with return_std or return_cov. Be aware that it will only accounts for uncertainty in the final estimator.    Returns \n \ny_predndarray of shape (n_samples,) or (n_samples, n_output) \n\nPredicted targets.","title":"sklearn.modules.generated.sklearn.ensemble.stackingregressor#sklearn.ensemble.StackingRegressor.predict"}]}
{"task_id":35945473,"prompt":"def f_35945473(myDictionary):\n\treturn ","suffix":"","canonical_solution":"{i[1]: i[0] for i in list(myDictionary.items())}","test_start":"\ndef check(candidate):","test":["\n    assert candidate({'a' : 'b', 'c' : 'd'}) == {'b': 'a', 'd': 'c'}\n"],"entry_point":"f_35945473","intent":"reverse the keys and values in a dictionary `myDictionary`","library":[],"docs":[]}
{"task_id":30729735,"prompt":"def f_30729735(myList):\n\treturn ","suffix":"","canonical_solution":"[i for i, j in enumerate(myList) if 'how' in j.lower() or 'what' in j.lower()]","test_start":"\ndef check(candidate):","test":["\n    assert candidate(['abc', 'how', 'what', 'def']) == [1, 2]\n"],"entry_point":"f_30729735","intent":"finding the index of elements containing substring 'how' and 'what' in a list of strings 'myList'.","library":[],"docs":[]}
{"task_id":1303243,"prompt":"def f_1303243(obj):\n\treturn ","suffix":"","canonical_solution":"isinstance(obj, str)","test_start":"\ndef check(candidate):","test":["\n    assert candidate('python3') == True\n","\n    assert candidate(1.23) == False\n"],"entry_point":"f_1303243","intent":"check if object `obj` is a string","library":[],"docs":[]}
{"task_id":1303243,"prompt":"def f_1303243(o):\n\treturn ","suffix":"","canonical_solution":"isinstance(o, str)","test_start":"\ndef check(candidate):","test":["\n    assert candidate(\"hello\") == True\n","\n    assert candidate(123) == False\n","\n    assert candidate([]) == False\n","\n    assert candidate({\"aa\", \"v\"}) == False\n","\n    assert candidate(\"123\") == True\n"],"entry_point":"f_1303243","intent":"check if object `o` is a string","library":[],"docs":[]}
{"task_id":1303243,"prompt":"def f_1303243(o):\n\treturn ","suffix":"","canonical_solution":"(type(o) is str)","test_start":"\ndef check(candidate):","test":["\n    assert candidate(\"hello\") == True\n","\n    assert candidate(123) == False\n","\n    assert candidate([]) == False\n","\n    assert candidate({\"aa\", \"v\"}) == False\n","\n    assert candidate(\"123\") == True\n"],"entry_point":"f_1303243","intent":"check if object `o` is a string","library":[],"docs":[]}
{"task_id":1303243,"prompt":"def f_1303243(obj_to_test):\n\treturn ","suffix":"","canonical_solution":"isinstance(obj_to_test, str)","test_start":"\ndef check(candidate):","test":["\n    assert candidate(\"hello\") == True\n","\n    assert candidate(123) == False\n","\n    assert candidate([]) == False\n","\n    assert candidate({\"aa\", \"v\"}) == False\n","\n    assert candidate(\"123\") == True\n"],"entry_point":"f_1303243","intent":"check if `obj_to_test` is a string","library":[],"docs":[]}
{"task_id":8177079,"prompt":"def f_8177079(list1, list2):\n\t","suffix":"\n\treturn ","canonical_solution":"list2.extend(list1)","test_start":"\ndef check(candidate):","test":["\n    a, b = [1, 2, 3], [4, 5, 6]\n    candidate(a, b)\n    assert b == [4, 5, 6, 1, 2, 3]\n","\n    a, c = [1, 2, 3], [7, 8, 9]\n    candidate(a, c)\n    assert c == [7, 8, 9, 1, 2, 3] \n","\n    b = [4, 5, 6, 1, 2, 3]\n    c = [7, 8, 9, 1, 2, 3] \n    candidate(b, c)\n    assert c == [7, 8, 9, 1, 2, 3, 4, 5, 6, 1, 2, 3]\n"],"entry_point":"f_8177079","intent":"append list `list1` to `list2`","library":[],"docs":[]}
{"task_id":8177079,"prompt":"def f_8177079(mylog, list1):\n\t","suffix":"\n\treturn ","canonical_solution":"list1.extend(mylog)","test_start":"\ndef check(candidate):","test":["\n    a, b = [1, 2, 3], [4, 5, 6]\n    candidate(a, b)\n    assert b == [4, 5, 6, 1, 2, 3]\n","\n    a, c = [1, 2, 3], [7, 8, 9]\n    candidate(a, c)\n    assert c == [7, 8, 9, 1, 2, 3] \n","\n    b = [4, 5, 6, 1, 2, 3]\n    c = [7, 8, 9, 1, 2, 3] \n    candidate(b, c)\n    assert c == [7, 8, 9, 1, 2, 3, 4, 5, 6, 1, 2, 3]\n"],"entry_point":"f_8177079","intent":"append list `mylog` to `list1`","library":[],"docs":[]}
{"task_id":8177079,"prompt":"def f_8177079(a, c):\n\t","suffix":"\n\treturn ","canonical_solution":"c.extend(a)","test_start":"\ndef check(candidate):","test":["\n    a, b = [1, 2, 3], [4, 5, 6]\n    candidate(a, b)\n    assert b == [4, 5, 6, 1, 2, 3]\n","\n    a, c = [1, 2, 3], [7, 8, 9]\n    candidate(a, c)\n    assert c == [7, 8, 9, 1, 2, 3] \n","\n    b = [4, 5, 6, 1, 2, 3]\n    c = [7, 8, 9, 1, 2, 3] \n    candidate(b, c)\n    assert c == [7, 8, 9, 1, 2, 3, 4, 5, 6, 1, 2, 3]\n"],"entry_point":"f_8177079","intent":"append list `a` to `c`","library":[],"docs":[]}
{"task_id":8177079,"prompt":"def f_8177079(mylog, list1):\n\t","suffix":"\n\treturn ","canonical_solution":"for line in mylog:\n\t    list1.append(line)","test_start":"\ndef check(candidate):","test":["\n    a, b = [1, 2, 3], [4, 5, 6]\n    candidate(a, b)\n    assert b == [4, 5, 6, 1, 2, 3]\n","\n    a, c = [1, 2, 3], [7, 8, 9]\n    candidate(a, c)\n    assert c == [7, 8, 9, 1, 2, 3] \n","\n    b = [4, 5, 6, 1, 2, 3]\n    c = [7, 8, 9, 1, 2, 3] \n    candidate(b, c)\n    assert c == [7, 8, 9, 1, 2, 3, 4, 5, 6, 1, 2, 3]\n"],"entry_point":"f_8177079","intent":"append items in list `mylog` to `list1`","library":[],"docs":[]}
{"task_id":4126227,"prompt":"def f_4126227(a, b):\n\t","suffix":"\n\treturn ","canonical_solution":"b.append((a[0][0], a[0][2]))","test_start":"\ndef check(candidate):","test":["\n    a = [(1,2,3),(4,5,6)]\n    b = [(0,0)]\n    candidate(a, b)\n    assert(b == [(0, 0), (1, 3)])\n"],"entry_point":"f_4126227","intent":"append a tuple of elements from list `a` with indexes '[0][0] [0][2]' to list `b`","library":[],"docs":[]}
{"task_id":34902378,"prompt":"def f_34902378(app):\n\t","suffix":"\n\treturn ","canonical_solution":"app.config['SECRET_KEY'] = 'Your_secret_string'","test_start":"\nfrom flask import Flask\n\ndef check(candidate):","test":["\n    app = Flask(\"test\")\n    candidate(app)\n    assert app.config['SECRET_KEY'] == 'Your_secret_string'\n"],"entry_point":"f_34902378","intent":"Initialize `SECRET_KEY` in flask config with `Your_secret_string `","library":["flask"],"docs":[{"text":"secret_key  \nIf a secret key is set, cryptographic components can use this to sign cookies and other things. Set this to a complex random value when you want to use the secure cookie for instance. This attribute can also be configured from the config with the SECRET_KEY configuration key. Defaults to None.","title":"flask.api.index#flask.Flask.secret_key"},{"text":"SECRET_KEY  \nA secret key that will be used for securely signing the session cookie and can be used for any other security related needs by extensions or your application. It should be a long random bytes or str. For example, copy the output of this to your config: $ python -c 'import os; print(os.urandom(16))'\nb'_5#y2L\"F4Q8z\\n\\xec]\/'\n Do not reveal the secret key when posting questions or committing code. Default: None","title":"flask.config.index#SECRET_KEY"},{"text":"salt = 'cookie-session'  \nthe salt that should be applied on top of the secret key for the signing of cookie based sessions.","title":"flask.api.index#flask.sessions.SecureCookieSessionInterface.salt"},{"text":"AppConfig.ready()  \nSubclasses can override this method to perform initialization tasks such as registering signals. It is called as soon as the registry is fully populated. Although you can\u2019t import models at the module-level where AppConfig classes are defined, you can import them in ready(), using either an import statement or get_model(). If you\u2019re registering model signals, you can refer to the sender by its string label instead of using the model class itself. Example: from django.apps import AppConfig\nfrom django.db.models.signals import pre_save\n\n\nclass RockNRollConfig(AppConfig):\n    # ...\n\n    def ready(self):\n        # importing model classes\n        from .models import MyModel  # or...\n        MyModel = self.get_model('MyModel')\n\n        # registering signals with the model's string label\n        pre_save.connect(receiver, sender='app_label.MyModel')\n  Warning Although you can access model classes as described above, avoid interacting with the database in your ready() implementation. This includes model methods that execute queries (save(), delete(), manager methods etc.), and also raw SQL queries via django.db.connection. Your ready() method will run during startup of every management command. For example, even though the test database configuration is separate from the production settings, manage.py test would still execute some queries against your production database!   Note In the usual initialization process, the ready method is only called once by Django. But in some corner cases, particularly in tests which are fiddling with installed applications, ready might be called more than once. In that case, either write idempotent methods, or put a flag on your AppConfig classes to prevent re-running code which should be executed exactly one time.","title":"django.ref.applications#django.apps.AppConfig.ready"},{"text":"ENV  \nWhat environment the app is running in. Flask and extensions may enable behaviors based on the environment, such as enabling debug mode. The env attribute maps to this config key. This is set by the FLASK_ENV environment variable and may not behave as expected if set in code. Do not enable development when deploying in production. Default: 'production'  Changelog New in version 1.0.","title":"flask.config.index#ENV"},{"text":"Extensions Extensions are extra packages that add functionality to a Flask application. For example, an extension might add support for sending email or connecting to a database. Some extensions add entire new frameworks to help build certain types of applications, like a REST API. Finding Extensions Flask extensions are usually named \u201cFlask-Foo\u201d or \u201cFoo-Flask\u201d. You can search PyPI for packages tagged with Framework :: Flask. Using Extensions Consult each extension\u2019s documentation for installation, configuration, and usage instructions. Generally, extensions pull their own configuration from app.config and are passed an application instance during initialization. For example, an extension called \u201cFlask-Foo\u201d might be used like this: from flask_foo import Foo\n\nfoo = Foo()\n\napp = Flask(__name__)\napp.config.update(\n    FOO_BAR='baz',\n    FOO_SPAM='eggs',\n)\n\nfoo.init_app(app)\n Building Extensions While the PyPI contains many Flask extensions, you may not find an extension that fits your need. If this is the case, you can create your own. Read Flask Extension Development to develop your own Flask extension.","title":"flask.extensions.index"},{"text":"class flask.sessions.SecureCookieSessionInterface  \nThe default session interface that stores sessions in signed cookies through the itsdangerous module.  \nstatic digest_method()  \nthe hash function to use for the signature. The default is sha1 \n  \nkey_derivation = 'hmac'  \nthe name of the itsdangerous supported key derivation. The default is hmac. \n  \nopen_session(app, request)  \nThis method has to be implemented and must either return None in case the loading failed because of a configuration error or an instance of a session object which implements a dictionary like interface + the methods and attributes on SessionMixin.  Parameters \n \napp (Flask) \u2013  \nrequest (Request) \u2013    Return type \nOptional[flask.sessions.SecureCookieSession]   \n  \nsalt = 'cookie-session'  \nthe salt that should be applied on top of the secret key for the signing of cookie based sessions. \n  \nsave_session(app, session, response)  \nThis is called for actual sessions returned by open_session() at the end of the request. This is still called during a request context so if you absolutely need access to the request you can do that.  Parameters \n \napp (Flask) \u2013  \nsession (flask.sessions.SessionMixin) \u2013  \nresponse (Response) \u2013    Return type \nNone   \n  \nserializer = <flask.json.tag.TaggedJSONSerializer object>  \nA python serializer for the payload. The default is a compact JSON derived serializer with support for some extra Python types such as datetime objects or tuples. \n  \nsession_class  \nalias of flask.sessions.SecureCookieSession","title":"flask.api.index#flask.sessions.SecureCookieSessionInterface"},{"text":"flask.got_request_exception  \nThis signal is sent when an unhandled exception happens during request processing, including when debugging. The exception is passed to the subscriber as exception. This signal is not sent for HTTPException, or other exceptions that have error handlers registered, unless the exception was raised from an error handler. This example shows how to do some extra logging if a theoretical SecurityException was raised: from flask import got_request_exception\n\ndef log_security_exception(sender, exception, **extra):\n    if not isinstance(exception, SecurityException):\n        return\n\n    security_logger.exception(\n        f\"SecurityException at {request.url!r}\",\n        exc_info=exception,\n    )\n\ngot_request_exception.connect(log_security_exception, app)","title":"flask.api.index#flask.got_request_exception"},{"text":"save_session(app, session, response)  \nThis is called for actual sessions returned by open_session() at the end of the request. This is still called during a request context so if you absolutely need access to the request you can do that.  Parameters \n \napp (Flask) \u2013  \nsession (flask.sessions.SessionMixin) \u2013  \nresponse (Response) \u2013    Return type \nNone","title":"flask.api.index#flask.sessions.SecureCookieSessionInterface.save_session"},{"text":"Field.concrete  \nBoolean flag that indicates if the field has a database column associated with it.","title":"django.ref.models.fields#django.db.models.Field.concrete"}]}
{"task_id":22799300,"prompt":"def f_22799300(out):\n\treturn ","suffix":"","canonical_solution":"pd.DataFrame(out.tolist(), columns=['out-1', 'out-2'], index=out.index)","test_start":"\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\n\ndef check(candidate):","test":["\n    df = pd.DataFrame(dict(x=np.random.randn(100), y=np.repeat(list(\"abcd\"), 25)))\n    out = df.groupby(\"y\").x.apply(stats.ttest_1samp, 0)\n    test = pd.DataFrame(out.tolist())\n    test.columns = ['out-1', 'out-2']\n    test.index = out.index\n    res = candidate(out)\n    assert(test.equals(res))\n"],"entry_point":"f_22799300","intent":"unpack a series of tuples in pandas `out` into a DataFrame with column names 'out-1' and 'out-2'","library":["numpy","pandas","scipy"],"docs":[{"text":"torch.column_stack(tensors, *, out=None) \u2192 Tensor  \nCreates a new tensor by horizontally stacking the tensors in tensors. Equivalent to torch.hstack(tensors), except each zero or one dimensional tensor t in tensors is first reshaped into a (t.numel(), 1) column before being stacked horizontally.  Parameters \ntensors (sequence of Tensors) \u2013 sequence of tensors to concatenate  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.tensor([1, 2, 3])\n>>> b = torch.tensor([4, 5, 6])\n>>> torch.column_stack((a, b))\ntensor([[1, 4],\n    [2, 5],\n    [3, 6]])\n>>> a = torch.arange(5)\n>>> b = torch.arange(10).reshape(5, 2)\n>>> torch.column_stack((a, b, b))\ntensor([[0, 0, 1, 0, 1],\n        [1, 2, 3, 2, 3],\n        [2, 4, 5, 4, 5],\n        [3, 6, 7, 6, 7],\n        [4, 8, 9, 8, 9]])","title":"torch.generated.torch.column_stack#torch.column_stack"},{"text":"pandas.wide_to_long   pandas.wide_to_long(df, stubnames, i, j, sep='', suffix='\\\\d+')[source]\n \nUnpivot a DataFrame from wide to long format. Less flexible but more user-friendly than melt. With stubnames [\u2018A\u2019, \u2018B\u2019], this function expects to find one or more group of columns with format A-suffix1, A-suffix2,\u2026, B-suffix1, B-suffix2,\u2026 You specify what you want to call this suffix in the resulting long format with j (for example j=\u2019year\u2019) Each row of these wide variables are assumed to be uniquely identified by i (can be a single column name or a list of column names) All remaining variables in the data frame are left intact.  Parameters \n \ndf:DataFrame\n\n\nThe wide-format DataFrame.  \nstubnames:str or list-like\n\n\nThe stub name(s). The wide format variables are assumed to start with the stub names.  \ni:str or list-like\n\n\nColumn(s) to use as id variable(s).  \nj:str\n\n\nThe name of the sub-observation variable. What you wish to name your suffix in the long format.  \nsep:str, default \u201c\u201d\n\n\nA character indicating the separation of the variable names in the wide format, to be stripped from the names in the long format. For example, if your column names are A-suffix1, A-suffix2, you can strip the hyphen by specifying sep=\u2019-\u2019.  \nsuffix:str, default \u2018\\d+\u2019\n\n\nA regular expression capturing the wanted suffixes. \u2018\\d+\u2019 captures numeric suffixes. Suffixes with no numbers could be specified with the negated character class \u2018\\D+\u2019. You can also further disambiguate suffixes, for example, if your wide variables are of the form A-one, B-two,.., and you have an unrelated column A-rating, you can ignore the last one by specifying suffix=\u2019(!?one|two)\u2019. When all suffixes are numeric, they are cast to int64\/float64.    Returns \n DataFrame\n\nA DataFrame that contains each stub name as a variable, with new index (i, j).      See also  melt\n\nUnpivot a DataFrame from wide to long format, optionally leaving identifiers set.  pivot\n\nCreate a spreadsheet-style pivot table as a DataFrame.  DataFrame.pivot\n\nPivot without aggregation that can handle non-numeric data.  DataFrame.pivot_table\n\nGeneralization of pivot that can handle duplicate values for one index\/column pair.  DataFrame.unstack\n\nPivot based on the index values instead of a column.    Notes All extra variables are left untouched. This simply uses pandas.melt under the hood, but is hard-coded to \u201cdo the right thing\u201d in a typical case. Examples \n>>> np.random.seed(123)\n>>> df = pd.DataFrame({\"A1970\" : {0 : \"a\", 1 : \"b\", 2 : \"c\"},\n...                    \"A1980\" : {0 : \"d\", 1 : \"e\", 2 : \"f\"},\n...                    \"B1970\" : {0 : 2.5, 1 : 1.2, 2 : .7},\n...                    \"B1980\" : {0 : 3.2, 1 : 1.3, 2 : .1},\n...                    \"X\"     : dict(zip(range(3), np.random.randn(3)))\n...                   })\n>>> df[\"id\"] = df.index\n>>> df\n  A1970 A1980  B1970  B1980         X  id\n0     a     d    2.5    3.2 -1.085631   0\n1     b     e    1.2    1.3  0.997345   1\n2     c     f    0.7    0.1  0.282978   2\n>>> pd.wide_to_long(df, [\"A\", \"B\"], i=\"id\", j=\"year\")\n... \n                X  A    B\nid year\n0  1970 -1.085631  a  2.5\n1  1970  0.997345  b  1.2\n2  1970  0.282978  c  0.7\n0  1980 -1.085631  d  3.2\n1  1980  0.997345  e  1.3\n2  1980  0.282978  f  0.1\n  With multiple id columns \n>>> df = pd.DataFrame({\n...     'famid': [1, 1, 1, 2, 2, 2, 3, 3, 3],\n...     'birth': [1, 2, 3, 1, 2, 3, 1, 2, 3],\n...     'ht1': [2.8, 2.9, 2.2, 2, 1.8, 1.9, 2.2, 2.3, 2.1],\n...     'ht2': [3.4, 3.8, 2.9, 3.2, 2.8, 2.4, 3.3, 3.4, 2.9]\n... })\n>>> df\n   famid  birth  ht1  ht2\n0      1      1  2.8  3.4\n1      1      2  2.9  3.8\n2      1      3  2.2  2.9\n3      2      1  2.0  3.2\n4      2      2  1.8  2.8\n5      2      3  1.9  2.4\n6      3      1  2.2  3.3\n7      3      2  2.3  3.4\n8      3      3  2.1  2.9\n>>> l = pd.wide_to_long(df, stubnames='ht', i=['famid', 'birth'], j='age')\n>>> l\n... \n                  ht\nfamid birth age\n1     1     1    2.8\n            2    3.4\n      2     1    2.9\n            2    3.8\n      3     1    2.2\n            2    2.9\n2     1     1    2.0\n            2    3.2\n      2     1    1.8\n            2    2.8\n      3     1    1.9\n            2    2.4\n3     1     1    2.2\n            2    3.3\n      2     1    2.3\n            2    3.4\n      3     1    2.1\n            2    2.9\n  Going from long back to wide just takes some creative use of unstack \n>>> w = l.unstack()\n>>> w.columns = w.columns.map('{0[0]}{0[1]}'.format)\n>>> w.reset_index()\n   famid  birth  ht1  ht2\n0      1      1  2.8  3.4\n1      1      2  2.9  3.8\n2      1      3  2.2  2.9\n3      2      1  2.0  3.2\n4      2      2  1.8  2.8\n5      2      3  1.9  2.4\n6      3      1  2.2  3.3\n7      3      2  2.3  3.4\n8      3      3  2.1  2.9\n  Less wieldy column names are also handled \n>>> np.random.seed(0)\n>>> df = pd.DataFrame({'A(weekly)-2010': np.random.rand(3),\n...                    'A(weekly)-2011': np.random.rand(3),\n...                    'B(weekly)-2010': np.random.rand(3),\n...                    'B(weekly)-2011': np.random.rand(3),\n...                    'X' : np.random.randint(3, size=3)})\n>>> df['id'] = df.index\n>>> df \n   A(weekly)-2010  A(weekly)-2011  B(weekly)-2010  B(weekly)-2011  X  id\n0        0.548814        0.544883        0.437587        0.383442  0   0\n1        0.715189        0.423655        0.891773        0.791725  1   1\n2        0.602763        0.645894        0.963663        0.528895  1   2\n  \n>>> pd.wide_to_long(df, ['A(weekly)', 'B(weekly)'], i='id',\n...                 j='year', sep='-')\n... \n         X  A(weekly)  B(weekly)\nid year\n0  2010  0   0.548814   0.437587\n1  2010  1   0.715189   0.891773\n2  2010  1   0.602763   0.963663\n0  2011  0   0.544883   0.383442\n1  2011  1   0.423655   0.791725\n2  2011  1   0.645894   0.528895\n  If we have many columns, we could also use a regex to find our stubnames and pass that list on to wide_to_long \n>>> stubnames = sorted(\n...     set([match[0] for match in df.columns.str.findall(\n...         r'[A-B]\\(.*\\)').values if match != []])\n... )\n>>> list(stubnames)\n['A(weekly)', 'B(weekly)']\n  All of the above examples have integers as suffixes. It is possible to have non-integers as suffixes. \n>>> df = pd.DataFrame({\n...     'famid': [1, 1, 1, 2, 2, 2, 3, 3, 3],\n...     'birth': [1, 2, 3, 1, 2, 3, 1, 2, 3],\n...     'ht_one': [2.8, 2.9, 2.2, 2, 1.8, 1.9, 2.2, 2.3, 2.1],\n...     'ht_two': [3.4, 3.8, 2.9, 3.2, 2.8, 2.4, 3.3, 3.4, 2.9]\n... })\n>>> df\n   famid  birth  ht_one  ht_two\n0      1      1     2.8     3.4\n1      1      2     2.9     3.8\n2      1      3     2.2     2.9\n3      2      1     2.0     3.2\n4      2      2     1.8     2.8\n5      2      3     1.9     2.4\n6      3      1     2.2     3.3\n7      3      2     2.3     3.4\n8      3      3     2.1     2.9\n  \n>>> l = pd.wide_to_long(df, stubnames='ht', i=['famid', 'birth'], j='age',\n...                     sep='_', suffix=r'\\w+')\n>>> l\n... \n                  ht\nfamid birth age\n1     1     one  2.8\n            two  3.4\n      2     one  2.9\n            two  3.8\n      3     one  2.2\n            two  2.9\n2     1     one  2.0\n            two  3.2\n      2     one  1.8\n            two  2.8\n      3     one  1.9\n            two  2.4\n3     1     one  2.2\n            two  3.3\n      2     one  2.3\n            two  3.4\n      3     one  2.1\n            two  2.9","title":"pandas.reference.api.pandas.wide_to_long"},{"text":"tf.raw_ops.OutfeedDequeueTupleV2 Retrieve multiple values from the computation outfeed. Device ordinal is a  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.raw_ops.OutfeedDequeueTupleV2  \ntf.raw_ops.OutfeedDequeueTupleV2(\n    device_ordinal, dtypes, shapes, name=None\n)\n tensor allowing dynamic outfeed. This operation will block indefinitely until data is available. Output i corresponds to XLA tuple element i. Args: device_ordinal: A Tensor of type int32. An int scalar tensor, representing the TPU device to use. This should be -1 when the Op is running on a TPU device, and >= 0 when the Op is running on the CPU device. dtypes: A list of tf.DTypes that has length >= 1. The element types of each element in outputs. shapes: A list of shapes (each a tf.TensorShape or list of ints). The shapes of each tensor in outputs. name: A name for the operation (optional). Returns: A list of Tensor objects of type dtypes.","title":"tensorflow.raw_ops.outfeeddequeuetuplev2"},{"text":"pandas.melt   pandas.melt(frame, id_vars=None, value_vars=None, var_name=None, value_name='value', col_level=None, ignore_index=True)[source]\n \nUnpivot a DataFrame from wide to long format, optionally leaving identifiers set. This function is useful to massage a DataFrame into a format where one or more columns are identifier variables (id_vars), while all other columns, considered measured variables (value_vars), are \u201cunpivoted\u201d to the row axis, leaving just two non-identifier columns, \u2018variable\u2019 and \u2018value\u2019.  Parameters \n \nid_vars:tuple, list, or ndarray, optional\n\n\nColumn(s) to use as identifier variables.  \nvalue_vars:tuple, list, or ndarray, optional\n\n\nColumn(s) to unpivot. If not specified, uses all columns that are not set as id_vars.  \nvar_name:scalar\n\n\nName to use for the \u2018variable\u2019 column. If None it uses frame.columns.name or \u2018variable\u2019.  \nvalue_name:scalar, default \u2018value\u2019\n\n\nName to use for the \u2018value\u2019 column.  \ncol_level:int or str, optional\n\n\nIf columns are a MultiIndex then use this level to melt.  \nignore_index:bool, default True\n\n\nIf True, original index is ignored. If False, the original index is retained. Index labels will be repeated as necessary.  New in version 1.1.0.     Returns \n DataFrame\n\nUnpivoted DataFrame.      See also  DataFrame.melt\n\nIdentical method.  pivot_table\n\nCreate a spreadsheet-style pivot table as a DataFrame.  DataFrame.pivot\n\nReturn reshaped DataFrame organized by given index \/ column values.  DataFrame.explode\n\nExplode a DataFrame from list-like columns to long format.    Examples \n>>> df = pd.DataFrame({'A': {0: 'a', 1: 'b', 2: 'c'},\n...                    'B': {0: 1, 1: 3, 2: 5},\n...                    'C': {0: 2, 1: 4, 2: 6}})\n>>> df\n   A  B  C\n0  a  1  2\n1  b  3  4\n2  c  5  6\n  \n>>> pd.melt(df, id_vars=['A'], value_vars=['B'])\n   A variable  value\n0  a        B      1\n1  b        B      3\n2  c        B      5\n  \n>>> pd.melt(df, id_vars=['A'], value_vars=['B', 'C'])\n   A variable  value\n0  a        B      1\n1  b        B      3\n2  c        B      5\n3  a        C      2\n4  b        C      4\n5  c        C      6\n  The names of \u2018variable\u2019 and \u2018value\u2019 columns can be customized: \n>>> pd.melt(df, id_vars=['A'], value_vars=['B'],\n...         var_name='myVarname', value_name='myValname')\n   A myVarname  myValname\n0  a         B          1\n1  b         B          3\n2  c         B          5\n  Original index values can be kept around: \n>>> pd.melt(df, id_vars=['A'], value_vars=['B', 'C'], ignore_index=False)\n   A variable  value\n0  a        B      1\n1  b        B      3\n2  c        B      5\n0  a        C      2\n1  b        C      4\n2  c        C      6\n  If you have multi-index columns: \n>>> df.columns = [list('ABC'), list('DEF')]\n>>> df\n   A  B  C\n   D  E  F\n0  a  1  2\n1  b  3  4\n2  c  5  6\n  \n>>> pd.melt(df, col_level=0, id_vars=['A'], value_vars=['B'])\n   A variable  value\n0  a        B      1\n1  b        B      3\n2  c        B      5\n  \n>>> pd.melt(df, id_vars=[('A', 'D')], value_vars=[('B', 'E')])\n  (A, D) variable_0 variable_1  value\n0      a          B          E      1\n1      b          B          E      3\n2      c          B          E      5","title":"pandas.reference.api.pandas.melt"},{"text":"tf.experimental.numpy.outer TensorFlow variant of NumPy's outer. \ntf.experimental.numpy.outer(\n    a, b\n)\n Unsupported arguments: out. See the NumPy documentation for numpy.outer.","title":"tensorflow.experimental.numpy.outer"},{"text":"property df","title":"torch.distributions#torch.distributions.chi2.Chi2.df"},{"text":"pandas.DataFrame.melt   DataFrame.melt(id_vars=None, value_vars=None, var_name=None, value_name='value', col_level=None, ignore_index=True)[source]\n \nUnpivot a DataFrame from wide to long format, optionally leaving identifiers set. This function is useful to massage a DataFrame into a format where one or more columns are identifier variables (id_vars), while all other columns, considered measured variables (value_vars), are \u201cunpivoted\u201d to the row axis, leaving just two non-identifier columns, \u2018variable\u2019 and \u2018value\u2019.  Parameters \n \nid_vars:tuple, list, or ndarray, optional\n\n\nColumn(s) to use as identifier variables.  \nvalue_vars:tuple, list, or ndarray, optional\n\n\nColumn(s) to unpivot. If not specified, uses all columns that are not set as id_vars.  \nvar_name:scalar\n\n\nName to use for the \u2018variable\u2019 column. If None it uses frame.columns.name or \u2018variable\u2019.  \nvalue_name:scalar, default \u2018value\u2019\n\n\nName to use for the \u2018value\u2019 column.  \ncol_level:int or str, optional\n\n\nIf columns are a MultiIndex then use this level to melt.  \nignore_index:bool, default True\n\n\nIf True, original index is ignored. If False, the original index is retained. Index labels will be repeated as necessary.  New in version 1.1.0.     Returns \n DataFrame\n\nUnpivoted DataFrame.      See also  melt\n\nIdentical method.  pivot_table\n\nCreate a spreadsheet-style pivot table as a DataFrame.  DataFrame.pivot\n\nReturn reshaped DataFrame organized by given index \/ column values.  DataFrame.explode\n\nExplode a DataFrame from list-like columns to long format.    Examples \n>>> df = pd.DataFrame({'A': {0: 'a', 1: 'b', 2: 'c'},\n...                    'B': {0: 1, 1: 3, 2: 5},\n...                    'C': {0: 2, 1: 4, 2: 6}})\n>>> df\n   A  B  C\n0  a  1  2\n1  b  3  4\n2  c  5  6\n  \n>>> df.melt(id_vars=['A'], value_vars=['B'])\n   A variable  value\n0  a        B      1\n1  b        B      3\n2  c        B      5\n  \n>>> df.melt(id_vars=['A'], value_vars=['B', 'C'])\n   A variable  value\n0  a        B      1\n1  b        B      3\n2  c        B      5\n3  a        C      2\n4  b        C      4\n5  c        C      6\n  The names of \u2018variable\u2019 and \u2018value\u2019 columns can be customized: \n>>> df.melt(id_vars=['A'], value_vars=['B'],\n...         var_name='myVarname', value_name='myValname')\n   A myVarname  myValname\n0  a         B          1\n1  b         B          3\n2  c         B          5\n  Original index values can be kept around: \n>>> df.melt(id_vars=['A'], value_vars=['B', 'C'], ignore_index=False)\n   A variable  value\n0  a        B      1\n1  b        B      3\n2  c        B      5\n0  a        C      2\n1  b        C      4\n2  c        C      6\n  If you have multi-index columns: \n>>> df.columns = [list('ABC'), list('DEF')]\n>>> df\n   A  B  C\n   D  E  F\n0  a  1  2\n1  b  3  4\n2  c  5  6\n  \n>>> df.melt(col_level=0, id_vars=['A'], value_vars=['B'])\n   A variable  value\n0  a        B      1\n1  b        B      3\n2  c        B      5\n  \n>>> df.melt(id_vars=[('A', 'D')], value_vars=[('B', 'E')])\n  (A, D) variable_0 variable_1  value\n0      a          B          E      1\n1      b          B          E      3\n2      c          B          E      5","title":"pandas.reference.api.pandas.dataframe.melt"},{"text":"numpy.column_stack   numpy.column_stack(tup)[source]\n \nStack 1-D arrays as columns into a 2-D array. Take a sequence of 1-D arrays and stack them as columns to make a single 2-D array. 2-D arrays are stacked as-is, just like with hstack. 1-D arrays are turned into 2-D columns first.  Parameters \n \ntupsequence of 1-D or 2-D arrays.\n\n\nArrays to stack. All of them must have the same first dimension.    Returns \n \nstacked2-D array\n\n\nThe array formed by stacking the given arrays.      See also  \nstack, hstack, vstack, concatenate\n\n  Examples >>> a = np.array((1,2,3))\n>>> b = np.array((2,3,4))\n>>> np.column_stack((a,b))\narray([[1, 2],\n       [2, 3],\n       [3, 4]])","title":"numpy.reference.generated.numpy.column_stack"},{"text":"tf.compat.v1.setdiff1d Computes the difference between two lists of numbers or strings. \ntf.compat.v1.setdiff1d(\n    x, y, index_dtype=tf.dtypes.int32, name=None\n)\n Given a list x and a list y, this operation returns a list out that represents all values that are in x but not in y. The returned list out is sorted in the same order that the numbers appear in x (duplicates are preserved). This operation also returns a list idx that represents the position of each out element in x. In other words: out[i] = x[idx[i]] for i in [0, 1, ..., len(out) - 1] For example, given this input: x = [1, 2, 3, 4, 5, 6]\ny = [1, 3, 5]\n This operation would return: out ==> [2, 4, 6]\nidx ==> [1, 3, 5]\n\n \n\n\n Args\n  x   A Tensor. 1-D. Values to keep.  \n  y   A Tensor. Must have the same type as x. 1-D. Values to remove.  \n  out_idx   An optional tf.DType from: tf.int32, tf.int64. Defaults to tf.int32.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A tuple of Tensor objects (out, idx).     out   A Tensor. Has the same type as x.  \n  idx   A Tensor of type out_idx.","title":"tensorflow.compat.v1.setdiff1d"},{"text":"pandas.core.window.expanding.Expanding.cov   Expanding.cov(other=None, pairwise=None, ddof=1, **kwargs)[source]\n \nCalculate the expanding sample covariance.  Parameters \n \nother:Series or DataFrame, optional\n\n\nIf not supplied then will default to self and produce pairwise output.  \npairwise:bool, default None\n\n\nIf False then only matching columns between self and other will be used and the output will be a DataFrame. If True then all pairwise combinations will be calculated and the output will be a MultiIndexed DataFrame in the case of DataFrame inputs. In the case of missing elements, only complete pairwise observations will be used.  \nddof:int, default 1\n\n\nDelta Degrees of Freedom. The divisor used in calculations is N - ddof, where N represents the number of elements.  **kwargs\n\nFor NumPy compatibility and will not have an effect on the result.    Returns \n Series or DataFrame\n\nReturn type is the same as the original object with np.float64 dtype.      See also  pandas.Series.expanding\n\nCalling expanding with Series data.  pandas.DataFrame.expanding\n\nCalling expanding with DataFrames.  pandas.Series.cov\n\nAggregating cov for Series.  pandas.DataFrame.cov\n\nAggregating cov for DataFrame.","title":"pandas.reference.api.pandas.core.window.expanding.expanding.cov"}]}
{"task_id":1762484,"prompt":"def f_1762484(stocks_list):\n\treturn ","suffix":"","canonical_solution":"[x for x in range(len(stocks_list)) if stocks_list[x] == 'MSFT']","test_start":"\ndef check(candidate):","test":["\n    stocks_list = ['AAPL', 'MSFT', 'GOOG', 'MSFT', 'MSFT']\n    assert(candidate(stocks_list) == [1,3,4])\n","\n    stocks_list = ['AAPL', 'MSXT', 'GOOG', 'MSAT', 'SFT']\n    assert(candidate(stocks_list) == [])\n"],"entry_point":"f_1762484","intent":"find the index of an element 'MSFT' in a list `stocks_list`","library":[],"docs":[]}
{"task_id":3464359,"prompt":"def f_3464359(ax, labels):\n\treturn ","suffix":"","canonical_solution":"ax.set_xticklabels(labels, rotation=45)","test_start":"\nimport matplotlib.pyplot as plt \n\ndef check(candidate):","test":["\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3, 4], [1, 4, 2, 3])\n    ret = candidate(ax, [f\"#{i}\" for i in range(7)])\n    assert [tt.get_rotation() == 45.0 for tt in ret]\n"],"entry_point":"f_3464359","intent":"rotate the xtick `labels` of matplotlib plot `ax` by `45` degrees to make long labels readable","library":["matplotlib"],"docs":[{"text":"get_rotate_label(text)[source]","title":"matplotlib._as_gen.mpl_toolkits.mplot3d.axis3d.axis#mpl_toolkits.mplot3d.axis3d.Axis.get_rotate_label"},{"text":"get_axislabel_pos_angle(axes)[source]","title":"matplotlib._as_gen.mpl_toolkits.axisartist.grid_helper_curvelinear.floatingaxisartisthelper#mpl_toolkits.axisartist.grid_helper_curvelinear.FloatingAxisArtistHelper.get_axislabel_pos_angle"},{"text":"invert_ticklabel_direction()[source]","title":"matplotlib._as_gen.mpl_toolkits.axisartist.axis_artist.axisartist#mpl_toolkits.axisartist.axis_artist.AxisArtist.invert_ticklabel_direction"},{"text":"set_axis_direction(label_direction)[source]\n \nAdjust the text angle and text alignment of ticklabels according to the matplotlib convention. The label_direction must be one of [left, right, bottom, top].   \nproperty left bottom right top   \nticklabels angle 90 0 -90 180  \nticklabel va center baseline center baseline  \nticklabel ha right center right center   Note that the text angles are actually relative to (90 + angle of the direction to the ticklabel), which gives 0 for bottom axis.","title":"matplotlib._as_gen.mpl_toolkits.axisartist.axis_artist.ticklabels#mpl_toolkits.axisartist.axis_artist.TickLabels.set_axis_direction"},{"text":"get_rlabel_position()[source]\n \n Returns \n float\n\nThe theta position of the radius labels in degrees.","title":"matplotlib.projections_api#matplotlib.projections.polar.PolarAxes.get_rlabel_position"},{"text":"autofmt_xdate(bottom=0.2, rotation=30, ha='right', which='major')[source]\n \nDate ticklabels often overlap, so it is useful to rotate them and right align them. Also, a common use case is a number of subplots with shared x-axis where the x-axis is date data. The ticklabels are often long, and it helps to rotate them on the bottom subplot and turn them off on other subplots, as well as turn off xlabels.  Parameters \n \nbottomfloat, default: 0.2\n\n\nThe bottom of the subplots for subplots_adjust.  \nrotationfloat, default: 30 degrees\n\n\nThe rotation angle of the xtick labels in degrees.  \nha{'left', 'center', 'right'}, default: 'right'\n\n\nThe horizontal alignment of the xticklabels.  \nwhich{'major', 'minor', 'both'}, default: 'major'\n\n\nSelects which ticklabels to rotate.","title":"matplotlib.figure_api#matplotlib.figure.Figure.autofmt_xdate"},{"text":"autofmt_xdate(bottom=0.2, rotation=30, ha='right', which='major')[source]\n \nDate ticklabels often overlap, so it is useful to rotate them and right align them. Also, a common use case is a number of subplots with shared x-axis where the x-axis is date data. The ticklabels are often long, and it helps to rotate them on the bottom subplot and turn them off on other subplots, as well as turn off xlabels.  Parameters \n \nbottomfloat, default: 0.2\n\n\nThe bottom of the subplots for subplots_adjust.  \nrotationfloat, default: 30 degrees\n\n\nThe rotation angle of the xtick labels in degrees.  \nha{'left', 'center', 'right'}, default: 'right'\n\n\nThe horizontal alignment of the xticklabels.  \nwhich{'major', 'minor', 'both'}, default: 'major'\n\n\nSelects which ticklabels to rotate.","title":"matplotlib.figure_api#matplotlib.figure.SubFigure.autofmt_xdate"},{"text":"set_rotate_label(val)[source]\n \nWhether to rotate the axis label: True, False or None. If set to None the label will be rotated if longer than 4 chars.","title":"matplotlib._as_gen.mpl_toolkits.mplot3d.axis3d.axis#mpl_toolkits.mplot3d.axis3d.Axis.set_rotate_label"},{"text":"matplotlib.axis.YAxis.tick_left   YAxis.tick_left()[source]\n \nMove ticks and ticklabels (if present) to the left of the axes. \n  Examples using matplotlib.axis.YAxis.tick_left\n \n   Bachelor's degrees by gender   \n\n   Set default y-axis tick labels on the right","title":"matplotlib._as_gen.matplotlib.axis.yaxis.tick_left"},{"text":"autofmt_xdate(bottom=0.2, rotation=30, ha='right', which='major')[source]\n \nDate ticklabels often overlap, so it is useful to rotate them and right align them. Also, a common use case is a number of subplots with shared x-axis where the x-axis is date data. The ticklabels are often long, and it helps to rotate them on the bottom subplot and turn them off on other subplots, as well as turn off xlabels.  Parameters \n \nbottomfloat, default: 0.2\n\n\nThe bottom of the subplots for subplots_adjust.  \nrotationfloat, default: 30 degrees\n\n\nThe rotation angle of the xtick labels in degrees.  \nha{'left', 'center', 'right'}, default: 'right'\n\n\nThe horizontal alignment of the xticklabels.  \nwhich{'major', 'minor', 'both'}, default: 'major'\n\n\nSelects which ticklabels to rotate.","title":"matplotlib.figure_api#matplotlib.figure.FigureBase.autofmt_xdate"}]}
{"task_id":875968,"prompt":"def f_875968(s):\n\treturn ","suffix":"","canonical_solution":"re.sub('[^\\\\w]', ' ', s)","test_start":"\nimport re\n\ndef check(candidate):","test":["\n    s = \"how much for the maple syrup? $20.99? That's ridiculous!!!\"\n    assert candidate(s) == 'how much for the maple syrup   20 99  That s ridiculous   '\n"],"entry_point":"f_875968","intent":"remove symbols from a string `s`","library":["re"],"docs":[{"text":"matplotlib.cbook.strip_math(s)[source]\n \nRemove latex formatting from mathtext. Only handles fully math and fully non-math strings.","title":"matplotlib.cbook_api#matplotlib.cbook.strip_math"},{"text":"staticfix_minus(s)[source]\n \nSome classes may want to replace a hyphen for minus with the proper unicode symbol (U+2212) for typographical correctness. This is a helper method to perform such a replacement when it is enabled via rcParams[\"axes.unicode_minus\"] (default: True).","title":"matplotlib.ticker_api#matplotlib.ticker.Formatter.fix_minus"},{"text":"str.rstrip([chars])  \nReturn a copy of the string with trailing characters removed. The chars argument is a string specifying the set of characters to be removed. If omitted or None, the chars argument defaults to removing whitespace. The chars argument is not a suffix; rather, all combinations of its values are stripped: >>> '   spacious   '.rstrip()\n'   spacious'\n>>> 'mississippi'.rstrip('ipz')\n'mississ'\n See str.removesuffix() for a method that will remove a single suffix string rather than all of a set of characters. For example: >>> 'Monty Python'.rstrip(' Python')\n'M'\n>>> 'Monty Python'.removesuffix(' Python')\n'Monty'","title":"python.library.stdtypes#str.rstrip"},{"text":"str.strip([chars])  \nReturn a copy of the string with the leading and trailing characters removed. The chars argument is a string specifying the set of characters to be removed. If omitted or None, the chars argument defaults to removing whitespace. The chars argument is not a prefix or suffix; rather, all combinations of its values are stripped: >>> '   spacious   '.strip()\n'spacious'\n>>> 'www.example.com'.strip('cmowz.')\n'example'\n The outermost leading and trailing chars argument values are stripped from the string. Characters are removed from the leading end until reaching a string character that is not contained in the set of characters in chars. A similar action takes place on the trailing end. For example: >>> comment_string = '#....... Section 3.2.1 Issue #32 .......'\n>>> comment_string.strip('.#! ')\n'Section 3.2.1 Issue #32'","title":"python.library.stdtypes#str.strip"},{"text":"mark_safe(s) [source]\n \nExplicitly mark a string as safe for (HTML) output purposes. The returned object can be used everywhere a string is appropriate. Can be called multiple times on a single string. Can also be used as a decorator. For building up fragments of HTML, you should normally be using django.utils.html.format_html() instead. String marked safe will become unsafe again if modified. For example: >>> mystr = '<b>Hello World<\/b>   '\n>>> mystr = mark_safe(mystr)\n>>> type(mystr)\n<class 'django.utils.safestring.SafeString'>\n\n>>> mystr = mystr.strip()  # removing whitespace\n>>> type(mystr)\n<type 'str'>","title":"django.ref.utils#django.utils.safestring.mark_safe"},{"text":"smart_str(s, encoding='utf-8', strings_only=False, errors='strict') [source]\n \nReturns a str object representing arbitrary object s. Treats bytestrings using the encoding codec. If strings_only is True, don\u2019t convert (some) non-string-like objects.","title":"django.ref.utils#django.utils.encoding.smart_str"},{"text":"sklearn.preprocessing.KBinsDiscretizer  \nclass sklearn.preprocessing.KBinsDiscretizer(n_bins=5, *, encode='onehot', strategy='quantile', dtype=None) [source]\n \nBin continuous data into intervals. Read more in the User Guide.  New in version 0.20.   Parameters \n \nn_binsint or array-like of shape (n_features,), default=5 \n\nThe number of bins to produce. Raises ValueError if n_bins < 2.  \nencode{\u2018onehot\u2019, \u2018onehot-dense\u2019, \u2018ordinal\u2019}, default=\u2019onehot\u2019 \n\nMethod used to encode the transformed result.  onehot\n\nEncode the transformed result with one-hot encoding and return a sparse matrix. Ignored features are always stacked to the right.  onehot-dense\n\nEncode the transformed result with one-hot encoding and return a dense array. Ignored features are always stacked to the right.  ordinal\n\nReturn the bin identifier encoded as an integer value.    \nstrategy{\u2018uniform\u2019, \u2018quantile\u2019, \u2018kmeans\u2019}, default=\u2019quantile\u2019 \n\nStrategy used to define the widths of the bins.  uniform\n\nAll bins in each feature have identical widths.  quantile\n\nAll bins in each feature have the same number of points.  kmeans\n\nValues in each bin have the same nearest center of a 1D k-means cluster.    \ndtype{np.float32, np.float64}, default=None \n\nThe desired data-type for the output. If None, output dtype is consistent with input dtype. Only np.float32 and np.float64 are supported.  New in version 0.24.     Attributes \n \nn_bins_ndarray of shape (n_features,), dtype=np.int_ \n\nNumber of bins per feature. Bins whose width are too small (i.e., <= 1e-8) are removed with a warning.  \nbin_edges_ndarray of ndarray of shape (n_features,) \n\nThe edges of each bin. Contain arrays of varying shapes (n_bins_, ) Ignored features will have empty arrays.      See also  \nBinarizer\n\n\nClass used to bin values as 0 or 1 based on a parameter threshold.    Notes In bin edges for feature i, the first and last values are used only for inverse_transform. During transform, bin edges are extended to: np.concatenate([-np.inf, bin_edges_[i][1:-1], np.inf])\n You can combine KBinsDiscretizer with ColumnTransformer if you only want to preprocess part of the features. KBinsDiscretizer might produce constant features (e.g., when encode = 'onehot' and certain bins do not contain any data). These features can be removed with feature selection algorithms (e.g., VarianceThreshold). Examples >>> X = [[-2, 1, -4,   -1],\n...      [-1, 2, -3, -0.5],\n...      [ 0, 3, -2,  0.5],\n...      [ 1, 4, -1,    2]]\n>>> est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n>>> est.fit(X)\nKBinsDiscretizer(...)\n>>> Xt = est.transform(X)\n>>> Xt  \narray([[ 0., 0., 0., 0.],\n       [ 1., 1., 1., 0.],\n       [ 2., 2., 2., 1.],\n       [ 2., 2., 2., 2.]])\n Sometimes it may be useful to convert the data back into the original feature space. The inverse_transform function converts the binned data into the original feature space. Each value will be equal to the mean of the two bin edges. >>> est.bin_edges_[0]\narray([-2., -1.,  0.,  1.])\n>>> est.inverse_transform(Xt)\narray([[-1.5,  1.5, -3.5, -0.5],\n       [-0.5,  2.5, -2.5, -0.5],\n       [ 0.5,  3.5, -1.5,  0.5],\n       [ 0.5,  3.5, -1.5,  1.5]])\n Methods  \nfit(X[, y]) Fit the estimator.  \nfit_transform(X[, y]) Fit to data, then transform it.  \nget_params([deep]) Get parameters for this estimator.  \ninverse_transform(Xt) Transform discretized data back to original feature space.  \nset_params(**params) Set the parameters of this estimator.  \ntransform(X) Discretize the data.    \nfit(X, y=None) [source]\n \nFit the estimator.  Parameters \n \nXarray-like of shape (n_samples, n_features) \n\nData to be discretized.  \nyNone \n\nIgnored. This parameter exists only for compatibility with Pipeline.    Returns \n self\n   \n  \nfit_transform(X, y=None, **fit_params) [source]\n \nFit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.  Parameters \n \nXarray-like of shape (n_samples, n_features) \n\nInput samples.  \nyarray-like of shape (n_samples,) or (n_samples, n_outputs), default=None \n\nTarget values (None for unsupervised transformations).  \n**fit_paramsdict \n\nAdditional fit parameters.    Returns \n \nX_newndarray array of shape (n_samples, n_features_new) \n\nTransformed array.     \n  \nget_params(deep=True) [source]\n \nGet parameters for this estimator.  Parameters \n \ndeepbool, default=True \n\nIf True, will return the parameters for this estimator and contained subobjects that are estimators.    Returns \n \nparamsdict \n\nParameter names mapped to their values.     \n  \ninverse_transform(Xt) [source]\n \nTransform discretized data back to original feature space. Note that this function does not regenerate the original data due to discretization rounding.  Parameters \n \nXtarray-like of shape (n_samples, n_features) \n\nTransformed data in the binned space.    Returns \n \nXinvndarray, dtype={np.float32, np.float64} \n\nData in the original feature space.     \n  \nset_params(**params) [source]\n \nSet the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as Pipeline). The latter have parameters of the form <component>__<parameter> so that it\u2019s possible to update each component of a nested object.  Parameters \n \n**paramsdict \n\nEstimator parameters.    Returns \n \nselfestimator instance \n\nEstimator instance.     \n  \ntransform(X) [source]\n \nDiscretize the data.  Parameters \n \nXarray-like of shape (n_samples, n_features) \n\nData to be discretized.    Returns \n \nXt{ndarray, sparse matrix}, dtype={np.float32, np.float64} \n\nData in the binned space. Will be a sparse matrix if self.encode='onehot' and ndarray otherwise.     \n \n Examples using sklearn.preprocessing.KBinsDiscretizer\n \n  Poisson regression and non-normal loss  \n\n  Tweedie regression on insurance claims  \n\n  Using KBinsDiscretizer to discretize continuous features  \n\n  Demonstrating the different strategies of KBinsDiscretizer  \n\n  Feature discretization","title":"sklearn.modules.generated.sklearn.preprocessing.kbinsdiscretizer"},{"text":"class SymDifference(expr1, expr2, **extra)","title":"django.ref.contrib.gis.functions#django.contrib.gis.db.models.functions.SymDifference"},{"text":"shlex.split(s, comments=False, posix=True)  \nSplit the string s using shell-like syntax. If comments is False (the default), the parsing of comments in the given string will be disabled (setting the commenters attribute of the shlex instance to the empty string). This function operates in POSIX mode by default, but uses non-POSIX mode if the posix argument is false.  Note Since the split() function instantiates a shlex instance, passing None for s will read the string to split from standard input.   Deprecated since version 3.9: Passing None for s will raise an exception in future Python versions.","title":"python.library.shlex#shlex.split"},{"text":"json.loads(s, *, cls=None, object_hook=None, parse_float=None, parse_int=None, parse_constant=None, object_pairs_hook=None, **kw)  \nDeserialize s (a str, bytes or bytearray instance containing a JSON document) to a Python object using this conversion table. The other arguments have the same meaning as in load(). If the data being deserialized is not a valid JSON document, a JSONDecodeError will be raised.  Changed in version 3.6: s can now be of type bytes or bytearray. The input encoding should be UTF-8, UTF-16 or UTF-32.   Changed in version 3.9: The keyword argument encoding has been removed.","title":"python.library.json#json.loads"}]}
{"task_id":34750084,"prompt":"def f_34750084(s):\n\treturn ","suffix":"","canonical_solution":"re.findall(\"'\\\\\\\\[0-7]{1,3}'\", s)","test_start":"\nimport re\n\ndef check(candidate):","test":["\n    assert candidate(r\"char x = '\\077';\") == [\"'\\\\077'\"]\n"],"entry_point":"f_34750084","intent":"Find octal characters matches from a string `s` using regex","library":["re"],"docs":[{"text":"string.octdigits  \nThe string '01234567'.","title":"python.library.string#string.octdigits"},{"text":"oct(x)  \nConvert an integer number to an octal string prefixed with \u201c0o\u201d. The result is a valid Python expression. If x is not a Python int object, it has to define an __index__() method that returns an integer. For example: >>> oct(8)\n'0o10'\n>>> oct(-56)\n'-0o70'\n If you want to convert an integer number to octal string either with prefix \u201c0o\u201d or not, you can use either of the following ways. >>> '%#o' % 10, '%o' % 10\n('0o12', '12')\n>>> format(10, '#o'), format(10, 'o')\n('0o12', '12')\n>>> f'{10:#o}', f'{10:o}'\n('0o12', '12')\n See also format() for more information.","title":"python.library.functions#oct"},{"text":"email.utils.decode_rfc2231(s)  \nDecode the string s according to RFC 2231.","title":"python.library.email.utils#email.utils.decode_rfc2231"},{"text":"statichexify(match)[source]","title":"matplotlib.backend_pdf_api#matplotlib.backends.backend_pdf.Name.hexify"},{"text":"re.S  \nre.DOTALL  \nMake the '.' special character match any character at all, including a newline; without this flag, '.' will match anything except a newline. Corresponds to the inline flag (?s).","title":"python.library.re#re.S"},{"text":"raw_decode(s)  \nDecode a JSON document from s (a str beginning with a JSON document) and return a 2-tuple of the Python representation and the index in s where the document ended. This can be used to decode a JSON document from a string that may have extraneous data at the end.","title":"python.library.json#json.JSONDecoder.raw_decode"},{"text":"re.A  \nre.ASCII  \nMake \\w, \\W, \\b, \\B, \\d, \\D, \\s and \\S perform ASCII-only matching instead of full Unicode matching. This is only meaningful for Unicode patterns, and is ignored for byte patterns. Corresponds to the inline flag (?a). Note that for backward compatibility, the re.U flag still exists (as well as its synonym re.UNICODE and its embedded counterpart (?u)), but these are redundant in Python 3 since matches are Unicode by default for strings (and Unicode matching isn\u2019t allowed for bytes).","title":"python.library.re#re.ASCII"},{"text":"re.S  \nre.DOTALL  \nMake the '.' special character match any character at all, including a newline; without this flag, '.' will match anything except a newline. Corresponds to the inline flag (?s).","title":"python.library.re#re.DOTALL"},{"text":"re.A  \nre.ASCII  \nMake \\w, \\W, \\b, \\B, \\d, \\D, \\s and \\S perform ASCII-only matching instead of full Unicode matching. This is only meaningful for Unicode patterns, and is ignored for byte patterns. Corresponds to the inline flag (?a). Note that for backward compatibility, the re.U flag still exists (as well as its synonym re.UNICODE and its embedded counterpart (?u)), but these are redundant in Python 3 since matches are Unicode by default for strings (and Unicode matching isn\u2019t allowed for bytes).","title":"python.library.re#re.A"},{"text":"string.printable  \nString of ASCII characters which are considered printable. This is a combination of digits, ascii_letters, punctuation, and whitespace.","title":"python.library.string#string.printable"}]}
{"task_id":13209288,"prompt":"def f_13209288(input):\n\treturn ","suffix":"","canonical_solution":"re.split(r'[ ](?=[A-Z]+\\b)', input)","test_start":"\nimport re\n\ndef check(candidate):","test":["\n    assert candidate('HELLO there HOW are YOU') == ['HELLO there', 'HOW are', 'YOU']\n","\n    assert candidate('hELLO there HoW are YOU') == ['hELLO there HoW are', 'YOU']\n","\n    assert candidate('7 is a NUMBER') == ['7 is a', 'NUMBER']\n","\n    assert candidate('NUMBER 7') == ['NUMBER 7']\n"],"entry_point":"f_13209288","intent":"split string `input` based on occurrences of regex pattern '[ ](?=[A-Z]+\\\\b)'","library":["re"],"docs":[{"text":"torch.nn.functional.instance_norm(input, running_mean=None, running_var=None, weight=None, bias=None, use_input_stats=True, momentum=0.1, eps=1e-05) [source]\n \nApplies Instance Normalization for each channel in each data sample in a batch. See InstanceNorm1d, InstanceNorm2d, InstanceNorm3d for details.","title":"torch.nn.functional#torch.nn.functional.instance_norm"},{"text":"torch.ravel(input) \u2192 Tensor  \nReturn a contiguous flattened tensor. A copy is made only if needed.  Parameters \ninput (Tensor) \u2013 the input tensor.   Example: >>> t = torch.tensor([[[1, 2],\n...                    [3, 4]],\n...                   [[5, 6],\n...                    [7, 8]]])\n>>> torch.ravel(t)\ntensor([1, 2, 3, 4, 5, 6, 7, 8])","title":"torch.generated.torch.ravel#torch.ravel"},{"text":"tf.sparse.split     View source on GitHub    Split a SparseTensor into num_split tensors along axis. \ntf.sparse.split(\n    sp_input=None, num_split=None, axis=None, name=None\n)\n If the sp_input.dense_shape[axis] is not an integer multiple of num_split each slice starting from 0:shape[axis] % num_split gets extra one dimension. For example: \nindices = [[0, 2], [0, 4], [0, 5], [1, 0], [1, 1]]\nvalues = [1, 2, 3, 4, 5]\nt = tf.SparseTensor(indices=indices, values=values, dense_shape=[2, 7])\ntf.sparse.to_dense(t)\n<tf.Tensor: shape=(2, 7), dtype=int32, numpy=\narray([[0, 0, 1, 0, 2, 3, 0],\n       [4, 5, 0, 0, 0, 0, 0]], dtype=int32)>\n \noutput = tf.sparse.split(sp_input=t, num_split=2, axis=1)\ntf.sparse.to_dense(output[0])\n<tf.Tensor: shape=(2, 4), dtype=int32, numpy=\narray([[0, 0, 1, 0],\n       [4, 5, 0, 0]], dtype=int32)>\ntf.sparse.to_dense(output[1])\n<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\narray([[2, 3, 0],\n       [0, 0, 0]], dtype=int32)>\n \noutput = tf.sparse.split(sp_input=t, num_split=2, axis=0)\ntf.sparse.to_dense(output[0])\n<tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[0, 0, 1, 0, 2, 3, 0]],\ndtype=int32)>\ntf.sparse.to_dense(output[1])\n<tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[4, 5, 0, 0, 0, 0, 0]],\ndtype=int32)>\n \noutput = tf.sparse.split(sp_input=t, num_split=2, axis=-1)\ntf.sparse.to_dense(output[0])\n<tf.Tensor: shape=(2, 4), dtype=int32, numpy=\narray([[0, 0, 1, 0],\n       [4, 5, 0, 0]], dtype=int32)>\ntf.sparse.to_dense(output[1])\n<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\narray([[2, 3, 0],\n       [0, 0, 0]], dtype=int32)>\n\n \n\n\n Args\n  sp_input   The SparseTensor to split.  \n  num_split   A Python integer. The number of ways to split.  \n  axis   A 0-D int32 Tensor. The dimension along which to split. Must be in range [-rank, rank), where rank is the number of dimensions in the input SparseTensor.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   num_split SparseTensor objects resulting from splitting value.  \n\n \n\n\n Raises\n  TypeError   If sp_input is not a SparseTensor.","title":"tensorflow.sparse.split"},{"text":"torch.atleast_3d(*tensors) [source]\n \nReturns a 3-dimensional view of each input tensor with zero dimensions. Input tensors with three or more dimensions are returned as-is. :param input: :type input: Tensor or list of Tensors  Returns \noutput (Tensor or tuple of Tensors)   Example >>> x = torch.tensor(0.5)\n>>> x\ntensor(0.5000)\n>>> torch.atleast_3d(x)\ntensor([[[0.5000]]])\n>>> y = torch.randn(2,2)\n>>> y\ntensor([[-0.8079,  0.7460],\n        [-1.1647,  1.4734]])\n>>> torch.atleast_3d(y)\ntensor([[[-0.8079],\n        [ 0.7460]],\n\n        [[-1.1647],\n        [ 1.4734]]])\n>>> x = torch.randn(1,1,1)\n>>> x\ntensor([[[-1.5689]]])\n>>> torch.atleast_3d(x)\ntensor([[[-1.5689]]])\n>>> x = torch.tensor(0.5)\n>>> y = torch.tensor(1.)\n>>> torch.atleast_3d((x,y))\n(tensor([[[0.5000]]]), tensor([[[1.]]]))","title":"torch.generated.torch.atleast_3d#torch.atleast_3d"},{"text":"torch.tile(input, reps) \u2192 Tensor  \nConstructs a tensor by repeating the elements of input. The reps argument specifies the number of repetitions in each dimension. If reps specifies fewer dimensions than input has, then ones are prepended to reps until all dimensions are specified. For example, if input has shape (8, 6, 4, 2) and reps is (2, 2), then reps is treated as (1, 1, 2, 2). Analogously, if input has fewer dimensions than reps specifies, then input is treated as if it were unsqueezed at dimension zero until it has as many dimensions as reps specifies. For example, if input has shape (4, 2) and reps is (3, 3, 2, 2), then input is treated as if it had the shape (1, 1, 4, 2).  Note This function is similar to NumPy\u2019s tile function.   Parameters \n \ninput (Tensor) \u2013 the tensor whose elements to repeat. \nreps (tuple) \u2013 the number of repetitions per dimension.    Example: >>> x = torch.tensor([1, 2, 3])\n>>> x.tile((2,))\ntensor([1, 2, 3, 1, 2, 3])\n>>> y = torch.tensor([[1, 2], [3, 4]])\n>>> torch.tile(y, (2, 2))\ntensor([[1, 2, 1, 2],\n        [3, 4, 3, 4],\n        [1, 2, 1, 2],\n        [3, 4, 3, 4]])","title":"torch.generated.torch.tile#torch.tile"},{"text":"torch.mvlgamma(input, p) \u2192 Tensor  \nComputes the multivariate log-gamma function) with dimension pp  element-wise, given by  log\u2061(\u0393p(a))=C+\u2211i=1plog\u2061(\u0393(a\u2212i\u221212))\\log(\\Gamma_{p}(a)) = C + \\displaystyle \\sum_{i=1}^{p} \\log\\left(\\Gamma\\left(a - \\frac{i - 1}{2}\\right)\\right)  \nwhere C=log\u2061(\u03c0)\u00d7p(p\u22121)4C = \\log(\\pi) \\times \\frac{p (p - 1)}{4}  and \u0393(\u22c5)\\Gamma(\\cdot)  is the Gamma function. All elements must be greater than p\u221212\\frac{p - 1}{2} , otherwise an error would be thrown.  Parameters \n \ninput (Tensor) \u2013 the tensor to compute the multivariate log-gamma function \np (int) \u2013 the number of dimensions    Example: >>> a = torch.empty(2, 3).uniform_(1, 2)\n>>> a\ntensor([[1.6835, 1.8474, 1.1929],\n        [1.0475, 1.7162, 1.4180]])\n>>> torch.mvlgamma(a, 2)\ntensor([[0.3928, 0.4007, 0.7586],\n        [1.0311, 0.3901, 0.5049]])","title":"torch.generated.torch.mvlgamma#torch.mvlgamma"},{"text":"add_graph(model, input_to_model=None, verbose=False) [source]\n \nAdd graph data to summary.  Parameters \n \nmodel (torch.nn.Module) \u2013 Model to draw. \ninput_to_model (torch.Tensor or list of torch.Tensor) \u2013 A variable or a tuple of variables to be fed. \nverbose (bool) \u2013 Whether to print graph structure in console.","title":"torch.tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_graph"},{"text":"tf.raw_ops.TileGrad Returns the gradient of Tile.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.raw_ops.TileGrad  \ntf.raw_ops.TileGrad(\n    input, multiples, name=None\n)\n Since Tile takes an input and repeats the input multiples times along each dimension, TileGrad takes in multiples and aggregates each repeated tile of input into output.\n \n\n\n Args\n  input   A Tensor.  \n  multiples   A Tensor of type int32.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor. Has the same type as input.","title":"tensorflow.raw_ops.tilegrad"},{"text":"torch.solve(input, A, *, out=None) -> (Tensor, Tensor)  \nThis function returns the solution to the system of linear equations represented by AX=BAX = B  and the LU factorization of A, in order as a namedtuple solution, LU. LU contains L and U factors for LU factorization of A. torch.solve(B, A) can take in 2D inputs B, A or inputs that are batches of 2D matrices. If the inputs are batches, then returns batched outputs solution, LU. Supports real-valued and complex-valued inputs.  Note Irrespective of the original strides, the returned matrices solution and LU will be transposed, i.e. with strides like B.contiguous().transpose(-1, -2).stride() and A.contiguous().transpose(-1, -2).stride() respectively.   Parameters \n \ninput (Tensor) \u2013 input matrix BB  of size (\u2217,m,k)(*, m, k)  , where \u2217*  is zero or more batch dimensions. \nA (Tensor) \u2013 input square matrix of size (\u2217,m,m)(*, m, m) , where \u2217*  is zero or more batch dimensions.   Keyword Arguments \nout ((Tensor, Tensor), optional) \u2013 optional output tuple.   Example: >>> A = torch.tensor([[6.80, -2.11,  5.66,  5.97,  8.23],\n...                   [-6.05, -3.30,  5.36, -4.44,  1.08],\n...                   [-0.45,  2.58, -2.70,  0.27,  9.04],\n...                   [8.32,  2.71,  4.35,  -7.17,  2.14],\n...                   [-9.67, -5.14, -7.26,  6.08, -6.87]]).t()\n>>> B = torch.tensor([[4.02,  6.19, -8.22, -7.57, -3.03],\n...                   [-1.56,  4.00, -8.67,  1.75,  2.86],\n...                   [9.81, -4.09, -4.57, -8.61,  8.99]]).t()\n>>> X, LU = torch.solve(B, A)\n>>> torch.dist(B, torch.mm(A, X))\ntensor(1.00000e-06 *\n       7.0977)\n\n>>> # Batched solver example\n>>> A = torch.randn(2, 3, 1, 4, 4)\n>>> B = torch.randn(2, 3, 1, 4, 6)\n>>> X, LU = torch.solve(B, A)\n>>> torch.dist(B, A.matmul(X))\ntensor(1.00000e-06 *\n   3.6386)","title":"torch.generated.torch.solve#torch.solve"},{"text":"Pattern.split(string, maxsplit=0)  \nIdentical to the split() function, using the compiled pattern.","title":"python.library.re#re.Pattern.split"}]}
{"task_id":13209288,"prompt":"def f_13209288(input):\n\treturn ","suffix":"","canonical_solution":"re.split('[ ](?=[A-Z])', input)","test_start":"\nimport re\n\ndef check(candidate):","test":["\n    assert candidate('HELLO there HOW are YOU') == ['HELLO there', 'HOW are', 'YOU']\n","\n    assert candidate('hELLO there HoW are YOU') == ['hELLO there', 'HoW are', 'YOU']\n","\n    assert candidate('7 is a NUMBER') == ['7 is a', 'NUMBER']\n","\n    assert candidate('NUMBER 7') == ['NUMBER 7']\n"],"entry_point":"f_13209288","intent":"Split string `input` at every space followed by an upper-case letter","library":["re"],"docs":[{"text":"tf.sparse.slice     View source on GitHub    Slice a SparseTensor based on the start and `size.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.sparse.slice, tf.compat.v1.sparse_slice  \ntf.sparse.slice(\n    sp_input, start, size, name=None\n)\n For example, if the input is input_tensor = shape = [2, 7]\n[    a   d e  ]\n[b c          ]\n Graphically the output tensors are: sparse.slice([0, 0], [2, 4]) = shape = [2, 4]\n[    a  ]\n[b c    ]\n\nsparse.slice([0, 4], [2, 3]) = shape = [2, 3]\n[ d e  ]\n[      ]\n\n \n\n\n Args\n  sp_input   The SparseTensor to split.  \n  start   1-D. tensor represents the start of the slice.  \n  size   1-D. tensor represents the size of the slice.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A SparseTensor objects resulting from splicing.  \n\n \n\n\n Raises\n  TypeError   If sp_input is not a SparseTensor.","title":"tensorflow.sparse.slice"},{"text":"tf.raw_ops.StringUpper Converts all lowercase characters into their respective uppercase replacements.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.raw_ops.StringUpper  \ntf.raw_ops.StringUpper(\n    input, encoding='', name=None\n)\n Example: \ntf.strings.upper(\"CamelCase string and ALL CAPS\")\n<tf.Tensor: shape=(), dtype=string, numpy=b'CAMELCASE STRING AND ALL CAPS'>\n\n \n\n\n Args\n  input   A Tensor of type string.  \n  encoding   An optional string. Defaults to \"\".  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor of type string.","title":"tensorflow.raw_ops.stringupper"},{"text":"torch.flipud(input) \u2192 Tensor  \nFlip tensor in the up\/down direction, returning a new tensor. Flip the entries in each column in the up\/down direction. Rows are preserved, but appear in a different order than before.  Note Requires the tensor to be at least 1-D.   Note torch.flipud makes a copy of input\u2019s data. This is different from NumPy\u2019s np.flipud, which returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data, torch.flipud is expected to be slower than np.flipud.   Parameters \ninput (Tensor) \u2013 Must be at least 1-dimensional.   Example: >>> x = torch.arange(4).view(2, 2)\n>>> x\ntensor([[0, 1],\n        [2, 3]])\n>>> torch.flipud(x)\ntensor([[2, 3],\n        [0, 1]])","title":"torch.generated.torch.flipud#torch.flipud"},{"text":"torch.chunk(input, chunks, dim=0) \u2192 List of Tensors  \nSplits a tensor into a specific number of chunks. Each chunk is a view of the input tensor. Last chunk will be smaller if the tensor size along the given dimension dim is not divisible by chunks.  Parameters \n \ninput (Tensor) \u2013 the tensor to split \nchunks (int) \u2013 number of chunks to return \ndim (int) \u2013 dimension along which to split the tensor","title":"torch.generated.torch.chunk#torch.chunk"},{"text":"tf.strings.upper Converts all lowercase characters into their respective uppercase replacements.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.strings.upper  \ntf.strings.upper(\n    input, encoding='', name=None\n)\n Example: \ntf.strings.upper(\"CamelCase string and ALL CAPS\")\n<tf.Tensor: shape=(), dtype=string, numpy=b'CAMELCASE STRING AND ALL CAPS'>\n\n \n\n\n Args\n  input   A Tensor of type string.  \n  encoding   An optional string. Defaults to \"\".  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor of type string.","title":"tensorflow.strings.upper"},{"text":"tf.sparse.split     View source on GitHub    Split a SparseTensor into num_split tensors along axis. \ntf.sparse.split(\n    sp_input=None, num_split=None, axis=None, name=None\n)\n If the sp_input.dense_shape[axis] is not an integer multiple of num_split each slice starting from 0:shape[axis] % num_split gets extra one dimension. For example: \nindices = [[0, 2], [0, 4], [0, 5], [1, 0], [1, 1]]\nvalues = [1, 2, 3, 4, 5]\nt = tf.SparseTensor(indices=indices, values=values, dense_shape=[2, 7])\ntf.sparse.to_dense(t)\n<tf.Tensor: shape=(2, 7), dtype=int32, numpy=\narray([[0, 0, 1, 0, 2, 3, 0],\n       [4, 5, 0, 0, 0, 0, 0]], dtype=int32)>\n \noutput = tf.sparse.split(sp_input=t, num_split=2, axis=1)\ntf.sparse.to_dense(output[0])\n<tf.Tensor: shape=(2, 4), dtype=int32, numpy=\narray([[0, 0, 1, 0],\n       [4, 5, 0, 0]], dtype=int32)>\ntf.sparse.to_dense(output[1])\n<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\narray([[2, 3, 0],\n       [0, 0, 0]], dtype=int32)>\n \noutput = tf.sparse.split(sp_input=t, num_split=2, axis=0)\ntf.sparse.to_dense(output[0])\n<tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[0, 0, 1, 0, 2, 3, 0]],\ndtype=int32)>\ntf.sparse.to_dense(output[1])\n<tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[4, 5, 0, 0, 0, 0, 0]],\ndtype=int32)>\n \noutput = tf.sparse.split(sp_input=t, num_split=2, axis=-1)\ntf.sparse.to_dense(output[0])\n<tf.Tensor: shape=(2, 4), dtype=int32, numpy=\narray([[0, 0, 1, 0],\n       [4, 5, 0, 0]], dtype=int32)>\ntf.sparse.to_dense(output[1])\n<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\narray([[2, 3, 0],\n       [0, 0, 0]], dtype=int32)>\n\n \n\n\n Args\n  sp_input   The SparseTensor to split.  \n  num_split   A Python integer. The number of ways to split.  \n  axis   A 0-D int32 Tensor. The dimension along which to split. Must be in range [-rank, rank), where rank is the number of dimensions in the input SparseTensor.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   num_split SparseTensor objects resulting from splitting value.  \n\n \n\n\n Raises\n  TypeError   If sp_input is not a SparseTensor.","title":"tensorflow.sparse.split"},{"text":"tf.raw_ops.Snapshot Returns a copy of the input tensor.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.raw_ops.Snapshot  \ntf.raw_ops.Snapshot(\n    input, name=None\n)\n\n \n\n\n Args\n  input   A Tensor.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor. Has the same type as input.","title":"tensorflow.raw_ops.snapshot"},{"text":"torch.atleast_3d(*tensors) [source]\n \nReturns a 3-dimensional view of each input tensor with zero dimensions. Input tensors with three or more dimensions are returned as-is. :param input: :type input: Tensor or list of Tensors  Returns \noutput (Tensor or tuple of Tensors)   Example >>> x = torch.tensor(0.5)\n>>> x\ntensor(0.5000)\n>>> torch.atleast_3d(x)\ntensor([[[0.5000]]])\n>>> y = torch.randn(2,2)\n>>> y\ntensor([[-0.8079,  0.7460],\n        [-1.1647,  1.4734]])\n>>> torch.atleast_3d(y)\ntensor([[[-0.8079],\n        [ 0.7460]],\n\n        [[-1.1647],\n        [ 1.4734]]])\n>>> x = torch.randn(1,1,1)\n>>> x\ntensor([[[-1.5689]]])\n>>> torch.atleast_3d(x)\ntensor([[[-1.5689]]])\n>>> x = torch.tensor(0.5)\n>>> y = torch.tensor(1.)\n>>> torch.atleast_3d((x,y))\n(tensor([[[0.5000]]]), tensor([[[1.]]]))","title":"torch.generated.torch.atleast_3d#torch.atleast_3d"},{"text":"torch.nn.functional.avg_pool3d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None) \u2192 Tensor  \nApplies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kW  regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sW  steps. The number of output features is equal to \u230ainput planessT\u230b\\lfloor\\frac{\\text{input planes}}{sT}\\rfloor . See AvgPool3d for details and output shape.  Parameters \n \ninput \u2013 input tensor (minibatch,in_channels,iT\u00d7iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iT \\times iH , iW) \n \nkernel_size \u2013 size of the pooling region. Can be a single number or a tuple (kT, kH, kW)\n \nstride \u2013 stride of the pooling operation. Can be a single number or a tuple (sT, sH, sW). Default: kernel_size\n \npadding \u2013 implicit zero paddings on both sides of the input. Can be a single number or a tuple (padT, padH, padW), Default: 0 \nceil_mode \u2013 when True, will use ceil instead of floor in the formula to compute the output shape \ncount_include_pad \u2013 when True, will include the zero-padding in the averaging calculation \ndivisor_override \u2013 if specified, it will be used as divisor, otherwise size of the pooling region will be used. Default: None","title":"torch.nn.functional#torch.nn.functional.avg_pool3d"},{"text":"torch.cholesky_inverse(input, upper=False, *, out=None) \u2192 Tensor  \nComputes the inverse of a symmetric positive-definite matrix AA  using its Cholesky factor uu : returns matrix inv. The inverse is computed using LAPACK routines dpotri and spotri (and the corresponding MAGMA routines). If upper is False, uu  is lower triangular such that the returned tensor is  inv=(uuT)\u22121inv = (uu^{{T}})^{{-1}}  \nIf upper is True or not provided, uu  is upper triangular such that the returned tensor is  inv=(uTu)\u22121inv = (u^T u)^{{-1}}  \n Parameters \n \ninput (Tensor) \u2013 the input 2-D tensor uu , a upper or lower triangular Cholesky factor \nupper (bool, optional) \u2013 whether to return a lower (default) or upper triangular matrix   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor for inv   Example: >>> a = torch.randn(3, 3)\n>>> a = torch.mm(a, a.t()) + 1e-05 * torch.eye(3) # make symmetric positive definite\n>>> u = torch.cholesky(a)\n>>> a\ntensor([[  0.9935,  -0.6353,   1.5806],\n        [ -0.6353,   0.8769,  -1.7183],\n        [  1.5806,  -1.7183,  10.6618]])\n>>> torch.cholesky_inverse(u)\ntensor([[ 1.9314,  1.2251, -0.0889],\n        [ 1.2251,  2.4439,  0.2122],\n        [-0.0889,  0.2122,  0.1412]])\n>>> a.inverse()\ntensor([[ 1.9314,  1.2251, -0.0889],\n        [ 1.2251,  2.4439,  0.2122],\n        [-0.0889,  0.2122,  0.1412]])","title":"torch.generated.torch.cholesky_inverse#torch.cholesky_inverse"}]}
{"task_id":24642040,"prompt":"def f_24642040(url, files, headers, data):\n\treturn ","suffix":"","canonical_solution":"requests.post(url, files=files, headers=headers, data=data)","test_start":"\nimport requests\nfrom unittest.mock import Mock\n\ndef check(candidate):","test":["\n    requests.post = Mock()\n    try:\n        candidate('https:\/\/www.google.com', ['a.txt'], {'accept': 'text\/json'}, {'name': 'abc'})\n    except:\n        assert False\n"],"entry_point":"f_24642040","intent":"send multipart encoded file `files` to url `url` with headers `headers` and metadata `data`","library":["requests"],"docs":[]}
{"task_id":4290716,"prompt":"def f_4290716(filename, bytes_):\n\treturn ","suffix":"","canonical_solution":"open(filename, 'wb').write(bytes_)","test_start":"\ndef check(candidate):","test":["\n    bytes_ = b'68 65 6c 6c 6f'\n    candidate(\"tmpfile\", bytes_)\n\n    with open(\"tmpfile\", 'rb') as fr:\n        assert fr.read() == bytes_\n"],"entry_point":"f_4290716","intent":"write bytes `bytes_` to a file `filename` in python 3","library":[],"docs":[]}
{"task_id":33078554,"prompt":"def f_33078554(lst, dct):\n\treturn ","suffix":"","canonical_solution":"[dct[k] for k in lst]","test_start":"\ndef check(candidate):","test":["\n    assert candidate(['c', 'd', 'a', 'b', 'd'], {'a': '3', 'b': '3', 'c': '5', 'd': '3'}) == ['5', '3', '3', '3', '3'] \n","\n    assert candidate(['c', 'd', 'a', 'b', 'd'], {'a': 3, 'b': 3, 'c': 5, 'd': 3}) == [5, 3, 3, 3, 3] \n","\n    assert candidate(['c', 'd', 'a', 'b'], {'a': 3, 'b': 3, 'c': 5, 'd': 3}) == [5, 3, 3, 3]\n"],"entry_point":"f_33078554","intent":"get a list from a list `lst` with values mapped into a dictionary `dct`","library":[],"docs":[]}
{"task_id":15247628,"prompt":"def f_15247628(x):\n\treturn ","suffix":"","canonical_solution":"x['name'][x.duplicated('name')]","test_start":"\nimport pandas as pd \n\ndef check(candidate): ","test":["\n    assert candidate(pd.DataFrame([{'name': 'willy', 'age': 10}, {'name': 'wilson', 'age': 11}, {'name': 'zoe', 'age': 10}])).tolist() == [] \n","\n    assert candidate(pd.DataFrame([{'name': 'willy', 'age': 10}, {'name': 'willy', 'age': 11}, {'name': 'zoe', 'age': 10}])).tolist() == ['willy'] \n","\n    assert candidate(pd.DataFrame([{'name': 'willy', 'age': 11}, {'name': 'willy', 'age': 11}, {'name': 'zoe', 'age': 10}])).tolist() == ['willy'] \n","\n    assert candidate(pd.DataFrame([{'name': 'Willy', 'age': 11}, {'name': 'willy', 'age': 11}, {'name': 'zoe', 'age': 10}])).tolist() == []\n"],"entry_point":"f_15247628","intent":"find duplicate names in column 'name' of the dataframe `x`","library":["pandas"],"docs":[{"text":"numpy.lib.recfunctions.find_duplicates(a, key=None, ignoremask=True, return_index=False)[source]\n \nFind the duplicates in a structured array along a given key  Parameters \n \naarray-like\n\n\nInput array  \nkey{string, None}, optional\n\n\nName of the fields along which to check the duplicates. If None, the search is performed by records  \nignoremask{True, False}, optional\n\n\nWhether masked data should be discarded or considered as duplicates.  \nreturn_index{False, True}, optional\n\n\nWhether to return the indices of the duplicated values.     Examples >>> from numpy.lib import recfunctions as rfn\n>>> ndtype = [('a', int)]\n>>> a = np.ma.array([1, 1, 1, 2, 2, 3, 3],\n...         mask=[0, 0, 1, 0, 0, 0, 1]).view(ndtype)\n>>> rfn.find_duplicates(a, ignoremask=True, return_index=True)\n(masked_array(data=[(1,), (1,), (2,), (2,)],\n             mask=[(False,), (False,), (False,), (False,)],\n       fill_value=(999999,),\n            dtype=[('a', '<i8')]), array([0, 1, 3, 4]))","title":"numpy.user.basics.rec#numpy.lib.recfunctions.find_duplicates"},{"text":"tf.raw_ops.DeepCopy Makes a copy of x.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.raw_ops.DeepCopy  \ntf.raw_ops.DeepCopy(\n    x, name=None\n)\n\n \n\n\n Args\n  x   A Tensor. The source tensor of type T.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor. Has the same type as x.","title":"tensorflow.raw_ops.deepcopy"},{"text":"copy.deepcopy(x[, memo])  \nReturn a deep copy of x.","title":"python.library.copy#copy.deepcopy"},{"text":"tf.compat.v1.flags.FlagNameConflictsWithMethodError Raised when a flag name conflicts with FlagValues methods. Inherits From: Error  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.app.flags.FlagNameConflictsWithMethodError","title":"tensorflow.compat.v1.flags.flagnameconflictswithmethoderror"},{"text":"pandas.Index.set_names   Index.set_names(names, level=None, inplace=False)[source]\n \nSet Index or MultiIndex name. Able to set new names partially and by level.  Parameters \n \nnames:label or list of label or dict-like for MultiIndex\n\n\nName(s) to set.  Changed in version 1.3.0.   \nlevel:int, label or list of int or label, optional\n\n\nIf the index is a MultiIndex and names is not dict-like, level(s) to set (None for all levels). Otherwise level must be None.  Changed in version 1.3.0.   \ninplace:bool, default False\n\n\nModifies the object directly, instead of creating a new Index or MultiIndex.    Returns \n Index or None\n\nThe same type as the caller or None if inplace=True.      See also  Index.rename\n\nAble to set new names without level.    Examples \n>>> idx = pd.Index([1, 2, 3, 4])\n>>> idx\nInt64Index([1, 2, 3, 4], dtype='int64')\n>>> idx.set_names('quarter')\nInt64Index([1, 2, 3, 4], dtype='int64', name='quarter')\n  \n>>> idx = pd.MultiIndex.from_product([['python', 'cobra'],\n...                                   [2018, 2019]])\n>>> idx\nMultiIndex([('python', 2018),\n            ('python', 2019),\n            ( 'cobra', 2018),\n            ( 'cobra', 2019)],\n           )\n>>> idx.set_names(['kind', 'year'], inplace=True)\n>>> idx\nMultiIndex([('python', 2018),\n            ('python', 2019),\n            ( 'cobra', 2018),\n            ( 'cobra', 2019)],\n           names=['kind', 'year'])\n>>> idx.set_names('species', level=0)\nMultiIndex([('python', 2018),\n            ('python', 2019),\n            ( 'cobra', 2018),\n            ( 'cobra', 2019)],\n           names=['species', 'year'])\n  When renaming levels with a dict, levels can not be passed. \n>>> idx.set_names({'kind': 'snake'})\nMultiIndex([('python', 2018),\n            ('python', 2019),\n            ( 'cobra', 2018),\n            ( 'cobra', 2019)],\n           names=['snake', 'year'])","title":"pandas.reference.api.pandas.index.set_names"},{"text":"tf.compat.v1.flags.DuplicateFlagError Raised if there is a flag naming conflict. Inherits From: Error  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.app.flags.DuplicateFlagError  Methods from_flag \n@classmethod\nfrom_flag(\n    flagname, flag_values, other_flag_values=None\n)\n Creates a DuplicateFlagError by providing flag name and values.\n \n\n\n Args\n  flagname   str, the name of the flag being redefined.  \n  flag_values   FlagValues, the FlagValues instance containing the first definition of flagname.  \n  other_flag_values   FlagValues, if it is not None, it should be the FlagValues object where the second definition of flagname occurs. If it is None, we assume that we're being called when attempting to create the flag a second time, and we use the module calling this one as the source of the second definition.   \n \n\n\n Returns   An instance of DuplicateFlagError.","title":"tensorflow.compat.v1.flags.duplicateflagerror"},{"text":"staticdefault_units(x, axis)[source]\n \nReturn the default unit for x or None for the given axis.","title":"matplotlib.units_api#matplotlib.units.ConversionInterface.default_units"},{"text":"pandas.Index.duplicated   Index.duplicated(keep='first')[source]\n \nIndicate duplicate index values. Duplicated values are indicated as True values in the resulting array. Either all duplicates, all except the first, or all except the last occurrence of duplicates can be indicated.  Parameters \n \nkeep:{\u2018first\u2019, \u2018last\u2019, False}, default \u2018first\u2019\n\n\nThe value or values in a set of duplicates to mark as missing.  \u2018first\u2019 : Mark duplicates as True except for the first occurrence. \u2018last\u2019 : Mark duplicates as True except for the last occurrence. False : Mark all duplicates as True.     Returns \n np.ndarray[bool]\n    See also  Series.duplicated\n\nEquivalent method on pandas.Series.  DataFrame.duplicated\n\nEquivalent method on pandas.DataFrame.  Index.drop_duplicates\n\nRemove duplicate values from Index.    Examples By default, for each set of duplicated values, the first occurrence is set to False and all others to True: \n>>> idx = pd.Index(['lama', 'cow', 'lama', 'beetle', 'lama'])\n>>> idx.duplicated()\narray([False, False,  True, False,  True])\n  which is equivalent to \n>>> idx.duplicated(keep='first')\narray([False, False,  True, False,  True])\n  By using \u2018last\u2019, the last occurrence of each set of duplicated values is set on False and all others on True: \n>>> idx.duplicated(keep='last')\narray([ True, False,  True, False, False])\n  By setting keep on False, all duplicates are True: \n>>> idx.duplicated(keep=False)\narray([ True, False,  True, False,  True])","title":"pandas.reference.api.pandas.index.duplicated"},{"text":"pandas.Index.names   propertyIndex.names","title":"pandas.reference.api.pandas.index.names"},{"text":"tf.raw_ops.UniqueV2 Finds unique elements along an axis of a tensor.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.raw_ops.UniqueV2  \ntf.raw_ops.UniqueV2(\n    x, axis, out_idx=tf.dtypes.int32, name=None\n)\n This operation either returns a tensor y containing unique elements along the axis of a tensor. The returned unique elements is sorted in the same order as they occur along axis in x. This operation also returns a tensor idx that is the same size as the number of the elements in x along the axis dimension. It contains the index in the unique output y. In other words, for an 1-D tensor x with `axis = None: y[idx[i]] = x[i] for i in [0, 1,...,rank(x) - 1] For example: # tensor 'x' is [1, 1, 2, 4, 4, 4, 7, 8, 8]\ny, idx = unique(x)\ny ==> [1, 2, 4, 7, 8]\nidx ==> [0, 0, 1, 2, 2, 2, 3, 4, 4]\n For an 2-D tensor x with axis = 0: # tensor 'x' is [[1, 0, 0],\n#                [1, 0, 0],\n#                [2, 0, 0]]\ny, idx = unique(x, axis=0)\ny ==> [[1, 0, 0],\n       [2, 0, 0]]\nidx ==> [0, 0, 1]\n For an 2-D tensor x with axis = 1: # tensor 'x' is [[1, 0, 0],\n#                [1, 0, 0],\n#                [2, 0, 0]]\ny, idx = unique(x, axis=1)\ny ==> [[1, 0],\n       [1, 0],\n       [2, 0]]\nidx ==> [0, 1, 1]\n\n \n\n\n Args\n  x   A Tensor. A Tensor.  \n  axis   A Tensor. Must be one of the following types: int32, int64. A Tensor of type int32 (default: None). The axis of the Tensor to find the unique elements.  \n  out_idx   An optional tf.DType from: tf.int32, tf.int64. Defaults to tf.int32.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A tuple of Tensor objects (y, idx).     y   A Tensor. Has the same type as x.  \n  idx   A Tensor of type out_idx.","title":"tensorflow.raw_ops.uniquev2"}]}
{"task_id":783897,"prompt":"def f_783897():\n\treturn ","suffix":"","canonical_solution":"round(1.923328437452, 3)","test_start":"\ndef check(candidate): ","test":["\n    assert candidate() == 1.923\n"],"entry_point":"f_783897","intent":"truncate float 1.923328437452 to 3 decimal places","library":[],"docs":[]}
{"task_id":22859493,"prompt":"def f_22859493(li):\n\treturn ","suffix":"","canonical_solution":"sorted(li, key=lambda x: datetime.strptime(x[1], '%d\/%m\/%Y'), reverse=True)","test_start":"\nfrom datetime import datetime\n\ndef check(candidate): ","test":["\n    assert candidate([['name', '01\/03\/2012', 'job'], ['name', '02\/05\/2013', 'job'], ['name', '03\/08\/2014', 'job']]) == [['name', '03\/08\/2014', 'job'], ['name', '02\/05\/2013', 'job'], ['name', '01\/03\/2012', 'job']] \n","\n    assert candidate([['name', '01\/03\/2012', 'job'], ['name', '02\/05\/2012', 'job'], ['name', '03\/08\/2012', 'job']]) == [['name', '03\/08\/2012', 'job'], ['name', '02\/05\/2012', 'job'], ['name', '01\/03\/2012', 'job']] \n","\n    assert candidate([['name', '01\/03\/2012', 'job'], ['name', '02\/03\/2012', 'job'], ['name', '03\/03\/2012', 'job']]) == [['name', '03\/03\/2012', 'job'], ['name', '02\/03\/2012', 'job'], ['name', '01\/03\/2012', 'job']] \n","\n    assert candidate([['name', '03\/03\/2012', 'job'], ['name', '03\/03\/2012', 'job'], ['name', '03\/03\/2012', 'job']]) == [['name', '03\/03\/2012', 'job'], ['name', '03\/03\/2012', 'job'], ['name', '03\/03\/2012', 'job']] \n"],"entry_point":"f_22859493","intent":"sort list `li` in descending order based on the date value in second element of each list in list `li`","library":["datetime"],"docs":[{"text":"dates(field, kind, order='ASC')","title":"django.ref.models.querysets#django.db.models.query.QuerySet.dates"},{"text":"get_date_list(queryset, date_type=None, ordering='ASC')  \nReturns the list of dates of type date_type for which queryset contains entries. For example, get_date_list(qs, 'year') will return the list of years for which qs has entries. If date_type isn\u2019t provided, the result of get_date_list_period() is used. date_type and ordering are passed to QuerySet.dates().","title":"django.ref.class-based-views.mixins-date-based#django.views.generic.dates.BaseDateListView.get_date_list"},{"text":"class BaseDateListView  \nA base class that provides common behavior for all date-based views. There won\u2019t normally be a reason to instantiate BaseDateListView; instantiate one of the subclasses instead. While this view (and its subclasses) are executing, self.object_list will contain the list of objects that the view is operating upon, and self.date_list will contain the list of dates for which data is available. Mixins  DateMixin MultipleObjectMixin  Methods and Attributes  \nallow_empty  \nA boolean specifying whether to display the page if no objects are available. If this is True and no objects are available, the view will display an empty page instead of raising a 404. This is identical to django.views.generic.list.MultipleObjectMixin.allow_empty, except for the default value, which is False. \n  \ndate_list_period  \nOptional A string defining the aggregation period for date_list. It must be one of 'year' (default), 'month', or 'day'. \n  \nget_dated_items()  \nReturns a 3-tuple containing (date_list, object_list, extra_context). date_list is the list of dates for which data is available. object_list is the list of objects. extra_context is a dictionary of context data that will be added to any context data provided by the MultipleObjectMixin. \n  \nget_dated_queryset(**lookup)  \nReturns a queryset, filtered using the query arguments defined by lookup. Enforces any restrictions on the queryset, such as allow_empty and allow_future. \n  \nget_date_list_period()  \nReturns the aggregation period for date_list. Returns date_list_period by default. \n  \nget_date_list(queryset, date_type=None, ordering='ASC')  \nReturns the list of dates of type date_type for which queryset contains entries. For example, get_date_list(qs, 'year') will return the list of years for which qs has entries. If date_type isn\u2019t provided, the result of get_date_list_period() is used. date_type and ordering are passed to QuerySet.dates().","title":"django.ref.class-based-views.mixins-date-based#django.views.generic.dates.BaseDateListView"},{"text":"invoke(cli=None, args=None, **kwargs)  \nInvokes a CLI command in an isolated environment. See CliRunner.invoke for full method documentation. See Testing CLI Commands for examples. If the obj argument is not given, passes an instance of ScriptInfo that knows how to load the Flask app being tested.  Parameters \n \ncli (Optional[Any]) \u2013 Command object to invoke. Default is the app\u2019s cli group. \nargs (Optional[Any]) \u2013 List of strings to invoke the command with. \nkwargs (Any) \u2013    Returns \na Result object.  Return type \nAny","title":"flask.api.index#flask.testing.FlaskCliRunner.invoke"},{"text":"logits [source]","title":"torch.distributions#torch.distributions.bernoulli.Bernoulli.logits"},{"text":"log_prob(value) [source]","title":"torch.distributions#torch.distributions.bernoulli.Bernoulli.log_prob"},{"text":"sample(sample_shape=torch.Size([])) [source]","title":"torch.distributions#torch.distributions.bernoulli.Bernoulli.sample"},{"text":"logits [source]","title":"torch.distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.logits"},{"text":"probs [source]","title":"torch.distributions#torch.distributions.bernoulli.Bernoulli.probs"},{"text":"log_prob(value) [source]","title":"torch.distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.log_prob"}]}
{"task_id":29394552,"prompt":"def f_29394552(ax):\n\t","suffix":"\n\treturn ","canonical_solution":"ax.set_rlabel_position(135)","test_start":"\nimport matplotlib.pyplot as plt \n\ndef check(candidate): ","test":["\n    ax = plt.subplot(111, polar=True)\n    candidate(ax)\n    assert ax.properties()['rlabel_position'] == 135.0\n"],"entry_point":"f_29394552","intent":"place the radial ticks in plot `ax` at 135 degrees","library":["matplotlib"],"docs":[{"text":"classmatplotlib.projections.polar.RadialTick(*args, **kwargs)[source]\n \nBases: matplotlib.axis.YTick A radial-axis tick. This subclass of YTick provides radial ticks with some small modification to their re-positioning such that ticks are rotated based on axes limits. This results in ticks that are correctly perpendicular to the spine. Labels are also rotated to be perpendicular to the spine, when 'auto' rotation is enabled. bbox is the Bound2D bounding box in display coords of the Axes loc is the tick location in data coords size is the tick size in points   set(*, agg_filter=<UNSET>, alpha=<UNSET>, animated=<UNSET>, clip_box=<UNSET>, clip_on=<UNSET>, clip_path=<UNSET>, gid=<UNSET>, in_layout=<UNSET>, label=<UNSET>, label1=<UNSET>, label2=<UNSET>, pad=<UNSET>, path_effects=<UNSET>, picker=<UNSET>, rasterized=<UNSET>, sketch_params=<UNSET>, snap=<UNSET>, transform=<UNSET>, url=<UNSET>, visible=<UNSET>, zorder=<UNSET>)[source]\n \nSet multiple properties at once. Supported properties are   \nProperty Description   \nagg_filter a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array  \nalpha scalar or None  \nanimated bool  \nclip_box Bbox  \nclip_on bool  \nclip_path Patch or (Path, Transform) or None  \nfigure Figure  \ngid str  \nin_layout bool  \nlabel str  \nlabel1 str  \nlabel2 str  \npad float  \npath_effects AbstractPathEffect  \npicker None or bool or float or callable  \nrasterized bool  \nsketch_params (scale: float, length: float, randomness: float)  \nsnap bool or None  \ntransform Transform  \nurl str  \nvisible bool  \nzorder float   \n   update_position(loc)[source]\n \nSet the location of tick in data coords with scalar loc.","title":"matplotlib.projections_api#matplotlib.projections.polar.RadialTick"},{"text":"set_rticks(*args, **kwargs)[source]","title":"matplotlib.projections_api#matplotlib.projections.polar.PolarAxes.set_rticks"},{"text":"matplotlib.pyplot.minorticks_on   matplotlib.pyplot.minorticks_on()[source]\n \nDisplay minor ticks on the Axes. Displaying minor ticks may reduce performance; you may turn them off using minorticks_off() if drawing speed is a problem.","title":"matplotlib._as_gen.matplotlib.pyplot.minorticks_on"},{"text":"classmatplotlib.projections.polar.ThetaTick(axes, *args, **kwargs)[source]\n \nBases: matplotlib.axis.XTick A theta-axis tick. This subclass of XTick provides angular ticks with some small modification to their re-positioning such that ticks are rotated based on tick location. This results in ticks that are correctly perpendicular to the arc spine. When 'auto' rotation is enabled, labels are also rotated to be parallel to the spine. The label padding is also applied here since it's not possible to use a generic axes transform to produce tick-specific padding. bbox is the Bound2D bounding box in display coords of the Axes loc is the tick location in data coords size is the tick size in points   set(*, agg_filter=<UNSET>, alpha=<UNSET>, animated=<UNSET>, clip_box=<UNSET>, clip_on=<UNSET>, clip_path=<UNSET>, gid=<UNSET>, in_layout=<UNSET>, label=<UNSET>, label1=<UNSET>, label2=<UNSET>, pad=<UNSET>, path_effects=<UNSET>, picker=<UNSET>, rasterized=<UNSET>, sketch_params=<UNSET>, snap=<UNSET>, transform=<UNSET>, url=<UNSET>, visible=<UNSET>, zorder=<UNSET>)[source]\n \nSet multiple properties at once. Supported properties are   \nProperty Description   \nagg_filter a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array  \nalpha scalar or None  \nanimated bool  \nclip_box Bbox  \nclip_on bool  \nclip_path Patch or (Path, Transform) or None  \nfigure Figure  \ngid str  \nin_layout bool  \nlabel str  \nlabel1 str  \nlabel2 str  \npad float  \npath_effects AbstractPathEffect  \npicker None or bool or float or callable  \nrasterized bool  \nsketch_params (scale: float, length: float, randomness: float)  \nsnap bool or None  \ntransform Transform  \nurl str  \nvisible bool  \nzorder float   \n   update_position(loc)[source]\n \nSet the location of tick in data coords with scalar loc.","title":"matplotlib.projections_api#matplotlib.projections.polar.ThetaTick"},{"text":"matplotlib.pyplot.minorticks_off   matplotlib.pyplot.minorticks_off()[source]\n \nRemove minor ticks from the Axes.","title":"matplotlib._as_gen.matplotlib.pyplot.minorticks_off"},{"text":"classmatplotlib.axis.XTick(*args, **kwargs)[source]\n \nContains all the Artists needed to make an x tick - the tick line, the label text and the grid line bbox is the Bound2D bounding box in display coords of the Axes loc is the tick location in data coords size is the tick size in points","title":"matplotlib.axis_api#matplotlib.axis.XTick"},{"text":"set_thetagrids(angles, labels=None, fmt=None, **kwargs)[source]\n \nSet the theta gridlines in a polar plot.  Parameters \n \nanglestuple with floats, degrees\n\n\nThe angles of the theta gridlines.  \nlabelstuple with strings or None\n\n\nThe labels to use at each theta gridline. The projections.polar.ThetaFormatter will be used if None.  \nfmtstr or None\n\n\nFormat string used in matplotlib.ticker.FormatStrFormatter. For example '%f'. Note that the angle that is used is in radians.    Returns \n \nlineslist of lines.Line2D\n\n\nThe theta gridlines.  \nlabelslist of text.Text\n\n\nThe tick labels.    Other Parameters \n **kwargs\n\nkwargs are optional Text properties for the labels.      See also  PolarAxes.set_rgrids\nAxis.get_gridlines\nAxis.get_ticklabels","title":"matplotlib.projections_api#matplotlib.projections.polar.PolarAxes.set_thetagrids"},{"text":"matplotlib.pyplot.thetagrids   matplotlib.pyplot.thetagrids(angles=None, labels=None, fmt=None, **kwargs)[source]\n \nGet or set the theta gridlines on the current polar plot. Call signatures: lines, labels = thetagrids()\nlines, labels = thetagrids(angles, labels=None, fmt=None, **kwargs)\n When called with no arguments, thetagrids simply returns the tuple (lines, labels). When called with arguments, the labels will appear at the specified angles.  Parameters \n \nanglestuple with floats, degrees\n\n\nThe angles of the theta gridlines.  \nlabelstuple with strings or None\n\n\nThe labels to use at each radial gridline. The projections.polar.ThetaFormatter will be used if None.  \nfmtstr or None\n\n\nFormat string used in matplotlib.ticker.FormatStrFormatter. For example '%f'. Note that the angle in radians will be used.    Returns \n \nlineslist of lines.Line2D\n\n\nThe theta gridlines.  \nlabelslist of text.Text\n\n\nThe tick labels.    Other Parameters \n **kwargs\n\nkwargs are optional Text properties for the labels.      See also  pyplot.rgrids\nprojections.polar.PolarAxes.set_thetagrids\nAxis.get_gridlines\nAxis.get_ticklabels\n  Examples # set the locations of the angular gridlines\nlines, labels = thetagrids(range(45, 360, 90))\n\n# set the locations and labels of the angular gridlines\nlines, labels = thetagrids(range(45, 360, 90), ('NE', 'NW', 'SW', 'SE'))","title":"matplotlib._as_gen.matplotlib.pyplot.thetagrids"},{"text":"matplotlib.axis.XAxis.tick_bottom   XAxis.tick_bottom()[source]\n \nMove ticks and ticklabels (if present) to the bottom of the axes. \n  Examples using matplotlib.axis.XAxis.tick_bottom\n \n   Broken Axis   \n\n   Bachelor's degrees by gender","title":"matplotlib._as_gen.matplotlib.axis.xaxis.tick_bottom"},{"text":"matplotlib.pyplot.rgrids   matplotlib.pyplot.rgrids(radii=None, labels=None, angle=None, fmt=None, **kwargs)[source]\n \nGet or set the radial gridlines on the current polar plot. Call signatures: lines, labels = rgrids()\nlines, labels = rgrids(radii, labels=None, angle=22.5, fmt=None, **kwargs)\n When called with no arguments, rgrids simply returns the tuple (lines, labels). When called with arguments, the labels will appear at the specified radial distances and angle.  Parameters \n \nradiituple with floats\n\n\nThe radii for the radial gridlines  \nlabelstuple with strings or None\n\n\nThe labels to use at each radial gridline. The matplotlib.ticker.ScalarFormatter will be used if None.  \nanglefloat\n\n\nThe angular position of the radius labels in degrees.  \nfmtstr or None\n\n\nFormat string used in matplotlib.ticker.FormatStrFormatter. For example '%f'.    Returns \n \nlineslist of lines.Line2D\n\n\nThe radial gridlines.  \nlabelslist of text.Text\n\n\nThe tick labels.    Other Parameters \n **kwargs\n\nkwargs are optional Text properties for the labels.      See also  pyplot.thetagrids\nprojections.polar.PolarAxes.set_rgrids\nAxis.get_gridlines\nAxis.get_ticklabels\n  Examples # set the locations of the radial gridlines\nlines, labels = rgrids( (0.25, 0.5, 1.0) )\n\n# set the locations and labels of the radial gridlines\nlines, labels = rgrids( (0.25, 0.5, 1.0), ('Tom', 'Dick', 'Harry' ))","title":"matplotlib._as_gen.matplotlib.pyplot.rgrids"}]}
{"task_id":3320406,"prompt":"def f_3320406(my_path):\n\treturn ","suffix":"","canonical_solution":"os.path.isabs(my_path)","test_start":"\nimport os\n\ndef check(candidate): ","test":["\n    assert candidate('.') == False \n","\n    assert candidate('\/') == True \n","\n    assert candidate('\/usr') == True\n"],"entry_point":"f_3320406","intent":"check if path `my_path` is an absolute path","library":["os"],"docs":[{"text":"os.path.isabs(path)  \nReturn True if path is an absolute pathname. On Unix, that means it begins with a slash, on Windows that it begins with a (back)slash after chopping off a potential drive letter.  Changed in version 3.6: Accepts a path-like object.","title":"python.library.os.path#os.path.isabs"},{"text":"PurePath.is_absolute()  \nReturn whether the path is absolute or not. A path is considered absolute if it has both a root and (if the flavour allows) a drive: >>> PurePosixPath('\/a\/b').is_absolute()\nTrue\n>>> PurePosixPath('a\/b').is_absolute()\nFalse\n\n>>> PureWindowsPath('c:\/a\/b').is_absolute()\nTrue\n>>> PureWindowsPath('\/a\/b').is_absolute()\nFalse\n>>> PureWindowsPath('c:').is_absolute()\nFalse\n>>> PureWindowsPath('\/\/some\/share').is_absolute()\nTrue","title":"python.library.pathlib#pathlib.PurePath.is_absolute"},{"text":"Path.is_mount()  \nReturn True if the path is a mount point: a point in a file system where a different file system has been mounted. On POSIX, the function checks whether path\u2019s parent, path\/.., is on a different device than path, or whether path\/.. and path point to the same i-node on the same device \u2014 this should detect mount points for all Unix and POSIX variants. Not implemented on Windows.  New in version 3.7.","title":"python.library.pathlib#pathlib.Path.is_mount"},{"text":"os.path.ismount(path)  \nReturn True if pathname path is a mount point: a point in a file system where a different file system has been mounted. On POSIX, the function checks whether path\u2019s parent, path\/.., is on a different device than path, or whether path\/.. and path point to the same i-node on the same device \u2014 this should detect mount points for all Unix and POSIX variants. It is not able to reliably detect bind mounts on the same filesystem. On Windows, a drive letter root and a share UNC are always mount points, and for any other path GetVolumePathName is called to see if it is different from the input path.  New in version 3.4: Support for detecting non-root mount points on Windows.   Changed in version 3.6: Accepts a path-like object.","title":"python.library.os.path#os.path.ismount"},{"text":"PurePath.is_relative_to(*other)  \nReturn whether or not this path is relative to the other path. >>> p = PurePath('\/etc\/passwd')\n>>> p.is_relative_to('\/etc')\nTrue\n>>> p.is_relative_to('\/usr')\nFalse\n  New in version 3.9.","title":"python.library.pathlib#pathlib.PurePath.is_relative_to"},{"text":"os.path.islink(path)  \nReturn True if path refers to an existing directory entry that is a symbolic link. Always False if symbolic links are not supported by the Python runtime.  Changed in version 3.6: Accepts a path-like object.","title":"python.library.os.path#os.path.islink"},{"text":"PurePath.is_reserved()  \nWith PureWindowsPath, return True if the path is considered reserved under Windows, False otherwise. With PurePosixPath, False is always returned. >>> PureWindowsPath('nul').is_reserved()\nTrue\n>>> PurePosixPath('nul').is_reserved()\nFalse\n File system calls on reserved paths can fail mysteriously or have unintended effects.","title":"python.library.pathlib#pathlib.PurePath.is_reserved"},{"text":"Path.exists()  \nWhether the path points to an existing file or directory: >>> Path('.').exists()\nTrue\n>>> Path('setup.py').exists()\nTrue\n>>> Path('\/etc').exists()\nTrue\n>>> Path('nonexistentfile').exists()\nFalse\n  Note If the path points to a symlink, exists() returns whether the symlink points to an existing file or directory.","title":"python.library.pathlib#pathlib.Path.exists"},{"text":"Path.is_dir()  \nReturn True if the current context references a directory.","title":"python.library.zipfile#zipfile.Path.is_dir"},{"text":"Path.is_socket()  \nReturn True if the path points to a Unix socket (or a symbolic link pointing to a Unix socket), False if it points to another kind of file. False is also returned if the path doesn\u2019t exist or is a broken symlink; other errors (such as permission errors) are propagated.","title":"python.library.pathlib#pathlib.Path.is_socket"}]}
{"task_id":2212433,"prompt":"def f_2212433(yourdict):\n\treturn ","suffix":"","canonical_solution":"len(list(yourdict.keys()))","test_start":"\ndef check(candidate): ","test":["\n    assert candidate({'a': 1, 'b': 2, 'c': 3}) == 3 \n","\n    assert candidate({'a': 2, 'c': 3}) == 2\n"],"entry_point":"f_2212433","intent":"get number of keys in dictionary `yourdict`","library":[],"docs":[]}
{"task_id":2212433,"prompt":"def f_2212433(yourdictfile):\n\treturn ","suffix":"","canonical_solution":"len(set(open(yourdictfile).read().split()))","test_start":"\ndef check(candidate): ","test":["\n    with open('dict.txt', 'w') as fw:\n        for w in [\"apple\", \"banana\", \"tv\", \"apple\", \"phone\"]:\n            fw.write(f\"{w}\\n\")\n    assert candidate('dict.txt') == 4\n"],"entry_point":"f_2212433","intent":"count the number of keys in dictionary `yourdictfile`","library":[],"docs":[]}
{"task_id":20067636,"prompt":"def f_20067636(df):\n\treturn ","suffix":"","canonical_solution":"df.groupby('id').first()","test_start":"\nimport pandas as pd \n\ndef check(candidate): ","test":["\n    df = pd.DataFrame({\n        'id': [1, 1, 1, 2, 2, 3, 3, 3, 3, 4, 4, 5, 6, 6, 6, 7, 7], \n        'value': ['first', 'second', 'second', 'first', 'second', 'first', 'third', 'fourth', 'fifth', 'second', 'fifth', 'first', 'first', 'second', 'third', 'fourth', 'fifth']\n    })\n    assert candidate(df).to_dict() == {'value': {1: 'first', 2: 'first', 3: 'first', 4: 'second', 5: 'first', 6: 'first', 7: 'fourth'}}\n"],"entry_point":"f_20067636","intent":"pandas dataframe `df` get first row of each group by 'id'","library":["pandas"],"docs":[{"text":"pandas.core.groupby.GroupBy.groups   propertyGroupBy.groups\n \nDict {group name -> group labels}.","title":"pandas.reference.api.pandas.core.groupby.groupby.groups"},{"text":"pandas.core.resample.Resampler.get_group   Resampler.get_group(name, obj=None)[source]\n \nConstruct DataFrame from group with provided name.  Parameters \n \nname:object\n\n\nThe name of the group to get as a DataFrame.  \nobj:DataFrame, default None\n\n\nThe DataFrame to take the DataFrame out of. If it is None, the object groupby was called on will be used.    Returns \n \ngroup:same type as obj","title":"pandas.reference.api.pandas.core.resample.resampler.get_group"},{"text":"pandas.core.groupby.GroupBy.head   finalGroupBy.head(n=5)[source]\n \nReturn first n rows of each group. Similar to .apply(lambda x: x.head(n)), but it returns a subset of rows from the original DataFrame with original index and order preserved (as_index flag is ignored).  Parameters \n \nn:int\n\n\nIf positive: number of entries to include from start of each group. If negative: number of entries to exclude from end of each group.    Returns \n Series or DataFrame\n\nSubset of original Series or DataFrame as determined by n.      See also  Series.groupby\n\nApply a function groupby to a Series.  DataFrame.groupby\n\nApply a function groupby to each row or column of a DataFrame.    Examples \n>>> df = pd.DataFrame([[1, 2], [1, 4], [5, 6]],\n...                   columns=['A', 'B'])\n>>> df.groupby('A').head(1)\n   A  B\n0  1  2\n2  5  6\n>>> df.groupby('A').head(-1)\n   A  B\n0  1  2","title":"pandas.reference.api.pandas.core.groupby.groupby.head"},{"text":"pandas.core.groupby.GroupBy.first   finalGroupBy.first(numeric_only=False, min_count=- 1)[source]\n \nCompute first of group values.  Parameters \n \nnumeric_only:bool, default False\n\n\nInclude only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data.  \nmin_count:int, default -1\n\n\nThe required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA.    Returns \n Series or DataFrame\n\nComputed first of values within each group.","title":"pandas.reference.api.pandas.core.groupby.groupby.first"},{"text":"pandas.core.groupby.GroupBy.get_group   GroupBy.get_group(name, obj=None)[source]\n \nConstruct DataFrame from group with provided name.  Parameters \n \nname:object\n\n\nThe name of the group to get as a DataFrame.  \nobj:DataFrame, default None\n\n\nThe DataFrame to take the DataFrame out of. If it is None, the object groupby was called on will be used.    Returns \n \ngroup:same type as obj","title":"pandas.reference.api.pandas.core.groupby.groupby.get_group"},{"text":"pandas.core.resample.Resampler.groups   propertyResampler.groups\n \nDict {group name -> group labels}.","title":"pandas.reference.api.pandas.core.resample.resampler.groups"},{"text":"pandas.core.groupby.DataFrameGroupBy.size   DataFrameGroupBy.size()[source]\n \nCompute group sizes.  Returns \n DataFrame or Series\n\nNumber of rows in each group as a Series if as_index is True or a DataFrame if as_index is False.      See also  Series.groupby\n\nApply a function groupby to a Series.  DataFrame.groupby\n\nApply a function groupby to each row or column of a DataFrame.","title":"pandas.reference.api.pandas.core.groupby.dataframegroupby.size"},{"text":"pandas.core.groupby.DataFrameGroupBy.all   DataFrameGroupBy.all(skipna=True)[source]\n \nReturn True if all values in the group are truthful, else False.  Parameters \n \nskipna:bool, default True\n\n\nFlag to ignore nan values during truth testing.    Returns \n Series or DataFrame\n\nDataFrame or Series of boolean values, where a value is True if all elements are True within its respective group, False otherwise.      See also  Series.groupby\n\nApply a function groupby to a Series.  DataFrame.groupby\n\nApply a function groupby to each row or column of a DataFrame.","title":"pandas.reference.api.pandas.core.groupby.dataframegroupby.all"},{"text":"pandas.core.resample.Resampler.size   Resampler.size()[source]\n \nCompute group sizes.  Returns \n DataFrame or Series\n\nNumber of rows in each group as a Series if as_index is True or a DataFrame if as_index is False.      See also  Series.groupby\n\nApply a function groupby to a Series.  DataFrame.groupby\n\nApply a function groupby to each row or column of a DataFrame.","title":"pandas.reference.api.pandas.core.resample.resampler.size"},{"text":"pandas.core.groupby.GroupBy.__iter__   GroupBy.__iter__()[source]\n \nGroupby iterator.  Returns \n Generator yielding sequence of (name, subsetted object)\nfor each group","title":"pandas.reference.api.pandas.core.groupby.groupby.__iter__"}]}
{"task_id":40924332,"prompt":"def f_40924332(df):\n\treturn ","suffix":"","canonical_solution":"pd.concat([df[0].apply(pd.Series), df[1]], axis=1)","test_start":"\nimport numpy as np\nimport pandas as pd \n\ndef check(callerFunction):","test":["\n    assert callerFunction(pd.DataFrame([[[8, 10, 12], 'A'], [[7, 9, 11], 'B']])).equals(pd.DataFrame([[8,10,12,'A'], [7,9,11,'B']], columns=[0,1,2,1]))\n","\n    assert callerFunction(pd.DataFrame([[[8, 10, 12], 'A'], [[7, 11], 'B']])).equals(pd.DataFrame([[8.0,10.0,12.0,'A'], [7.0,11.0,np.nan,'B']], columns=[0,1,2,1]))\n","\n    assert callerFunction(pd.DataFrame([[[8, 10, 12]], [[7, 9, 11], 'B']])).equals(pd.DataFrame([[8,10,12,None], [7,9,11,'B']], columns=[0,1,2,1]))\n"],"entry_point":"f_40924332","intent":"split a list in first column into multiple columns keeping other columns as well in pandas data frame `df`","library":["numpy","pandas"],"docs":[{"text":"pandas.DataFrame.explode   DataFrame.explode(column, ignore_index=False)[source]\n \nTransform each element of a list-like to a row, replicating index values.  New in version 0.25.0.   Parameters \n \ncolumn:IndexLabel\n\n\nColumn(s) to explode. For multiple columns, specify a non-empty list with each element be str or tuple, and all specified columns their list-like data on same row of the frame must have matching length.  New in version 1.3.0: Multi-column explode   \nignore_index:bool, default False\n\n\nIf True, the resulting index will be labeled 0, 1, \u2026, n - 1.  New in version 1.1.0.     Returns \n DataFrame\n\nExploded lists to rows of the subset columns; index will be duplicated for these rows.    Raises \n ValueError :\n\n If columns of the frame are not unique. If specified columns to explode is empty list. If specified columns to explode have not matching count of elements rowwise in the frame.       See also  DataFrame.unstack\n\nPivot a level of the (necessarily hierarchical) index labels.  DataFrame.melt\n\nUnpivot a DataFrame from wide format to long format.  Series.explode\n\nExplode a DataFrame from list-like columns to long format.    Notes This routine will explode list-likes including lists, tuples, sets, Series, and np.ndarray. The result dtype of the subset rows will be object. Scalars will be returned unchanged, and empty list-likes will result in a np.nan for that row. In addition, the ordering of rows in the output will be non-deterministic when exploding sets. Examples \n>>> df = pd.DataFrame({'A': [[0, 1, 2], 'foo', [], [3, 4]],\n...                    'B': 1,\n...                    'C': [['a', 'b', 'c'], np.nan, [], ['d', 'e']]})\n>>> df\n           A  B          C\n0  [0, 1, 2]  1  [a, b, c]\n1        foo  1        NaN\n2         []  1         []\n3     [3, 4]  1     [d, e]\n  Single-column explode. \n>>> df.explode('A')\n     A  B          C\n0    0  1  [a, b, c]\n0    1  1  [a, b, c]\n0    2  1  [a, b, c]\n1  foo  1        NaN\n2  NaN  1         []\n3    3  1     [d, e]\n3    4  1     [d, e]\n  Multi-column explode. \n>>> df.explode(list('AC'))\n     A  B    C\n0    0  1    a\n0    1  1    b\n0    2  1    c\n1  foo  1  NaN\n2  NaN  1  NaN\n3    3  1    d\n3    4  1    e","title":"pandas.reference.api.pandas.dataframe.explode"},{"text":"split(split_size, dim=0) [source]\n \nSee torch.split()","title":"torch.tensors#torch.Tensor.split"},{"text":"class sklearn.compose.ColumnTransformer(transformers, *, remainder='drop', sparse_threshold=0.3, n_jobs=None, transformer_weights=None, verbose=False) [source]\n \nApplies transformers to columns of an array or pandas DataFrame. This estimator allows different columns or column subsets of the input to be transformed separately and the features generated by each transformer will be concatenated to form a single feature space. This is useful for heterogeneous or columnar data, to combine several feature extraction mechanisms or transformations into a single transformer. Read more in the User Guide.  New in version 0.20.   Parameters \n \ntransformerslist of tuples \n\nList of (name, transformer, columns) tuples specifying the transformer objects to be applied to subsets of the data.  \nnamestr \n\nLike in Pipeline and FeatureUnion, this allows the transformer and its parameters to be set using set_params and searched in grid search.  \ntransformer{\u2018drop\u2019, \u2018passthrough\u2019} or estimator \n\nEstimator must support fit and transform. Special-cased strings \u2018drop\u2019 and \u2018passthrough\u2019 are accepted as well, to indicate to drop the columns or to pass them through untransformed, respectively.  \ncolumnsstr, array-like of str, int, array-like of int, array-like of bool, slice or callable \n\nIndexes the data on its second axis. Integers are interpreted as positional columns, while strings can reference DataFrame columns by name. A scalar string or int should be used where transformer expects X to be a 1d array-like (vector), otherwise a 2d array will be passed to the transformer. A callable is passed the input data X and can return any of the above. To select multiple columns by name or dtype, you can use make_column_selector.    \nremainder{\u2018drop\u2019, \u2018passthrough\u2019} or estimator, default=\u2019drop\u2019 \n\nBy default, only the specified columns in transformers are transformed and combined in the output, and the non-specified columns are dropped. (default of 'drop'). By specifying remainder='passthrough', all remaining columns that were not specified in transformers will be automatically passed through. This subset of columns is concatenated with the output of the transformers. By setting remainder to be an estimator, the remaining non-specified columns will use the remainder estimator. The estimator must support fit and transform. Note that using this feature requires that the DataFrame columns input at fit and transform have identical order.  \nsparse_thresholdfloat, default=0.3 \n\nIf the output of the different transformers contains sparse matrices, these will be stacked as a sparse matrix if the overall density is lower than this value. Use sparse_threshold=0 to always return dense. When the transformed output consists of all dense data, the stacked result will be dense, and this keyword will be ignored.  \nn_jobsint, default=None \n\nNumber of jobs to run in parallel. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details.  \ntransformer_weightsdict, default=None \n\nMultiplicative weights for features per transformer. The output of the transformer is multiplied by these weights. Keys are transformer names, values the weights.  \nverbosebool, default=False \n\nIf True, the time elapsed while fitting each transformer will be printed as it is completed.    Attributes \n \ntransformers_list \n\nThe collection of fitted transformers as tuples of (name, fitted_transformer, column). fitted_transformer can be an estimator, \u2018drop\u2019, or \u2018passthrough\u2019. In case there were no columns selected, this will be the unfitted transformer. If there are remaining columns, the final element is a tuple of the form: (\u2018remainder\u2019, transformer, remaining_columns) corresponding to the remainder parameter. If there are remaining columns, then len(transformers_)==len(transformers)+1, otherwise len(transformers_)==len(transformers).  \nnamed_transformers_Bunch \n\nAccess the fitted transformer by name.  \nsparse_output_bool \n\nBoolean flag indicating whether the output of transform is a sparse matrix or a dense numpy array, which depends on the output of the individual transformers and the sparse_threshold keyword.      See also  \nmake_column_transformer\n\n\nConvenience function for combining the outputs of multiple transformer objects applied to column subsets of the original feature space.  \nmake_column_selector\n\n\nConvenience function for selecting columns based on datatype or the columns name with a regex pattern.    Notes The order of the columns in the transformed feature matrix follows the order of how the columns are specified in the transformers list. Columns of the original feature matrix that are not specified are dropped from the resulting transformed feature matrix, unless specified in the passthrough keyword. Those columns specified with passthrough are added at the right to the output of the transformers. Examples >>> import numpy as np\n>>> from sklearn.compose import ColumnTransformer\n>>> from sklearn.preprocessing import Normalizer\n>>> ct = ColumnTransformer(\n...     [(\"norm1\", Normalizer(norm='l1'), [0, 1]),\n...      (\"norm2\", Normalizer(norm='l1'), slice(2, 4))])\n>>> X = np.array([[0., 1., 2., 2.],\n...               [1., 1., 0., 1.]])\n>>> # Normalizer scales each row of X to unit norm. A separate scaling\n>>> # is applied for the two first and two last elements of each\n>>> # row independently.\n>>> ct.fit_transform(X)\narray([[0. , 1. , 0.5, 0.5],\n       [0.5, 0.5, 0. , 1. ]])\n Methods  \nfit(X[, y]) Fit all transformers using X.  \nfit_transform(X[, y]) Fit all transformers, transform the data and concatenate results.  \nget_feature_names() Get feature names from all transformers.  \nget_params([deep]) Get parameters for this estimator.  \nset_params(**kwargs) Set the parameters of this estimator.  \ntransform(X) Transform X separately by each transformer, concatenate results.    \nfit(X, y=None) [source]\n \nFit all transformers using X.  Parameters \n \nX{array-like, dataframe} of shape (n_samples, n_features) \n\nInput data, of which specified subsets are used to fit the transformers.  \nyarray-like of shape (n_samples,\u2026), default=None \n\nTargets for supervised learning.    Returns \n \nselfColumnTransformer \n\nThis estimator     \n  \nfit_transform(X, y=None) [source]\n \nFit all transformers, transform the data and concatenate results.  Parameters \n \nX{array-like, dataframe} of shape (n_samples, n_features) \n\nInput data, of which specified subsets are used to fit the transformers.  \nyarray-like of shape (n_samples,), default=None \n\nTargets for supervised learning.    Returns \n \nX_t{array-like, sparse matrix} of shape (n_samples, sum_n_components) \n\nhstack of results of transformers. sum_n_components is the sum of n_components (output dimension) over transformers. If any result is a sparse matrix, everything will be converted to sparse matrices.     \n  \nget_feature_names() [source]\n \nGet feature names from all transformers.  Returns \n \nfeature_nameslist of strings \n\nNames of the features produced by transform.     \n  \nget_params(deep=True) [source]\n \nGet parameters for this estimator. Returns the parameters given in the constructor as well as the estimators contained within the transformers of the ColumnTransformer.  Parameters \n \ndeepbool, default=True \n\nIf True, will return the parameters for this estimator and contained subobjects that are estimators.    Returns \n \nparamsdict \n\nParameter names mapped to their values.     \n  \nproperty named_transformers_  \nAccess the fitted transformer by name. Read-only attribute to access any transformer by given name. Keys are transformer names and values are the fitted transformer objects. \n  \nset_params(**kwargs) [source]\n \nSet the parameters of this estimator. Valid parameter keys can be listed with get_params(). Note that you can directly set the parameters of the estimators contained in transformers of ColumnTransformer.  Returns \n self\n   \n  \ntransform(X) [source]\n \nTransform X separately by each transformer, concatenate results.  Parameters \n \nX{array-like, dataframe} of shape (n_samples, n_features) \n\nThe data to be transformed by subset.    Returns \n \nX_t{array-like, sparse matrix} of shape (n_samples, sum_n_components) \n\nhstack of results of transformers. sum_n_components is the sum of n_components (output dimension) over transformers. If any result is a sparse matrix, everything will be converted to sparse matrices.","title":"sklearn.modules.generated.sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer"},{"text":"pandas.melt   pandas.melt(frame, id_vars=None, value_vars=None, var_name=None, value_name='value', col_level=None, ignore_index=True)[source]\n \nUnpivot a DataFrame from wide to long format, optionally leaving identifiers set. This function is useful to massage a DataFrame into a format where one or more columns are identifier variables (id_vars), while all other columns, considered measured variables (value_vars), are \u201cunpivoted\u201d to the row axis, leaving just two non-identifier columns, \u2018variable\u2019 and \u2018value\u2019.  Parameters \n \nid_vars:tuple, list, or ndarray, optional\n\n\nColumn(s) to use as identifier variables.  \nvalue_vars:tuple, list, or ndarray, optional\n\n\nColumn(s) to unpivot. If not specified, uses all columns that are not set as id_vars.  \nvar_name:scalar\n\n\nName to use for the \u2018variable\u2019 column. If None it uses frame.columns.name or \u2018variable\u2019.  \nvalue_name:scalar, default \u2018value\u2019\n\n\nName to use for the \u2018value\u2019 column.  \ncol_level:int or str, optional\n\n\nIf columns are a MultiIndex then use this level to melt.  \nignore_index:bool, default True\n\n\nIf True, original index is ignored. If False, the original index is retained. Index labels will be repeated as necessary.  New in version 1.1.0.     Returns \n DataFrame\n\nUnpivoted DataFrame.      See also  DataFrame.melt\n\nIdentical method.  pivot_table\n\nCreate a spreadsheet-style pivot table as a DataFrame.  DataFrame.pivot\n\nReturn reshaped DataFrame organized by given index \/ column values.  DataFrame.explode\n\nExplode a DataFrame from list-like columns to long format.    Examples \n>>> df = pd.DataFrame({'A': {0: 'a', 1: 'b', 2: 'c'},\n...                    'B': {0: 1, 1: 3, 2: 5},\n...                    'C': {0: 2, 1: 4, 2: 6}})\n>>> df\n   A  B  C\n0  a  1  2\n1  b  3  4\n2  c  5  6\n  \n>>> pd.melt(df, id_vars=['A'], value_vars=['B'])\n   A variable  value\n0  a        B      1\n1  b        B      3\n2  c        B      5\n  \n>>> pd.melt(df, id_vars=['A'], value_vars=['B', 'C'])\n   A variable  value\n0  a        B      1\n1  b        B      3\n2  c        B      5\n3  a        C      2\n4  b        C      4\n5  c        C      6\n  The names of \u2018variable\u2019 and \u2018value\u2019 columns can be customized: \n>>> pd.melt(df, id_vars=['A'], value_vars=['B'],\n...         var_name='myVarname', value_name='myValname')\n   A myVarname  myValname\n0  a         B          1\n1  b         B          3\n2  c         B          5\n  Original index values can be kept around: \n>>> pd.melt(df, id_vars=['A'], value_vars=['B', 'C'], ignore_index=False)\n   A variable  value\n0  a        B      1\n1  b        B      3\n2  c        B      5\n0  a        C      2\n1  b        C      4\n2  c        C      6\n  If you have multi-index columns: \n>>> df.columns = [list('ABC'), list('DEF')]\n>>> df\n   A  B  C\n   D  E  F\n0  a  1  2\n1  b  3  4\n2  c  5  6\n  \n>>> pd.melt(df, col_level=0, id_vars=['A'], value_vars=['B'])\n   A variable  value\n0  a        B      1\n1  b        B      3\n2  c        B      5\n  \n>>> pd.melt(df, id_vars=[('A', 'D')], value_vars=[('B', 'E')])\n  (A, D) variable_0 variable_1  value\n0      a          B          E      1\n1      b          B          E      3\n2      c          B          E      5","title":"pandas.reference.api.pandas.melt"},{"text":"staticget_default_size()[source]\n \nReturn the default font size.","title":"matplotlib.font_manager_api#matplotlib.font_manager.FontManager.get_default_size"},{"text":"sklearn.compose.ColumnTransformer  \nclass sklearn.compose.ColumnTransformer(transformers, *, remainder='drop', sparse_threshold=0.3, n_jobs=None, transformer_weights=None, verbose=False) [source]\n \nApplies transformers to columns of an array or pandas DataFrame. This estimator allows different columns or column subsets of the input to be transformed separately and the features generated by each transformer will be concatenated to form a single feature space. This is useful for heterogeneous or columnar data, to combine several feature extraction mechanisms or transformations into a single transformer. Read more in the User Guide.  New in version 0.20.   Parameters \n \ntransformerslist of tuples \n\nList of (name, transformer, columns) tuples specifying the transformer objects to be applied to subsets of the data.  \nnamestr \n\nLike in Pipeline and FeatureUnion, this allows the transformer and its parameters to be set using set_params and searched in grid search.  \ntransformer{\u2018drop\u2019, \u2018passthrough\u2019} or estimator \n\nEstimator must support fit and transform. Special-cased strings \u2018drop\u2019 and \u2018passthrough\u2019 are accepted as well, to indicate to drop the columns or to pass them through untransformed, respectively.  \ncolumnsstr, array-like of str, int, array-like of int, array-like of bool, slice or callable \n\nIndexes the data on its second axis. Integers are interpreted as positional columns, while strings can reference DataFrame columns by name. A scalar string or int should be used where transformer expects X to be a 1d array-like (vector), otherwise a 2d array will be passed to the transformer. A callable is passed the input data X and can return any of the above. To select multiple columns by name or dtype, you can use make_column_selector.    \nremainder{\u2018drop\u2019, \u2018passthrough\u2019} or estimator, default=\u2019drop\u2019 \n\nBy default, only the specified columns in transformers are transformed and combined in the output, and the non-specified columns are dropped. (default of 'drop'). By specifying remainder='passthrough', all remaining columns that were not specified in transformers will be automatically passed through. This subset of columns is concatenated with the output of the transformers. By setting remainder to be an estimator, the remaining non-specified columns will use the remainder estimator. The estimator must support fit and transform. Note that using this feature requires that the DataFrame columns input at fit and transform have identical order.  \nsparse_thresholdfloat, default=0.3 \n\nIf the output of the different transformers contains sparse matrices, these will be stacked as a sparse matrix if the overall density is lower than this value. Use sparse_threshold=0 to always return dense. When the transformed output consists of all dense data, the stacked result will be dense, and this keyword will be ignored.  \nn_jobsint, default=None \n\nNumber of jobs to run in parallel. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details.  \ntransformer_weightsdict, default=None \n\nMultiplicative weights for features per transformer. The output of the transformer is multiplied by these weights. Keys are transformer names, values the weights.  \nverbosebool, default=False \n\nIf True, the time elapsed while fitting each transformer will be printed as it is completed.    Attributes \n \ntransformers_list \n\nThe collection of fitted transformers as tuples of (name, fitted_transformer, column). fitted_transformer can be an estimator, \u2018drop\u2019, or \u2018passthrough\u2019. In case there were no columns selected, this will be the unfitted transformer. If there are remaining columns, the final element is a tuple of the form: (\u2018remainder\u2019, transformer, remaining_columns) corresponding to the remainder parameter. If there are remaining columns, then len(transformers_)==len(transformers)+1, otherwise len(transformers_)==len(transformers).  \nnamed_transformers_Bunch \n\nAccess the fitted transformer by name.  \nsparse_output_bool \n\nBoolean flag indicating whether the output of transform is a sparse matrix or a dense numpy array, which depends on the output of the individual transformers and the sparse_threshold keyword.      See also  \nmake_column_transformer\n\n\nConvenience function for combining the outputs of multiple transformer objects applied to column subsets of the original feature space.  \nmake_column_selector\n\n\nConvenience function for selecting columns based on datatype or the columns name with a regex pattern.    Notes The order of the columns in the transformed feature matrix follows the order of how the columns are specified in the transformers list. Columns of the original feature matrix that are not specified are dropped from the resulting transformed feature matrix, unless specified in the passthrough keyword. Those columns specified with passthrough are added at the right to the output of the transformers. Examples >>> import numpy as np\n>>> from sklearn.compose import ColumnTransformer\n>>> from sklearn.preprocessing import Normalizer\n>>> ct = ColumnTransformer(\n...     [(\"norm1\", Normalizer(norm='l1'), [0, 1]),\n...      (\"norm2\", Normalizer(norm='l1'), slice(2, 4))])\n>>> X = np.array([[0., 1., 2., 2.],\n...               [1., 1., 0., 1.]])\n>>> # Normalizer scales each row of X to unit norm. A separate scaling\n>>> # is applied for the two first and two last elements of each\n>>> # row independently.\n>>> ct.fit_transform(X)\narray([[0. , 1. , 0.5, 0.5],\n       [0.5, 0.5, 0. , 1. ]])\n Methods  \nfit(X[, y]) Fit all transformers using X.  \nfit_transform(X[, y]) Fit all transformers, transform the data and concatenate results.  \nget_feature_names() Get feature names from all transformers.  \nget_params([deep]) Get parameters for this estimator.  \nset_params(**kwargs) Set the parameters of this estimator.  \ntransform(X) Transform X separately by each transformer, concatenate results.    \nfit(X, y=None) [source]\n \nFit all transformers using X.  Parameters \n \nX{array-like, dataframe} of shape (n_samples, n_features) \n\nInput data, of which specified subsets are used to fit the transformers.  \nyarray-like of shape (n_samples,\u2026), default=None \n\nTargets for supervised learning.    Returns \n \nselfColumnTransformer \n\nThis estimator     \n  \nfit_transform(X, y=None) [source]\n \nFit all transformers, transform the data and concatenate results.  Parameters \n \nX{array-like, dataframe} of shape (n_samples, n_features) \n\nInput data, of which specified subsets are used to fit the transformers.  \nyarray-like of shape (n_samples,), default=None \n\nTargets for supervised learning.    Returns \n \nX_t{array-like, sparse matrix} of shape (n_samples, sum_n_components) \n\nhstack of results of transformers. sum_n_components is the sum of n_components (output dimension) over transformers. If any result is a sparse matrix, everything will be converted to sparse matrices.     \n  \nget_feature_names() [source]\n \nGet feature names from all transformers.  Returns \n \nfeature_nameslist of strings \n\nNames of the features produced by transform.     \n  \nget_params(deep=True) [source]\n \nGet parameters for this estimator. Returns the parameters given in the constructor as well as the estimators contained within the transformers of the ColumnTransformer.  Parameters \n \ndeepbool, default=True \n\nIf True, will return the parameters for this estimator and contained subobjects that are estimators.    Returns \n \nparamsdict \n\nParameter names mapped to their values.     \n  \nproperty named_transformers_  \nAccess the fitted transformer by name. Read-only attribute to access any transformer by given name. Keys are transformer names and values are the fitted transformer objects. \n  \nset_params(**kwargs) [source]\n \nSet the parameters of this estimator. Valid parameter keys can be listed with get_params(). Note that you can directly set the parameters of the estimators contained in transformers of ColumnTransformer.  Returns \n self\n   \n  \ntransform(X) [source]\n \nTransform X separately by each transformer, concatenate results.  Parameters \n \nX{array-like, dataframe} of shape (n_samples, n_features) \n\nThe data to be transformed by subset.    Returns \n \nX_t{array-like, sparse matrix} of shape (n_samples, sum_n_components) \n\nhstack of results of transformers. sum_n_components is the sum of n_components (output dimension) over transformers. If any result is a sparse matrix, everything will be converted to sparse matrices.     \n \n Examples using sklearn.compose.ColumnTransformer\n \n  Poisson regression and non-normal loss  \n\n  Tweedie regression on insurance claims  \n\n  Permutation Importance vs Random Forest Feature Importance (MDI)  \n\n  Column Transformer with Mixed Types  \n\n  Column Transformer with Heterogeneous Data Sources","title":"sklearn.modules.generated.sklearn.compose.columntransformer"},{"text":"tf.raw_ops.TensorListSplit Splits a tensor into a list.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.raw_ops.TensorListSplit  \ntf.raw_ops.TensorListSplit(\n    tensor, element_shape, lengths, name=None\n)\n list[i] corresponds to lengths[i] tensors from the input tensor. The tensor must have rank at least 1 and contain exactly sum(lengths) elements. tensor: The input tensor. element_shape: A shape compatible with that of elements in the tensor. lengths: Vector of sizes of the 0th dimension of tensors in the list. output_handle: The list.\n \n\n\n Args\n  tensor   A Tensor.  \n  element_shape   A Tensor. Must be one of the following types: int32, int64.  \n  lengths   A Tensor of type int64.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor of type variant.","title":"tensorflow.raw_ops.tensorlistsplit"},{"text":"pandas.MultiIndex.from_frame   classmethodMultiIndex.from_frame(df, sortorder=None, names=None)[source]\n \nMake a MultiIndex from a DataFrame.  Parameters \n \ndf:DataFrame\n\n\nDataFrame to be converted to MultiIndex.  \nsortorder:int, optional\n\n\nLevel of sortedness (must be lexicographically sorted by that level).  \nnames:list-like, optional\n\n\nIf no names are provided, use the column names, or tuple of column names if the columns is a MultiIndex. If a sequence, overwrite names with the given sequence.    Returns \n MultiIndex\n\nThe MultiIndex representation of the given DataFrame.      See also  MultiIndex.from_arrays\n\nConvert list of arrays to MultiIndex.  MultiIndex.from_tuples\n\nConvert list of tuples to MultiIndex.  MultiIndex.from_product\n\nMake a MultiIndex from cartesian product of iterables.    Examples \n>>> df = pd.DataFrame([['HI', 'Temp'], ['HI', 'Precip'],\n...                    ['NJ', 'Temp'], ['NJ', 'Precip']],\n...                   columns=['a', 'b'])\n>>> df\n      a       b\n0    HI    Temp\n1    HI  Precip\n2    NJ    Temp\n3    NJ  Precip\n  \n>>> pd.MultiIndex.from_frame(df)\nMultiIndex([('HI',   'Temp'),\n            ('HI', 'Precip'),\n            ('NJ',   'Temp'),\n            ('NJ', 'Precip')],\n           names=['a', 'b'])\n  Using explicit names, instead of the column names \n>>> pd.MultiIndex.from_frame(df, names=['state', 'observation'])\nMultiIndex([('HI',   'Temp'),\n            ('HI', 'Precip'),\n            ('NJ',   'Temp'),\n            ('NJ', 'Precip')],\n           names=['state', 'observation'])","title":"pandas.reference.api.pandas.multiindex.from_frame"},{"text":"tf.debugging.experimental.disable_dump_debug_info Disable the currently-enabled debugging dumping.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.debugging.experimental.disable_dump_debug_info  \ntf.debugging.experimental.disable_dump_debug_info()\n If the enable_dump_debug_info() method under the same Python namespace has been invoked before, calling this method disables it. If no call to enable_dump_debug_info() has been made, calling this method is a no-op. Calling this method more than once is idempotent.","title":"tensorflow.debugging.experimental.disable_dump_debug_info"},{"text":"torch.split(tensor, split_size_or_sections, dim=0) [source]\n \nSplits the tensor into chunks. Each chunk is a view of the original tensor. If split_size_or_sections is an integer type, then tensor will be split into equally sized chunks (if possible). Last chunk will be smaller if the tensor size along the given dimension dim is not divisible by split_size. If split_size_or_sections is a list, then tensor will be split into len(split_size_or_sections) chunks with sizes in dim according to split_size_or_sections.  Parameters \n \ntensor (Tensor) \u2013 tensor to split. \nsplit_size_or_sections (int) or (list(int)) \u2013 size of a single chunk or list of sizes for each chunk \ndim (int) \u2013 dimension along which to split the tensor.     Example::\n\n>>> a = torch.arange(10).reshape(5,2)\n>>> a\ntensor([[0, 1],\n        [2, 3],\n        [4, 5],\n        [6, 7],\n        [8, 9]])\n>>> torch.split(a, 2)\n(tensor([[0, 1],\n         [2, 3]]),\n tensor([[4, 5],\n         [6, 7]]),\n tensor([[8, 9]]))\n>>> torch.split(a, [1,4])\n(tensor([[0, 1]]),\n tensor([[2, 3],\n         [4, 5],\n         [6, 7],\n         [8, 9]]))","title":"torch.generated.torch.split#torch.split"}]}
{"task_id":30759776,"prompt":"def f_30759776(data):\n\treturn ","suffix":"","canonical_solution":"re.findall('src=\"js\/([^\"]*\\\\bjquery\\\\b[^\"]*)\"', data)","test_start":"\nimport re \n\ndef check(candidate):","test":["\n    data = '<script type=\"text\/javascript\" src=\"js\/jquery-1.9.1.min.js\"\/><script type=\"text\/javascript\" src=\"js\/jquery-migrate-1.2.1.min.js\"\/><script type=\"text\/javascript\" src=\"js\/jquery-ui.min.js\"\/><script type=\"text\/javascript\" src=\"js\/abc_bsub.js\"\/><script type=\"text\/javascript\" src=\"js\/abc_core.js\"\/>            <script type=\"text\/javascript\" src=\"js\/abc_explore.js\"\/><script type=\"text\/javascript\" src=\"js\/abc_qaa.js\"\/>'\n    assert candidate(data) == ['jquery-1.9.1.min.js', 'jquery-migrate-1.2.1.min.js', 'jquery-ui.min.js']\n"],"entry_point":"f_30759776","intent":"extract attributes 'src=\"js\/([^\"]*\\\\bjquery\\\\b[^\"]*)\"' from string `data`","library":["re"],"docs":[{"text":"get_script_prefix()","title":"django.ref.urlresolvers#django.urls.get_script_prefix"},{"text":"numpy.distutils.misc_util.get_script_files(scripts)[source]","title":"numpy.reference.distutils.misc_util#numpy.distutils.misc_util.get_script_files"},{"text":"extra_js","title":"django.ref.contrib.gis.admin#django.contrib.gis.admin.GeoModelAdmin.extra_js"},{"text":"numpy.distutils.misc_util.get_data_files(data)[source]","title":"numpy.reference.distutils.misc_util#numpy.distutils.misc_util.get_data_files"},{"text":"static source_to_code(data, path='<string>')  \nCreate a code object from Python source. The data argument can be whatever the compile() function supports (i.e. string or bytes). The path argument should be the \u201cpath\u201d to where the source code originated from, which can be an abstract concept (e.g. location in a zip file). With the subsequent code object one can execute it in a module by running exec(code, module.__dict__).  New in version 3.4.   Changed in version 3.5: Made the method static.","title":"python.library.importlib#importlib.abc.InspectLoader.source_to_code"},{"text":"data  \nA dictionary with arbitrary data that can be associated with this script info.","title":"flask.api.index#flask.cli.ScriptInfo.data"},{"text":"DataHandler.data_open(req)  \nRead a data URL. This kind of URL contains the content encoded in the URL itself. The data URL syntax is specified in RFC 2397. This implementation ignores white spaces in base64 encoded data URLs so the URL may be wrapped in whatever source file it comes from. But even though some browsers don\u2019t mind about a missing padding at the end of a base64 encoded data URL, this implementation will raise an ValueError in that case.","title":"python.library.urllib.request#urllib.request.DataHandler.data_open"},{"text":"django.template.Library.filter()","title":"django.howto.custom-template-tags#django.template.Library.filter"},{"text":"display_js()[source]","title":"matplotlib.backend_nbagg_api#matplotlib.backends.backend_nbagg.FigureManagerNbAgg.display_js"},{"text":"configure_mock(**kwargs)  \nSet attributes on the mock through keyword arguments. Attributes plus return values and side effects can be set on child mocks using standard dot notation and unpacking a dictionary in the method call: >>> mock = Mock()\n>>> attrs = {'method.return_value': 3, 'other.side_effect': KeyError}\n>>> mock.configure_mock(**attrs)\n>>> mock.method()\n3\n>>> mock.other()\nTraceback (most recent call last):\n  ...\nKeyError\n The same thing can be achieved in the constructor call to mocks: >>> attrs = {'method.return_value': 3, 'other.side_effect': KeyError}\n>>> mock = Mock(some_attribute='eggs', **attrs)\n>>> mock.some_attribute\n'eggs'\n>>> mock.method()\n3\n>>> mock.other()\nTraceback (most recent call last):\n  ...\nKeyError\n configure_mock() exists to make it easier to do configuration after the mock has been created.","title":"python.library.unittest.mock#unittest.mock.Mock.configure_mock"}]}
{"task_id":25388796,"prompt":"def f_25388796():\n\treturn ","suffix":"","canonical_solution":"sum(int(float(item)) for item in [_f for _f in ['', '3.4', '', '', '1.0'] if _f])","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == 4\n"],"entry_point":"f_25388796","intent":"Sum integers contained in strings in list `['', '3.4', '', '', '1.0']`","library":[],"docs":[]}
{"task_id":804995,"prompt":"def f_804995():\n\treturn ","suffix":"","canonical_solution":"subprocess.Popen(['c:\\\\Program Files\\\\VMware\\\\VMware Server\\\\vmware-cmd.bat'])","test_start":"\nimport subprocess\nfrom unittest.mock import Mock\n\ndef check(candidate):","test":["\n    subprocess.Popen = Mock(return_value = 0)\n    assert candidate() == 0\n"],"entry_point":"f_804995","intent":"Call a subprocess with arguments `c:\\\\Program Files\\\\VMware\\\\VMware Server\\\\vmware-cmd.bat` that may contain spaces","library":["subprocess"],"docs":[{"text":"coroutine asyncio.create_subprocess_shell(cmd, stdin=None, stdout=None, stderr=None, loop=None, limit=None, **kwds)  \nRun the cmd shell command. The limit argument sets the buffer limit for StreamReader wrappers for Process.stdout and Process.stderr (if subprocess.PIPE is passed to stdout and stderr arguments). Return a Process instance. See the documentation of loop.subprocess_shell() for other parameters.  Important It is the application\u2019s responsibility to ensure that all whitespace and special characters are quoted appropriately to avoid shell injection vulnerabilities. The shlex.quote() function can be used to properly escape whitespace and special shell characters in strings that are going to be used to construct shell commands.   Deprecated since version 3.8, will be removed in version 3.10: The loop parameter.","title":"python.library.asyncio-subprocess#asyncio.create_subprocess_shell"},{"text":"ctypes.util.find_msvcrt()  \nWindows only: return the filename of the VC runtime library used by Python, and by the extension modules. If the name of the library cannot be determined, None is returned. If you need to free memory, for example, allocated by an extension module with a call to the free(void *), it is important that you use the function in the same library that allocated the memory.","title":"python.library.ctypes#ctypes.util.find_msvcrt"},{"text":"socket.AF_VSOCK  \nsocket.IOCTL_VM_SOCKETS_GET_LOCAL_CID  \nVMADDR*  \nSO_VM*  \nConstants for Linux host\/guest communication. Availability: Linux >= 4.8.  New in version 3.7.","title":"python.library.socket#socket.AF_VSOCK"},{"text":"msvcrt \u2014 Useful routines from the MS VC++ runtime These functions provide access to some useful capabilities on Windows platforms. Some higher-level modules use these functions to build the Windows implementations of their services. For example, the getpass module uses this in the implementation of the getpass() function. Further documentation on these functions can be found in the Platform API documentation. The module implements both the normal and wide char variants of the console I\/O api. The normal API deals only with ASCII characters and is of limited use for internationalized applications. The wide char API should be used where ever possible.  Changed in version 3.3: Operations in this module now raise OSError where IOError was raised.  File Operations  \nmsvcrt.locking(fd, mode, nbytes)  \nLock part of a file based on file descriptor fd from the C runtime. Raises OSError on failure. The locked region of the file extends from the current file position for nbytes bytes, and may continue beyond the end of the file. mode must be one of the LK_* constants listed below. Multiple regions in a file may be locked at the same time, but may not overlap. Adjacent regions are not merged; they must be unlocked individually. Raises an auditing event msvcrt.locking with arguments fd, mode, nbytes. \n  \nmsvcrt.LK_LOCK  \nmsvcrt.LK_RLCK  \nLocks the specified bytes. If the bytes cannot be locked, the program immediately tries again after 1 second. If, after 10 attempts, the bytes cannot be locked, OSError is raised. \n  \nmsvcrt.LK_NBLCK  \nmsvcrt.LK_NBRLCK  \nLocks the specified bytes. If the bytes cannot be locked, OSError is raised. \n  \nmsvcrt.LK_UNLCK  \nUnlocks the specified bytes, which must have been previously locked. \n  \nmsvcrt.setmode(fd, flags)  \nSet the line-end translation mode for the file descriptor fd. To set it to text mode, flags should be os.O_TEXT; for binary, it should be os.O_BINARY. \n  \nmsvcrt.open_osfhandle(handle, flags)  \nCreate a C runtime file descriptor from the file handle handle. The flags parameter should be a bitwise OR of os.O_APPEND, os.O_RDONLY, and os.O_TEXT. The returned file descriptor may be used as a parameter to os.fdopen() to create a file object. Raises an auditing event msvcrt.open_osfhandle with arguments handle, flags. \n  \nmsvcrt.get_osfhandle(fd)  \nReturn the file handle for the file descriptor fd. Raises OSError if fd is not recognized. Raises an auditing event msvcrt.get_osfhandle with argument fd. \n Console I\/O  \nmsvcrt.kbhit()  \nReturn True if a keypress is waiting to be read. \n  \nmsvcrt.getch()  \nRead a keypress and return the resulting character as a byte string. Nothing is echoed to the console. This call will block if a keypress is not already available, but will not wait for Enter to be pressed. If the pressed key was a special function key, this will return '\\000' or '\\xe0'; the next call will return the keycode. The Control-C keypress cannot be read with this function. \n  \nmsvcrt.getwch()  \nWide char variant of getch(), returning a Unicode value. \n  \nmsvcrt.getche()  \nSimilar to getch(), but the keypress will be echoed if it represents a printable character. \n  \nmsvcrt.getwche()  \nWide char variant of getche(), returning a Unicode value. \n  \nmsvcrt.putch(char)  \nPrint the byte string char to the console without buffering. \n  \nmsvcrt.putwch(unicode_char)  \nWide char variant of putch(), accepting a Unicode value. \n  \nmsvcrt.ungetch(char)  \nCause the byte string char to be \u201cpushed back\u201d into the console buffer; it will be the next character read by getch() or getche(). \n  \nmsvcrt.ungetwch(unicode_char)  \nWide char variant of ungetch(), accepting a Unicode value. \n Other Functions  \nmsvcrt.heapmin()  \nForce the malloc() heap to clean itself up and return unused blocks to the operating system. On failure, this raises OSError.","title":"python.library.msvcrt"},{"text":"socket.AF_VSOCK  \nsocket.IOCTL_VM_SOCKETS_GET_LOCAL_CID  \nVMADDR*  \nSO_VM*  \nConstants for Linux host\/guest communication. Availability: Linux >= 4.8.  New in version 3.7.","title":"python.library.socket#socket.IOCTL_VM_SOCKETS_GET_LOCAL_CID"},{"text":"gibbs(v) [source]\n \nPerform one Gibbs sampling step.  Parameters \n \nvndarray of shape (n_samples, n_features) \n\nValues of the visible layer to start from.    Returns \n \nv_newndarray of shape (n_samples, n_features) \n\nValues of the visible layer after one Gibbs step.","title":"sklearn.modules.generated.sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM.gibbs"},{"text":"Subprocesses Source code: Lib\/asyncio\/subprocess.py, Lib\/asyncio\/base_subprocess.py This section describes high-level async\/await asyncio APIs to create and manage subprocesses. Here\u2019s an example of how asyncio can run a shell command and obtain its result: import asyncio\n\nasync def run(cmd):\n    proc = await asyncio.create_subprocess_shell(\n        cmd,\n        stdout=asyncio.subprocess.PIPE,\n        stderr=asyncio.subprocess.PIPE)\n\n    stdout, stderr = await proc.communicate()\n\n    print(f'[{cmd!r} exited with {proc.returncode}]')\n    if stdout:\n        print(f'[stdout]\\n{stdout.decode()}')\n    if stderr:\n        print(f'[stderr]\\n{stderr.decode()}')\n\nasyncio.run(run('ls \/zzz'))\n will print: ['ls \/zzz' exited with 1]\n[stderr]\nls: \/zzz: No such file or directory\n Because all asyncio subprocess functions are asynchronous and asyncio provides many tools to work with such functions, it is easy to execute and monitor multiple subprocesses in parallel. It is indeed trivial to modify the above example to run several commands simultaneously: async def main():\n    await asyncio.gather(\n        run('ls \/zzz'),\n        run('sleep 1; echo \"hello\"'))\n\nasyncio.run(main())\n See also the Examples subsection. Creating Subprocesses  \ncoroutine asyncio.create_subprocess_exec(program, *args, stdin=None, stdout=None, stderr=None, loop=None, limit=None, **kwds)  \nCreate a subprocess. The limit argument sets the buffer limit for StreamReader wrappers for Process.stdout and Process.stderr (if subprocess.PIPE is passed to stdout and stderr arguments). Return a Process instance. See the documentation of loop.subprocess_exec() for other parameters.  Deprecated since version 3.8, will be removed in version 3.10: The loop parameter.  \n  \ncoroutine asyncio.create_subprocess_shell(cmd, stdin=None, stdout=None, stderr=None, loop=None, limit=None, **kwds)  \nRun the cmd shell command. The limit argument sets the buffer limit for StreamReader wrappers for Process.stdout and Process.stderr (if subprocess.PIPE is passed to stdout and stderr arguments). Return a Process instance. See the documentation of loop.subprocess_shell() for other parameters.  Important It is the application\u2019s responsibility to ensure that all whitespace and special characters are quoted appropriately to avoid shell injection vulnerabilities. The shlex.quote() function can be used to properly escape whitespace and special shell characters in strings that are going to be used to construct shell commands.   Deprecated since version 3.8, will be removed in version 3.10: The loop parameter.  \n  Note Subprocesses are available for Windows if a ProactorEventLoop is used. See Subprocess Support on Windows for details.   See also asyncio also has the following low-level APIs to work with subprocesses: loop.subprocess_exec(), loop.subprocess_shell(), loop.connect_read_pipe(), loop.connect_write_pipe(), as well as the Subprocess Transports and Subprocess Protocols.  Constants  \nasyncio.subprocess.PIPE  \nCan be passed to the stdin, stdout or stderr parameters. If PIPE is passed to stdin argument, the Process.stdin attribute will point to a StreamWriter instance. If PIPE is passed to stdout or stderr arguments, the Process.stdout and Process.stderr attributes will point to StreamReader instances. \n  \nasyncio.subprocess.STDOUT  \nSpecial value that can be used as the stderr argument and indicates that standard error should be redirected into standard output. \n  \nasyncio.subprocess.DEVNULL  \nSpecial value that can be used as the stdin, stdout or stderr argument to process creation functions. It indicates that the special file os.devnull will be used for the corresponding subprocess stream. \n Interacting with Subprocesses Both create_subprocess_exec() and create_subprocess_shell() functions return instances of the Process class. Process is a high-level wrapper that allows communicating with subprocesses and watching for their completion.  \nclass asyncio.subprocess.Process  \nAn object that wraps OS processes created by the create_subprocess_exec() and create_subprocess_shell() functions. This class is designed to have a similar API to the subprocess.Popen class, but there are some notable differences:  unlike Popen, Process instances do not have an equivalent to the poll() method; the communicate() and wait() methods don\u2019t have a timeout parameter: use the wait_for() function; the Process.wait() method is asynchronous, whereas subprocess.Popen.wait() method is implemented as a blocking busy loop; the universal_newlines parameter is not supported.  This class is not thread safe. See also the Subprocess and Threads section.  \ncoroutine wait()  \nWait for the child process to terminate. Set and return the returncode attribute.  Note This method can deadlock when using stdout=PIPE or stderr=PIPE and the child process generates so much output that it blocks waiting for the OS pipe buffer to accept more data. Use the communicate() method when using pipes to avoid this condition.  \n  \ncoroutine communicate(input=None)  \nInteract with process:  send data to stdin (if input is not None); read data from stdout and stderr, until EOF is reached; wait for process to terminate.  The optional input argument is the data (bytes object) that will be sent to the child process. Return a tuple (stdout_data, stderr_data). If either BrokenPipeError or ConnectionResetError exception is raised when writing input into stdin, the exception is ignored. This condition occurs when the process exits before all data are written into stdin. If it is desired to send data to the process\u2019 stdin, the process needs to be created with stdin=PIPE. Similarly, to get anything other than None in the result tuple, the process has to be created with stdout=PIPE and\/or stderr=PIPE arguments. Note, that the data read is buffered in memory, so do not use this method if the data size is large or unlimited. \n  \nsend_signal(signal)  \nSends the signal signal to the child process.  Note On Windows, SIGTERM is an alias for terminate(). CTRL_C_EVENT and CTRL_BREAK_EVENT can be sent to processes started with a creationflags parameter which includes CREATE_NEW_PROCESS_GROUP.  \n  \nterminate()  \nStop the child process. On POSIX systems this method sends signal.SIGTERM to the child process. On Windows the Win32 API function TerminateProcess() is called to stop the child process. \n  \nkill()  \nKill the child process. On POSIX systems this method sends SIGKILL to the child process. On Windows this method is an alias for terminate(). \n  \nstdin  \nStandard input stream (StreamWriter) or None if the process was created with stdin=None. \n  \nstdout  \nStandard output stream (StreamReader) or None if the process was created with stdout=None. \n  \nstderr  \nStandard error stream (StreamReader) or None if the process was created with stderr=None. \n  Warning Use the communicate() method rather than process.stdin.write(), await process.stdout.read() or await process.stderr.read. This avoids deadlocks due to streams pausing reading or writing and blocking the child process.   \npid  \nProcess identification number (PID). Note that for processes created by the create_subprocess_shell() function, this attribute is the PID of the spawned shell. \n  \nreturncode  \nReturn code of the process when it exits. A None value indicates that the process has not terminated yet. A negative value -N indicates that the child was terminated by signal N (POSIX only). \n \n Subprocess and Threads Standard asyncio event loop supports running subprocesses from different threads by default. On Windows subprocesses are provided by ProactorEventLoop only (default), SelectorEventLoop has no subprocess support. On UNIX child watchers are used for subprocess finish waiting, see Process Watchers for more info.  Changed in version 3.8: UNIX switched to use ThreadedChildWatcher for spawning subprocesses from different threads without any limitation. Spawning a subprocess with inactive current child watcher raises RuntimeError.  Note that alternative event loop implementations might have own limitations; please refer to their documentation.  See also The Concurrency and multithreading in asyncio section.  Examples An example using the Process class to control a subprocess and the StreamReader class to read from its standard output. The subprocess is created by the create_subprocess_exec() function: import asyncio\nimport sys\n\nasync def get_date():\n    code = 'import datetime; print(datetime.datetime.now())'\n\n    # Create the subprocess; redirect the standard output\n    # into a pipe.\n    proc = await asyncio.create_subprocess_exec(\n        sys.executable, '-c', code,\n        stdout=asyncio.subprocess.PIPE)\n\n    # Read one line of output.\n    data = await proc.stdout.readline()\n    line = data.decode('ascii').rstrip()\n\n    # Wait for the subprocess exit.\n    await proc.wait()\n    return line\n\ndate = asyncio.run(get_date())\nprint(f\"Current date: {date}\")\n See also the same example written using low-level APIs.","title":"python.library.asyncio-subprocess"},{"text":"msvcrt.getch()  \nRead a keypress and return the resulting character as a byte string. Nothing is echoed to the console. This call will block if a keypress is not already available, but will not wait for Enter to be pressed. If the pressed key was a special function key, this will return '\\000' or '\\xe0'; the next call will return the keycode. The Control-C keypress cannot be read with this function.","title":"python.library.msvcrt#msvcrt.getch"},{"text":"sample(sample_shape=torch.Size([])) [source]\n \nThe sampling algorithm for the von Mises distribution is based on the following paper: Best, D. J., and Nicholas I. Fisher. \u201cEfficient simulation of the von Mises distribution.\u201d Applied Statistics (1979): 152-157.","title":"torch.distributions#torch.distributions.von_mises.VonMises.sample"},{"text":"msilib.gen_uuid()  \nReturn a new UUID, in the format that MSI typically requires (i.e. in curly braces, and with all hexdigits in upper-case).","title":"python.library.msilib#msilib.gen_uuid"}]}
{"task_id":26441253,"prompt":"def f_26441253(q):\n\t","suffix":"\n\treturn q","canonical_solution":"for n in [1,3,4,2]: q.put((-n, n))","test_start":"\nfrom queue import PriorityQueue\n\ndef check(candidate):","test":["\n    q = PriorityQueue()\n    q = candidate(q)\n    expected = [4, 3, 2, 1]\n    for i in range(0, len(expected)):\n        assert q.get()[1] == expected[i]\n"],"entry_point":"f_26441253","intent":"reverse a priority queue `q` in python without using classes","library":["queue"],"docs":[{"text":"heapq \u2014 Heap queue algorithm Source code: Lib\/heapq.py This module provides an implementation of the heap queue algorithm, also known as the priority queue algorithm. Heaps are binary trees for which every parent node has a value less than or equal to any of its children. This implementation uses arrays for which heap[k] <= heap[2*k+1] and heap[k] <= heap[2*k+2] for all k, counting elements from zero. For the sake of comparison, non-existing elements are considered to be infinite. The interesting property of a heap is that its smallest element is always the root, heap[0]. The API below differs from textbook heap algorithms in two aspects: (a) We use zero-based indexing. This makes the relationship between the index for a node and the indexes for its children slightly less obvious, but is more suitable since Python uses zero-based indexing. (b) Our pop method returns the smallest item, not the largest (called a \u201cmin heap\u201d in textbooks; a \u201cmax heap\u201d is more common in texts because of its suitability for in-place sorting). These two make it possible to view the heap as a regular Python list without surprises: heap[0] is the smallest item, and heap.sort() maintains the heap invariant! To create a heap, use a list initialized to [], or you can transform a populated list into a heap via function heapify(). The following functions are provided:  \nheapq.heappush(heap, item)  \nPush the value item onto the heap, maintaining the heap invariant. \n  \nheapq.heappop(heap)  \nPop and return the smallest item from the heap, maintaining the heap invariant. If the heap is empty, IndexError is raised. To access the smallest item without popping it, use heap[0]. \n  \nheapq.heappushpop(heap, item)  \nPush item on the heap, then pop and return the smallest item from the heap. The combined action runs more efficiently than heappush() followed by a separate call to heappop(). \n  \nheapq.heapify(x)  \nTransform list x into a heap, in-place, in linear time. \n  \nheapq.heapreplace(heap, item)  \nPop and return the smallest item from the heap, and also push the new item. The heap size doesn\u2019t change. If the heap is empty, IndexError is raised. This one step operation is more efficient than a heappop() followed by heappush() and can be more appropriate when using a fixed-size heap. The pop\/push combination always returns an element from the heap and replaces it with item. The value returned may be larger than the item added. If that isn\u2019t desired, consider using heappushpop() instead. Its push\/pop combination returns the smaller of the two values, leaving the larger value on the heap. \n The module also offers three general purpose functions based on heaps.  \nheapq.merge(*iterables, key=None, reverse=False)  \nMerge multiple sorted inputs into a single sorted output (for example, merge timestamped entries from multiple log files). Returns an iterator over the sorted values. Similar to sorted(itertools.chain(*iterables)) but returns an iterable, does not pull the data into memory all at once, and assumes that each of the input streams is already sorted (smallest to largest). Has two optional arguments which must be specified as keyword arguments. key specifies a key function of one argument that is used to extract a comparison key from each input element. The default value is None (compare the elements directly). reverse is a boolean value. If set to True, then the input elements are merged as if each comparison were reversed. To achieve behavior similar to sorted(itertools.chain(*iterables), reverse=True), all iterables must be sorted from largest to smallest.  Changed in version 3.5: Added the optional key and reverse parameters.  \n  \nheapq.nlargest(n, iterable, key=None)  \nReturn a list with the n largest elements from the dataset defined by iterable. key, if provided, specifies a function of one argument that is used to extract a comparison key from each element in iterable (for example, key=str.lower). Equivalent to: sorted(iterable, key=key,\nreverse=True)[:n]. \n  \nheapq.nsmallest(n, iterable, key=None)  \nReturn a list with the n smallest elements from the dataset defined by iterable. key, if provided, specifies a function of one argument that is used to extract a comparison key from each element in iterable (for example, key=str.lower). Equivalent to: sorted(iterable, key=key)[:n]. \n The latter two functions perform best for smaller values of n. For larger values, it is more efficient to use the sorted() function. Also, when n==1, it is more efficient to use the built-in min() and max() functions. If repeated usage of these functions is required, consider turning the iterable into an actual heap. Basic Examples A heapsort can be implemented by pushing all values onto a heap and then popping off the smallest values one at a time: >>> def heapsort(iterable):\n...     h = []\n...     for value in iterable:\n...         heappush(h, value)\n...     return [heappop(h) for i in range(len(h))]\n...\n>>> heapsort([1, 3, 5, 7, 9, 2, 4, 6, 8, 0])\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n This is similar to sorted(iterable), but unlike sorted(), this implementation is not stable. Heap elements can be tuples. This is useful for assigning comparison values (such as task priorities) alongside the main record being tracked: >>> h = []\n>>> heappush(h, (5, 'write code'))\n>>> heappush(h, (7, 'release product'))\n>>> heappush(h, (1, 'write spec'))\n>>> heappush(h, (3, 'create tests'))\n>>> heappop(h)\n(1, 'write spec')\n Priority Queue Implementation Notes A priority queue is common use for a heap, and it presents several implementation challenges:  Sort stability: how do you get two tasks with equal priorities to be returned in the order they were originally added? Tuple comparison breaks for (priority, task) pairs if the priorities are equal and the tasks do not have a default comparison order. If the priority of a task changes, how do you move it to a new position in the heap? Or if a pending task needs to be deleted, how do you find it and remove it from the queue?  A solution to the first two challenges is to store entries as 3-element list including the priority, an entry count, and the task. The entry count serves as a tie-breaker so that two tasks with the same priority are returned in the order they were added. And since no two entry counts are the same, the tuple comparison will never attempt to directly compare two tasks. Another solution to the problem of non-comparable tasks is to create a wrapper class that ignores the task item and only compares the priority field: from dataclasses import dataclass, field\nfrom typing import Any\n\n@dataclass(order=True)\nclass PrioritizedItem:\n    priority: int\n    item: Any=field(compare=False)\n The remaining challenges revolve around finding a pending task and making changes to its priority or removing it entirely. Finding a task can be done with a dictionary pointing to an entry in the queue. Removing the entry or changing its priority is more difficult because it would break the heap structure invariants. So, a possible solution is to mark the entry as removed and add a new entry with the revised priority: pq = []                         # list of entries arranged in a heap\nentry_finder = {}               # mapping of tasks to entries\nREMOVED = '<removed-task>'      # placeholder for a removed task\ncounter = itertools.count()     # unique sequence count\n\ndef add_task(task, priority=0):\n    'Add a new task or update the priority of an existing task'\n    if task in entry_finder:\n        remove_task(task)\n    count = next(counter)\n    entry = [priority, count, task]\n    entry_finder[task] = entry\n    heappush(pq, entry)\n\ndef remove_task(task):\n    'Mark an existing task as REMOVED.  Raise KeyError if not found.'\n    entry = entry_finder.pop(task)\n    entry[-1] = REMOVED\n\ndef pop_task():\n    'Remove and return the lowest priority task. Raise KeyError if empty.'\n    while pq:\n        priority, count, task = heappop(pq)\n        if task is not REMOVED:\n            del entry_finder[task]\n            return task\n    raise KeyError('pop from an empty priority queue')\n Theory Heaps are arrays for which a[k] <= a[2*k+1] and a[k] <= a[2*k+2] for all k, counting elements from 0. For the sake of comparison, non-existing elements are considered to be infinite. The interesting property of a heap is that a[0] is always its smallest element. The strange invariant above is meant to be an efficient memory representation for a tournament. The numbers below are k, not a[k]:                                0\n\n              1                                 2\n\n      3               4                5               6\n\n  7       8       9       10      11      12      13      14\n\n15 16   17 18   19 20   21 22   23 24   25 26   27 28   29 30\n In the tree above, each cell k is topping 2*k+1 and 2*k+2. In a usual binary tournament we see in sports, each cell is the winner over the two cells it tops, and we can trace the winner down the tree to see all opponents s\/he had. However, in many computer applications of such tournaments, we do not need to trace the history of a winner. To be more memory efficient, when a winner is promoted, we try to replace it by something else at a lower level, and the rule becomes that a cell and the two cells it tops contain three different items, but the top cell \u201cwins\u201d over the two topped cells. If this heap invariant is protected at all time, index 0 is clearly the overall winner. The simplest algorithmic way to remove it and find the \u201cnext\u201d winner is to move some loser (let\u2019s say cell 30 in the diagram above) into the 0 position, and then percolate this new 0 down the tree, exchanging values, until the invariant is re-established. This is clearly logarithmic on the total number of items in the tree. By iterating over all items, you get an O(n log n) sort. A nice feature of this sort is that you can efficiently insert new items while the sort is going on, provided that the inserted items are not \u201cbetter\u201d than the last 0\u2019th element you extracted. This is especially useful in simulation contexts, where the tree holds all incoming events, and the \u201cwin\u201d condition means the smallest scheduled time. When an event schedules other events for execution, they are scheduled into the future, so they can easily go into the heap. So, a heap is a good structure for implementing schedulers (this is what I used for my MIDI sequencer :-). Various structures for implementing schedulers have been extensively studied, and heaps are good for this, as they are reasonably speedy, the speed is almost constant, and the worst case is not much different than the average case. However, there are other representations which are more efficient overall, yet the worst cases might be terrible. Heaps are also very useful in big disk sorts. You most probably all know that a big sort implies producing \u201cruns\u201d (which are pre-sorted sequences, whose size is usually related to the amount of CPU memory), followed by a merging passes for these runs, which merging is often very cleverly organised 1. It is very important that the initial sort produces the longest runs possible. Tournaments are a good way to achieve that. If, using all the memory available to hold a tournament, you replace and percolate items that happen to fit the current run, you\u2019ll produce runs which are twice the size of the memory for random input, and much better for input fuzzily ordered. Moreover, if you output the 0\u2019th item on disk and get an input which may not fit in the current tournament (because the value \u201cwins\u201d over the last output value), it cannot fit in the heap, so the size of the heap decreases. The freed memory could be cleverly reused immediately for progressively building a second heap, which grows at exactly the same rate the first heap is melting. When the first heap completely vanishes, you switch heaps and start a new run. Clever and quite effective! In a word, heaps are useful memory structures to know. I use them in a few applications, and I think it is good to keep a \u2018heap\u2019 module around. :-) Footnotes  \n1  \nThe disk balancing algorithms which are current, nowadays, are more annoying than clever, and this is a consequence of the seeking capabilities of the disks. On devices which cannot seek, like big tape drives, the story was quite different, and one had to be very clever to ensure (far in advance) that each tape movement will be the most effective possible (that is, will best participate at \u201cprogressing\u201d the merge). Some tapes were even able to read backwards, and this was also used to avoid the rewinding time. Believe me, real good tape sorts were quite spectacular to watch! From all times, sorting has always been a Great Art! :-)","title":"python.library.heapq"},{"text":"reverse()  \nReverse the elements of the deque in-place and then return None.  New in version 3.2.","title":"python.library.collections#collections.deque.reverse"},{"text":"reverse()","title":"django.ref.models.querysets#django.db.models.query.QuerySet.reverse"},{"text":"heapq.heapify(x)  \nTransform list x into a heap, in-place, in linear time.","title":"python.library.heapq#heapq.heapify"},{"text":"reverse_order()  \nThis method for the Stats class reverses the ordering of the basic list within the object. Note that by default ascending vs descending order is properly selected based on the sort key of choice.","title":"python.library.profile#pstats.Stats.reverse_order"},{"text":"reversed(seq)  \nReturn a reverse iterator. seq must be an object which has a __reversed__() method or supports the sequence protocol (the __len__() method and the __getitem__() method with integer arguments starting at 0).","title":"python.library.functions#reversed"},{"text":"class asyncio.PriorityQueue  \nA variant of Queue; retrieves entries in priority order (lowest first). Entries are typically tuples of the form (priority_number, data).","title":"python.library.asyncio-queue#asyncio.PriorityQueue"},{"text":"tf.raw_ops.PriorityQueue A queue that produces elements sorted by the first component value.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.raw_ops.PriorityQueue  \ntf.raw_ops.PriorityQueue(\n    shapes, component_types=[], capacity=-1, container='',\n    shared_name='', name=None\n)\n Note that the PriorityQueue requires the first component of any element to be a scalar int64, in addition to the other elements declared by component_types. Therefore calls to Enqueue and EnqueueMany (resp. Dequeue and DequeueMany) on a PriorityQueue will all require (resp. output) one extra entry in their input (resp. output) lists.\n \n\n\n Args\n  shapes   A list of shapes (each a tf.TensorShape or list of ints). The shape of each component in a value. The length of this attr must be either 0 or the same as the length of component_types. If the length of this attr is 0, the shapes of queue elements are not constrained, and only one element may be dequeued at a time.  \n  component_types   An optional list of tf.DTypes. Defaults to []. The type of each component in a value.  \n  capacity   An optional int. Defaults to -1. The upper bound on the number of elements in this queue. Negative numbers mean no limit.  \n  container   An optional string. Defaults to \"\". If non-empty, this queue is placed in the given container. Otherwise, a default container is used.  \n  shared_name   An optional string. Defaults to \"\". If non-empty, this queue will be shared under the given name across multiple sessions.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor of type mutable string.","title":"tensorflow.raw_ops.priorityqueue"},{"text":"heapq.merge(*iterables, key=None, reverse=False)  \nMerge multiple sorted inputs into a single sorted output (for example, merge timestamped entries from multiple log files). Returns an iterator over the sorted values. Similar to sorted(itertools.chain(*iterables)) but returns an iterable, does not pull the data into memory all at once, and assumes that each of the input streams is already sorted (smallest to largest). Has two optional arguments which must be specified as keyword arguments. key specifies a key function of one argument that is used to extract a comparison key from each input element. The default value is None (compare the elements directly). reverse is a boolean value. If set to True, then the input elements are merged as if each comparison were reversed. To achieve behavior similar to sorted(itertools.chain(*iterables), reverse=True), all iterables must be sorted from largest to smallest.  Changed in version 3.5: Added the optional key and reverse parameters.","title":"python.library.heapq#heapq.merge"},{"text":"class queue.PriorityQueue(maxsize=0)  \nConstructor for a priority queue. maxsize is an integer that sets the upperbound limit on the number of items that can be placed in the queue. Insertion will block once this size has been reached, until queue items are consumed. If maxsize is less than or equal to zero, the queue size is infinite. The lowest valued entries are retrieved first (the lowest valued entry is the one returned by sorted(list(entries))[0]). A typical pattern for entries is a tuple in the form: (priority_number, data). If the data elements are not comparable, the data can be wrapped in a class that ignores the data item and only compares the priority number: from dataclasses import dataclass, field\nfrom typing import Any\n\n@dataclass(order=True)\nclass PrioritizedItem:\n    priority: int\n    item: Any=field(compare=False)","title":"python.library.queue#queue.PriorityQueue"}]}
{"task_id":18897261,"prompt":"def f_18897261(df):\n\treturn ","suffix":"","canonical_solution":"df['group'].plot(kind='bar', color=['r', 'g', 'b', 'r', 'g', 'b', 'r'])","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    df = pd.DataFrame([1, 3, 4, 5, 7, 9], columns = ['group'])\n    a = candidate(df)\n    assert 'AxesSubplot' in str(type(a))\n"],"entry_point":"f_18897261","intent":"make a barplot of data in column `group` of dataframe `df` colour-coded according to list `color`","library":["pandas"],"docs":[{"text":"pandas.core.groupby.DataFrameGroupBy.boxplot   DataFrameGroupBy.boxplot(subplots=True, column=None, fontsize=None, rot=0, grid=True, ax=None, figsize=None, layout=None, sharex=False, sharey=True, backend=None, **kwargs)[source]\n \nMake box plots from DataFrameGroupBy data.  Parameters \n \ngrouped:Grouped DataFrame\n\n\nsubplots:bool\n\n\n False - no subplots will be used True - create a subplot for each group.   \ncolumn:column name or list of names, or vector\n\n\nCan be any valid input to groupby.  \nfontsize:int or str\n\n\nrot:label rotation angle\n\n\ngrid:Setting this to True will show the grid\n\n\nax:Matplotlib axis object, default None\n\n\nfigsize:A tuple (width, height) in inches\n\n\nlayout:tuple (optional)\n\n\nThe layout of the plot: (rows, columns).  \nsharex:bool, default False\n\n\nWhether x-axes will be shared among subplots.  \nsharey:bool, default True\n\n\nWhether y-axes will be shared among subplots.  \nbackend:str, default None\n\n\nBackend to use instead of the backend specified in the option plotting.backend. For instance, \u2018matplotlib\u2019. Alternatively, to specify the plotting.backend for the whole session, set pd.options.plotting.backend.  New in version 1.0.0.   **kwargs\n\nAll other plotting keyword arguments to be passed to matplotlib\u2019s boxplot function.    Returns \n dict of key\/value = group key\/DataFrame.boxplot return value\nor DataFrame.boxplot return value in case subplots=figures=False\n   Examples You can create boxplots for grouped data and show them as separate subplots: \n>>> import itertools\n>>> tuples = [t for t in itertools.product(range(1000), range(4))]\n>>> index = pd.MultiIndex.from_tuples(tuples, names=['lvl0', 'lvl1'])\n>>> data = np.random.randn(len(index),4)\n>>> df = pd.DataFrame(data, columns=list('ABCD'), index=index)\n>>> grouped = df.groupby(level='lvl1')\n>>> grouped.boxplot(rot=45, fontsize=12, figsize=(8,10))  \n     The subplots=False option shows the boxplots in a single figure. \n>>> grouped.boxplot(subplots=False, rot=45, fontsize=12)","title":"pandas.reference.api.pandas.core.groupby.dataframegroupby.boxplot"},{"text":"pandas.DataFrame.plot.bar   DataFrame.plot.bar(x=None, y=None, **kwargs)[source]\n \nVertical bar plot. A bar plot is a plot that presents categorical data with rectangular bars with lengths proportional to the values that they represent. A bar plot shows comparisons among discrete categories. One axis of the plot shows the specific categories being compared, and the other axis represents a measured value.  Parameters \n \nx:label or position, optional\n\n\nAllows plotting of one column versus another. If not specified, the index of the DataFrame is used.  \ny:label or position, optional\n\n\nAllows plotting of one column versus another. If not specified, all numerical columns are used.  \ncolor:str, array-like, or dict, optional\n\n\nThe color for each of the DataFrame\u2019s columns. Possible values are:  \n A single color string referred to by name, RGB or RGBA code,\n\nfor instance \u2018red\u2019 or \u2018#a98d19\u2019.    \n A sequence of color strings referred to by name, RGB or RGBA\n\ncode, which will be used for each column recursively. For instance [\u2018green\u2019,\u2019yellow\u2019] each column\u2019s bar will be filled in green or yellow, alternatively. If there is only a single column to be plotted, then only the first color from the color list will be used.    \n A dict of the form {column name:color}, so that each column will be\n\n\ncolored accordingly. For example, if your columns are called a and b, then passing {\u2018a\u2019: \u2018green\u2019, \u2018b\u2019: \u2018red\u2019} will color bars for column a in green and bars for column b in red.      New in version 1.1.0.   **kwargs\n\nAdditional keyword arguments are documented in DataFrame.plot().    Returns \n matplotlib.axes.Axes or np.ndarray of them\n\nAn ndarray is returned with one matplotlib.axes.Axes per column when subplots=True.      See also  DataFrame.plot.barh\n\nHorizontal bar plot.  DataFrame.plot\n\nMake plots of a DataFrame.  matplotlib.pyplot.bar\n\nMake a bar plot with matplotlib.    Examples Basic plot. \n>>> df = pd.DataFrame({'lab':['A', 'B', 'C'], 'val':[10, 30, 20]})\n>>> ax = df.plot.bar(x='lab', y='val', rot=0)\n     Plot a whole dataframe to a bar plot. Each column is assigned a distinct color, and each row is nested in a group along the horizontal axis. \n>>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n>>> index = ['snail', 'pig', 'elephant',\n...          'rabbit', 'giraffe', 'coyote', 'horse']\n>>> df = pd.DataFrame({'speed': speed,\n...                    'lifespan': lifespan}, index=index)\n>>> ax = df.plot.bar(rot=0)\n     Plot stacked bar charts for the DataFrame \n>>> ax = df.plot.bar(stacked=True)\n     Instead of nesting, the figure can be split by column with subplots=True. In this case, a numpy.ndarray of matplotlib.axes.Axes are returned. \n>>> axes = df.plot.bar(rot=0, subplots=True)\n>>> axes[1].legend(loc=2)  \n     If you don\u2019t like the default colours, you can specify how you\u2019d like each column to be colored. \n>>> axes = df.plot.bar(\n...     rot=0, subplots=True, color={\"speed\": \"red\", \"lifespan\": \"green\"}\n... )\n>>> axes[1].legend(loc=2)  \n     Plot a single column. \n>>> ax = df.plot.bar(y='speed', rot=0)\n     Plot only selected categories for the DataFrame. \n>>> ax = df.plot.bar(x='lifespan', rot=0)","title":"pandas.reference.api.pandas.dataframe.plot.bar"},{"text":"pandas.Series.plot.bar   Series.plot.bar(x=None, y=None, **kwargs)[source]\n \nVertical bar plot. A bar plot is a plot that presents categorical data with rectangular bars with lengths proportional to the values that they represent. A bar plot shows comparisons among discrete categories. One axis of the plot shows the specific categories being compared, and the other axis represents a measured value.  Parameters \n \nx:label or position, optional\n\n\nAllows plotting of one column versus another. If not specified, the index of the DataFrame is used.  \ny:label or position, optional\n\n\nAllows plotting of one column versus another. If not specified, all numerical columns are used.  \ncolor:str, array-like, or dict, optional\n\n\nThe color for each of the DataFrame\u2019s columns. Possible values are:  \n A single color string referred to by name, RGB or RGBA code,\n\nfor instance \u2018red\u2019 or \u2018#a98d19\u2019.    \n A sequence of color strings referred to by name, RGB or RGBA\n\ncode, which will be used for each column recursively. For instance [\u2018green\u2019,\u2019yellow\u2019] each column\u2019s bar will be filled in green or yellow, alternatively. If there is only a single column to be plotted, then only the first color from the color list will be used.    \n A dict of the form {column name:color}, so that each column will be\n\n\ncolored accordingly. For example, if your columns are called a and b, then passing {\u2018a\u2019: \u2018green\u2019, \u2018b\u2019: \u2018red\u2019} will color bars for column a in green and bars for column b in red.      New in version 1.1.0.   **kwargs\n\nAdditional keyword arguments are documented in DataFrame.plot().    Returns \n matplotlib.axes.Axes or np.ndarray of them\n\nAn ndarray is returned with one matplotlib.axes.Axes per column when subplots=True.      See also  DataFrame.plot.barh\n\nHorizontal bar plot.  DataFrame.plot\n\nMake plots of a DataFrame.  matplotlib.pyplot.bar\n\nMake a bar plot with matplotlib.    Examples Basic plot. \n>>> df = pd.DataFrame({'lab':['A', 'B', 'C'], 'val':[10, 30, 20]})\n>>> ax = df.plot.bar(x='lab', y='val', rot=0)\n     Plot a whole dataframe to a bar plot. Each column is assigned a distinct color, and each row is nested in a group along the horizontal axis. \n>>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n>>> index = ['snail', 'pig', 'elephant',\n...          'rabbit', 'giraffe', 'coyote', 'horse']\n>>> df = pd.DataFrame({'speed': speed,\n...                    'lifespan': lifespan}, index=index)\n>>> ax = df.plot.bar(rot=0)\n     Plot stacked bar charts for the DataFrame \n>>> ax = df.plot.bar(stacked=True)\n     Instead of nesting, the figure can be split by column with subplots=True. In this case, a numpy.ndarray of matplotlib.axes.Axes are returned. \n>>> axes = df.plot.bar(rot=0, subplots=True)\n>>> axes[1].legend(loc=2)  \n     If you don\u2019t like the default colours, you can specify how you\u2019d like each column to be colored. \n>>> axes = df.plot.bar(\n...     rot=0, subplots=True, color={\"speed\": \"red\", \"lifespan\": \"green\"}\n... )\n>>> axes[1].legend(loc=2)  \n     Plot a single column. \n>>> ax = df.plot.bar(y='speed', rot=0)\n     Plot only selected categories for the DataFrame. \n>>> ax = df.plot.bar(x='lifespan', rot=0)","title":"pandas.reference.api.pandas.series.plot.bar"},{"text":"pandas.DataFrame.plot.barh   DataFrame.plot.barh(x=None, y=None, **kwargs)[source]\n \nMake a horizontal bar plot. A horizontal bar plot is a plot that presents quantitative data with rectangular bars with lengths proportional to the values that they represent. A bar plot shows comparisons among discrete categories. One axis of the plot shows the specific categories being compared, and the other axis represents a measured value.  Parameters \n \nx:label or position, optional\n\n\nAllows plotting of one column versus another. If not specified, the index of the DataFrame is used.  \ny:label or position, optional\n\n\nAllows plotting of one column versus another. If not specified, all numerical columns are used.  \ncolor:str, array-like, or dict, optional\n\n\nThe color for each of the DataFrame\u2019s columns. Possible values are:  \n A single color string referred to by name, RGB or RGBA code,\n\nfor instance \u2018red\u2019 or \u2018#a98d19\u2019.    \n A sequence of color strings referred to by name, RGB or RGBA\n\ncode, which will be used for each column recursively. For instance [\u2018green\u2019,\u2019yellow\u2019] each column\u2019s bar will be filled in green or yellow, alternatively. If there is only a single column to be plotted, then only the first color from the color list will be used.    \n A dict of the form {column name:color}, so that each column will be\n\n\ncolored accordingly. For example, if your columns are called a and b, then passing {\u2018a\u2019: \u2018green\u2019, \u2018b\u2019: \u2018red\u2019} will color bars for column a in green and bars for column b in red.      New in version 1.1.0.   **kwargs\n\nAdditional keyword arguments are documented in DataFrame.plot().    Returns \n matplotlib.axes.Axes or np.ndarray of them\n\nAn ndarray is returned with one matplotlib.axes.Axes per column when subplots=True.      See also  DataFrame.plot.bar\n\nVertical bar plot.  DataFrame.plot\n\nMake plots of DataFrame using matplotlib.  matplotlib.axes.Axes.bar\n\nPlot a vertical bar plot using matplotlib.    Examples Basic example \n>>> df = pd.DataFrame({'lab': ['A', 'B', 'C'], 'val': [10, 30, 20]})\n>>> ax = df.plot.barh(x='lab', y='val')\n     Plot a whole DataFrame to a horizontal bar plot \n>>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n>>> index = ['snail', 'pig', 'elephant',\n...          'rabbit', 'giraffe', 'coyote', 'horse']\n>>> df = pd.DataFrame({'speed': speed,\n...                    'lifespan': lifespan}, index=index)\n>>> ax = df.plot.barh()\n     Plot stacked barh charts for the DataFrame \n>>> ax = df.plot.barh(stacked=True)\n     We can specify colors for each column \n>>> ax = df.plot.barh(color={\"speed\": \"red\", \"lifespan\": \"green\"})\n     Plot a column of the DataFrame to a horizontal bar plot \n>>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n>>> index = ['snail', 'pig', 'elephant',\n...          'rabbit', 'giraffe', 'coyote', 'horse']\n>>> df = pd.DataFrame({'speed': speed,\n...                    'lifespan': lifespan}, index=index)\n>>> ax = df.plot.barh(y='speed')\n     Plot DataFrame versus the desired column \n>>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n>>> index = ['snail', 'pig', 'elephant',\n...          'rabbit', 'giraffe', 'coyote', 'horse']\n>>> df = pd.DataFrame({'speed': speed,\n...                    'lifespan': lifespan}, index=index)\n>>> ax = df.plot.barh(x='lifespan')","title":"pandas.reference.api.pandas.dataframe.plot.barh"},{"text":"pandas.Series.plot.barh   Series.plot.barh(x=None, y=None, **kwargs)[source]\n \nMake a horizontal bar plot. A horizontal bar plot is a plot that presents quantitative data with rectangular bars with lengths proportional to the values that they represent. A bar plot shows comparisons among discrete categories. One axis of the plot shows the specific categories being compared, and the other axis represents a measured value.  Parameters \n \nx:label or position, optional\n\n\nAllows plotting of one column versus another. If not specified, the index of the DataFrame is used.  \ny:label or position, optional\n\n\nAllows plotting of one column versus another. If not specified, all numerical columns are used.  \ncolor:str, array-like, or dict, optional\n\n\nThe color for each of the DataFrame\u2019s columns. Possible values are:  \n A single color string referred to by name, RGB or RGBA code,\n\nfor instance \u2018red\u2019 or \u2018#a98d19\u2019.    \n A sequence of color strings referred to by name, RGB or RGBA\n\ncode, which will be used for each column recursively. For instance [\u2018green\u2019,\u2019yellow\u2019] each column\u2019s bar will be filled in green or yellow, alternatively. If there is only a single column to be plotted, then only the first color from the color list will be used.    \n A dict of the form {column name:color}, so that each column will be\n\n\ncolored accordingly. For example, if your columns are called a and b, then passing {\u2018a\u2019: \u2018green\u2019, \u2018b\u2019: \u2018red\u2019} will color bars for column a in green and bars for column b in red.      New in version 1.1.0.   **kwargs\n\nAdditional keyword arguments are documented in DataFrame.plot().    Returns \n matplotlib.axes.Axes or np.ndarray of them\n\nAn ndarray is returned with one matplotlib.axes.Axes per column when subplots=True.      See also  DataFrame.plot.bar\n\nVertical bar plot.  DataFrame.plot\n\nMake plots of DataFrame using matplotlib.  matplotlib.axes.Axes.bar\n\nPlot a vertical bar plot using matplotlib.    Examples Basic example \n>>> df = pd.DataFrame({'lab': ['A', 'B', 'C'], 'val': [10, 30, 20]})\n>>> ax = df.plot.barh(x='lab', y='val')\n     Plot a whole DataFrame to a horizontal bar plot \n>>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n>>> index = ['snail', 'pig', 'elephant',\n...          'rabbit', 'giraffe', 'coyote', 'horse']\n>>> df = pd.DataFrame({'speed': speed,\n...                    'lifespan': lifespan}, index=index)\n>>> ax = df.plot.barh()\n     Plot stacked barh charts for the DataFrame \n>>> ax = df.plot.barh(stacked=True)\n     We can specify colors for each column \n>>> ax = df.plot.barh(color={\"speed\": \"red\", \"lifespan\": \"green\"})\n     Plot a column of the DataFrame to a horizontal bar plot \n>>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n>>> index = ['snail', 'pig', 'elephant',\n...          'rabbit', 'giraffe', 'coyote', 'horse']\n>>> df = pd.DataFrame({'speed': speed,\n...                    'lifespan': lifespan}, index=index)\n>>> ax = df.plot.barh(y='speed')\n     Plot DataFrame versus the desired column \n>>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n>>> index = ['snail', 'pig', 'elephant',\n...          'rabbit', 'giraffe', 'coyote', 'horse']\n>>> df = pd.DataFrame({'speed': speed,\n...                    'lifespan': lifespan}, index=index)\n>>> ax = df.plot.barh(x='lifespan')","title":"pandas.reference.api.pandas.series.plot.barh"},{"text":"pandas.DataFrame.plot.box   DataFrame.plot.box(by=None, **kwargs)[source]\n \nMake a box plot of the DataFrame columns. A box plot is a method for graphically depicting groups of numerical data through their quartiles. The box extends from the Q1 to Q3 quartile values of the data, with a line at the median (Q2). The whiskers extend from the edges of box to show the range of the data. The position of the whiskers is set by default to 1.5*IQR (IQR = Q3 - Q1) from the edges of the box. Outlier points are those past the end of the whiskers. For further details see Wikipedia\u2019s entry for boxplot. A consideration when using this chart is that the box and the whiskers can overlap, which is very common when plotting small sets of data.  Parameters \n \nby:str or sequence\n\n\nColumn in the DataFrame to group by.  Changed in version 1.4.0: Previously, by is silently ignore and makes no groupings   **kwargs\n\nAdditional keywords are documented in DataFrame.plot().    Returns \n \nmatplotlib.axes.Axes or numpy.ndarray of them\n    See also  DataFrame.boxplot\n\nAnother method to draw a box plot.  Series.plot.box\n\nDraw a box plot from a Series object.  matplotlib.pyplot.boxplot\n\nDraw a box plot in matplotlib.    Examples Draw a box plot from a DataFrame with four columns of randomly generated data. \n>>> data = np.random.randn(25, 4)\n>>> df = pd.DataFrame(data, columns=list('ABCD'))\n>>> ax = df.plot.box()\n     You can also generate groupings if you specify the by parameter (which can take a column name, or a list or tuple of column names):  Changed in version 1.4.0.  \n>>> age_list = [8, 10, 12, 14, 72, 74, 76, 78, 20, 25, 30, 35, 60, 85]\n>>> df = pd.DataFrame({\"gender\": list(\"MMMMMMMMFFFFFF\"), \"age\": age_list})\n>>> ax = df.plot.box(column=\"age\", by=\"gender\", figsize=(10, 8))","title":"pandas.reference.api.pandas.dataframe.plot.box"},{"text":"pandas.Series.plot.box   Series.plot.box(by=None, **kwargs)[source]\n \nMake a box plot of the DataFrame columns. A box plot is a method for graphically depicting groups of numerical data through their quartiles. The box extends from the Q1 to Q3 quartile values of the data, with a line at the median (Q2). The whiskers extend from the edges of box to show the range of the data. The position of the whiskers is set by default to 1.5*IQR (IQR = Q3 - Q1) from the edges of the box. Outlier points are those past the end of the whiskers. For further details see Wikipedia\u2019s entry for boxplot. A consideration when using this chart is that the box and the whiskers can overlap, which is very common when plotting small sets of data.  Parameters \n \nby:str or sequence\n\n\nColumn in the DataFrame to group by.  Changed in version 1.4.0: Previously, by is silently ignore and makes no groupings   **kwargs\n\nAdditional keywords are documented in DataFrame.plot().    Returns \n \nmatplotlib.axes.Axes or numpy.ndarray of them\n    See also  DataFrame.boxplot\n\nAnother method to draw a box plot.  Series.plot.box\n\nDraw a box plot from a Series object.  matplotlib.pyplot.boxplot\n\nDraw a box plot in matplotlib.    Examples Draw a box plot from a DataFrame with four columns of randomly generated data. \n>>> data = np.random.randn(25, 4)\n>>> df = pd.DataFrame(data, columns=list('ABCD'))\n>>> ax = df.plot.box()\n     You can also generate groupings if you specify the by parameter (which can take a column name, or a list or tuple of column names):  Changed in version 1.4.0.  \n>>> age_list = [8, 10, 12, 14, 72, 74, 76, 78, 20, 25, 30, 35, 60, 85]\n>>> df = pd.DataFrame({\"gender\": list(\"MMMMMMMMFFFFFF\"), \"age\": age_list})\n>>> ax = df.plot.box(column=\"age\", by=\"gender\", figsize=(10, 8))","title":"pandas.reference.api.pandas.series.plot.box"},{"text":"pandas.core.groupby.SeriesGroupBy.hist   propertySeriesGroupBy.hist\n \nDraw histogram of the input series using matplotlib.  Parameters \n \nby:object, optional\n\n\nIf passed, then used to form histograms for separate groups.  \nax:matplotlib axis object\n\n\nIf not passed, uses gca().  \ngrid:bool, default True\n\n\nWhether to show axis grid lines.  \nxlabelsize:int, default None\n\n\nIf specified changes the x-axis label size.  \nxrot:float, default None\n\n\nRotation of x axis labels.  \nylabelsize:int, default None\n\n\nIf specified changes the y-axis label size.  \nyrot:float, default None\n\n\nRotation of y axis labels.  \nfigsize:tuple, default None\n\n\nFigure size in inches by default.  \nbins:int or sequence, default 10\n\n\nNumber of histogram bins to be used. If an integer is given, bins + 1 bin edges are calculated and returned. If bins is a sequence, gives bin edges, including left edge of first bin and right edge of last bin. In this case, bins is returned unmodified.  \nbackend:str, default None\n\n\nBackend to use instead of the backend specified in the option plotting.backend. For instance, \u2018matplotlib\u2019. Alternatively, to specify the plotting.backend for the whole session, set pd.options.plotting.backend.  New in version 1.0.0.   \nlegend:bool, default False\n\n\nWhether to show the legend.  New in version 1.1.0.   **kwargs\n\nTo be passed to the actual plotting function.    Returns \n matplotlib.AxesSubplot\n\nA histogram plot.      See also  matplotlib.axes.Axes.hist\n\nPlot a histogram using matplotlib.","title":"pandas.reference.api.pandas.core.groupby.seriesgroupby.hist"},{"text":"pandas.DataFrame.boxplot   DataFrame.boxplot(column=None, by=None, ax=None, fontsize=None, rot=0, grid=True, figsize=None, layout=None, return_type=None, backend=None, **kwargs)[source]\n \nMake a box plot from DataFrame columns. Make a box-and-whisker plot from DataFrame columns, optionally grouped by some other columns. A box plot is a method for graphically depicting groups of numerical data through their quartiles. The box extends from the Q1 to Q3 quartile values of the data, with a line at the median (Q2). The whiskers extend from the edges of box to show the range of the data. By default, they extend no more than 1.5 * IQR (IQR = Q3 - Q1) from the edges of the box, ending at the farthest data point within that interval. Outliers are plotted as separate dots. For further details see Wikipedia\u2019s entry for boxplot.  Parameters \n \ncolumn:str or list of str, optional\n\n\nColumn name or list of names, or vector. Can be any valid input to pandas.DataFrame.groupby().  \nby:str or array-like, optional\n\n\nColumn in the DataFrame to pandas.DataFrame.groupby(). One box-plot will be done per value of columns in by.  \nax:object of class matplotlib.axes.Axes, optional\n\n\nThe matplotlib axes to be used by boxplot.  \nfontsize:float or str\n\n\nTick label font size in points or as a string (e.g., large).  \nrot:int or float, default 0\n\n\nThe rotation angle of labels (in degrees) with respect to the screen coordinate system.  \ngrid:bool, default True\n\n\nSetting this to True will show the grid.  \nfigsize:A tuple (width, height) in inches\n\n\nThe size of the figure to create in matplotlib.  \nlayout:tuple (rows, columns), optional\n\n\nFor example, (3, 5) will display the subplots using 3 columns and 5 rows, starting from the top-left.  \nreturn_type:{\u2018axes\u2019, \u2018dict\u2019, \u2018both\u2019} or None, default \u2018axes\u2019\n\n\nThe kind of object to return. The default is axes.  \u2018axes\u2019 returns the matplotlib axes the boxplot is drawn on. \u2018dict\u2019 returns a dictionary whose values are the matplotlib Lines of the boxplot. \u2018both\u2019 returns a namedtuple with the axes and dict. \nwhen grouping with by, a Series mapping columns to return_type is returned. If return_type is None, a NumPy array of axes with the same shape as layout is returned.    \nbackend:str, default None\n\n\nBackend to use instead of the backend specified in the option plotting.backend. For instance, \u2018matplotlib\u2019. Alternatively, to specify the plotting.backend for the whole session, set pd.options.plotting.backend.  New in version 1.0.0.   **kwargs\n\nAll other plotting keyword arguments to be passed to matplotlib.pyplot.boxplot().    Returns \n result\n\nSee Notes.      See also  Series.plot.hist\n\nMake a histogram.  matplotlib.pyplot.boxplot\n\nMatplotlib equivalent plot.    Notes The return type depends on the return_type parameter:  \u2018axes\u2019 : object of class matplotlib.axes.Axes \u2018dict\u2019 : dict of matplotlib.lines.Line2D objects \u2018both\u2019 : a namedtuple with structure (ax, lines)  For data grouped with by, return a Series of the above or a numpy array:  Series array (for return_type = None)  Use return_type='dict' when you want to tweak the appearance of the lines after plotting. In this case a dict containing the Lines making up the boxes, caps, fliers, medians, and whiskers is returned. Examples Boxplots can be created for every column in the dataframe by df.boxplot() or indicating the columns to be used: \n>>> np.random.seed(1234)\n>>> df = pd.DataFrame(np.random.randn(10, 4),\n...                   columns=['Col1', 'Col2', 'Col3', 'Col4'])\n>>> boxplot = df.boxplot(column=['Col1', 'Col2', 'Col3'])  \n     Boxplots of variables distributions grouped by the values of a third variable can be created using the option by. For instance: \n>>> df = pd.DataFrame(np.random.randn(10, 2),\n...                   columns=['Col1', 'Col2'])\n>>> df['X'] = pd.Series(['A', 'A', 'A', 'A', 'A',\n...                      'B', 'B', 'B', 'B', 'B'])\n>>> boxplot = df.boxplot(by='X')\n     A list of strings (i.e. ['X', 'Y']) can be passed to boxplot in order to group the data by combination of the variables in the x-axis: \n>>> df = pd.DataFrame(np.random.randn(10, 3),\n...                   columns=['Col1', 'Col2', 'Col3'])\n>>> df['X'] = pd.Series(['A', 'A', 'A', 'A', 'A',\n...                      'B', 'B', 'B', 'B', 'B'])\n>>> df['Y'] = pd.Series(['A', 'B', 'A', 'B', 'A',\n...                      'B', 'A', 'B', 'A', 'B'])\n>>> boxplot = df.boxplot(column=['Col1', 'Col2'], by=['X', 'Y'])\n     The layout of boxplot can be adjusted giving a tuple to layout: \n>>> boxplot = df.boxplot(column=['Col1', 'Col2'], by='X',\n...                      layout=(2, 1))\n     Additional formatting can be done to the boxplot, like suppressing the grid (grid=False), rotating the labels in the x-axis (i.e. rot=45) or changing the fontsize (i.e. fontsize=15): \n>>> boxplot = df.boxplot(grid=False, rot=45, fontsize=15)  \n     The parameter return_type can be used to select the type of element returned by boxplot. When return_type='axes' is selected, the matplotlib axes on which the boxplot is drawn are returned: \n>>> boxplot = df.boxplot(column=['Col1', 'Col2'], return_type='axes')\n>>> type(boxplot)\n<class 'matplotlib.axes._subplots.AxesSubplot'>\n  When grouping with by, a Series mapping columns to return_type is returned: \n>>> boxplot = df.boxplot(column=['Col1', 'Col2'], by='X',\n...                      return_type='axes')\n>>> type(boxplot)\n<class 'pandas.core.series.Series'>\n  If return_type is None, a NumPy array of axes with the same shape as layout is returned: \n>>> boxplot = df.boxplot(column=['Col1', 'Col2'], by='X',\n...                      return_type=None)\n>>> type(boxplot)\n<class 'numpy.ndarray'>","title":"pandas.reference.api.pandas.dataframe.boxplot"},{"text":"bar3d(x, y, z, dx, dy, dz, color=None, zsort='average', shade=True, lightsource=None, *args, data=None, **kwargs)[source]\n \nGenerate a 3D barplot. This method creates three dimensional barplot where the width, depth, height, and color of the bars can all be uniquely set.  Parameters \n \nx, y, zarray-like\n\n\nThe coordinates of the anchor point of the bars.  \ndx, dy, dzfloat or array-like\n\n\nThe width, depth, and height of the bars, respectively.  \ncolorsequence of colors, optional\n\n\nThe color of the bars can be specified globally or individually. This parameter can be:  A single color, to color all bars the same color. An array of colors of length N bars, to color each bar independently. An array of colors of length 6, to color the faces of the bars similarly. An array of colors of length 6 * N bars, to color each face independently.  When coloring the faces of the boxes specifically, this is the order of the coloring:  -Z (bottom of box) +Z (top of box) -Y +Y -X +X   \nzsortstr, optional\n\n\nThe z-axis sorting scheme passed onto Poly3DCollection  \nshadebool, default: True\n\n\nWhen true, this shades the dark sides of the bars (relative to the plot's source of light).  \nlightsourceLightSource\n\n\nThe lightsource to use when shade is True.  \ndataindexable object, optional\n\n\nIf given, all parameters also accept a string s, which is interpreted as data[s] (unless this raises an exception).  **kwargs\n\nAny additional keyword arguments are passed onto Poly3DCollection.    Returns \n \ncollectionPoly3DCollection\n\n\nA collection of three dimensional polygons representing the bars.","title":"matplotlib._as_gen.mpl_toolkits.mplot3d.axes3d.axes3d#mpl_toolkits.mplot3d.axes3d.Axes3D.bar3d"}]}
{"task_id":373194,"prompt":"def f_373194(data):\n\treturn ","suffix":"","canonical_solution":"re.findall('([a-fA-F\\\\d]{32})', data)","test_start":"\nimport re \n\ndef check(candidate):","test":["\n    assert candidate('6f96cfdfe5ccc627cadf24b41725caa4 gorilla') ==         ['6f96cfdfe5ccc627cadf24b41725caa4']\n"],"entry_point":"f_373194","intent":"find all matches of regex pattern '([a-fA-F\\\\d]{32})' in string `data`","library":["re"],"docs":[{"text":"statichexify(match)[source]","title":"matplotlib.backend_pdf_api#matplotlib.backends.backend_pdf.Name.hexify"},{"text":"statistics.multimode(data)  \nReturn a list of the most frequently occurring values in the order they were first encountered in the data. Will return more than one result if there are multiple modes or an empty list if the data is empty: >>> multimode('aabbbbccddddeeffffgg')\n['b', 'd', 'f']\n>>> multimode('')\n[]\n  New in version 3.8.","title":"python.library.statistics#statistics.multimode"},{"text":"residuals(data, params=None) [source]\n \nDetermine residuals of data to model. For each point, the shortest (orthogonal) distance to the line is returned. It is obtained by projecting the data onto the line.  Parameters \n \ndata(N, dim) array \n\nN points in a space of dimension dim.  \nparams(2, ) array, optional \n\nOptional custom parameter set in the form (origin, direction).    Returns \n \nresiduals(N, ) array \n\nResidual for each data point.","title":"skimage.api.skimage.measure#skimage.measure.LineModelND.residuals"},{"text":"estimate(data) [source]\n \nEstimate line model from data. This minimizes the sum of shortest (orthogonal) distances from the given data points to the estimated line.  Parameters \n \ndata(N, dim) array \n\nN points in a space of dimensionality dim >= 2.    Returns \n \nsuccessbool \n\nTrue, if model estimation succeeds.","title":"skimage.api.skimage.measure#skimage.measure.LineModelND.estimate"},{"text":"werkzeug.http.generate_etag(data)  \nGenerate an etag for some data.  Changed in version 2.0: Use SHA-1. MD5 may not be available in some environments.   Parameters \ndata (bytes) \u2013   Return type \nstr","title":"werkzeug.http.index#werkzeug.http.generate_etag"},{"text":"tf.raw_ops.RefSwitch Forwards the ref tensor data to the output port determined by pred.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.raw_ops.RefSwitch  \ntf.raw_ops.RefSwitch(\n    data, pred, name=None\n)\n If pred is true, the data input is forwarded to output_true. Otherwise, the data goes to output_false. See also Switch and Merge.\n \n\n\n Args\n  data   A mutable Tensor. The ref tensor to be forwarded to the appropriate output.  \n  pred   A Tensor of type bool. A scalar that specifies which output port will receive data.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A tuple of Tensor objects (output_false, output_true).     output_false   A mutable Tensor. Has the same type as data.  \n  output_true   A mutable Tensor. Has the same type as data.","title":"tensorflow.raw_ops.refswitch"},{"text":"tf.debugging.Assert     View source on GitHub    Asserts that the given condition is true.  View aliases  Main aliases \ntf.Assert Compat aliases for migration See Migration guide for more details. tf.compat.v1.Assert, tf.compat.v1.debugging.Assert  \ntf.debugging.Assert(\n    condition, data, summarize=None, name=None\n)\n If condition evaluates to false, print the list of tensors in data. summarize determines how many entries of the tensors to print.\n \n\n\n Args\n  condition   The condition to evaluate.  \n  data   The tensors to print out when condition is false.  \n  summarize   Print this many entries of each tensor.  \n  name   A name for this operation (optional).   \n \n\n\n Returns\n  assert_op   An Operation that, when executed, raises a tf.errors.InvalidArgumentError if condition is not true.   \n \n\n\n Raises\n \nNote: The output of this function should be used. If it is not, a warning will be logged or an error may be raised. To mark the output as used, call its .mark_used() method.\n Tf1 Compatibility When in TF V1 mode (that is, outside tf.function) Assert needs a control dependency on the output to ensure the assertion executes: # Ensure maximum element of x is smaller or equal to 1\nassert_op = tf.Assert(tf.less_equal(tf.reduce_max(x), 1.), [x])\nwith tf.control_dependencies([assert_op]):\n  ... code using x ...\n Eager Compatibility returns None","title":"tensorflow.debugging.assert"},{"text":"tf.raw_ops.Assert Asserts that the given condition is true.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.raw_ops.Assert  \ntf.raw_ops.Assert(\n    condition, data, summarize=3, name=None\n)\n If condition evaluates to false, print the list of tensors in data. summarize determines how many entries of the tensors to print.\n \n\n\n Args\n  condition   A Tensor of type bool. The condition to evaluate.  \n  data   A list of Tensor objects. The tensors to print out when condition is false.  \n  summarize   An optional int. Defaults to 3. Print this many entries of each tensor.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   The created Operation.","title":"tensorflow.raw_ops.assert"},{"text":"InteractiveInterpreter.write(data)  \nWrite a string to the standard error stream (sys.stderr). Derived classes should override this to provide the appropriate output handling as needed.","title":"python.library.code#code.InteractiveInterpreter.write"},{"text":"lzma.compress(data, format=FORMAT_XZ, check=-1, preset=None, filters=None)  \nCompress data (a bytes object), returning the compressed data as a bytes object. See LZMACompressor above for a description of the format, check, preset and filters arguments.","title":"python.library.lzma#lzma.compress"}]}
{"task_id":518021,"prompt":"def f_518021(my_list):\n\treturn ","suffix":"","canonical_solution":"len(my_list)","test_start":"\ndef check(candidate):","test":["\n    assert candidate([]) == 0\n","\n    assert candidate([1]) == 1\n","\n    assert candidate([1, 2]) == 2\n"],"entry_point":"f_518021","intent":"Get the length of list `my_list`","library":[],"docs":[]}
{"task_id":518021,"prompt":"def f_518021(l):\n\treturn ","suffix":"","canonical_solution":"len(l)","test_start":"\nimport numpy as np \n\ndef check(candidate):","test":["\n    assert candidate([]) == 0\n","\n    assert candidate(np.array([1])) == 1\n","\n    assert candidate(np.array([1, 2])) == 2\n"],"entry_point":"f_518021","intent":"Getting the length of array `l`","library":["numpy"],"docs":[]}
{"task_id":518021,"prompt":"def f_518021(s):\n\treturn ","suffix":"","canonical_solution":"len(s)","test_start":"\nimport numpy as np \n\ndef check(candidate):","test":["\n    assert candidate([]) == 0\n","\n    assert candidate(np.array([1])) == 1\n","\n    assert candidate(np.array([1, 2])) == 2\n"],"entry_point":"f_518021","intent":"Getting the length of array `s`","library":["numpy"],"docs":[]}
{"task_id":518021,"prompt":"def f_518021(my_tuple):\n\treturn ","suffix":"","canonical_solution":"len(my_tuple)","test_start":"\ndef check(candidate):","test":["\n    assert candidate(()) == 0\n","\n    assert candidate(('aa', 'wfseg', '')) == 3\n","\n    assert candidate(('apple',)) == 1\n"],"entry_point":"f_518021","intent":"Getting the length of `my_tuple`","library":[],"docs":[]}
{"task_id":518021,"prompt":"def f_518021(my_string):\n\treturn ","suffix":"","canonical_solution":"len(my_string)","test_start":"\ndef check(candidate):","test":["\n    assert candidate(\"sedfgbdjofgljnh\") == 15\n","\n    assert candidate(\"             \") == 13\n","\n    assert candidate(\"vsdh4'cdf'\") == 10\n"],"entry_point":"f_518021","intent":"Getting the length of `my_string`","library":[],"docs":[]}
{"task_id":40452956,"prompt":"def f_40452956():\n\treturn ","suffix":"","canonical_solution":"b'\\\\a'.decode('unicode-escape')","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == '\\x07'\n"],"entry_point":"f_40452956","intent":"remove escape character from string \"\\\\a\"","library":[],"docs":[]}
{"task_id":8687018,"prompt":"def f_8687018():\n\treturn ","suffix":"","canonical_solution":"\"\"\"obama\"\"\".replace('a', '%temp%').replace('b', 'a').replace('%temp%', 'b')","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == 'oabmb'\n"],"entry_point":"f_8687018","intent":"replace each 'a' with 'b' and each 'b' with 'a' in the string 'obama' in a single pass.","library":[],"docs":[]}
{"task_id":303200,"prompt":"def f_303200():\n\t","suffix":"\n\treturn ","canonical_solution":"shutil.rmtree('\/folder_name')","test_start":"\nimport os\nimport shutil\nfrom unittest.mock import Mock\n\ndef check(candidate):","test":["\n    shutil.rmtree = Mock()\n    os.walk = Mock(return_value = [])\n    candidate()\n    assert os.walk('\/') == []\n"],"entry_point":"f_303200","intent":"remove directory tree '\/folder_name'","library":["os","shutil"],"docs":[{"text":"remove_folder(folder)  \nDelete the folder whose name is folder. If the folder contains any messages, a NotEmptyError exception will be raised and the folder will not be deleted.","title":"python.library.mailbox#mailbox.MH.remove_folder"},{"text":"remove_folder(folder)  \nDelete the folder whose name is folder. If the folder contains any messages, a NotEmptyError exception will be raised and the folder will not be deleted.","title":"python.library.mailbox#mailbox.Maildir.remove_folder"},{"text":"test.support.rmtree(path)  \nCall shutil.rmtree() on path or call os.lstat() and os.rmdir() to remove a path and its contents. On Windows platforms, this is wrapped with a wait loop that checks for the existence of the files.","title":"python.library.test#test.support.rmtree"},{"text":"tf.raw_ops.IFFT3D Inverse 3D fast Fourier transform.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.raw_ops.IFFT3D  \ntf.raw_ops.IFFT3D(\n    input, name=None\n)\n Computes the inverse 3-dimensional discrete Fourier transform over the inner-most 3 dimensions of input.\n \n\n\n Args\n  input   A Tensor. Must be one of the following types: complex64, complex128. A complex tensor.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor. Has the same type as input.","title":"tensorflow.raw_ops.ifft3d"},{"text":"tf.raw_ops.IFFT2D Inverse 2D fast Fourier transform.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.raw_ops.IFFT2D  \ntf.raw_ops.IFFT2D(\n    input, name=None\n)\n Computes the inverse 2-dimensional discrete Fourier transform over the inner-most 2 dimensions of input.\n \n\n\n Args\n  input   A Tensor. Must be one of the following types: complex64, complex128. A complex tensor.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor. Has the same type as input.","title":"tensorflow.raw_ops.ifft2d"},{"text":"InlineModelAdmin.fk_name  \nThe name of the foreign key on the model. In most cases this will be dealt with automatically, but fk_name must be specified explicitly if there are more than one foreign key to the same parent model.","title":"django.ref.contrib.admin.index#django.contrib.admin.InlineModelAdmin.fk_name"},{"text":"itertools.dropwhile(predicate, iterable)  \nMake an iterator that drops elements from the iterable as long as the predicate is true; afterwards, returns every element. Note, the iterator does not produce any output until the predicate first becomes false, so it may have a lengthy start-up time. Roughly equivalent to: def dropwhile(predicate, iterable):\n    # dropwhile(lambda x: x<5, [1,4,6,4,1]) --> 6 4 1\n    iterable = iter(iterable)\n    for x in iterable:\n        if not predicate(x):\n            yield x\n            break\n    for x in iterable:\n        yield x","title":"python.library.itertools#itertools.dropwhile"},{"text":"tf.signal.ifft3d Inverse 3D fast Fourier transform.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.ifft3d, tf.compat.v1.signal.ifft3d, tf.compat.v1.spectral.ifft3d  \ntf.signal.ifft3d(\n    input, name=None\n)\n Computes the inverse 3-dimensional discrete Fourier transform over the inner-most 3 dimensions of input.\n \n\n\n Args\n  input   A Tensor. Must be one of the following types: complex64, complex128. A complex tensor.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor. Has the same type as input.","title":"tensorflow.signal.ifft3d"},{"text":"itertools.filterfalse(predicate, iterable)  \nMake an iterator that filters elements from iterable returning only those for which the predicate is False. If predicate is None, return the items that are false. Roughly equivalent to: def filterfalse(predicate, iterable):\n    # filterfalse(lambda x: x%2, range(10)) --> 0 2 4 6 8\n    if predicate is None:\n        predicate = bool\n    for x in iterable:\n        if not predicate(x):\n            yield x","title":"python.library.itertools#itertools.filterfalse"},{"text":"tf.signal.irfft3d     View source on GitHub    Inverse 3D real-valued fast Fourier transform.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.signal.irfft3d, tf.compat.v1.spectral.irfft3d  \ntf.signal.irfft3d(\n    input_tensor, fft_length=None, name=None\n)\n Computes the inverse 3-dimensional discrete Fourier transform of a real-valued signal over the inner-most 3 dimensions of input. The inner-most 3 dimensions of input are assumed to be the result of RFFT3D: The inner-most dimension contains the fft_length \/ 2 + 1 unique components of the DFT of a real-valued signal. If fft_length is not provided, it is computed from the size of the inner-most 3 dimensions of input. If the FFT length used to compute input is odd, it should be provided since it cannot be inferred properly. Along each axis IRFFT3D is computed on, if fft_length (or fft_length \/ 2 + 1 for the inner-most dimension) is smaller than the corresponding dimension of input, the dimension is cropped. If it is larger, the dimension is padded with zeros.\n \n\n\n Args\n  input   A Tensor. Must be one of the following types: complex64, complex128. A complex tensor.  \n  fft_length   A Tensor of type int32. An int32 tensor of shape [3]. The FFT length for each dimension.  \n  Treal   An optional tf.DType from: tf.float32, tf.float64. Defaults to tf.float32.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor of type Treal.","title":"tensorflow.signal.irfft3d"}]}
{"task_id":13740672,"prompt":"def f_13740672(data):\n\t","suffix":"\n\treturn data","canonical_solution":"\n    def weekday(i):\n        if i >=1 and i <= 5: return True\n        else: return False\n    data['weekday'] = data['my_dt'].apply(lambda x: weekday(x))\n","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    data = pd.DataFrame([1, 2, 3, 4, 5, 6, 7], columns = ['my_dt'])\n    data = candidate(data)\n    assert data['weekday'][5] == False\n    assert data['weekday'][6] == False\n    for i in range (0, 5):\n        assert data['weekday'][i]\n"],"entry_point":"f_13740672","intent":"create a new column `weekday` in pandas data frame `data` based on the values in column `my_dt`","library":["pandas"],"docs":[{"text":"pandas.tseries.offsets.FY5253.weekday   FY5253.weekday","title":"pandas.reference.api.pandas.tseries.offsets.fy5253.weekday"},{"text":"pandas.tseries.offsets.FY5253Quarter.weekday   FY5253Quarter.weekday","title":"pandas.reference.api.pandas.tseries.offsets.fy5253quarter.weekday"},{"text":"pandas.tseries.offsets.WeekOfMonth.weekday   WeekOfMonth.weekday","title":"pandas.reference.api.pandas.tseries.offsets.weekofmonth.weekday"},{"text":"pandas.tseries.offsets.Week.weekday   Week.weekday","title":"pandas.reference.api.pandas.tseries.offsets.week.weekday"},{"text":"pandas.PeriodIndex.weekday   propertyPeriodIndex.weekday\n \nThe day of the week with Monday=0, Sunday=6.","title":"pandas.reference.api.pandas.periodindex.weekday"},{"text":"pandas.Timestamp.weekday   Timestamp.weekday()\n \nReturn the day of the week represented by the date. Monday == 0 \u2026 Sunday == 6.","title":"pandas.reference.api.pandas.timestamp.weekday"},{"text":"pandas.tseries.offsets.WeekOfMonth.apply   WeekOfMonth.apply()","title":"pandas.reference.api.pandas.tseries.offsets.weekofmonth.apply"},{"text":"pandas.Series.dt.weekday   Series.dt.weekday\n \nThe day of the week with Monday=0, Sunday=6. Return the day of the week. It is assumed the week starts on Monday, which is denoted by 0 and ends on Sunday which is denoted by 6. This method is available on both Series with datetime values (using the dt accessor) or DatetimeIndex.  Returns \n Series or Index\n\nContaining integers indicating the day number.      See also  Series.dt.dayofweek\n\nAlias.  Series.dt.weekday\n\nAlias.  Series.dt.day_name\n\nReturns the name of the day of the week.    Examples \n>>> s = pd.date_range('2016-12-31', '2017-01-08', freq='D').to_series()\n>>> s.dt.dayofweek\n2016-12-31    5\n2017-01-01    6\n2017-01-02    0\n2017-01-03    1\n2017-01-04    2\n2017-01-05    3\n2017-01-06    4\n2017-01-07    5\n2017-01-08    6\nFreq: D, dtype: int64","title":"pandas.reference.api.pandas.series.dt.weekday"},{"text":"pandas.tseries.offsets.WeekOfMonth.__call__   WeekOfMonth.__call__(*args, **kwargs)\n \nCall self as a function.","title":"pandas.reference.api.pandas.tseries.offsets.weekofmonth.__call__"},{"text":"class ExtractWeekDay(expression, tzinfo=None, **extra)  \n \nlookup_name = 'week_day'","title":"django.ref.models.database-functions#django.db.models.functions.ExtractWeekDay"}]}
{"task_id":20950650,"prompt":"def f_20950650(x):\n\treturn ","suffix":"","canonical_solution":"sorted(x, key=x.get, reverse=True)","test_start":"\nfrom collections import Counter\n\ndef check(candidate):","test":["\n    x = Counter({'blue': 1, 'red': 2, 'green': 3})\n    assert candidate(x) == ['green', 'red', 'blue']\n","\n    x = Counter({'blue': 1.234, 'red': 1.35, 'green': 1.789})\n    assert candidate(x) == ['green', 'red', 'blue']\n","\n    x = Counter({'blue': \"b\", 'red': \"r\", 'green': \"g\"})\n    assert candidate(x) == ['red', 'green', 'blue']\n"],"entry_point":"f_20950650","intent":"reverse sort Counter `x` by values","library":["collections"],"docs":[]}
{"task_id":20950650,"prompt":"def f_20950650(x):\n\treturn ","suffix":"","canonical_solution":"sorted(list(x.items()), key=lambda pair: pair[1], reverse=True)","test_start":"\nfrom collections import Counter\n\ndef check(candidate):","test":["\n    x = Counter({'blue': 1, 'red': 2, 'green': 3})\n    assert candidate(x) == [('green', 3), ('red', 2), ('blue', 1)]\n","\n    x = Counter({'blue': 1.234, 'red': 1.35, 'green': 1.789})\n    assert candidate(x) == [('green', 1.789), ('red', 1.35), ('blue', 1.234)]\n","\n    x = Counter({'blue': \"b\", 'red': \"r\", 'green': \"g\"})\n    assert candidate(x) == [('red', \"r\"), ('green', \"g\"), ('blue', \"b\")]\n"],"entry_point":"f_20950650","intent":"reverse sort counter `x` by value","library":["collections"],"docs":[]}
{"task_id":9775297,"prompt":"def f_9775297(a, b):\n\treturn ","suffix":"","canonical_solution":"np.vstack((a, b))","test_start":"\nimport numpy as np\n\ndef check(candidate):","test":["\n    a = np.array([[1, 2, 3], [4, 5, 6]])\n    b = np.array([[9, 8, 7], [6, 5, 4]])\n    assert np.array_equal(candidate(a, b), np.array([[1, 2, 3], [4, 5, 6], [9, 8, 7], [6, 5, 4]]))\n","\n    a = np.array([[1, 2.45, 3], [4, 0.55, 612]])\n    b = np.array([[988, 8, 7], [6, 512, 4]])\n    assert np.array_equal(candidate(a, b), np.array([[1, 2.45, 3], [4, 0.55, 612], [988, 8, 7], [6, 512, 4]]))\n"],"entry_point":"f_9775297","intent":"append a numpy array 'b' to a numpy array 'a'","library":["numpy"],"docs":[{"text":"numpy.ma.append   ma.append(a, b, axis=None)[source]\n \nAppend values to the end of an array.  New in version 1.9.0.   Parameters \n \naarray_like\n\n\nValues are appended to a copy of this array.  \nbarray_like\n\n\nThese values are appended to a copy of a. It must be of the correct shape (the same shape as a, excluding axis). If axis is not specified, b can be any shape and will be flattened before use.  \naxisint, optional\n\n\nThe axis along which v are appended. If axis is not given, both a and b are flattened before use.    Returns \n \nappendMaskedArray\n\n\nA copy of a with b appended to axis. Note that append does not occur in-place: a new array is allocated and filled. If axis is None, the result is a flattened array.      See also  numpy.append\n\nEquivalent function in the top-level NumPy module.    Examples >>> import numpy.ma as ma\n>>> a = ma.masked_values([1, 2, 3], 2)\n>>> b = ma.masked_values([[4, 5, 6], [7, 8, 9]], 7)\n>>> ma.append(a, b)\nmasked_array(data=[1, --, 3, 4, 5, 6, --, 8, 9],\n             mask=[False,  True, False, False, False, False,  True, False,\n                   False],\n       fill_value=999999)","title":"numpy.reference.generated.numpy.ma.append"},{"text":"tf.experimental.numpy.append TensorFlow variant of NumPy's append. \ntf.experimental.numpy.append(\n    arr, values, axis=None\n)\n See the NumPy documentation for numpy.append.","title":"tensorflow.experimental.numpy.append"},{"text":"numpy.append   numpy.append(arr, values, axis=None)[source]\n \nAppend values to the end of an array.  Parameters \n \narrarray_like\n\n\nValues are appended to a copy of this array.  \nvaluesarray_like\n\n\nThese values are appended to a copy of arr. It must be of the correct shape (the same shape as arr, excluding axis). If axis is not specified, values can be any shape and will be flattened before use.  \naxisint, optional\n\n\nThe axis along which values are appended. If axis is not given, both arr and values are flattened before use.    Returns \n \nappendndarray\n\n\nA copy of arr with values appended to axis. Note that append does not occur in-place: a new array is allocated and filled. If axis is None, out is a flattened array.      See also  insert\n\nInsert elements into an array.  delete\n\nDelete elements from an array.    Examples >>> np.append([1, 2, 3], [[4, 5, 6], [7, 8, 9]])\narray([1, 2, 3, ..., 7, 8, 9])\n When axis is specified, values must have the correct shape. >>> np.append([[1, 2, 3], [4, 5, 6]], [[7, 8, 9]], axis=0)\narray([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n>>> np.append([[1, 2, 3], [4, 5, 6]], [7, 8, 9], axis=0)\nTraceback (most recent call last):\n    ...\nValueError: all the input arrays must have same number of dimensions, but\nthe array at index 0 has 2 dimension(s) and the array at index 1 has 1\ndimension(s)","title":"numpy.reference.generated.numpy.append"},{"text":"operator.iconcat(a, b)  \noperator.__iconcat__(a, b)  \na = iconcat(a, b) is equivalent to a += b for a and b sequences.","title":"python.library.operator#operator.iconcat"},{"text":"operator.iconcat(a, b)  \noperator.__iconcat__(a, b)  \na = iconcat(a, b) is equivalent to a += b for a and b sequences.","title":"python.library.operator#operator.__iconcat__"},{"text":"numpy.concatenate   numpy.concatenate((a1, a2, ...), axis=0, out=None, dtype=None, casting=\"same_kind\")\n \nJoin a sequence of arrays along an existing axis.  Parameters \n \na1, a2, \u2026sequence of array_like\n\n\nThe arrays must have the same shape, except in the dimension corresponding to axis (the first, by default).  \naxisint, optional\n\n\nThe axis along which the arrays will be joined. If axis is None, arrays are flattened before use. Default is 0.  \noutndarray, optional\n\n\nIf provided, the destination to place the result. The shape must be correct, matching that of what concatenate would have returned if no out argument were specified.  \ndtypestr or dtype\n\n\nIf provided, the destination array will have this dtype. Cannot be provided together with out.  New in version 1.20.0.   \ncasting{\u2018no\u2019, \u2018equiv\u2019, \u2018safe\u2019, \u2018same_kind\u2019, \u2018unsafe\u2019}, optional\n\n\nControls what kind of data casting may occur. Defaults to \u2018same_kind\u2019.  New in version 1.20.0.     Returns \n \nresndarray\n\n\nThe concatenated array.      See also  ma.concatenate\n\nConcatenate function that preserves input masks.  array_split\n\nSplit an array into multiple sub-arrays of equal or near-equal size.  split\n\nSplit array into a list of multiple sub-arrays of equal size.  hsplit\n\nSplit array into multiple sub-arrays horizontally (column wise).  vsplit\n\nSplit array into multiple sub-arrays vertically (row wise).  dsplit\n\nSplit array into multiple sub-arrays along the 3rd axis (depth).  stack\n\nStack a sequence of arrays along a new axis.  block\n\nAssemble arrays from blocks.  hstack\n\nStack arrays in sequence horizontally (column wise).  vstack\n\nStack arrays in sequence vertically (row wise).  dstack\n\nStack arrays in sequence depth wise (along third dimension).  column_stack\n\nStack 1-D arrays as columns into a 2-D array.    Notes When one or more of the arrays to be concatenated is a MaskedArray, this function will return a MaskedArray object instead of an ndarray, but the input masks are not preserved. In cases where a MaskedArray is expected as input, use the ma.concatenate function from the masked array module instead. Examples >>> a = np.array([[1, 2], [3, 4]])\n>>> b = np.array([[5, 6]])\n>>> np.concatenate((a, b), axis=0)\narray([[1, 2],\n       [3, 4],\n       [5, 6]])\n>>> np.concatenate((a, b.T), axis=1)\narray([[1, 2, 5],\n       [3, 4, 6]])\n>>> np.concatenate((a, b), axis=None)\narray([1, 2, 3, 4, 5, 6])\n This function will not preserve masking of MaskedArray inputs. >>> a = np.ma.arange(3)\n>>> a[1] = np.ma.masked\n>>> b = np.arange(2, 5)\n>>> a\nmasked_array(data=[0, --, 2],\n             mask=[False,  True, False],\n       fill_value=999999)\n>>> b\narray([2, 3, 4])\n>>> np.concatenate([a, b])\nmasked_array(data=[0, 1, 2, 2, 3, 4],\n             mask=False,\n       fill_value=999999)\n>>> np.ma.concatenate([a, b])\nmasked_array(data=[0, --, 2, 2, 3, 4],\n             mask=[False,  True, False, False, False, False],\n       fill_value=999999)","title":"numpy.reference.generated.numpy.concatenate"},{"text":"tf.experimental.numpy.concatenate TensorFlow variant of NumPy's concatenate. \ntf.experimental.numpy.concatenate(\n    arys, axis=0\n)\n See the NumPy documentation for numpy.concatenate.","title":"tensorflow.experimental.numpy.concatenate"},{"text":"numpy.char.add   char.add(x1, x2)[source]\n \nReturn element-wise string concatenation for two arrays of str or unicode. Arrays x1 and x2 must have the same shape.  Parameters \n \nx1array_like of str or unicode\n\n\nInput array.  \nx2array_like of str or unicode\n\n\nInput array.    Returns \n \naddndarray\n\n\nOutput array of string_ or unicode_, depending on input types of the same shape as x1 and x2.","title":"numpy.reference.generated.numpy.char.add"},{"text":"numpy.ndarray.__iadd__ method   ndarray.__iadd__(value, \/)\n \nReturn self+=value.","title":"numpy.reference.generated.numpy.ndarray.__iadd__"},{"text":"numpy.lib.recfunctions.rec_append_fields(base, names, data, dtypes=None)[source]\n \nAdd new fields to an existing array. The names of the fields are given with the names arguments, the corresponding values with the data arguments. If a single field is appended, names, data and dtypes do not have to be lists but just values.  Parameters \n \nbasearray\n\n\nInput array to extend.  \nnamesstring, sequence\n\n\nString or sequence of strings corresponding to the names of the new fields.  \ndataarray or sequence of arrays\n\n\nArray or sequence of arrays storing the fields to add to the base.  \ndtypessequence of datatypes, optional\n\n\nDatatype or sequence of datatypes. If None, the datatypes are estimated from the data.    Returns \n \nappended_arraynp.recarray\n\n    See also  append_fields","title":"numpy.user.basics.rec#numpy.lib.recfunctions.rec_append_fields"}]}
{"task_id":21887754,"prompt":"def f_21887754(a, b):\n\treturn ","suffix":"","canonical_solution":"np.concatenate((a, b), axis=0)","test_start":"\nimport numpy as np\n\ndef check(candidate):","test":["\n    a = np.array([[1, 5, 9], [2, 6, 10]])\n    b = np.array([[3, 7, 11], [4, 8, 12]])\n    assert np.array_equal(candidate(a, b), np.array([[1, 5, 9], [2, 6, 10], [3, 7, 11], [4, 8, 12]]))\n","\n    a = np.array([[1, 2.45, 3], [4, 0.55, 612]])\n    b = np.array([[988, 8, 7], [6, 512, 4]])\n    assert np.array_equal(candidate(a, b), np.array([[1, 2.45, 3], [4, 0.55, 612], [988, 8, 7], [6, 512, 4]]))\n"],"entry_point":"f_21887754","intent":"numpy concatenate two arrays `a` and `b` along the first axis","library":["numpy"],"docs":[{"text":"numpy.ma.append   ma.append(a, b, axis=None)[source]\n \nAppend values to the end of an array.  New in version 1.9.0.   Parameters \n \naarray_like\n\n\nValues are appended to a copy of this array.  \nbarray_like\n\n\nThese values are appended to a copy of a. It must be of the correct shape (the same shape as a, excluding axis). If axis is not specified, b can be any shape and will be flattened before use.  \naxisint, optional\n\n\nThe axis along which v are appended. If axis is not given, both a and b are flattened before use.    Returns \n \nappendMaskedArray\n\n\nA copy of a with b appended to axis. Note that append does not occur in-place: a new array is allocated and filled. If axis is None, the result is a flattened array.      See also  numpy.append\n\nEquivalent function in the top-level NumPy module.    Examples >>> import numpy.ma as ma\n>>> a = ma.masked_values([1, 2, 3], 2)\n>>> b = ma.masked_values([[4, 5, 6], [7, 8, 9]], 7)\n>>> ma.append(a, b)\nmasked_array(data=[1, --, 3, 4, 5, 6, --, 8, 9],\n             mask=[False,  True, False, False, False, False,  True, False,\n                   False],\n       fill_value=999999)","title":"numpy.reference.generated.numpy.ma.append"},{"text":"tf.experimental.numpy.concatenate TensorFlow variant of NumPy's concatenate. \ntf.experimental.numpy.concatenate(\n    arys, axis=0\n)\n See the NumPy documentation for numpy.concatenate.","title":"tensorflow.experimental.numpy.concatenate"},{"text":"numpy.c_   numpy.c_ = <numpy.lib.index_tricks.CClass object>\n \nTranslates slice objects to concatenation along the second axis. This is short-hand for np.r_['-1,2,0', index expression], which is useful because of its common occurrence. In particular, arrays will be stacked along their last axis after being upgraded to at least 2-D with 1\u2019s post-pended to the shape (column vectors made out of 1-D arrays).  See also  column_stack\n\nStack 1-D arrays as columns into a 2-D array.  r_\n\nFor more detailed documentation.    Examples >>> np.c_[np.array([1,2,3]), np.array([4,5,6])]\narray([[1, 4],\n       [2, 5],\n       [3, 6]])\n>>> np.c_[np.array([[1,2,3]]), 0, 0, np.array([[4,5,6]])]\narray([[1, 2, 3, ..., 4, 5, 6]])","title":"numpy.reference.generated.numpy.c_"},{"text":"numpy.concatenate   numpy.concatenate((a1, a2, ...), axis=0, out=None, dtype=None, casting=\"same_kind\")\n \nJoin a sequence of arrays along an existing axis.  Parameters \n \na1, a2, \u2026sequence of array_like\n\n\nThe arrays must have the same shape, except in the dimension corresponding to axis (the first, by default).  \naxisint, optional\n\n\nThe axis along which the arrays will be joined. If axis is None, arrays are flattened before use. Default is 0.  \noutndarray, optional\n\n\nIf provided, the destination to place the result. The shape must be correct, matching that of what concatenate would have returned if no out argument were specified.  \ndtypestr or dtype\n\n\nIf provided, the destination array will have this dtype. Cannot be provided together with out.  New in version 1.20.0.   \ncasting{\u2018no\u2019, \u2018equiv\u2019, \u2018safe\u2019, \u2018same_kind\u2019, \u2018unsafe\u2019}, optional\n\n\nControls what kind of data casting may occur. Defaults to \u2018same_kind\u2019.  New in version 1.20.0.     Returns \n \nresndarray\n\n\nThe concatenated array.      See also  ma.concatenate\n\nConcatenate function that preserves input masks.  array_split\n\nSplit an array into multiple sub-arrays of equal or near-equal size.  split\n\nSplit array into a list of multiple sub-arrays of equal size.  hsplit\n\nSplit array into multiple sub-arrays horizontally (column wise).  vsplit\n\nSplit array into multiple sub-arrays vertically (row wise).  dsplit\n\nSplit array into multiple sub-arrays along the 3rd axis (depth).  stack\n\nStack a sequence of arrays along a new axis.  block\n\nAssemble arrays from blocks.  hstack\n\nStack arrays in sequence horizontally (column wise).  vstack\n\nStack arrays in sequence vertically (row wise).  dstack\n\nStack arrays in sequence depth wise (along third dimension).  column_stack\n\nStack 1-D arrays as columns into a 2-D array.    Notes When one or more of the arrays to be concatenated is a MaskedArray, this function will return a MaskedArray object instead of an ndarray, but the input masks are not preserved. In cases where a MaskedArray is expected as input, use the ma.concatenate function from the masked array module instead. Examples >>> a = np.array([[1, 2], [3, 4]])\n>>> b = np.array([[5, 6]])\n>>> np.concatenate((a, b), axis=0)\narray([[1, 2],\n       [3, 4],\n       [5, 6]])\n>>> np.concatenate((a, b.T), axis=1)\narray([[1, 2, 5],\n       [3, 4, 6]])\n>>> np.concatenate((a, b), axis=None)\narray([1, 2, 3, 4, 5, 6])\n This function will not preserve masking of MaskedArray inputs. >>> a = np.ma.arange(3)\n>>> a[1] = np.ma.masked\n>>> b = np.arange(2, 5)\n>>> a\nmasked_array(data=[0, --, 2],\n             mask=[False,  True, False],\n       fill_value=999999)\n>>> b\narray([2, 3, 4])\n>>> np.concatenate([a, b])\nmasked_array(data=[0, 1, 2, 2, 3, 4],\n             mask=False,\n       fill_value=999999)\n>>> np.ma.concatenate([a, b])\nmasked_array(data=[0, --, 2, 2, 3, 4],\n             mask=[False,  True, False, False, False, False],\n       fill_value=999999)","title":"numpy.reference.generated.numpy.concatenate"},{"text":"numpy.ma.concatenate   ma.concatenate(arrays, axis=0)[source]\n \nConcatenate a sequence of arrays along the given axis.  Parameters \n \narrayssequence of array_like\n\n\nThe arrays must have the same shape, except in the dimension corresponding to axis (the first, by default).  \naxisint, optional\n\n\nThe axis along which the arrays will be joined. Default is 0.    Returns \n \nresultMaskedArray\n\n\nThe concatenated array with any masked entries preserved.      See also  numpy.concatenate\n\nEquivalent function in the top-level NumPy module.    Examples >>> import numpy.ma as ma\n>>> a = ma.arange(3)\n>>> a[1] = ma.masked\n>>> b = ma.arange(2, 5)\n>>> a\nmasked_array(data=[0, --, 2],\n             mask=[False,  True, False],\n       fill_value=999999)\n>>> b\nmasked_array(data=[2, 3, 4],\n             mask=False,\n       fill_value=999999)\n>>> ma.concatenate([a, b])\nmasked_array(data=[0, --, 2, 2, 3, 4],\n             mask=[False,  True, False, False, False, False],\n       fill_value=999999)","title":"numpy.reference.generated.numpy.ma.concatenate"},{"text":"numpy.append   numpy.append(arr, values, axis=None)[source]\n \nAppend values to the end of an array.  Parameters \n \narrarray_like\n\n\nValues are appended to a copy of this array.  \nvaluesarray_like\n\n\nThese values are appended to a copy of arr. It must be of the correct shape (the same shape as arr, excluding axis). If axis is not specified, values can be any shape and will be flattened before use.  \naxisint, optional\n\n\nThe axis along which values are appended. If axis is not given, both arr and values are flattened before use.    Returns \n \nappendndarray\n\n\nA copy of arr with values appended to axis. Note that append does not occur in-place: a new array is allocated and filled. If axis is None, out is a flattened array.      See also  insert\n\nInsert elements into an array.  delete\n\nDelete elements from an array.    Examples >>> np.append([1, 2, 3], [[4, 5, 6], [7, 8, 9]])\narray([1, 2, 3, ..., 7, 8, 9])\n When axis is specified, values must have the correct shape. >>> np.append([[1, 2, 3], [4, 5, 6]], [[7, 8, 9]], axis=0)\narray([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n>>> np.append([[1, 2, 3], [4, 5, 6]], [7, 8, 9], axis=0)\nTraceback (most recent call last):\n    ...\nValueError: all the input arrays must have same number of dimensions, but\nthe array at index 0 has 2 dimension(s) and the array at index 1 has 1\ndimension(s)","title":"numpy.reference.generated.numpy.append"},{"text":"numpy.ma.inner   ma.inner(a, b, \/)[source]\n \nInner product of two arrays. Ordinary inner product of vectors for 1-D arrays (without complex conjugation), in higher dimensions a sum product over the last axes.  Parameters \n \na, barray_like\n\n\nIf a and b are nonscalar, their last dimensions must match.    Returns \n \noutndarray\n\n\nIf a and b are both scalars or both 1-D arrays then a scalar is returned; otherwise an array is returned. out.shape = (*a.shape[:-1], *b.shape[:-1])    Raises \n ValueError\n\nIf both a and b are nonscalar and their last dimensions have different sizes.      See also  tensordot\n\nSum products over arbitrary axes.  dot\n\nGeneralised matrix product, using second last dimension of b.  einsum\n\nEinstein summation convention.    Notes Masked values are replaced by 0. For vectors (1-D arrays) it computes the ordinary inner-product: np.inner(a, b) = sum(a[:]*b[:])\n More generally, if ndim(a) = r > 0 and ndim(b) = s > 0: np.inner(a, b) = np.tensordot(a, b, axes=(-1,-1))\n or explicitly: np.inner(a, b)[i0,...,ir-2,j0,...,js-2]\n     = sum(a[i0,...,ir-2,:]*b[j0,...,js-2,:])\n In addition a or b may be scalars, in which case: np.inner(a,b) = a*b\n Examples Ordinary inner product for vectors: >>> a = np.array([1,2,3])\n>>> b = np.array([0,1,0])\n>>> np.inner(a, b)\n2\n Some multidimensional examples: >>> a = np.arange(24).reshape((2,3,4))\n>>> b = np.arange(4)\n>>> c = np.inner(a, b)\n>>> c.shape\n(2, 3)\n>>> c\narray([[ 14,  38,  62],\n       [ 86, 110, 134]])\n >>> a = np.arange(2).reshape((1,1,2))\n>>> b = np.arange(6).reshape((3,2))\n>>> c = np.inner(a, b)\n>>> c.shape\n(1, 1, 3)\n>>> c\narray([[[1, 3, 5]]])\n An example where b is a scalar: >>> np.inner(np.eye(2), 7)\narray([[7., 0.],\n       [0., 7.]])","title":"numpy.reference.generated.numpy.ma.inner"},{"text":"numpy.char.add   char.add(x1, x2)[source]\n \nReturn element-wise string concatenation for two arrays of str or unicode. Arrays x1 and x2 must have the same shape.  Parameters \n \nx1array_like of str or unicode\n\n\nInput array.  \nx2array_like of str or unicode\n\n\nInput array.    Returns \n \naddndarray\n\n\nOutput array of string_ or unicode_, depending on input types of the same shape as x1 and x2.","title":"numpy.reference.generated.numpy.char.add"},{"text":"numpy.inner   numpy.inner(a, b, \/)\n \nInner product of two arrays. Ordinary inner product of vectors for 1-D arrays (without complex conjugation), in higher dimensions a sum product over the last axes.  Parameters \n \na, barray_like\n\n\nIf a and b are nonscalar, their last dimensions must match.    Returns \n \noutndarray\n\n\nIf a and b are both scalars or both 1-D arrays then a scalar is returned; otherwise an array is returned. out.shape = (*a.shape[:-1], *b.shape[:-1])    Raises \n ValueError\n\nIf both a and b are nonscalar and their last dimensions have different sizes.      See also  tensordot\n\nSum products over arbitrary axes.  dot\n\nGeneralised matrix product, using second last dimension of b.  einsum\n\nEinstein summation convention.    Notes For vectors (1-D arrays) it computes the ordinary inner-product: np.inner(a, b) = sum(a[:]*b[:])\n More generally, if ndim(a) = r > 0 and ndim(b) = s > 0: np.inner(a, b) = np.tensordot(a, b, axes=(-1,-1))\n or explicitly: np.inner(a, b)[i0,...,ir-2,j0,...,js-2]\n     = sum(a[i0,...,ir-2,:]*b[j0,...,js-2,:])\n In addition a or b may be scalars, in which case: np.inner(a,b) = a*b\n Examples Ordinary inner product for vectors: >>> a = np.array([1,2,3])\n>>> b = np.array([0,1,0])\n>>> np.inner(a, b)\n2\n Some multidimensional examples: >>> a = np.arange(24).reshape((2,3,4))\n>>> b = np.arange(4)\n>>> c = np.inner(a, b)\n>>> c.shape\n(2, 3)\n>>> c\narray([[ 14,  38,  62],\n       [ 86, 110, 134]])\n >>> a = np.arange(2).reshape((1,1,2))\n>>> b = np.arange(6).reshape((3,2))\n>>> c = np.inner(a, b)\n>>> c.shape\n(1, 1, 3)\n>>> c\narray([[[1, 3, 5]]])\n An example where b is a scalar: >>> np.inner(np.eye(2), 7)\narray([[7., 0.],\n       [0., 7.]])","title":"numpy.reference.generated.numpy.inner"},{"text":"numpy.r_   numpy.r_ = <numpy.lib.index_tricks.RClass object>\n \nTranslates slice objects to concatenation along the first axis. This is a simple way to build up arrays quickly. There are two use cases.  If the index expression contains comma separated arrays, then stack them along their first axis. If the index expression contains slice notation or scalars then create a 1-D array with a range indicated by the slice notation.  If slice notation is used, the syntax start:stop:step is equivalent to np.arange(start, stop, step) inside of the brackets. However, if step is an imaginary number (i.e. 100j) then its integer portion is interpreted as a number-of-points desired and the start and stop are inclusive. In other words start:stop:stepj is interpreted as np.linspace(start, stop, step, endpoint=1) inside of the brackets. After expansion of slice notation, all comma separated sequences are concatenated together. Optional character strings placed as the first element of the index expression can be used to change the output. The strings \u2018r\u2019 or \u2018c\u2019 result in matrix output. If the result is 1-D and \u2018r\u2019 is specified a 1 x N (row) matrix is produced. If the result is 1-D and \u2018c\u2019 is specified, then a N x 1 (column) matrix is produced. If the result is 2-D then both provide the same matrix result. A string integer specifies which axis to stack multiple comma separated arrays along. A string of two comma-separated integers allows indication of the minimum number of dimensions to force each entry into as the second integer (the axis to concatenate along is still the first integer). A string with three comma-separated integers allows specification of the axis to concatenate along, the minimum number of dimensions to force the entries to, and which axis should contain the start of the arrays which are less than the specified number of dimensions. In other words the third integer allows you to specify where the 1\u2019s should be placed in the shape of the arrays that have their shapes upgraded. By default, they are placed in the front of the shape tuple. The third argument allows you to specify where the start of the array should be instead. Thus, a third argument of \u20180\u2019 would place the 1\u2019s at the end of the array shape. Negative integers specify where in the new shape tuple the last dimension of upgraded arrays should be placed, so the default is \u2018-1\u2019.  Parameters \n Not a function, so takes no parameters\n  Returns \n A concatenated ndarray or matrix.\n    See also  concatenate\n\nJoin a sequence of arrays along an existing axis.  c_\n\nTranslates slice objects to concatenation along the second axis.    Examples >>> np.r_[np.array([1,2,3]), 0, 0, np.array([4,5,6])]\narray([1, 2, 3, ..., 4, 5, 6])\n>>> np.r_[-1:1:6j, [0]*3, 5, 6]\narray([-1. , -0.6, -0.2,  0.2,  0.6,  1. ,  0. ,  0. ,  0. ,  5. ,  6. ])\n String integers specify the axis to concatenate along or the minimum number of dimensions to force entries into. >>> a = np.array([[0, 1, 2], [3, 4, 5]])\n>>> np.r_['-1', a, a] # concatenate along last axis\narray([[0, 1, 2, 0, 1, 2],\n       [3, 4, 5, 3, 4, 5]])\n>>> np.r_['0,2', [1,2,3], [4,5,6]] # concatenate along first axis, dim>=2\narray([[1, 2, 3],\n       [4, 5, 6]])\n >>> np.r_['0,2,0', [1,2,3], [4,5,6]]\narray([[1],\n       [2],\n       [3],\n       [4],\n       [5],\n       [6]])\n>>> np.r_['1,2,0', [1,2,3], [4,5,6]]\narray([[1, 4],\n       [2, 5],\n       [3, 6]])\n Using \u2018r\u2019 or \u2018c\u2019 as a first string argument creates a matrix. >>> np.r_['r',[1,2,3], [4,5,6]]\nmatrix([[1, 2, 3, 4, 5, 6]])","title":"numpy.reference.generated.numpy.r_"}]}
{"task_id":21887754,"prompt":"def f_21887754(a, b):\n\treturn ","suffix":"","canonical_solution":"np.concatenate((a, b), axis=1)","test_start":"\nimport numpy as np\n\ndef check(candidate):","test":["\n    a = np.array([[1, 5, 9], [2, 6, 10]])\n    b = np.array([[3, 7, 11], [4, 8, 12]])\n    assert np.array_equal(candidate(a, b), np.array([[1, 5, 9, 3, 7, 11], [2, 6, 10, 4, 8, 12]]))\n","\n    a = np.array([[1, 2.45, 3], [4, 0.55, 612]])\n    b = np.array([[988, 8, 7], [6, 512, 4]])\n    assert np.array_equal(candidate(a, b), np.array([[1, 2.45, 3, 988, 8, 7], [4, 0.55, 612, 6, 512, 4]]))\n"],"entry_point":"f_21887754","intent":"numpy concatenate two arrays `a` and `b` along the second axis","library":["numpy"],"docs":[{"text":"numpy.ma.append   ma.append(a, b, axis=None)[source]\n \nAppend values to the end of an array.  New in version 1.9.0.   Parameters \n \naarray_like\n\n\nValues are appended to a copy of this array.  \nbarray_like\n\n\nThese values are appended to a copy of a. It must be of the correct shape (the same shape as a, excluding axis). If axis is not specified, b can be any shape and will be flattened before use.  \naxisint, optional\n\n\nThe axis along which v are appended. If axis is not given, both a and b are flattened before use.    Returns \n \nappendMaskedArray\n\n\nA copy of a with b appended to axis. Note that append does not occur in-place: a new array is allocated and filled. If axis is None, the result is a flattened array.      See also  numpy.append\n\nEquivalent function in the top-level NumPy module.    Examples >>> import numpy.ma as ma\n>>> a = ma.masked_values([1, 2, 3], 2)\n>>> b = ma.masked_values([[4, 5, 6], [7, 8, 9]], 7)\n>>> ma.append(a, b)\nmasked_array(data=[1, --, 3, 4, 5, 6, --, 8, 9],\n             mask=[False,  True, False, False, False, False,  True, False,\n                   False],\n       fill_value=999999)","title":"numpy.reference.generated.numpy.ma.append"},{"text":"numpy.c_   numpy.c_ = <numpy.lib.index_tricks.CClass object>\n \nTranslates slice objects to concatenation along the second axis. This is short-hand for np.r_['-1,2,0', index expression], which is useful because of its common occurrence. In particular, arrays will be stacked along their last axis after being upgraded to at least 2-D with 1\u2019s post-pended to the shape (column vectors made out of 1-D arrays).  See also  column_stack\n\nStack 1-D arrays as columns into a 2-D array.  r_\n\nFor more detailed documentation.    Examples >>> np.c_[np.array([1,2,3]), np.array([4,5,6])]\narray([[1, 4],\n       [2, 5],\n       [3, 6]])\n>>> np.c_[np.array([[1,2,3]]), 0, 0, np.array([[4,5,6]])]\narray([[1, 2, 3, ..., 4, 5, 6]])","title":"numpy.reference.generated.numpy.c_"},{"text":"tf.experimental.numpy.concatenate TensorFlow variant of NumPy's concatenate. \ntf.experimental.numpy.concatenate(\n    arys, axis=0\n)\n See the NumPy documentation for numpy.concatenate.","title":"tensorflow.experimental.numpy.concatenate"},{"text":"numpy.concatenate   numpy.concatenate((a1, a2, ...), axis=0, out=None, dtype=None, casting=\"same_kind\")\n \nJoin a sequence of arrays along an existing axis.  Parameters \n \na1, a2, \u2026sequence of array_like\n\n\nThe arrays must have the same shape, except in the dimension corresponding to axis (the first, by default).  \naxisint, optional\n\n\nThe axis along which the arrays will be joined. If axis is None, arrays are flattened before use. Default is 0.  \noutndarray, optional\n\n\nIf provided, the destination to place the result. The shape must be correct, matching that of what concatenate would have returned if no out argument were specified.  \ndtypestr or dtype\n\n\nIf provided, the destination array will have this dtype. Cannot be provided together with out.  New in version 1.20.0.   \ncasting{\u2018no\u2019, \u2018equiv\u2019, \u2018safe\u2019, \u2018same_kind\u2019, \u2018unsafe\u2019}, optional\n\n\nControls what kind of data casting may occur. Defaults to \u2018same_kind\u2019.  New in version 1.20.0.     Returns \n \nresndarray\n\n\nThe concatenated array.      See also  ma.concatenate\n\nConcatenate function that preserves input masks.  array_split\n\nSplit an array into multiple sub-arrays of equal or near-equal size.  split\n\nSplit array into a list of multiple sub-arrays of equal size.  hsplit\n\nSplit array into multiple sub-arrays horizontally (column wise).  vsplit\n\nSplit array into multiple sub-arrays vertically (row wise).  dsplit\n\nSplit array into multiple sub-arrays along the 3rd axis (depth).  stack\n\nStack a sequence of arrays along a new axis.  block\n\nAssemble arrays from blocks.  hstack\n\nStack arrays in sequence horizontally (column wise).  vstack\n\nStack arrays in sequence vertically (row wise).  dstack\n\nStack arrays in sequence depth wise (along third dimension).  column_stack\n\nStack 1-D arrays as columns into a 2-D array.    Notes When one or more of the arrays to be concatenated is a MaskedArray, this function will return a MaskedArray object instead of an ndarray, but the input masks are not preserved. In cases where a MaskedArray is expected as input, use the ma.concatenate function from the masked array module instead. Examples >>> a = np.array([[1, 2], [3, 4]])\n>>> b = np.array([[5, 6]])\n>>> np.concatenate((a, b), axis=0)\narray([[1, 2],\n       [3, 4],\n       [5, 6]])\n>>> np.concatenate((a, b.T), axis=1)\narray([[1, 2, 5],\n       [3, 4, 6]])\n>>> np.concatenate((a, b), axis=None)\narray([1, 2, 3, 4, 5, 6])\n This function will not preserve masking of MaskedArray inputs. >>> a = np.ma.arange(3)\n>>> a[1] = np.ma.masked\n>>> b = np.arange(2, 5)\n>>> a\nmasked_array(data=[0, --, 2],\n             mask=[False,  True, False],\n       fill_value=999999)\n>>> b\narray([2, 3, 4])\n>>> np.concatenate([a, b])\nmasked_array(data=[0, 1, 2, 2, 3, 4],\n             mask=False,\n       fill_value=999999)\n>>> np.ma.concatenate([a, b])\nmasked_array(data=[0, --, 2, 2, 3, 4],\n             mask=[False,  True, False, False, False, False],\n       fill_value=999999)","title":"numpy.reference.generated.numpy.concatenate"},{"text":"numpy.ma.concatenate   ma.concatenate(arrays, axis=0)[source]\n \nConcatenate a sequence of arrays along the given axis.  Parameters \n \narrayssequence of array_like\n\n\nThe arrays must have the same shape, except in the dimension corresponding to axis (the first, by default).  \naxisint, optional\n\n\nThe axis along which the arrays will be joined. Default is 0.    Returns \n \nresultMaskedArray\n\n\nThe concatenated array with any masked entries preserved.      See also  numpy.concatenate\n\nEquivalent function in the top-level NumPy module.    Examples >>> import numpy.ma as ma\n>>> a = ma.arange(3)\n>>> a[1] = ma.masked\n>>> b = ma.arange(2, 5)\n>>> a\nmasked_array(data=[0, --, 2],\n             mask=[False,  True, False],\n       fill_value=999999)\n>>> b\nmasked_array(data=[2, 3, 4],\n             mask=False,\n       fill_value=999999)\n>>> ma.concatenate([a, b])\nmasked_array(data=[0, --, 2, 2, 3, 4],\n             mask=[False,  True, False, False, False, False],\n       fill_value=999999)","title":"numpy.reference.generated.numpy.ma.concatenate"},{"text":"numpy.append   numpy.append(arr, values, axis=None)[source]\n \nAppend values to the end of an array.  Parameters \n \narrarray_like\n\n\nValues are appended to a copy of this array.  \nvaluesarray_like\n\n\nThese values are appended to a copy of arr. It must be of the correct shape (the same shape as arr, excluding axis). If axis is not specified, values can be any shape and will be flattened before use.  \naxisint, optional\n\n\nThe axis along which values are appended. If axis is not given, both arr and values are flattened before use.    Returns \n \nappendndarray\n\n\nA copy of arr with values appended to axis. Note that append does not occur in-place: a new array is allocated and filled. If axis is None, out is a flattened array.      See also  insert\n\nInsert elements into an array.  delete\n\nDelete elements from an array.    Examples >>> np.append([1, 2, 3], [[4, 5, 6], [7, 8, 9]])\narray([1, 2, 3, ..., 7, 8, 9])\n When axis is specified, values must have the correct shape. >>> np.append([[1, 2, 3], [4, 5, 6]], [[7, 8, 9]], axis=0)\narray([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n>>> np.append([[1, 2, 3], [4, 5, 6]], [7, 8, 9], axis=0)\nTraceback (most recent call last):\n    ...\nValueError: all the input arrays must have same number of dimensions, but\nthe array at index 0 has 2 dimension(s) and the array at index 1 has 1\ndimension(s)","title":"numpy.reference.generated.numpy.append"},{"text":"numpy.char.add   char.add(x1, x2)[source]\n \nReturn element-wise string concatenation for two arrays of str or unicode. Arrays x1 and x2 must have the same shape.  Parameters \n \nx1array_like of str or unicode\n\n\nInput array.  \nx2array_like of str or unicode\n\n\nInput array.    Returns \n \naddndarray\n\n\nOutput array of string_ or unicode_, depending on input types of the same shape as x1 and x2.","title":"numpy.reference.generated.numpy.char.add"},{"text":"numpy.ma.inner   ma.inner(a, b, \/)[source]\n \nInner product of two arrays. Ordinary inner product of vectors for 1-D arrays (without complex conjugation), in higher dimensions a sum product over the last axes.  Parameters \n \na, barray_like\n\n\nIf a and b are nonscalar, their last dimensions must match.    Returns \n \noutndarray\n\n\nIf a and b are both scalars or both 1-D arrays then a scalar is returned; otherwise an array is returned. out.shape = (*a.shape[:-1], *b.shape[:-1])    Raises \n ValueError\n\nIf both a and b are nonscalar and their last dimensions have different sizes.      See also  tensordot\n\nSum products over arbitrary axes.  dot\n\nGeneralised matrix product, using second last dimension of b.  einsum\n\nEinstein summation convention.    Notes Masked values are replaced by 0. For vectors (1-D arrays) it computes the ordinary inner-product: np.inner(a, b) = sum(a[:]*b[:])\n More generally, if ndim(a) = r > 0 and ndim(b) = s > 0: np.inner(a, b) = np.tensordot(a, b, axes=(-1,-1))\n or explicitly: np.inner(a, b)[i0,...,ir-2,j0,...,js-2]\n     = sum(a[i0,...,ir-2,:]*b[j0,...,js-2,:])\n In addition a or b may be scalars, in which case: np.inner(a,b) = a*b\n Examples Ordinary inner product for vectors: >>> a = np.array([1,2,3])\n>>> b = np.array([0,1,0])\n>>> np.inner(a, b)\n2\n Some multidimensional examples: >>> a = np.arange(24).reshape((2,3,4))\n>>> b = np.arange(4)\n>>> c = np.inner(a, b)\n>>> c.shape\n(2, 3)\n>>> c\narray([[ 14,  38,  62],\n       [ 86, 110, 134]])\n >>> a = np.arange(2).reshape((1,1,2))\n>>> b = np.arange(6).reshape((3,2))\n>>> c = np.inner(a, b)\n>>> c.shape\n(1, 1, 3)\n>>> c\narray([[[1, 3, 5]]])\n An example where b is a scalar: >>> np.inner(np.eye(2), 7)\narray([[7., 0.],\n       [0., 7.]])","title":"numpy.reference.generated.numpy.ma.inner"},{"text":"numpy.inner   numpy.inner(a, b, \/)\n \nInner product of two arrays. Ordinary inner product of vectors for 1-D arrays (without complex conjugation), in higher dimensions a sum product over the last axes.  Parameters \n \na, barray_like\n\n\nIf a and b are nonscalar, their last dimensions must match.    Returns \n \noutndarray\n\n\nIf a and b are both scalars or both 1-D arrays then a scalar is returned; otherwise an array is returned. out.shape = (*a.shape[:-1], *b.shape[:-1])    Raises \n ValueError\n\nIf both a and b are nonscalar and their last dimensions have different sizes.      See also  tensordot\n\nSum products over arbitrary axes.  dot\n\nGeneralised matrix product, using second last dimension of b.  einsum\n\nEinstein summation convention.    Notes For vectors (1-D arrays) it computes the ordinary inner-product: np.inner(a, b) = sum(a[:]*b[:])\n More generally, if ndim(a) = r > 0 and ndim(b) = s > 0: np.inner(a, b) = np.tensordot(a, b, axes=(-1,-1))\n or explicitly: np.inner(a, b)[i0,...,ir-2,j0,...,js-2]\n     = sum(a[i0,...,ir-2,:]*b[j0,...,js-2,:])\n In addition a or b may be scalars, in which case: np.inner(a,b) = a*b\n Examples Ordinary inner product for vectors: >>> a = np.array([1,2,3])\n>>> b = np.array([0,1,0])\n>>> np.inner(a, b)\n2\n Some multidimensional examples: >>> a = np.arange(24).reshape((2,3,4))\n>>> b = np.arange(4)\n>>> c = np.inner(a, b)\n>>> c.shape\n(2, 3)\n>>> c\narray([[ 14,  38,  62],\n       [ 86, 110, 134]])\n >>> a = np.arange(2).reshape((1,1,2))\n>>> b = np.arange(6).reshape((3,2))\n>>> c = np.inner(a, b)\n>>> c.shape\n(1, 1, 3)\n>>> c\narray([[[1, 3, 5]]])\n An example where b is a scalar: >>> np.inner(np.eye(2), 7)\narray([[7., 0.],\n       [0., 7.]])","title":"numpy.reference.generated.numpy.inner"},{"text":"numpy.r_   numpy.r_ = <numpy.lib.index_tricks.RClass object>\n \nTranslates slice objects to concatenation along the first axis. This is a simple way to build up arrays quickly. There are two use cases.  If the index expression contains comma separated arrays, then stack them along their first axis. If the index expression contains slice notation or scalars then create a 1-D array with a range indicated by the slice notation.  If slice notation is used, the syntax start:stop:step is equivalent to np.arange(start, stop, step) inside of the brackets. However, if step is an imaginary number (i.e. 100j) then its integer portion is interpreted as a number-of-points desired and the start and stop are inclusive. In other words start:stop:stepj is interpreted as np.linspace(start, stop, step, endpoint=1) inside of the brackets. After expansion of slice notation, all comma separated sequences are concatenated together. Optional character strings placed as the first element of the index expression can be used to change the output. The strings \u2018r\u2019 or \u2018c\u2019 result in matrix output. If the result is 1-D and \u2018r\u2019 is specified a 1 x N (row) matrix is produced. If the result is 1-D and \u2018c\u2019 is specified, then a N x 1 (column) matrix is produced. If the result is 2-D then both provide the same matrix result. A string integer specifies which axis to stack multiple comma separated arrays along. A string of two comma-separated integers allows indication of the minimum number of dimensions to force each entry into as the second integer (the axis to concatenate along is still the first integer). A string with three comma-separated integers allows specification of the axis to concatenate along, the minimum number of dimensions to force the entries to, and which axis should contain the start of the arrays which are less than the specified number of dimensions. In other words the third integer allows you to specify where the 1\u2019s should be placed in the shape of the arrays that have their shapes upgraded. By default, they are placed in the front of the shape tuple. The third argument allows you to specify where the start of the array should be instead. Thus, a third argument of \u20180\u2019 would place the 1\u2019s at the end of the array shape. Negative integers specify where in the new shape tuple the last dimension of upgraded arrays should be placed, so the default is \u2018-1\u2019.  Parameters \n Not a function, so takes no parameters\n  Returns \n A concatenated ndarray or matrix.\n    See also  concatenate\n\nJoin a sequence of arrays along an existing axis.  c_\n\nTranslates slice objects to concatenation along the second axis.    Examples >>> np.r_[np.array([1,2,3]), 0, 0, np.array([4,5,6])]\narray([1, 2, 3, ..., 4, 5, 6])\n>>> np.r_[-1:1:6j, [0]*3, 5, 6]\narray([-1. , -0.6, -0.2,  0.2,  0.6,  1. ,  0. ,  0. ,  0. ,  5. ,  6. ])\n String integers specify the axis to concatenate along or the minimum number of dimensions to force entries into. >>> a = np.array([[0, 1, 2], [3, 4, 5]])\n>>> np.r_['-1', a, a] # concatenate along last axis\narray([[0, 1, 2, 0, 1, 2],\n       [3, 4, 5, 3, 4, 5]])\n>>> np.r_['0,2', [1,2,3], [4,5,6]] # concatenate along first axis, dim>=2\narray([[1, 2, 3],\n       [4, 5, 6]])\n >>> np.r_['0,2,0', [1,2,3], [4,5,6]]\narray([[1],\n       [2],\n       [3],\n       [4],\n       [5],\n       [6]])\n>>> np.r_['1,2,0', [1,2,3], [4,5,6]]\narray([[1, 4],\n       [2, 5],\n       [3, 6]])\n Using \u2018r\u2019 or \u2018c\u2019 as a first string argument creates a matrix. >>> np.r_['r',[1,2,3], [4,5,6]]\nmatrix([[1, 2, 3, 4, 5, 6]])","title":"numpy.reference.generated.numpy.r_"}]}
{"task_id":21887754,"prompt":"def f_21887754(a, b):\n\treturn ","suffix":"","canonical_solution":"np.r_[(a[None, :], b[None, :])]","test_start":"\nimport numpy as np\n\ndef check(candidate):","test":["\n    a = np.array([[1, 5, 9], [2, 6, 10]])\n    b = np.array([[3, 7, 11], [4, 8, 12]])\n    assert np.array_equal(candidate(a, b), np.array([[[1, 5, 9], [2, 6, 10]], [[3, 7, 11], [4, 8, 12]]]))\n","\n    a = np.array([[1, 2.45, 3], [4, 0.55, 612]])\n    b = np.array([[988, 8, 7], [6, 512, 4]])\n    assert np.array_equal(candidate(a, b), np.array([[[1, 2.45, 3], [4, 0.55, 612]], [[988, 8 , 7], [6, 512, 4]]]))\n"],"entry_point":"f_21887754","intent":"numpy concatenate two arrays `a` and `b` along the first axis","library":["numpy"],"docs":[{"text":"numpy.ma.append   ma.append(a, b, axis=None)[source]\n \nAppend values to the end of an array.  New in version 1.9.0.   Parameters \n \naarray_like\n\n\nValues are appended to a copy of this array.  \nbarray_like\n\n\nThese values are appended to a copy of a. It must be of the correct shape (the same shape as a, excluding axis). If axis is not specified, b can be any shape and will be flattened before use.  \naxisint, optional\n\n\nThe axis along which v are appended. If axis is not given, both a and b are flattened before use.    Returns \n \nappendMaskedArray\n\n\nA copy of a with b appended to axis. Note that append does not occur in-place: a new array is allocated and filled. If axis is None, the result is a flattened array.      See also  numpy.append\n\nEquivalent function in the top-level NumPy module.    Examples >>> import numpy.ma as ma\n>>> a = ma.masked_values([1, 2, 3], 2)\n>>> b = ma.masked_values([[4, 5, 6], [7, 8, 9]], 7)\n>>> ma.append(a, b)\nmasked_array(data=[1, --, 3, 4, 5, 6, --, 8, 9],\n             mask=[False,  True, False, False, False, False,  True, False,\n                   False],\n       fill_value=999999)","title":"numpy.reference.generated.numpy.ma.append"},{"text":"tf.experimental.numpy.concatenate TensorFlow variant of NumPy's concatenate. \ntf.experimental.numpy.concatenate(\n    arys, axis=0\n)\n See the NumPy documentation for numpy.concatenate.","title":"tensorflow.experimental.numpy.concatenate"},{"text":"numpy.c_   numpy.c_ = <numpy.lib.index_tricks.CClass object>\n \nTranslates slice objects to concatenation along the second axis. This is short-hand for np.r_['-1,2,0', index expression], which is useful because of its common occurrence. In particular, arrays will be stacked along their last axis after being upgraded to at least 2-D with 1\u2019s post-pended to the shape (column vectors made out of 1-D arrays).  See also  column_stack\n\nStack 1-D arrays as columns into a 2-D array.  r_\n\nFor more detailed documentation.    Examples >>> np.c_[np.array([1,2,3]), np.array([4,5,6])]\narray([[1, 4],\n       [2, 5],\n       [3, 6]])\n>>> np.c_[np.array([[1,2,3]]), 0, 0, np.array([[4,5,6]])]\narray([[1, 2, 3, ..., 4, 5, 6]])","title":"numpy.reference.generated.numpy.c_"},{"text":"numpy.concatenate   numpy.concatenate((a1, a2, ...), axis=0, out=None, dtype=None, casting=\"same_kind\")\n \nJoin a sequence of arrays along an existing axis.  Parameters \n \na1, a2, \u2026sequence of array_like\n\n\nThe arrays must have the same shape, except in the dimension corresponding to axis (the first, by default).  \naxisint, optional\n\n\nThe axis along which the arrays will be joined. If axis is None, arrays are flattened before use. Default is 0.  \noutndarray, optional\n\n\nIf provided, the destination to place the result. The shape must be correct, matching that of what concatenate would have returned if no out argument were specified.  \ndtypestr or dtype\n\n\nIf provided, the destination array will have this dtype. Cannot be provided together with out.  New in version 1.20.0.   \ncasting{\u2018no\u2019, \u2018equiv\u2019, \u2018safe\u2019, \u2018same_kind\u2019, \u2018unsafe\u2019}, optional\n\n\nControls what kind of data casting may occur. Defaults to \u2018same_kind\u2019.  New in version 1.20.0.     Returns \n \nresndarray\n\n\nThe concatenated array.      See also  ma.concatenate\n\nConcatenate function that preserves input masks.  array_split\n\nSplit an array into multiple sub-arrays of equal or near-equal size.  split\n\nSplit array into a list of multiple sub-arrays of equal size.  hsplit\n\nSplit array into multiple sub-arrays horizontally (column wise).  vsplit\n\nSplit array into multiple sub-arrays vertically (row wise).  dsplit\n\nSplit array into multiple sub-arrays along the 3rd axis (depth).  stack\n\nStack a sequence of arrays along a new axis.  block\n\nAssemble arrays from blocks.  hstack\n\nStack arrays in sequence horizontally (column wise).  vstack\n\nStack arrays in sequence vertically (row wise).  dstack\n\nStack arrays in sequence depth wise (along third dimension).  column_stack\n\nStack 1-D arrays as columns into a 2-D array.    Notes When one or more of the arrays to be concatenated is a MaskedArray, this function will return a MaskedArray object instead of an ndarray, but the input masks are not preserved. In cases where a MaskedArray is expected as input, use the ma.concatenate function from the masked array module instead. Examples >>> a = np.array([[1, 2], [3, 4]])\n>>> b = np.array([[5, 6]])\n>>> np.concatenate((a, b), axis=0)\narray([[1, 2],\n       [3, 4],\n       [5, 6]])\n>>> np.concatenate((a, b.T), axis=1)\narray([[1, 2, 5],\n       [3, 4, 6]])\n>>> np.concatenate((a, b), axis=None)\narray([1, 2, 3, 4, 5, 6])\n This function will not preserve masking of MaskedArray inputs. >>> a = np.ma.arange(3)\n>>> a[1] = np.ma.masked\n>>> b = np.arange(2, 5)\n>>> a\nmasked_array(data=[0, --, 2],\n             mask=[False,  True, False],\n       fill_value=999999)\n>>> b\narray([2, 3, 4])\n>>> np.concatenate([a, b])\nmasked_array(data=[0, 1, 2, 2, 3, 4],\n             mask=False,\n       fill_value=999999)\n>>> np.ma.concatenate([a, b])\nmasked_array(data=[0, --, 2, 2, 3, 4],\n             mask=[False,  True, False, False, False, False],\n       fill_value=999999)","title":"numpy.reference.generated.numpy.concatenate"},{"text":"numpy.ma.concatenate   ma.concatenate(arrays, axis=0)[source]\n \nConcatenate a sequence of arrays along the given axis.  Parameters \n \narrayssequence of array_like\n\n\nThe arrays must have the same shape, except in the dimension corresponding to axis (the first, by default).  \naxisint, optional\n\n\nThe axis along which the arrays will be joined. Default is 0.    Returns \n \nresultMaskedArray\n\n\nThe concatenated array with any masked entries preserved.      See also  numpy.concatenate\n\nEquivalent function in the top-level NumPy module.    Examples >>> import numpy.ma as ma\n>>> a = ma.arange(3)\n>>> a[1] = ma.masked\n>>> b = ma.arange(2, 5)\n>>> a\nmasked_array(data=[0, --, 2],\n             mask=[False,  True, False],\n       fill_value=999999)\n>>> b\nmasked_array(data=[2, 3, 4],\n             mask=False,\n       fill_value=999999)\n>>> ma.concatenate([a, b])\nmasked_array(data=[0, --, 2, 2, 3, 4],\n             mask=[False,  True, False, False, False, False],\n       fill_value=999999)","title":"numpy.reference.generated.numpy.ma.concatenate"},{"text":"numpy.append   numpy.append(arr, values, axis=None)[source]\n \nAppend values to the end of an array.  Parameters \n \narrarray_like\n\n\nValues are appended to a copy of this array.  \nvaluesarray_like\n\n\nThese values are appended to a copy of arr. It must be of the correct shape (the same shape as arr, excluding axis). If axis is not specified, values can be any shape and will be flattened before use.  \naxisint, optional\n\n\nThe axis along which values are appended. If axis is not given, both arr and values are flattened before use.    Returns \n \nappendndarray\n\n\nA copy of arr with values appended to axis. Note that append does not occur in-place: a new array is allocated and filled. If axis is None, out is a flattened array.      See also  insert\n\nInsert elements into an array.  delete\n\nDelete elements from an array.    Examples >>> np.append([1, 2, 3], [[4, 5, 6], [7, 8, 9]])\narray([1, 2, 3, ..., 7, 8, 9])\n When axis is specified, values must have the correct shape. >>> np.append([[1, 2, 3], [4, 5, 6]], [[7, 8, 9]], axis=0)\narray([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n>>> np.append([[1, 2, 3], [4, 5, 6]], [7, 8, 9], axis=0)\nTraceback (most recent call last):\n    ...\nValueError: all the input arrays must have same number of dimensions, but\nthe array at index 0 has 2 dimension(s) and the array at index 1 has 1\ndimension(s)","title":"numpy.reference.generated.numpy.append"},{"text":"numpy.ma.inner   ma.inner(a, b, \/)[source]\n \nInner product of two arrays. Ordinary inner product of vectors for 1-D arrays (without complex conjugation), in higher dimensions a sum product over the last axes.  Parameters \n \na, barray_like\n\n\nIf a and b are nonscalar, their last dimensions must match.    Returns \n \noutndarray\n\n\nIf a and b are both scalars or both 1-D arrays then a scalar is returned; otherwise an array is returned. out.shape = (*a.shape[:-1], *b.shape[:-1])    Raises \n ValueError\n\nIf both a and b are nonscalar and their last dimensions have different sizes.      See also  tensordot\n\nSum products over arbitrary axes.  dot\n\nGeneralised matrix product, using second last dimension of b.  einsum\n\nEinstein summation convention.    Notes Masked values are replaced by 0. For vectors (1-D arrays) it computes the ordinary inner-product: np.inner(a, b) = sum(a[:]*b[:])\n More generally, if ndim(a) = r > 0 and ndim(b) = s > 0: np.inner(a, b) = np.tensordot(a, b, axes=(-1,-1))\n or explicitly: np.inner(a, b)[i0,...,ir-2,j0,...,js-2]\n     = sum(a[i0,...,ir-2,:]*b[j0,...,js-2,:])\n In addition a or b may be scalars, in which case: np.inner(a,b) = a*b\n Examples Ordinary inner product for vectors: >>> a = np.array([1,2,3])\n>>> b = np.array([0,1,0])\n>>> np.inner(a, b)\n2\n Some multidimensional examples: >>> a = np.arange(24).reshape((2,3,4))\n>>> b = np.arange(4)\n>>> c = np.inner(a, b)\n>>> c.shape\n(2, 3)\n>>> c\narray([[ 14,  38,  62],\n       [ 86, 110, 134]])\n >>> a = np.arange(2).reshape((1,1,2))\n>>> b = np.arange(6).reshape((3,2))\n>>> c = np.inner(a, b)\n>>> c.shape\n(1, 1, 3)\n>>> c\narray([[[1, 3, 5]]])\n An example where b is a scalar: >>> np.inner(np.eye(2), 7)\narray([[7., 0.],\n       [0., 7.]])","title":"numpy.reference.generated.numpy.ma.inner"},{"text":"numpy.char.add   char.add(x1, x2)[source]\n \nReturn element-wise string concatenation for two arrays of str or unicode. Arrays x1 and x2 must have the same shape.  Parameters \n \nx1array_like of str or unicode\n\n\nInput array.  \nx2array_like of str or unicode\n\n\nInput array.    Returns \n \naddndarray\n\n\nOutput array of string_ or unicode_, depending on input types of the same shape as x1 and x2.","title":"numpy.reference.generated.numpy.char.add"},{"text":"numpy.inner   numpy.inner(a, b, \/)\n \nInner product of two arrays. Ordinary inner product of vectors for 1-D arrays (without complex conjugation), in higher dimensions a sum product over the last axes.  Parameters \n \na, barray_like\n\n\nIf a and b are nonscalar, their last dimensions must match.    Returns \n \noutndarray\n\n\nIf a and b are both scalars or both 1-D arrays then a scalar is returned; otherwise an array is returned. out.shape = (*a.shape[:-1], *b.shape[:-1])    Raises \n ValueError\n\nIf both a and b are nonscalar and their last dimensions have different sizes.      See also  tensordot\n\nSum products over arbitrary axes.  dot\n\nGeneralised matrix product, using second last dimension of b.  einsum\n\nEinstein summation convention.    Notes For vectors (1-D arrays) it computes the ordinary inner-product: np.inner(a, b) = sum(a[:]*b[:])\n More generally, if ndim(a) = r > 0 and ndim(b) = s > 0: np.inner(a, b) = np.tensordot(a, b, axes=(-1,-1))\n or explicitly: np.inner(a, b)[i0,...,ir-2,j0,...,js-2]\n     = sum(a[i0,...,ir-2,:]*b[j0,...,js-2,:])\n In addition a or b may be scalars, in which case: np.inner(a,b) = a*b\n Examples Ordinary inner product for vectors: >>> a = np.array([1,2,3])\n>>> b = np.array([0,1,0])\n>>> np.inner(a, b)\n2\n Some multidimensional examples: >>> a = np.arange(24).reshape((2,3,4))\n>>> b = np.arange(4)\n>>> c = np.inner(a, b)\n>>> c.shape\n(2, 3)\n>>> c\narray([[ 14,  38,  62],\n       [ 86, 110, 134]])\n >>> a = np.arange(2).reshape((1,1,2))\n>>> b = np.arange(6).reshape((3,2))\n>>> c = np.inner(a, b)\n>>> c.shape\n(1, 1, 3)\n>>> c\narray([[[1, 3, 5]]])\n An example where b is a scalar: >>> np.inner(np.eye(2), 7)\narray([[7., 0.],\n       [0., 7.]])","title":"numpy.reference.generated.numpy.inner"},{"text":"numpy.r_   numpy.r_ = <numpy.lib.index_tricks.RClass object>\n \nTranslates slice objects to concatenation along the first axis. This is a simple way to build up arrays quickly. There are two use cases.  If the index expression contains comma separated arrays, then stack them along their first axis. If the index expression contains slice notation or scalars then create a 1-D array with a range indicated by the slice notation.  If slice notation is used, the syntax start:stop:step is equivalent to np.arange(start, stop, step) inside of the brackets. However, if step is an imaginary number (i.e. 100j) then its integer portion is interpreted as a number-of-points desired and the start and stop are inclusive. In other words start:stop:stepj is interpreted as np.linspace(start, stop, step, endpoint=1) inside of the brackets. After expansion of slice notation, all comma separated sequences are concatenated together. Optional character strings placed as the first element of the index expression can be used to change the output. The strings \u2018r\u2019 or \u2018c\u2019 result in matrix output. If the result is 1-D and \u2018r\u2019 is specified a 1 x N (row) matrix is produced. If the result is 1-D and \u2018c\u2019 is specified, then a N x 1 (column) matrix is produced. If the result is 2-D then both provide the same matrix result. A string integer specifies which axis to stack multiple comma separated arrays along. A string of two comma-separated integers allows indication of the minimum number of dimensions to force each entry into as the second integer (the axis to concatenate along is still the first integer). A string with three comma-separated integers allows specification of the axis to concatenate along, the minimum number of dimensions to force the entries to, and which axis should contain the start of the arrays which are less than the specified number of dimensions. In other words the third integer allows you to specify where the 1\u2019s should be placed in the shape of the arrays that have their shapes upgraded. By default, they are placed in the front of the shape tuple. The third argument allows you to specify where the start of the array should be instead. Thus, a third argument of \u20180\u2019 would place the 1\u2019s at the end of the array shape. Negative integers specify where in the new shape tuple the last dimension of upgraded arrays should be placed, so the default is \u2018-1\u2019.  Parameters \n Not a function, so takes no parameters\n  Returns \n A concatenated ndarray or matrix.\n    See also  concatenate\n\nJoin a sequence of arrays along an existing axis.  c_\n\nTranslates slice objects to concatenation along the second axis.    Examples >>> np.r_[np.array([1,2,3]), 0, 0, np.array([4,5,6])]\narray([1, 2, 3, ..., 4, 5, 6])\n>>> np.r_[-1:1:6j, [0]*3, 5, 6]\narray([-1. , -0.6, -0.2,  0.2,  0.6,  1. ,  0. ,  0. ,  0. ,  5. ,  6. ])\n String integers specify the axis to concatenate along or the minimum number of dimensions to force entries into. >>> a = np.array([[0, 1, 2], [3, 4, 5]])\n>>> np.r_['-1', a, a] # concatenate along last axis\narray([[0, 1, 2, 0, 1, 2],\n       [3, 4, 5, 3, 4, 5]])\n>>> np.r_['0,2', [1,2,3], [4,5,6]] # concatenate along first axis, dim>=2\narray([[1, 2, 3],\n       [4, 5, 6]])\n >>> np.r_['0,2,0', [1,2,3], [4,5,6]]\narray([[1],\n       [2],\n       [3],\n       [4],\n       [5],\n       [6]])\n>>> np.r_['1,2,0', [1,2,3], [4,5,6]]\narray([[1, 4],\n       [2, 5],\n       [3, 6]])\n Using \u2018r\u2019 or \u2018c\u2019 as a first string argument creates a matrix. >>> np.r_['r',[1,2,3], [4,5,6]]\nmatrix([[1, 2, 3, 4, 5, 6]])","title":"numpy.reference.generated.numpy.r_"}]}
{"task_id":21887754,"prompt":"def f_21887754(a, b):\n\treturn ","suffix":"","canonical_solution":"np.array((a, b))","test_start":"\nimport numpy as np\n\ndef check(candidate):","test":["\n    a = np.array([[1, 5, 9], [2, 6, 10]])\n    b = np.array([[3, 7, 11], [4, 8, 12]])\n    assert np.array_equal(candidate(a, b), np.array([[[1, 5, 9], [2, 6, 10]], [[3, 7, 11], [4, 8, 12]]]))\n","\n    a = np.array([[1, 2.45, 3], [4, 0.55, 612]])\n    b = np.array([[988, 8, 7], [6, 512, 4]])\n    assert np.array_equal(candidate(a, b), np.array([[[1, 2.45, 3], [4, 0.55, 612]], [[988, 8 , 7], [6, 512, 4]]]))\n"],"entry_point":"f_21887754","intent":"numpy concatenate two arrays `a` and `b` along the first axis","library":["numpy"],"docs":[{"text":"numpy.ma.append   ma.append(a, b, axis=None)[source]\n \nAppend values to the end of an array.  New in version 1.9.0.   Parameters \n \naarray_like\n\n\nValues are appended to a copy of this array.  \nbarray_like\n\n\nThese values are appended to a copy of a. It must be of the correct shape (the same shape as a, excluding axis). If axis is not specified, b can be any shape and will be flattened before use.  \naxisint, optional\n\n\nThe axis along which v are appended. If axis is not given, both a and b are flattened before use.    Returns \n \nappendMaskedArray\n\n\nA copy of a with b appended to axis. Note that append does not occur in-place: a new array is allocated and filled. If axis is None, the result is a flattened array.      See also  numpy.append\n\nEquivalent function in the top-level NumPy module.    Examples >>> import numpy.ma as ma\n>>> a = ma.masked_values([1, 2, 3], 2)\n>>> b = ma.masked_values([[4, 5, 6], [7, 8, 9]], 7)\n>>> ma.append(a, b)\nmasked_array(data=[1, --, 3, 4, 5, 6, --, 8, 9],\n             mask=[False,  True, False, False, False, False,  True, False,\n                   False],\n       fill_value=999999)","title":"numpy.reference.generated.numpy.ma.append"},{"text":"tf.experimental.numpy.concatenate TensorFlow variant of NumPy's concatenate. \ntf.experimental.numpy.concatenate(\n    arys, axis=0\n)\n See the NumPy documentation for numpy.concatenate.","title":"tensorflow.experimental.numpy.concatenate"},{"text":"numpy.c_   numpy.c_ = <numpy.lib.index_tricks.CClass object>\n \nTranslates slice objects to concatenation along the second axis. This is short-hand for np.r_['-1,2,0', index expression], which is useful because of its common occurrence. In particular, arrays will be stacked along their last axis after being upgraded to at least 2-D with 1\u2019s post-pended to the shape (column vectors made out of 1-D arrays).  See also  column_stack\n\nStack 1-D arrays as columns into a 2-D array.  r_\n\nFor more detailed documentation.    Examples >>> np.c_[np.array([1,2,3]), np.array([4,5,6])]\narray([[1, 4],\n       [2, 5],\n       [3, 6]])\n>>> np.c_[np.array([[1,2,3]]), 0, 0, np.array([[4,5,6]])]\narray([[1, 2, 3, ..., 4, 5, 6]])","title":"numpy.reference.generated.numpy.c_"},{"text":"numpy.concatenate   numpy.concatenate((a1, a2, ...), axis=0, out=None, dtype=None, casting=\"same_kind\")\n \nJoin a sequence of arrays along an existing axis.  Parameters \n \na1, a2, \u2026sequence of array_like\n\n\nThe arrays must have the same shape, except in the dimension corresponding to axis (the first, by default).  \naxisint, optional\n\n\nThe axis along which the arrays will be joined. If axis is None, arrays are flattened before use. Default is 0.  \noutndarray, optional\n\n\nIf provided, the destination to place the result. The shape must be correct, matching that of what concatenate would have returned if no out argument were specified.  \ndtypestr or dtype\n\n\nIf provided, the destination array will have this dtype. Cannot be provided together with out.  New in version 1.20.0.   \ncasting{\u2018no\u2019, \u2018equiv\u2019, \u2018safe\u2019, \u2018same_kind\u2019, \u2018unsafe\u2019}, optional\n\n\nControls what kind of data casting may occur. Defaults to \u2018same_kind\u2019.  New in version 1.20.0.     Returns \n \nresndarray\n\n\nThe concatenated array.      See also  ma.concatenate\n\nConcatenate function that preserves input masks.  array_split\n\nSplit an array into multiple sub-arrays of equal or near-equal size.  split\n\nSplit array into a list of multiple sub-arrays of equal size.  hsplit\n\nSplit array into multiple sub-arrays horizontally (column wise).  vsplit\n\nSplit array into multiple sub-arrays vertically (row wise).  dsplit\n\nSplit array into multiple sub-arrays along the 3rd axis (depth).  stack\n\nStack a sequence of arrays along a new axis.  block\n\nAssemble arrays from blocks.  hstack\n\nStack arrays in sequence horizontally (column wise).  vstack\n\nStack arrays in sequence vertically (row wise).  dstack\n\nStack arrays in sequence depth wise (along third dimension).  column_stack\n\nStack 1-D arrays as columns into a 2-D array.    Notes When one or more of the arrays to be concatenated is a MaskedArray, this function will return a MaskedArray object instead of an ndarray, but the input masks are not preserved. In cases where a MaskedArray is expected as input, use the ma.concatenate function from the masked array module instead. Examples >>> a = np.array([[1, 2], [3, 4]])\n>>> b = np.array([[5, 6]])\n>>> np.concatenate((a, b), axis=0)\narray([[1, 2],\n       [3, 4],\n       [5, 6]])\n>>> np.concatenate((a, b.T), axis=1)\narray([[1, 2, 5],\n       [3, 4, 6]])\n>>> np.concatenate((a, b), axis=None)\narray([1, 2, 3, 4, 5, 6])\n This function will not preserve masking of MaskedArray inputs. >>> a = np.ma.arange(3)\n>>> a[1] = np.ma.masked\n>>> b = np.arange(2, 5)\n>>> a\nmasked_array(data=[0, --, 2],\n             mask=[False,  True, False],\n       fill_value=999999)\n>>> b\narray([2, 3, 4])\n>>> np.concatenate([a, b])\nmasked_array(data=[0, 1, 2, 2, 3, 4],\n             mask=False,\n       fill_value=999999)\n>>> np.ma.concatenate([a, b])\nmasked_array(data=[0, --, 2, 2, 3, 4],\n             mask=[False,  True, False, False, False, False],\n       fill_value=999999)","title":"numpy.reference.generated.numpy.concatenate"},{"text":"numpy.ma.concatenate   ma.concatenate(arrays, axis=0)[source]\n \nConcatenate a sequence of arrays along the given axis.  Parameters \n \narrayssequence of array_like\n\n\nThe arrays must have the same shape, except in the dimension corresponding to axis (the first, by default).  \naxisint, optional\n\n\nThe axis along which the arrays will be joined. Default is 0.    Returns \n \nresultMaskedArray\n\n\nThe concatenated array with any masked entries preserved.      See also  numpy.concatenate\n\nEquivalent function in the top-level NumPy module.    Examples >>> import numpy.ma as ma\n>>> a = ma.arange(3)\n>>> a[1] = ma.masked\n>>> b = ma.arange(2, 5)\n>>> a\nmasked_array(data=[0, --, 2],\n             mask=[False,  True, False],\n       fill_value=999999)\n>>> b\nmasked_array(data=[2, 3, 4],\n             mask=False,\n       fill_value=999999)\n>>> ma.concatenate([a, b])\nmasked_array(data=[0, --, 2, 2, 3, 4],\n             mask=[False,  True, False, False, False, False],\n       fill_value=999999)","title":"numpy.reference.generated.numpy.ma.concatenate"},{"text":"numpy.append   numpy.append(arr, values, axis=None)[source]\n \nAppend values to the end of an array.  Parameters \n \narrarray_like\n\n\nValues are appended to a copy of this array.  \nvaluesarray_like\n\n\nThese values are appended to a copy of arr. It must be of the correct shape (the same shape as arr, excluding axis). If axis is not specified, values can be any shape and will be flattened before use.  \naxisint, optional\n\n\nThe axis along which values are appended. If axis is not given, both arr and values are flattened before use.    Returns \n \nappendndarray\n\n\nA copy of arr with values appended to axis. Note that append does not occur in-place: a new array is allocated and filled. If axis is None, out is a flattened array.      See also  insert\n\nInsert elements into an array.  delete\n\nDelete elements from an array.    Examples >>> np.append([1, 2, 3], [[4, 5, 6], [7, 8, 9]])\narray([1, 2, 3, ..., 7, 8, 9])\n When axis is specified, values must have the correct shape. >>> np.append([[1, 2, 3], [4, 5, 6]], [[7, 8, 9]], axis=0)\narray([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n>>> np.append([[1, 2, 3], [4, 5, 6]], [7, 8, 9], axis=0)\nTraceback (most recent call last):\n    ...\nValueError: all the input arrays must have same number of dimensions, but\nthe array at index 0 has 2 dimension(s) and the array at index 1 has 1\ndimension(s)","title":"numpy.reference.generated.numpy.append"},{"text":"numpy.ma.inner   ma.inner(a, b, \/)[source]\n \nInner product of two arrays. Ordinary inner product of vectors for 1-D arrays (without complex conjugation), in higher dimensions a sum product over the last axes.  Parameters \n \na, barray_like\n\n\nIf a and b are nonscalar, their last dimensions must match.    Returns \n \noutndarray\n\n\nIf a and b are both scalars or both 1-D arrays then a scalar is returned; otherwise an array is returned. out.shape = (*a.shape[:-1], *b.shape[:-1])    Raises \n ValueError\n\nIf both a and b are nonscalar and their last dimensions have different sizes.      See also  tensordot\n\nSum products over arbitrary axes.  dot\n\nGeneralised matrix product, using second last dimension of b.  einsum\n\nEinstein summation convention.    Notes Masked values are replaced by 0. For vectors (1-D arrays) it computes the ordinary inner-product: np.inner(a, b) = sum(a[:]*b[:])\n More generally, if ndim(a) = r > 0 and ndim(b) = s > 0: np.inner(a, b) = np.tensordot(a, b, axes=(-1,-1))\n or explicitly: np.inner(a, b)[i0,...,ir-2,j0,...,js-2]\n     = sum(a[i0,...,ir-2,:]*b[j0,...,js-2,:])\n In addition a or b may be scalars, in which case: np.inner(a,b) = a*b\n Examples Ordinary inner product for vectors: >>> a = np.array([1,2,3])\n>>> b = np.array([0,1,0])\n>>> np.inner(a, b)\n2\n Some multidimensional examples: >>> a = np.arange(24).reshape((2,3,4))\n>>> b = np.arange(4)\n>>> c = np.inner(a, b)\n>>> c.shape\n(2, 3)\n>>> c\narray([[ 14,  38,  62],\n       [ 86, 110, 134]])\n >>> a = np.arange(2).reshape((1,1,2))\n>>> b = np.arange(6).reshape((3,2))\n>>> c = np.inner(a, b)\n>>> c.shape\n(1, 1, 3)\n>>> c\narray([[[1, 3, 5]]])\n An example where b is a scalar: >>> np.inner(np.eye(2), 7)\narray([[7., 0.],\n       [0., 7.]])","title":"numpy.reference.generated.numpy.ma.inner"},{"text":"numpy.char.add   char.add(x1, x2)[source]\n \nReturn element-wise string concatenation for two arrays of str or unicode. Arrays x1 and x2 must have the same shape.  Parameters \n \nx1array_like of str or unicode\n\n\nInput array.  \nx2array_like of str or unicode\n\n\nInput array.    Returns \n \naddndarray\n\n\nOutput array of string_ or unicode_, depending on input types of the same shape as x1 and x2.","title":"numpy.reference.generated.numpy.char.add"},{"text":"numpy.inner   numpy.inner(a, b, \/)\n \nInner product of two arrays. Ordinary inner product of vectors for 1-D arrays (without complex conjugation), in higher dimensions a sum product over the last axes.  Parameters \n \na, barray_like\n\n\nIf a and b are nonscalar, their last dimensions must match.    Returns \n \noutndarray\n\n\nIf a and b are both scalars or both 1-D arrays then a scalar is returned; otherwise an array is returned. out.shape = (*a.shape[:-1], *b.shape[:-1])    Raises \n ValueError\n\nIf both a and b are nonscalar and their last dimensions have different sizes.      See also  tensordot\n\nSum products over arbitrary axes.  dot\n\nGeneralised matrix product, using second last dimension of b.  einsum\n\nEinstein summation convention.    Notes For vectors (1-D arrays) it computes the ordinary inner-product: np.inner(a, b) = sum(a[:]*b[:])\n More generally, if ndim(a) = r > 0 and ndim(b) = s > 0: np.inner(a, b) = np.tensordot(a, b, axes=(-1,-1))\n or explicitly: np.inner(a, b)[i0,...,ir-2,j0,...,js-2]\n     = sum(a[i0,...,ir-2,:]*b[j0,...,js-2,:])\n In addition a or b may be scalars, in which case: np.inner(a,b) = a*b\n Examples Ordinary inner product for vectors: >>> a = np.array([1,2,3])\n>>> b = np.array([0,1,0])\n>>> np.inner(a, b)\n2\n Some multidimensional examples: >>> a = np.arange(24).reshape((2,3,4))\n>>> b = np.arange(4)\n>>> c = np.inner(a, b)\n>>> c.shape\n(2, 3)\n>>> c\narray([[ 14,  38,  62],\n       [ 86, 110, 134]])\n >>> a = np.arange(2).reshape((1,1,2))\n>>> b = np.arange(6).reshape((3,2))\n>>> c = np.inner(a, b)\n>>> c.shape\n(1, 1, 3)\n>>> c\narray([[[1, 3, 5]]])\n An example where b is a scalar: >>> np.inner(np.eye(2), 7)\narray([[7., 0.],\n       [0., 7.]])","title":"numpy.reference.generated.numpy.inner"},{"text":"numpy.r_   numpy.r_ = <numpy.lib.index_tricks.RClass object>\n \nTranslates slice objects to concatenation along the first axis. This is a simple way to build up arrays quickly. There are two use cases.  If the index expression contains comma separated arrays, then stack them along their first axis. If the index expression contains slice notation or scalars then create a 1-D array with a range indicated by the slice notation.  If slice notation is used, the syntax start:stop:step is equivalent to np.arange(start, stop, step) inside of the brackets. However, if step is an imaginary number (i.e. 100j) then its integer portion is interpreted as a number-of-points desired and the start and stop are inclusive. In other words start:stop:stepj is interpreted as np.linspace(start, stop, step, endpoint=1) inside of the brackets. After expansion of slice notation, all comma separated sequences are concatenated together. Optional character strings placed as the first element of the index expression can be used to change the output. The strings \u2018r\u2019 or \u2018c\u2019 result in matrix output. If the result is 1-D and \u2018r\u2019 is specified a 1 x N (row) matrix is produced. If the result is 1-D and \u2018c\u2019 is specified, then a N x 1 (column) matrix is produced. If the result is 2-D then both provide the same matrix result. A string integer specifies which axis to stack multiple comma separated arrays along. A string of two comma-separated integers allows indication of the minimum number of dimensions to force each entry into as the second integer (the axis to concatenate along is still the first integer). A string with three comma-separated integers allows specification of the axis to concatenate along, the minimum number of dimensions to force the entries to, and which axis should contain the start of the arrays which are less than the specified number of dimensions. In other words the third integer allows you to specify where the 1\u2019s should be placed in the shape of the arrays that have their shapes upgraded. By default, they are placed in the front of the shape tuple. The third argument allows you to specify where the start of the array should be instead. Thus, a third argument of \u20180\u2019 would place the 1\u2019s at the end of the array shape. Negative integers specify where in the new shape tuple the last dimension of upgraded arrays should be placed, so the default is \u2018-1\u2019.  Parameters \n Not a function, so takes no parameters\n  Returns \n A concatenated ndarray or matrix.\n    See also  concatenate\n\nJoin a sequence of arrays along an existing axis.  c_\n\nTranslates slice objects to concatenation along the second axis.    Examples >>> np.r_[np.array([1,2,3]), 0, 0, np.array([4,5,6])]\narray([1, 2, 3, ..., 4, 5, 6])\n>>> np.r_[-1:1:6j, [0]*3, 5, 6]\narray([-1. , -0.6, -0.2,  0.2,  0.6,  1. ,  0. ,  0. ,  0. ,  5. ,  6. ])\n String integers specify the axis to concatenate along or the minimum number of dimensions to force entries into. >>> a = np.array([[0, 1, 2], [3, 4, 5]])\n>>> np.r_['-1', a, a] # concatenate along last axis\narray([[0, 1, 2, 0, 1, 2],\n       [3, 4, 5, 3, 4, 5]])\n>>> np.r_['0,2', [1,2,3], [4,5,6]] # concatenate along first axis, dim>=2\narray([[1, 2, 3],\n       [4, 5, 6]])\n >>> np.r_['0,2,0', [1,2,3], [4,5,6]]\narray([[1],\n       [2],\n       [3],\n       [4],\n       [5],\n       [6]])\n>>> np.r_['1,2,0', [1,2,3], [4,5,6]]\narray([[1, 4],\n       [2, 5],\n       [3, 6]])\n Using \u2018r\u2019 or \u2018c\u2019 as a first string argument creates a matrix. >>> np.r_['r',[1,2,3], [4,5,6]]\nmatrix([[1, 2, 3, 4, 5, 6]])","title":"numpy.reference.generated.numpy.r_"}]}
{"task_id":2805231,"prompt":"def f_2805231():\n\treturn ","suffix":"","canonical_solution":"socket.getaddrinfo('google.com', 80)","test_start":"\nimport socket\n\ndef check(candidate):","test":["\n    res = candidate()\n    assert all([(add[4][1] == 80) for add in res])\n"],"entry_point":"f_2805231","intent":"fetch address information for host 'google.com' ion port 80","library":["socket"],"docs":[{"text":"coroutine loop.getnameinfo(sockaddr, flags=0)  \nAsynchronous version of socket.getnameinfo().","title":"python.library.asyncio-eventloop#asyncio.loop.getnameinfo"},{"text":"coroutine loop.getaddrinfo(host, port, *, family=0, type=0, proto=0, flags=0)  \nAsynchronous version of socket.getaddrinfo().","title":"python.library.asyncio-eventloop#asyncio.loop.getaddrinfo"},{"text":"socket.gethostbyaddr(ip_address)  \nReturn a triple (hostname, aliaslist, ipaddrlist) where hostname is the primary host name responding to the given ip_address, aliaslist is a (possibly empty) list of alternative host names for the same address, and ipaddrlist is a list of IPv4\/v6 addresses for the same interface on the same host (most likely containing only a single address). To find the fully qualified domain name, use the function getfqdn(). gethostbyaddr() supports both IPv4 and IPv6. Raises an auditing event socket.gethostbyaddr with argument ip_address.","title":"python.library.socket#socket.gethostbyaddr"},{"text":"socket.gethostbyname_ex(hostname)  \nTranslate a host name to IPv4 address format, extended interface. Return a triple (hostname, aliaslist, ipaddrlist) where hostname is the primary host name responding to the given ip_address, aliaslist is a (possibly empty) list of alternative host names for the same address, and ipaddrlist is a list of IPv4 addresses for the same interface on the same host (often but not always a single address). gethostbyname_ex() does not support IPv6 name resolution, and getaddrinfo() should be used instead for IPv4\/v6 dual stack support. Raises an auditing event socket.gethostbyname with argument hostname.","title":"python.library.socket#socket.gethostbyname_ex"},{"text":"socket.getaddrinfo(host, port, family=0, type=0, proto=0, flags=0)  \nTranslate the host\/port argument into a sequence of 5-tuples that contain all the necessary arguments for creating a socket connected to that service. host is a domain name, a string representation of an IPv4\/v6 address or None. port is a string service name such as 'http', a numeric port number or None. By passing None as the value of host and port, you can pass NULL to the underlying C API. The family, type and proto arguments can be optionally specified in order to narrow the list of addresses returned. Passing zero as a value for each of these arguments selects the full range of results. The flags argument can be one or several of the AI_* constants, and will influence how results are computed and returned. For example, AI_NUMERICHOST will disable domain name resolution and will raise an error if host is a domain name. The function returns a list of 5-tuples with the following structure: (family, type, proto, canonname, sockaddr) In these tuples, family, type, proto are all integers and are meant to be passed to the socket() function. canonname will be a string representing the canonical name of the host if AI_CANONNAME is part of the flags argument; else canonname will be empty. sockaddr is a tuple describing a socket address, whose format depends on the returned family (a (address, port) 2-tuple for AF_INET, a (address, port, flowinfo, scope_id) 4-tuple for AF_INET6), and is meant to be passed to the socket.connect() method. Raises an auditing event socket.getaddrinfo with arguments host, port, family, type, protocol. The following example fetches address information for a hypothetical TCP connection to example.org on port 80 (results may differ on your system if IPv6 isn\u2019t enabled): >>> socket.getaddrinfo(\"example.org\", 80, proto=socket.IPPROTO_TCP)\n[(<AddressFamily.AF_INET6: 10>, <SocketType.SOCK_STREAM: 1>,\n 6, '', ('2606:2800:220:1:248:1893:25c8:1946', 80, 0, 0)),\n (<AddressFamily.AF_INET: 2>, <SocketType.SOCK_STREAM: 1>,\n 6, '', ('93.184.216.34', 80))]\n  Changed in version 3.2: parameters can now be passed using keyword arguments.   Changed in version 3.7: for IPv6 multicast addresses, string representing an address will not contain %scope_id part.","title":"python.library.socket#socket.getaddrinfo"},{"text":"Module: tf.compat.v1.gfile Import router for file_io. Classes class FastGFile: File I\/O wrappers without thread locking. class GFile: File I\/O wrappers without thread locking. class Open: File I\/O wrappers without thread locking. Functions Copy(...): Copies data from oldpath to newpath. DeleteRecursively(...): Deletes everything under dirname recursively. Exists(...): Determines whether a path exists or not. Glob(...): Returns a list of files that match the given pattern(s). IsDirectory(...): Returns whether the path is a directory or not. ListDirectory(...): Returns a list of entries contained within a directory. MakeDirs(...): Creates a directory and all parent\/intermediate directories. MkDir(...): Creates a directory with the name dirname. Remove(...): Deletes the file located at 'filename'. Rename(...): Rename or move a file \/ directory. Stat(...): Returns file statistics for a given path. Walk(...): Recursive directory tree generator for directories.","title":"tensorflow.compat.v1.gfile"},{"text":"matplotlib.pyplot.ion   matplotlib.pyplot.ion()[source]\n \nEnable interactive mode. See pyplot.isinteractive for more details.  See also  ioff\n\nDisable interactive mode.  isinteractive\n\nWhether interactive mode is enabled.  show\n\nShow all figures (and maybe block).  pause\n\nShow all figures, and block for a time.    Notes For a temporary change, this can be used as a context manager: # if interactive mode is off\n# then figures will not be shown on creation\nplt.ioff()\n# This figure will not be shown immediately\nfig = plt.figure()\n\nwith plt.ion():\n    # interactive mode will be on\n    # figures will automatically be shown\n    fig2 = plt.figure()\n    # ...\n To enable usage as a context manager, this function returns an _IonContext object. The return value is not intended to be stored or accessed by the user.","title":"matplotlib._as_gen.matplotlib.pyplot.ion"},{"text":"ping_google(sitemap_url=None, ping_url=PING_URL, sitemap_uses_https=True)  \nping_google takes these optional arguments:  \nsitemap_url - The absolute path to your site\u2019s sitemap (e.g., '\/sitemap.xml'). If this argument isn\u2019t provided, ping_google will perform a reverse lookup in your URLconf, for URLs named 'django.contrib.sitemaps.views.index' and then 'django.contrib.sitemaps.views.sitemap' (without further arguments) to automatically determine the sitemap URL.  \nping_url - Defaults to Google\u2019s Ping Tool: https:\/\/www.google.com\/webmasters\/tools\/ping.  \nsitemap_uses_https - Set to False if your site uses http rather than https.   ping_google() raises the exception django.contrib.sitemaps.SitemapNotFound if it cannot determine your sitemap URL.","title":"django.ref.contrib.sitemaps#django.contrib.sitemaps.ping_google"},{"text":"xml.etree.ElementTree.register_namespace(prefix, uri)  \nRegisters a namespace prefix. The registry is global, and any existing mapping for either the given prefix or the namespace URI will be removed. prefix is a namespace prefix. uri is a namespace uri. Tags and attributes in this namespace will be serialized with the given prefix, if at all possible.  New in version 3.2.","title":"python.library.xml.etree.elementtree#xml.etree.ElementTree.register_namespace"},{"text":"tf.raw_ops.IsotonicRegression Solves a batch of isotonic regression problems.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.raw_ops.IsotonicRegression  \ntf.raw_ops.IsotonicRegression(\n    input, output_dtype=tf.dtypes.float32, name=None\n)\n\n \n\n\n Args\n  input   A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64. A (batch_size, dim)-tensor holding a batch of inputs.  \n  output_dtype   An optional tf.DType from: tf.half, tf.bfloat16, tf.float32, tf.float64. Defaults to tf.float32. Dtype of output.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A tuple of Tensor objects (output, segments).     output   A Tensor of type output_dtype.  \n  segments   A Tensor of type int32.","title":"tensorflow.raw_ops.isotonicregression"}]}
{"task_id":17552997,"prompt":"def f_17552997(df):\n\treturn ","suffix":"","canonical_solution":"df.xs('sat', level='day', drop_level=False)","test_start":"\nimport pandas as pd \n\ndef check(candidate):","test":["\n    df = pd.DataFrame({'year':[2008,2008,2008,2008,2009,2009,2009,2009], \n                      'flavour':['strawberry','strawberry','banana','banana',\n                      'strawberry','strawberry','banana','banana'],\n                      'day':['sat','sun','sat','sun','sat','sun','sat','sun'],\n                      'sales':[10,12,22,23,11,13,23,24]})\n    df = df.set_index(['year','flavour','day'])\n    assert candidate(df).to_dict() == {'sales': {(2008, 'strawberry', 'sat'): 10, (2008, 'banana', 'sat'): 22, (2009, 'strawberry', 'sat'): 11, (2009, 'banana', 'sat'): 23}}\n"],"entry_point":"f_17552997","intent":"add a column 'day' with value 'sat' to dataframe `df`","library":["pandas"],"docs":[{"text":"pandas.tseries.offsets.CDay   pandas.tseries.offsets.CDay\n \nalias of pandas._libs.tslibs.offsets.CustomBusinessDay","title":"pandas.reference.api.pandas.tseries.offsets.cday"},{"text":"pandas.tseries.offsets.CustomBusinessDay.name   CustomBusinessDay.name","title":"pandas.reference.api.pandas.tseries.offsets.custombusinessday.name"},{"text":"pandas.tseries.offsets.CustomBusinessDay.apply   CustomBusinessDay.apply()","title":"pandas.reference.api.pandas.tseries.offsets.custombusinessday.apply"},{"text":"tf.image.adjust_saturation     View source on GitHub    Adjust saturation of RGB images.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.image.adjust_saturation  \ntf.image.adjust_saturation(\n    image, saturation_factor, name=None\n)\n This is a convenience method that converts RGB images to float representation, converts them to HSV, adds an offset to the saturation channel, converts back to RGB and then back to the original data type. If several adjustments are chained it is advisable to minimize the number of redundant conversions. image is an RGB image or images. The image saturation is adjusted by converting the images to HSV and multiplying the saturation (S) channel by saturation_factor and clipping. The images are then converted back to RGB. Usage Example: \nx = [[[1.0, 2.0, 3.0],\n      [4.0, 5.0, 6.0]],\n    [[7.0, 8.0, 9.0],\n      [10.0, 11.0, 12.0]]]\ntf.image.adjust_saturation(x, 0.5)\n<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=\narray([[[ 2. ,  2.5,  3. ],\n        [ 5. ,  5.5,  6. ]],\n       [[ 8. ,  8.5,  9. ],\n        [11. , 11.5, 12. ]]], dtype=float32)>\n\n \n\n\n Args\n  image   RGB image or images. The size of the last dimension must be 3.  \n  saturation_factor   float. Factor to multiply the saturation by.  \n  name   A name for this operation (optional).   \n \n\n\n Returns   Adjusted image(s), same shape and DType as image.  \n\n \n\n\n Raises\n  InvalidArgumentError   input must have 3 channels","title":"tensorflow.image.adjust_saturation"},{"text":"pandas.tseries.offsets.CustomBusinessDay.__call__   CustomBusinessDay.__call__(*args, **kwargs)\n \nCall self as a function.","title":"pandas.reference.api.pandas.tseries.offsets.custombusinessday.__call__"},{"text":"pandas.tseries.offsets.Day.name   Day.name","title":"pandas.reference.api.pandas.tseries.offsets.day.name"},{"text":"class TruncDay(expression, output_field=None, tzinfo=None, is_dst=None, **extra)  \n \nkind = 'day'","title":"django.ref.models.database-functions#django.db.models.functions.TruncDay"},{"text":"pandas.core.window.expanding.Expanding.sem   Expanding.sem(ddof=1, *args, **kwargs)[source]\n \nCalculate the expanding standard error of mean.  Parameters \n \nddof:int, default 1\n\n\nDelta Degrees of Freedom. The divisor used in calculations is N - ddof, where N represents the number of elements.  *args\n\nFor NumPy compatibility and will not have an effect on the result.  **kwargs\n\nFor NumPy compatibility and will not have an effect on the result.    Returns \n Series or DataFrame\n\nReturn type is the same as the original object with np.float64 dtype.      See also  pandas.Series.expanding\n\nCalling expanding with Series data.  pandas.DataFrame.expanding\n\nCalling expanding with DataFrames.  pandas.Series.sem\n\nAggregating sem for Series.  pandas.DataFrame.sem\n\nAggregating sem for DataFrame.    Notes A minimum of one period is required for the calculation. Examples \n>>> s = pd.Series([0, 1, 2, 3])\n  \n>>> s.expanding().sem()\n0         NaN\n1    0.707107\n2    0.707107\n3    0.745356\ndtype: float64","title":"pandas.reference.api.pandas.core.window.expanding.expanding.sem"},{"text":"pandas.tseries.offsets.CustomBusinessDay.n   CustomBusinessDay.n","title":"pandas.reference.api.pandas.tseries.offsets.custombusinessday.n"},{"text":"pandas.tseries.offsets.BusinessDay.__call__   BusinessDay.__call__(*args, **kwargs)\n \nCall self as a function.","title":"pandas.reference.api.pandas.tseries.offsets.businessday.__call__"}]}
{"task_id":4356842,"prompt":"def f_4356842():\n\treturn ","suffix":"","canonical_solution":"HttpResponse('Unauthorized', status=401)","test_start":"\nfrom django.http import HttpResponse\nfrom django.conf import settings\n\nif not settings.configured:\n    settings.configure(DEBUG=True)\n\ndef check(candidate):","test":["\n    assert candidate().status_code == 401\n"],"entry_point":"f_4356842","intent":"return a 401 unauthorized in django","library":["django"],"docs":[{"text":"Exceptions  Exceptions\u2026 allow error handling to be organized cleanly in a central or high-level place within the program structure. \u2014 Doug Hellmann, Python Exception Handling Techniques  Exception handling in REST framework views REST framework's views handle various exceptions, and deal with returning appropriate error responses. The handled exceptions are:  Subclasses of APIException raised inside REST framework. Django's Http404 exception. Django's PermissionDenied exception.  In each case, REST framework will return a response with an appropriate status code and content-type. The body of the response will include any additional details regarding the nature of the error. Most error responses will include a key detail in the body of the response. For example, the following request: DELETE http:\/\/api.example.com\/foo\/bar HTTP\/1.1\nAccept: application\/json\n Might receive an error response indicating that the DELETE method is not allowed on that resource: HTTP\/1.1 405 Method Not Allowed\nContent-Type: application\/json\nContent-Length: 42\n\n{\"detail\": \"Method 'DELETE' not allowed.\"}\n Validation errors are handled slightly differently, and will include the field names as the keys in the response. If the validation error was not specific to a particular field then it will use the \"non_field_errors\" key, or whatever string value has been set for the NON_FIELD_ERRORS_KEY setting. An example validation error might look like this: HTTP\/1.1 400 Bad Request\nContent-Type: application\/json\nContent-Length: 94\n\n{\"amount\": [\"A valid integer is required.\"], \"description\": [\"This field may not be blank.\"]}\n Custom exception handling You can implement custom exception handling by creating a handler function that converts exceptions raised in your API views into response objects. This allows you to control the style of error responses used by your API. The function must take a pair of arguments, the first is the exception to be handled, and the second is a dictionary containing any extra context such as the view currently being handled. The exception handler function should either return a Response object, or return None if the exception cannot be handled. If the handler returns None then the exception will be re-raised and Django will return a standard HTTP 500 'server error' response. For example, you might want to ensure that all error responses include the HTTP status code in the body of the response, like so: HTTP\/1.1 405 Method Not Allowed\nContent-Type: application\/json\nContent-Length: 62\n\n{\"status_code\": 405, \"detail\": \"Method 'DELETE' not allowed.\"}\n In order to alter the style of the response, you could write the following custom exception handler: from rest_framework.views import exception_handler\n\ndef custom_exception_handler(exc, context):\n    # Call REST framework's default exception handler first,\n    # to get the standard error response.\n    response = exception_handler(exc, context)\n\n    #\u00a0Now add the HTTP status code to the response.\n    if response is not None:\n        response.data['status_code'] = response.status_code\n\n    return response\n The context argument is not used by the default handler, but can be useful if the exception handler needs further information such as the view currently being handled, which can be accessed as context['view']. The exception handler must also be configured in your settings, using the EXCEPTION_HANDLER setting key. For example: REST_FRAMEWORK = {\n    'EXCEPTION_HANDLER': 'my_project.my_app.utils.custom_exception_handler'\n}\n If not specified, the 'EXCEPTION_HANDLER' setting defaults to the standard exception handler provided by REST framework: REST_FRAMEWORK = {\n    'EXCEPTION_HANDLER': 'rest_framework.views.exception_handler'\n}\n Note that the exception handler will only be called for responses generated by raised exceptions. It will not be used for any responses returned directly by the view, such as the HTTP_400_BAD_REQUEST responses that are returned by the generic views when serializer validation fails. API Reference APIException Signature: APIException() The base class for all exceptions raised inside an APIView class or @api_view. To provide a custom exception, subclass APIException and set the .status_code, .default_detail, and default_code attributes on the class. For example, if your API relies on a third party service that may sometimes be unreachable, you might want to implement an exception for the \"503 Service Unavailable\" HTTP response code. You could do this like so: from rest_framework.exceptions import APIException\n\nclass ServiceUnavailable(APIException):\n    status_code = 503\n    default_detail = 'Service temporarily unavailable, try again later.'\n    default_code = 'service_unavailable'\n Inspecting API exceptions There are a number of different properties available for inspecting the status of an API exception. You can use these to build custom exception handling for your project. The available attributes and methods are:  \n.detail - Return the textual description of the error. \n.get_codes() - Return the code identifier of the error. \n.get_full_details() - Return both the textual description and the code identifier.  In most cases the error detail will be a simple item: >>> print(exc.detail)\nYou do not have permission to perform this action.\n>>> print(exc.get_codes())\npermission_denied\n>>> print(exc.get_full_details())\n{'message':'You do not have permission to perform this action.','code':'permission_denied'}\n In the case of validation errors the error detail will be either a list or dictionary of items: >>> print(exc.detail)\n{\"name\":\"This field is required.\",\"age\":\"A valid integer is required.\"}\n>>> print(exc.get_codes())\n{\"name\":\"required\",\"age\":\"invalid\"}\n>>> print(exc.get_full_details())\n{\"name\":{\"message\":\"This field is required.\",\"code\":\"required\"},\"age\":{\"message\":\"A valid integer is required.\",\"code\":\"invalid\"}}\n ParseError Signature: ParseError(detail=None, code=None) Raised if the request contains malformed data when accessing request.data. By default this exception results in a response with the HTTP status code \"400 Bad Request\". AuthenticationFailed Signature: AuthenticationFailed(detail=None, code=None) Raised when an incoming request includes incorrect authentication. By default this exception results in a response with the HTTP status code \"401 Unauthenticated\", but it may also result in a \"403 Forbidden\" response, depending on the authentication scheme in use. See the authentication documentation for more details. NotAuthenticated Signature: NotAuthenticated(detail=None, code=None) Raised when an unauthenticated request fails the permission checks. By default this exception results in a response with the HTTP status code \"401 Unauthenticated\", but it may also result in a \"403 Forbidden\" response, depending on the authentication scheme in use. See the authentication documentation for more details. PermissionDenied Signature: PermissionDenied(detail=None, code=None) Raised when an authenticated request fails the permission checks. By default this exception results in a response with the HTTP status code \"403 Forbidden\". NotFound Signature: NotFound(detail=None, code=None) Raised when a resource does not exists at the given URL. This exception is equivalent to the standard Http404 Django exception. By default this exception results in a response with the HTTP status code \"404 Not Found\". MethodNotAllowed Signature: MethodNotAllowed(method, detail=None, code=None) Raised when an incoming request occurs that does not map to a handler method on the view. By default this exception results in a response with the HTTP status code \"405 Method Not Allowed\". NotAcceptable Signature: NotAcceptable(detail=None, code=None) Raised when an incoming request occurs with an Accept header that cannot be satisfied by any of the available renderers. By default this exception results in a response with the HTTP status code \"406 Not Acceptable\". UnsupportedMediaType Signature: UnsupportedMediaType(media_type, detail=None, code=None) Raised if there are no parsers that can handle the content type of the request data when accessing request.data. By default this exception results in a response with the HTTP status code \"415 Unsupported Media Type\". Throttled Signature: Throttled(wait=None, detail=None, code=None) Raised when an incoming request fails the throttling checks. By default this exception results in a response with the HTTP status code \"429 Too Many Requests\". ValidationError Signature: ValidationError(detail, code=None) The ValidationError exception is slightly different from the other APIException classes:  The detail argument is mandatory, not optional. The detail argument may be a list or dictionary of error details, and may also be a nested data structure. By using a dictionary, you can specify field-level errors while performing object-level validation in the validate() method of a serializer. For example. raise serializers.ValidationError({'name': 'Please enter a valid name.'})\n By convention you should import the serializers module and use a fully qualified ValidationError style, in order to differentiate it from Django's built-in validation error. For example. raise serializers.ValidationError('This field must be an integer value.')\n  The ValidationError class should be used for serializer and field validation, and by validator classes. It is also raised when calling serializer.is_valid with the raise_exception keyword argument: serializer.is_valid(raise_exception=True)\n The generic views use the raise_exception=True flag, which means that you can override the style of validation error responses globally in your API. To do so, use a custom exception handler, as described above. By default this exception results in a response with the HTTP status code \"400 Bad Request\". Generic Error Views Django REST Framework provides two error views suitable for providing generic JSON 500 Server Error and 400 Bad Request responses. (Django's default error views provide HTML responses, which may not be appropriate for an API-only application.) Use these as per Django's Customizing error views documentation. rest_framework.exceptions.server_error Returns a response with status code 500 and application\/json content type. Set as handler500: handler500 = 'rest_framework.exceptions.server_error'\n rest_framework.exceptions.bad_request Returns a response with status code 400 and application\/json content type. Set as handler400: handler400 = 'rest_framework.exceptions.bad_request'\n exceptions.py","title":"django_rest_framework.api-guide.exceptions.index#notauthenticated"},{"text":"HTTPBasicAuthHandler.http_error_401(req, fp, code, msg, hdrs)  \nRetry the request with authentication information, if available.","title":"python.library.urllib.request#urllib.request.HTTPBasicAuthHandler.http_error_401"},{"text":"HTTPDigestAuthHandler.http_error_401(req, fp, code, msg, hdrs)  \nRetry the request with authentication information, if available.","title":"python.library.urllib.request#urllib.request.HTTPDigestAuthHandler.http_error_401"},{"text":"Exceptions  Exceptions\u2026 allow error handling to be organized cleanly in a central or high-level place within the program structure. \u2014 Doug Hellmann, Python Exception Handling Techniques  Exception handling in REST framework views REST framework's views handle various exceptions, and deal with returning appropriate error responses. The handled exceptions are:  Subclasses of APIException raised inside REST framework. Django's Http404 exception. Django's PermissionDenied exception.  In each case, REST framework will return a response with an appropriate status code and content-type. The body of the response will include any additional details regarding the nature of the error. Most error responses will include a key detail in the body of the response. For example, the following request: DELETE http:\/\/api.example.com\/foo\/bar HTTP\/1.1\nAccept: application\/json\n Might receive an error response indicating that the DELETE method is not allowed on that resource: HTTP\/1.1 405 Method Not Allowed\nContent-Type: application\/json\nContent-Length: 42\n\n{\"detail\": \"Method 'DELETE' not allowed.\"}\n Validation errors are handled slightly differently, and will include the field names as the keys in the response. If the validation error was not specific to a particular field then it will use the \"non_field_errors\" key, or whatever string value has been set for the NON_FIELD_ERRORS_KEY setting. An example validation error might look like this: HTTP\/1.1 400 Bad Request\nContent-Type: application\/json\nContent-Length: 94\n\n{\"amount\": [\"A valid integer is required.\"], \"description\": [\"This field may not be blank.\"]}\n Custom exception handling You can implement custom exception handling by creating a handler function that converts exceptions raised in your API views into response objects. This allows you to control the style of error responses used by your API. The function must take a pair of arguments, the first is the exception to be handled, and the second is a dictionary containing any extra context such as the view currently being handled. The exception handler function should either return a Response object, or return None if the exception cannot be handled. If the handler returns None then the exception will be re-raised and Django will return a standard HTTP 500 'server error' response. For example, you might want to ensure that all error responses include the HTTP status code in the body of the response, like so: HTTP\/1.1 405 Method Not Allowed\nContent-Type: application\/json\nContent-Length: 62\n\n{\"status_code\": 405, \"detail\": \"Method 'DELETE' not allowed.\"}\n In order to alter the style of the response, you could write the following custom exception handler: from rest_framework.views import exception_handler\n\ndef custom_exception_handler(exc, context):\n    # Call REST framework's default exception handler first,\n    # to get the standard error response.\n    response = exception_handler(exc, context)\n\n    #\u00a0Now add the HTTP status code to the response.\n    if response is not None:\n        response.data['status_code'] = response.status_code\n\n    return response\n The context argument is not used by the default handler, but can be useful if the exception handler needs further information such as the view currently being handled, which can be accessed as context['view']. The exception handler must also be configured in your settings, using the EXCEPTION_HANDLER setting key. For example: REST_FRAMEWORK = {\n    'EXCEPTION_HANDLER': 'my_project.my_app.utils.custom_exception_handler'\n}\n If not specified, the 'EXCEPTION_HANDLER' setting defaults to the standard exception handler provided by REST framework: REST_FRAMEWORK = {\n    'EXCEPTION_HANDLER': 'rest_framework.views.exception_handler'\n}\n Note that the exception handler will only be called for responses generated by raised exceptions. It will not be used for any responses returned directly by the view, such as the HTTP_400_BAD_REQUEST responses that are returned by the generic views when serializer validation fails. API Reference APIException Signature: APIException() The base class for all exceptions raised inside an APIView class or @api_view. To provide a custom exception, subclass APIException and set the .status_code, .default_detail, and default_code attributes on the class. For example, if your API relies on a third party service that may sometimes be unreachable, you might want to implement an exception for the \"503 Service Unavailable\" HTTP response code. You could do this like so: from rest_framework.exceptions import APIException\n\nclass ServiceUnavailable(APIException):\n    status_code = 503\n    default_detail = 'Service temporarily unavailable, try again later.'\n    default_code = 'service_unavailable'\n Inspecting API exceptions There are a number of different properties available for inspecting the status of an API exception. You can use these to build custom exception handling for your project. The available attributes and methods are:  \n.detail - Return the textual description of the error. \n.get_codes() - Return the code identifier of the error. \n.get_full_details() - Return both the textual description and the code identifier.  In most cases the error detail will be a simple item: >>> print(exc.detail)\nYou do not have permission to perform this action.\n>>> print(exc.get_codes())\npermission_denied\n>>> print(exc.get_full_details())\n{'message':'You do not have permission to perform this action.','code':'permission_denied'}\n In the case of validation errors the error detail will be either a list or dictionary of items: >>> print(exc.detail)\n{\"name\":\"This field is required.\",\"age\":\"A valid integer is required.\"}\n>>> print(exc.get_codes())\n{\"name\":\"required\",\"age\":\"invalid\"}\n>>> print(exc.get_full_details())\n{\"name\":{\"message\":\"This field is required.\",\"code\":\"required\"},\"age\":{\"message\":\"A valid integer is required.\",\"code\":\"invalid\"}}\n ParseError Signature: ParseError(detail=None, code=None) Raised if the request contains malformed data when accessing request.data. By default this exception results in a response with the HTTP status code \"400 Bad Request\". AuthenticationFailed Signature: AuthenticationFailed(detail=None, code=None) Raised when an incoming request includes incorrect authentication. By default this exception results in a response with the HTTP status code \"401 Unauthenticated\", but it may also result in a \"403 Forbidden\" response, depending on the authentication scheme in use. See the authentication documentation for more details. NotAuthenticated Signature: NotAuthenticated(detail=None, code=None) Raised when an unauthenticated request fails the permission checks. By default this exception results in a response with the HTTP status code \"401 Unauthenticated\", but it may also result in a \"403 Forbidden\" response, depending on the authentication scheme in use. See the authentication documentation for more details. PermissionDenied Signature: PermissionDenied(detail=None, code=None) Raised when an authenticated request fails the permission checks. By default this exception results in a response with the HTTP status code \"403 Forbidden\". NotFound Signature: NotFound(detail=None, code=None) Raised when a resource does not exists at the given URL. This exception is equivalent to the standard Http404 Django exception. By default this exception results in a response with the HTTP status code \"404 Not Found\". MethodNotAllowed Signature: MethodNotAllowed(method, detail=None, code=None) Raised when an incoming request occurs that does not map to a handler method on the view. By default this exception results in a response with the HTTP status code \"405 Method Not Allowed\". NotAcceptable Signature: NotAcceptable(detail=None, code=None) Raised when an incoming request occurs with an Accept header that cannot be satisfied by any of the available renderers. By default this exception results in a response with the HTTP status code \"406 Not Acceptable\". UnsupportedMediaType Signature: UnsupportedMediaType(media_type, detail=None, code=None) Raised if there are no parsers that can handle the content type of the request data when accessing request.data. By default this exception results in a response with the HTTP status code \"415 Unsupported Media Type\". Throttled Signature: Throttled(wait=None, detail=None, code=None) Raised when an incoming request fails the throttling checks. By default this exception results in a response with the HTTP status code \"429 Too Many Requests\". ValidationError Signature: ValidationError(detail, code=None) The ValidationError exception is slightly different from the other APIException classes:  The detail argument is mandatory, not optional. The detail argument may be a list or dictionary of error details, and may also be a nested data structure. By using a dictionary, you can specify field-level errors while performing object-level validation in the validate() method of a serializer. For example. raise serializers.ValidationError({'name': 'Please enter a valid name.'})\n By convention you should import the serializers module and use a fully qualified ValidationError style, in order to differentiate it from Django's built-in validation error. For example. raise serializers.ValidationError('This field must be an integer value.')\n  The ValidationError class should be used for serializer and field validation, and by validator classes. It is also raised when calling serializer.is_valid with the raise_exception keyword argument: serializer.is_valid(raise_exception=True)\n The generic views use the raise_exception=True flag, which means that you can override the style of validation error responses globally in your API. To do so, use a custom exception handler, as described above. By default this exception results in a response with the HTTP status code \"400 Bad Request\". Generic Error Views Django REST Framework provides two error views suitable for providing generic JSON 500 Server Error and 400 Bad Request responses. (Django's default error views provide HTML responses, which may not be appropriate for an API-only application.) Use these as per Django's Customizing error views documentation. rest_framework.exceptions.server_error Returns a response with status code 500 and application\/json content type. Set as handler500: handler500 = 'rest_framework.exceptions.server_error'\n rest_framework.exceptions.bad_request Returns a response with status code 400 and application\/json content type. Set as handler400: handler400 = 'rest_framework.exceptions.bad_request'\n exceptions.py","title":"django_rest_framework.api-guide.exceptions.index#authenticationfailed"},{"text":"Authentication  Auth needs to be pluggable. \u2014 Jacob Kaplan-Moss, \"REST worst practices\"  Authentication is the mechanism of associating an incoming request with a set of identifying credentials, such as the user the request came from, or the token that it was signed with. The permission and throttling policies can then use those credentials to determine if the request should be permitted. REST framework provides several authentication schemes out of the box, and also allows you to implement custom schemes. Authentication always runs at the very start of the view, before the permission and throttling checks occur, and before any other code is allowed to proceed. The request.user property will typically be set to an instance of the contrib.auth package's User class. The request.auth property is used for any additional authentication information, for example, it may be used to represent an authentication token that the request was signed with. Note: Don't forget that authentication by itself won't allow or disallow an incoming request, it simply identifies the credentials that the request was made with. For information on how to set up the permission policies for your API please see the permissions documentation. How authentication is determined The authentication schemes are always defined as a list of classes. REST framework will attempt to authenticate with each class in the list, and will set request.user and request.auth using the return value of the first class that successfully authenticates. If no class authenticates, request.user will be set to an instance of django.contrib.auth.models.AnonymousUser, and request.auth will be set to None. The value of request.user and request.auth for unauthenticated requests can be modified using the UNAUTHENTICATED_USER and UNAUTHENTICATED_TOKEN settings. Setting the authentication scheme The default authentication schemes may be set globally, using the DEFAULT_AUTHENTICATION_CLASSES setting. For example. REST_FRAMEWORK = {\n    'DEFAULT_AUTHENTICATION_CLASSES': [\n        'rest_framework.authentication.BasicAuthentication',\n        'rest_framework.authentication.SessionAuthentication',\n    ]\n}\n You can also set the authentication scheme on a per-view or per-viewset basis, using the APIView class-based views. from rest_framework.authentication import SessionAuthentication, BasicAuthentication\nfrom rest_framework.permissions import IsAuthenticated\nfrom rest_framework.response import Response\nfrom rest_framework.views import APIView\n\nclass ExampleView(APIView):\n    authentication_classes = [SessionAuthentication, BasicAuthentication]\n    permission_classes = [IsAuthenticated]\n\n    def get(self, request, format=None):\n        content = {\n            'user': str(request.user),  # `django.contrib.auth.User` instance.\n            'auth': str(request.auth),  # None\n        }\n        return Response(content)\n Or, if you're using the @api_view decorator with function based views. @api_view(['GET'])\n@authentication_classes([SessionAuthentication, BasicAuthentication])\n@permission_classes([IsAuthenticated])\ndef example_view(request, format=None):\n    content = {\n        'user': str(request.user),  # `django.contrib.auth.User` instance.\n        'auth': str(request.auth),  # None\n    }\n    return Response(content)\n Unauthorized and Forbidden responses When an unauthenticated request is denied permission there are two different error codes that may be appropriate.  HTTP 401 Unauthorized HTTP 403 Permission Denied  HTTP 401 responses must always include a WWW-Authenticate header, that instructs the client how to authenticate. HTTP 403 responses do not include the WWW-Authenticate header. The kind of response that will be used depends on the authentication scheme. Although multiple authentication schemes may be in use, only one scheme may be used to determine the type of response. The first authentication class set on the view is used when determining the type of response. Note that when a request may successfully authenticate, but still be denied permission to perform the request, in which case a 403 Permission Denied response will always be used, regardless of the authentication scheme. Apache mod_wsgi specific configuration Note that if deploying to Apache using mod_wsgi, the authorization header is not passed through to a WSGI application by default, as it is assumed that authentication will be handled by Apache, rather than at an application level. If you are deploying to Apache, and using any non-session based authentication, you will need to explicitly configure mod_wsgi to pass the required headers through to the application. This can be done by specifying the WSGIPassAuthorization directive in the appropriate context and setting it to 'On'. # this can go in either server config, virtual host, directory or .htaccess\nWSGIPassAuthorization On\n API Reference BasicAuthentication This authentication scheme uses HTTP Basic Authentication, signed against a user's username and password. Basic authentication is generally only appropriate for testing. If successfully authenticated, BasicAuthentication provides the following credentials.  \nrequest.user will be a Django User instance. \nrequest.auth will be None.  Unauthenticated responses that are denied permission will result in an HTTP 401 Unauthorized response with an appropriate WWW-Authenticate header. For example: WWW-Authenticate: Basic realm=\"api\"\n Note: If you use BasicAuthentication in production you must ensure that your API is only available over https. You should also ensure that your API clients will always re-request the username and password at login, and will never store those details to persistent storage. TokenAuthentication This authentication scheme uses a simple token-based HTTP Authentication scheme. Token authentication is appropriate for client-server setups, such as native desktop and mobile clients. To use the TokenAuthentication scheme you'll need to configure the authentication classes to include TokenAuthentication, and additionally include rest_framework.authtoken in your INSTALLED_APPS setting: INSTALLED_APPS = [\n    ...\n    'rest_framework.authtoken'\n]\n Note: Make sure to run manage.py migrate after changing your settings. The rest_framework.authtoken app provides Django database migrations. You'll also need to create tokens for your users. from rest_framework.authtoken.models import Token\n\ntoken = Token.objects.create(user=...)\nprint(token.key)\n For clients to authenticate, the token key should be included in the Authorization HTTP header. The key should be prefixed by the string literal \"Token\", with whitespace separating the two strings. For example: Authorization: Token 9944b09199c62bcf9418ad846dd0e4bbdfc6ee4b\n Note: If you want to use a different keyword in the header, such as Bearer, simply subclass TokenAuthentication and set the keyword class variable. If successfully authenticated, TokenAuthentication provides the following credentials.  \nrequest.user will be a Django User instance. \nrequest.auth will be a rest_framework.authtoken.models.Token instance.  Unauthenticated responses that are denied permission will result in an HTTP 401 Unauthorized response with an appropriate WWW-Authenticate header. For example: WWW-Authenticate: Token\n The curl command line tool may be useful for testing token authenticated APIs. For example: curl -X GET http:\/\/127.0.0.1:8000\/api\/example\/ -H 'Authorization: Token 9944b09199c62bcf9418ad846dd0e4bbdfc6ee4b'\n Note: If you use TokenAuthentication in production you must ensure that your API is only available over https. Generating Tokens By using signals If you want every user to have an automatically generated Token, you can simply catch the User's post_save signal. from django.conf import settings\nfrom django.db.models.signals import post_save\nfrom django.dispatch import receiver\nfrom rest_framework.authtoken.models import Token\n\n@receiver(post_save, sender=settings.AUTH_USER_MODEL)\ndef create_auth_token(sender, instance=None, created=False, **kwargs):\n    if created:\n        Token.objects.create(user=instance)\n Note that you'll want to ensure you place this code snippet in an installed models.py module, or some other location that will be imported by Django on startup. If you've already created some users, you can generate tokens for all existing users like this: from django.contrib.auth.models import User\nfrom rest_framework.authtoken.models import Token\n\nfor user in User.objects.all():\n    Token.objects.get_or_create(user=user)\n By exposing an api endpoint When using TokenAuthentication, you may want to provide a mechanism for clients to obtain a token given the username and password. REST framework provides a built-in view to provide this behaviour. To use it, add the obtain_auth_token view to your URLconf: from rest_framework.authtoken import views\nurlpatterns += [\n    path('api-token-auth\/', views.obtain_auth_token)\n]\n Note that the URL part of the pattern can be whatever you want to use. The obtain_auth_token view will return a JSON response when valid username and password fields are POSTed to the view using form data or JSON: { 'token' : '9944b09199c62bcf9418ad846dd0e4bbdfc6ee4b' }\n Note that the default obtain_auth_token view explicitly uses JSON requests and responses, rather than using default renderer and parser classes in your settings. By default, there are no permissions or throttling applied to the obtain_auth_token view. If you do wish to apply to throttle you'll need to override the view class, and include them using the throttle_classes attribute. If you need a customized version of the obtain_auth_token view, you can do so by subclassing the ObtainAuthToken view class, and using that in your url conf instead. For example, you may return additional user information beyond the token value: from rest_framework.authtoken.views import ObtainAuthToken\nfrom rest_framework.authtoken.models import Token\nfrom rest_framework.response import Response\n\nclass CustomAuthToken(ObtainAuthToken):\n\n    def post(self, request, *args, **kwargs):\n        serializer = self.serializer_class(data=request.data,\n                                           context={'request': request})\n        serializer.is_valid(raise_exception=True)\n        user = serializer.validated_data['user']\n        token, created = Token.objects.get_or_create(user=user)\n        return Response({\n            'token': token.key,\n            'user_id': user.pk,\n            'email': user.email\n        })\n And in your urls.py: urlpatterns += [\n    path('api-token-auth\/', CustomAuthToken.as_view())\n]\n With Django admin It is also possible to create Tokens manually through the admin interface. In case you are using a large user base, we recommend that you monkey patch the TokenAdmin class customize it to your needs, more specifically by declaring the user field as raw_field. your_app\/admin.py: from rest_framework.authtoken.admin import TokenAdmin\n\nTokenAdmin.raw_id_fields = ['user']\n Using Django manage.py command Since version 3.6.4 it's possible to generate a user token using the following command: .\/manage.py drf_create_token <username>\n this command will return the API token for the given user, creating it if it doesn't exist: Generated token 9944b09199c62bcf9418ad846dd0e4bbdfc6ee4b for user user1\n In case you want to regenerate the token (for example if it has been compromised or leaked) you can pass an additional parameter: .\/manage.py drf_create_token -r <username>\n SessionAuthentication This authentication scheme uses Django's default session backend for authentication. Session authentication is appropriate for AJAX clients that are running in the same session context as your website. If successfully authenticated, SessionAuthentication provides the following credentials.  \nrequest.user will be a Django User instance. \nrequest.auth will be None.  Unauthenticated responses that are denied permission will result in an HTTP 403 Forbidden response. If you're using an AJAX-style API with SessionAuthentication, you'll need to make sure you include a valid CSRF token for any \"unsafe\" HTTP method calls, such as PUT, PATCH, POST or DELETE requests. See the Django CSRF documentation for more details. Warning: Always use Django's standard login view when creating login pages. This will ensure your login views are properly protected. CSRF validation in REST framework works slightly differently from standard Django due to the need to support both session and non-session based authentication to the same views. This means that only authenticated requests require CSRF tokens, and anonymous requests may be sent without CSRF tokens. This behaviour is not suitable for login views, which should always have CSRF validation applied. RemoteUserAuthentication This authentication scheme allows you to delegate authentication to your web server, which sets the REMOTE_USER environment variable. To use it, you must have django.contrib.auth.backends.RemoteUserBackend (or a subclass) in your AUTHENTICATION_BACKENDS setting. By default, RemoteUserBackend creates User objects for usernames that don't already exist. To change this and other behaviour, consult the Django documentation. If successfully authenticated, RemoteUserAuthentication provides the following credentials:  \nrequest.user will be a Django User instance. \nrequest.auth will be None.  Consult your web server's documentation for information about configuring an authentication method, e.g.:  Apache Authentication How-To NGINX (Restricting Access)  Custom authentication To implement a custom authentication scheme, subclass BaseAuthentication and override the .authenticate(self, request) method. The method should return a two-tuple of (user, auth) if authentication succeeds, or None otherwise. In some circumstances instead of returning None, you may want to raise an AuthenticationFailed exception from the .authenticate() method. Typically the approach you should take is:  If authentication is not attempted, return None. Any other authentication schemes also in use will still be checked. If authentication is attempted but fails, raise an AuthenticationFailed exception. An error response will be returned immediately, regardless of any permissions checks, and without checking any other authentication schemes.  You may also override the .authenticate_header(self, request) method. If implemented, it should return a string that will be used as the value of the WWW-Authenticate header in a HTTP 401 Unauthorized response. If the .authenticate_header() method is not overridden, the authentication scheme will return HTTP 403 Forbidden responses when an unauthenticated request is denied access. Note: When your custom authenticator is invoked by the request object's .user or .auth properties, you may see an AttributeError re-raised as a WrappedAttributeError. This is necessary to prevent the original exception from being suppressed by the outer property access. Python will not recognize that the AttributeError originates from your custom authenticator and will instead assume that the request object does not have a .user or .auth property. These errors should be fixed or otherwise handled by your authenticator. Example The following example will authenticate any incoming request as the user given by the username in a custom request header named 'X-USERNAME'. from django.contrib.auth.models import User\nfrom rest_framework import authentication\nfrom rest_framework import exceptions\n\nclass ExampleAuthentication(authentication.BaseAuthentication):\n    def authenticate(self, request):\n        username = request.META.get('HTTP_X_USERNAME')\n        if not username:\n            return None\n\n        try:\n            user = User.objects.get(username=username)\n        except User.DoesNotExist:\n            raise exceptions.AuthenticationFailed('No such user')\n\n        return (user, None)\n Third party packages The following third-party packages are also available. Django OAuth Toolkit The Django OAuth Toolkit package provides OAuth 2.0 support and works with Python 3.4+. The package is maintained by jazzband and uses the excellent OAuthLib. The package is well documented, and well supported and is currently our recommended package for OAuth 2.0 support. Installation & configuration Install using pip. pip install django-oauth-toolkit\n Add the package to your INSTALLED_APPS and modify your REST framework settings. INSTALLED_APPS = [\n    ...\n    'oauth2_provider',\n]\n\nREST_FRAMEWORK = {\n    'DEFAULT_AUTHENTICATION_CLASSES': [\n        'oauth2_provider.contrib.rest_framework.OAuth2Authentication',\n    ]\n}\n For more details see the Django REST framework - Getting started documentation. Django REST framework OAuth The Django REST framework OAuth package provides both OAuth1 and OAuth2 support for REST framework. This package was previously included directly in the REST framework but is now supported and maintained as a third-party package. Installation & configuration Install the package using pip. pip install djangorestframework-oauth\n For details on configuration and usage see the Django REST framework OAuth documentation for authentication and permissions. JSON Web Token Authentication JSON Web Token is a fairly new standard which can be used for token-based authentication. Unlike the built-in TokenAuthentication scheme, JWT Authentication doesn't need to use a database to validate a token. A package for JWT authentication is djangorestframework-simplejwt which provides some features as well as a pluggable token blacklist app. Hawk HTTP Authentication The HawkREST library builds on the Mohawk library to let you work with Hawk signed requests and responses in your API. Hawk lets two parties securely communicate with each other using messages signed by a shared key. It is based on HTTP MAC access authentication (which was based on parts of OAuth 1.0). HTTP Signature Authentication HTTP Signature (currently a IETF draft) provides a way to achieve origin authentication and message integrity for HTTP messages. Similar to Amazon's HTTP Signature scheme, used by many of its services, it permits stateless, per-request authentication. Elvio Toccalino maintains the djangorestframework-httpsignature (outdated) package which provides an easy to use HTTP Signature Authentication mechanism. You can use the updated fork version of djangorestframework-httpsignature, which is drf-httpsig. Djoser Djoser library provides a set of views to handle basic actions such as registration, login, logout, password reset and account activation. The package works with a custom user model and uses token-based authentication. This is ready to use REST implementation of the Django authentication system. django-rest-auth \/ dj-rest-auth This library provides a set of REST API endpoints for registration, authentication (including social media authentication), password reset, retrieve and update user details, etc. By having these API endpoints, your client apps such as AngularJS, iOS, Android, and others can communicate to your Django backend site independently via REST APIs for user management. There are currently two forks of this project.  \nDjango-rest-auth is the original project, but is not currently receiving updates. \nDj-rest-auth is a newer fork of the project.  django-rest-framework-social-oauth2 Django-rest-framework-social-oauth2 library provides an easy way to integrate social plugins (facebook, twitter, google, etc.) to your authentication system and an easy oauth2 setup. With this library, you will be able to authenticate users based on external tokens (e.g. facebook access token), convert these tokens to \"in-house\" oauth2 tokens and use and generate oauth2 tokens to authenticate your users. django-rest-knox Django-rest-knox library provides models and views to handle token-based authentication in a more secure and extensible way than the built-in TokenAuthentication scheme - with Single Page Applications and Mobile clients in mind. It provides per-client tokens, and views to generate them when provided some other authentication (usually basic authentication), to delete the token (providing a server enforced logout) and to delete all tokens (logs out all clients that a user is logged into). drfpasswordless drfpasswordless adds (Medium, Square Cash inspired) passwordless support to Django REST Framework's TokenAuthentication scheme. Users log in and sign up with a token sent to a contact point like an email address or a mobile number. django-rest-authemail django-rest-authemail provides a RESTful API interface for user signup and authentication. Email addresses are used for authentication, rather than usernames. API endpoints are available for signup, signup email verification, login, logout, password reset, password reset verification, email change, email change verification, password change, and user detail. A fully functional example project and detailed instructions are included. Django-Rest-Durin Django-Rest-Durin is built with the idea to have one library that does token auth for multiple Web\/CLI\/Mobile API clients via one interface but allows different token configuration for each API Client that consumes the API. It provides support for multiple tokens per user via custom models, views, permissions that work with Django-Rest-Framework. The token expiration time can be different per API client and is customizable via the Django Admin Interface. More information can be found in the Documentation. authentication.py","title":"django_rest_framework.api-guide.authentication.index#tokenauthentication"},{"text":"Exceptions  Exceptions\u2026 allow error handling to be organized cleanly in a central or high-level place within the program structure. \u2014 Doug Hellmann, Python Exception Handling Techniques  Exception handling in REST framework views REST framework's views handle various exceptions, and deal with returning appropriate error responses. The handled exceptions are:  Subclasses of APIException raised inside REST framework. Django's Http404 exception. Django's PermissionDenied exception.  In each case, REST framework will return a response with an appropriate status code and content-type. The body of the response will include any additional details regarding the nature of the error. Most error responses will include a key detail in the body of the response. For example, the following request: DELETE http:\/\/api.example.com\/foo\/bar HTTP\/1.1\nAccept: application\/json\n Might receive an error response indicating that the DELETE method is not allowed on that resource: HTTP\/1.1 405 Method Not Allowed\nContent-Type: application\/json\nContent-Length: 42\n\n{\"detail\": \"Method 'DELETE' not allowed.\"}\n Validation errors are handled slightly differently, and will include the field names as the keys in the response. If the validation error was not specific to a particular field then it will use the \"non_field_errors\" key, or whatever string value has been set for the NON_FIELD_ERRORS_KEY setting. An example validation error might look like this: HTTP\/1.1 400 Bad Request\nContent-Type: application\/json\nContent-Length: 94\n\n{\"amount\": [\"A valid integer is required.\"], \"description\": [\"This field may not be blank.\"]}\n Custom exception handling You can implement custom exception handling by creating a handler function that converts exceptions raised in your API views into response objects. This allows you to control the style of error responses used by your API. The function must take a pair of arguments, the first is the exception to be handled, and the second is a dictionary containing any extra context such as the view currently being handled. The exception handler function should either return a Response object, or return None if the exception cannot be handled. If the handler returns None then the exception will be re-raised and Django will return a standard HTTP 500 'server error' response. For example, you might want to ensure that all error responses include the HTTP status code in the body of the response, like so: HTTP\/1.1 405 Method Not Allowed\nContent-Type: application\/json\nContent-Length: 62\n\n{\"status_code\": 405, \"detail\": \"Method 'DELETE' not allowed.\"}\n In order to alter the style of the response, you could write the following custom exception handler: from rest_framework.views import exception_handler\n\ndef custom_exception_handler(exc, context):\n    # Call REST framework's default exception handler first,\n    # to get the standard error response.\n    response = exception_handler(exc, context)\n\n    #\u00a0Now add the HTTP status code to the response.\n    if response is not None:\n        response.data['status_code'] = response.status_code\n\n    return response\n The context argument is not used by the default handler, but can be useful if the exception handler needs further information such as the view currently being handled, which can be accessed as context['view']. The exception handler must also be configured in your settings, using the EXCEPTION_HANDLER setting key. For example: REST_FRAMEWORK = {\n    'EXCEPTION_HANDLER': 'my_project.my_app.utils.custom_exception_handler'\n}\n If not specified, the 'EXCEPTION_HANDLER' setting defaults to the standard exception handler provided by REST framework: REST_FRAMEWORK = {\n    'EXCEPTION_HANDLER': 'rest_framework.views.exception_handler'\n}\n Note that the exception handler will only be called for responses generated by raised exceptions. It will not be used for any responses returned directly by the view, such as the HTTP_400_BAD_REQUEST responses that are returned by the generic views when serializer validation fails. API Reference APIException Signature: APIException() The base class for all exceptions raised inside an APIView class or @api_view. To provide a custom exception, subclass APIException and set the .status_code, .default_detail, and default_code attributes on the class. For example, if your API relies on a third party service that may sometimes be unreachable, you might want to implement an exception for the \"503 Service Unavailable\" HTTP response code. You could do this like so: from rest_framework.exceptions import APIException\n\nclass ServiceUnavailable(APIException):\n    status_code = 503\n    default_detail = 'Service temporarily unavailable, try again later.'\n    default_code = 'service_unavailable'\n Inspecting API exceptions There are a number of different properties available for inspecting the status of an API exception. You can use these to build custom exception handling for your project. The available attributes and methods are:  \n.detail - Return the textual description of the error. \n.get_codes() - Return the code identifier of the error. \n.get_full_details() - Return both the textual description and the code identifier.  In most cases the error detail will be a simple item: >>> print(exc.detail)\nYou do not have permission to perform this action.\n>>> print(exc.get_codes())\npermission_denied\n>>> print(exc.get_full_details())\n{'message':'You do not have permission to perform this action.','code':'permission_denied'}\n In the case of validation errors the error detail will be either a list or dictionary of items: >>> print(exc.detail)\n{\"name\":\"This field is required.\",\"age\":\"A valid integer is required.\"}\n>>> print(exc.get_codes())\n{\"name\":\"required\",\"age\":\"invalid\"}\n>>> print(exc.get_full_details())\n{\"name\":{\"message\":\"This field is required.\",\"code\":\"required\"},\"age\":{\"message\":\"A valid integer is required.\",\"code\":\"invalid\"}}\n ParseError Signature: ParseError(detail=None, code=None) Raised if the request contains malformed data when accessing request.data. By default this exception results in a response with the HTTP status code \"400 Bad Request\". AuthenticationFailed Signature: AuthenticationFailed(detail=None, code=None) Raised when an incoming request includes incorrect authentication. By default this exception results in a response with the HTTP status code \"401 Unauthenticated\", but it may also result in a \"403 Forbidden\" response, depending on the authentication scheme in use. See the authentication documentation for more details. NotAuthenticated Signature: NotAuthenticated(detail=None, code=None) Raised when an unauthenticated request fails the permission checks. By default this exception results in a response with the HTTP status code \"401 Unauthenticated\", but it may also result in a \"403 Forbidden\" response, depending on the authentication scheme in use. See the authentication documentation for more details. PermissionDenied Signature: PermissionDenied(detail=None, code=None) Raised when an authenticated request fails the permission checks. By default this exception results in a response with the HTTP status code \"403 Forbidden\". NotFound Signature: NotFound(detail=None, code=None) Raised when a resource does not exists at the given URL. This exception is equivalent to the standard Http404 Django exception. By default this exception results in a response with the HTTP status code \"404 Not Found\". MethodNotAllowed Signature: MethodNotAllowed(method, detail=None, code=None) Raised when an incoming request occurs that does not map to a handler method on the view. By default this exception results in a response with the HTTP status code \"405 Method Not Allowed\". NotAcceptable Signature: NotAcceptable(detail=None, code=None) Raised when an incoming request occurs with an Accept header that cannot be satisfied by any of the available renderers. By default this exception results in a response with the HTTP status code \"406 Not Acceptable\". UnsupportedMediaType Signature: UnsupportedMediaType(media_type, detail=None, code=None) Raised if there are no parsers that can handle the content type of the request data when accessing request.data. By default this exception results in a response with the HTTP status code \"415 Unsupported Media Type\". Throttled Signature: Throttled(wait=None, detail=None, code=None) Raised when an incoming request fails the throttling checks. By default this exception results in a response with the HTTP status code \"429 Too Many Requests\". ValidationError Signature: ValidationError(detail, code=None) The ValidationError exception is slightly different from the other APIException classes:  The detail argument is mandatory, not optional. The detail argument may be a list or dictionary of error details, and may also be a nested data structure. By using a dictionary, you can specify field-level errors while performing object-level validation in the validate() method of a serializer. For example. raise serializers.ValidationError({'name': 'Please enter a valid name.'})\n By convention you should import the serializers module and use a fully qualified ValidationError style, in order to differentiate it from Django's built-in validation error. For example. raise serializers.ValidationError('This field must be an integer value.')\n  The ValidationError class should be used for serializer and field validation, and by validator classes. It is also raised when calling serializer.is_valid with the raise_exception keyword argument: serializer.is_valid(raise_exception=True)\n The generic views use the raise_exception=True flag, which means that you can override the style of validation error responses globally in your API. To do so, use a custom exception handler, as described above. By default this exception results in a response with the HTTP status code \"400 Bad Request\". Generic Error Views Django REST Framework provides two error views suitable for providing generic JSON 500 Server Error and 400 Bad Request responses. (Django's default error views provide HTML responses, which may not be appropriate for an API-only application.) Use these as per Django's Customizing error views documentation. rest_framework.exceptions.server_error Returns a response with status code 500 and application\/json content type. Set as handler500: handler500 = 'rest_framework.exceptions.server_error'\n rest_framework.exceptions.bad_request Returns a response with status code 400 and application\/json content type. Set as handler400: handler400 = 'rest_framework.exceptions.bad_request'\n exceptions.py","title":"django_rest_framework.api-guide.exceptions.index#permissiondenied"},{"text":"defaults.permission_denied(request, exception, template_name='403.html')","title":"django.ref.views#django.views.defaults.permission_denied"},{"text":"AbstractDigestAuthHandler.http_error_auth_reqed(authreq, host, req, headers)  \nauthreq should be the name of the header where the information about the realm is included in the request, host should be the host to authenticate to, req should be the (failed) Request object, and headers should be the error headers.","title":"python.library.urllib.request#urllib.request.AbstractDigestAuthHandler.http_error_auth_reqed"},{"text":"class AuthenticationMiddleware","title":"django.ref.middleware#django.contrib.auth.middleware.AuthenticationMiddleware"},{"text":"Authentication  Auth needs to be pluggable. \u2014 Jacob Kaplan-Moss, \"REST worst practices\"  Authentication is the mechanism of associating an incoming request with a set of identifying credentials, such as the user the request came from, or the token that it was signed with. The permission and throttling policies can then use those credentials to determine if the request should be permitted. REST framework provides several authentication schemes out of the box, and also allows you to implement custom schemes. Authentication always runs at the very start of the view, before the permission and throttling checks occur, and before any other code is allowed to proceed. The request.user property will typically be set to an instance of the contrib.auth package's User class. The request.auth property is used for any additional authentication information, for example, it may be used to represent an authentication token that the request was signed with. Note: Don't forget that authentication by itself won't allow or disallow an incoming request, it simply identifies the credentials that the request was made with. For information on how to set up the permission policies for your API please see the permissions documentation. How authentication is determined The authentication schemes are always defined as a list of classes. REST framework will attempt to authenticate with each class in the list, and will set request.user and request.auth using the return value of the first class that successfully authenticates. If no class authenticates, request.user will be set to an instance of django.contrib.auth.models.AnonymousUser, and request.auth will be set to None. The value of request.user and request.auth for unauthenticated requests can be modified using the UNAUTHENTICATED_USER and UNAUTHENTICATED_TOKEN settings. Setting the authentication scheme The default authentication schemes may be set globally, using the DEFAULT_AUTHENTICATION_CLASSES setting. For example. REST_FRAMEWORK = {\n    'DEFAULT_AUTHENTICATION_CLASSES': [\n        'rest_framework.authentication.BasicAuthentication',\n        'rest_framework.authentication.SessionAuthentication',\n    ]\n}\n You can also set the authentication scheme on a per-view or per-viewset basis, using the APIView class-based views. from rest_framework.authentication import SessionAuthentication, BasicAuthentication\nfrom rest_framework.permissions import IsAuthenticated\nfrom rest_framework.response import Response\nfrom rest_framework.views import APIView\n\nclass ExampleView(APIView):\n    authentication_classes = [SessionAuthentication, BasicAuthentication]\n    permission_classes = [IsAuthenticated]\n\n    def get(self, request, format=None):\n        content = {\n            'user': str(request.user),  # `django.contrib.auth.User` instance.\n            'auth': str(request.auth),  # None\n        }\n        return Response(content)\n Or, if you're using the @api_view decorator with function based views. @api_view(['GET'])\n@authentication_classes([SessionAuthentication, BasicAuthentication])\n@permission_classes([IsAuthenticated])\ndef example_view(request, format=None):\n    content = {\n        'user': str(request.user),  # `django.contrib.auth.User` instance.\n        'auth': str(request.auth),  # None\n    }\n    return Response(content)\n Unauthorized and Forbidden responses When an unauthenticated request is denied permission there are two different error codes that may be appropriate.  HTTP 401 Unauthorized HTTP 403 Permission Denied  HTTP 401 responses must always include a WWW-Authenticate header, that instructs the client how to authenticate. HTTP 403 responses do not include the WWW-Authenticate header. The kind of response that will be used depends on the authentication scheme. Although multiple authentication schemes may be in use, only one scheme may be used to determine the type of response. The first authentication class set on the view is used when determining the type of response. Note that when a request may successfully authenticate, but still be denied permission to perform the request, in which case a 403 Permission Denied response will always be used, regardless of the authentication scheme. Apache mod_wsgi specific configuration Note that if deploying to Apache using mod_wsgi, the authorization header is not passed through to a WSGI application by default, as it is assumed that authentication will be handled by Apache, rather than at an application level. If you are deploying to Apache, and using any non-session based authentication, you will need to explicitly configure mod_wsgi to pass the required headers through to the application. This can be done by specifying the WSGIPassAuthorization directive in the appropriate context and setting it to 'On'. # this can go in either server config, virtual host, directory or .htaccess\nWSGIPassAuthorization On\n API Reference BasicAuthentication This authentication scheme uses HTTP Basic Authentication, signed against a user's username and password. Basic authentication is generally only appropriate for testing. If successfully authenticated, BasicAuthentication provides the following credentials.  \nrequest.user will be a Django User instance. \nrequest.auth will be None.  Unauthenticated responses that are denied permission will result in an HTTP 401 Unauthorized response with an appropriate WWW-Authenticate header. For example: WWW-Authenticate: Basic realm=\"api\"\n Note: If you use BasicAuthentication in production you must ensure that your API is only available over https. You should also ensure that your API clients will always re-request the username and password at login, and will never store those details to persistent storage. TokenAuthentication This authentication scheme uses a simple token-based HTTP Authentication scheme. Token authentication is appropriate for client-server setups, such as native desktop and mobile clients. To use the TokenAuthentication scheme you'll need to configure the authentication classes to include TokenAuthentication, and additionally include rest_framework.authtoken in your INSTALLED_APPS setting: INSTALLED_APPS = [\n    ...\n    'rest_framework.authtoken'\n]\n Note: Make sure to run manage.py migrate after changing your settings. The rest_framework.authtoken app provides Django database migrations. You'll also need to create tokens for your users. from rest_framework.authtoken.models import Token\n\ntoken = Token.objects.create(user=...)\nprint(token.key)\n For clients to authenticate, the token key should be included in the Authorization HTTP header. The key should be prefixed by the string literal \"Token\", with whitespace separating the two strings. For example: Authorization: Token 9944b09199c62bcf9418ad846dd0e4bbdfc6ee4b\n Note: If you want to use a different keyword in the header, such as Bearer, simply subclass TokenAuthentication and set the keyword class variable. If successfully authenticated, TokenAuthentication provides the following credentials.  \nrequest.user will be a Django User instance. \nrequest.auth will be a rest_framework.authtoken.models.Token instance.  Unauthenticated responses that are denied permission will result in an HTTP 401 Unauthorized response with an appropriate WWW-Authenticate header. For example: WWW-Authenticate: Token\n The curl command line tool may be useful for testing token authenticated APIs. For example: curl -X GET http:\/\/127.0.0.1:8000\/api\/example\/ -H 'Authorization: Token 9944b09199c62bcf9418ad846dd0e4bbdfc6ee4b'\n Note: If you use TokenAuthentication in production you must ensure that your API is only available over https. Generating Tokens By using signals If you want every user to have an automatically generated Token, you can simply catch the User's post_save signal. from django.conf import settings\nfrom django.db.models.signals import post_save\nfrom django.dispatch import receiver\nfrom rest_framework.authtoken.models import Token\n\n@receiver(post_save, sender=settings.AUTH_USER_MODEL)\ndef create_auth_token(sender, instance=None, created=False, **kwargs):\n    if created:\n        Token.objects.create(user=instance)\n Note that you'll want to ensure you place this code snippet in an installed models.py module, or some other location that will be imported by Django on startup. If you've already created some users, you can generate tokens for all existing users like this: from django.contrib.auth.models import User\nfrom rest_framework.authtoken.models import Token\n\nfor user in User.objects.all():\n    Token.objects.get_or_create(user=user)\n By exposing an api endpoint When using TokenAuthentication, you may want to provide a mechanism for clients to obtain a token given the username and password. REST framework provides a built-in view to provide this behaviour. To use it, add the obtain_auth_token view to your URLconf: from rest_framework.authtoken import views\nurlpatterns += [\n    path('api-token-auth\/', views.obtain_auth_token)\n]\n Note that the URL part of the pattern can be whatever you want to use. The obtain_auth_token view will return a JSON response when valid username and password fields are POSTed to the view using form data or JSON: { 'token' : '9944b09199c62bcf9418ad846dd0e4bbdfc6ee4b' }\n Note that the default obtain_auth_token view explicitly uses JSON requests and responses, rather than using default renderer and parser classes in your settings. By default, there are no permissions or throttling applied to the obtain_auth_token view. If you do wish to apply to throttle you'll need to override the view class, and include them using the throttle_classes attribute. If you need a customized version of the obtain_auth_token view, you can do so by subclassing the ObtainAuthToken view class, and using that in your url conf instead. For example, you may return additional user information beyond the token value: from rest_framework.authtoken.views import ObtainAuthToken\nfrom rest_framework.authtoken.models import Token\nfrom rest_framework.response import Response\n\nclass CustomAuthToken(ObtainAuthToken):\n\n    def post(self, request, *args, **kwargs):\n        serializer = self.serializer_class(data=request.data,\n                                           context={'request': request})\n        serializer.is_valid(raise_exception=True)\n        user = serializer.validated_data['user']\n        token, created = Token.objects.get_or_create(user=user)\n        return Response({\n            'token': token.key,\n            'user_id': user.pk,\n            'email': user.email\n        })\n And in your urls.py: urlpatterns += [\n    path('api-token-auth\/', CustomAuthToken.as_view())\n]\n With Django admin It is also possible to create Tokens manually through the admin interface. In case you are using a large user base, we recommend that you monkey patch the TokenAdmin class customize it to your needs, more specifically by declaring the user field as raw_field. your_app\/admin.py: from rest_framework.authtoken.admin import TokenAdmin\n\nTokenAdmin.raw_id_fields = ['user']\n Using Django manage.py command Since version 3.6.4 it's possible to generate a user token using the following command: .\/manage.py drf_create_token <username>\n this command will return the API token for the given user, creating it if it doesn't exist: Generated token 9944b09199c62bcf9418ad846dd0e4bbdfc6ee4b for user user1\n In case you want to regenerate the token (for example if it has been compromised or leaked) you can pass an additional parameter: .\/manage.py drf_create_token -r <username>\n SessionAuthentication This authentication scheme uses Django's default session backend for authentication. Session authentication is appropriate for AJAX clients that are running in the same session context as your website. If successfully authenticated, SessionAuthentication provides the following credentials.  \nrequest.user will be a Django User instance. \nrequest.auth will be None.  Unauthenticated responses that are denied permission will result in an HTTP 403 Forbidden response. If you're using an AJAX-style API with SessionAuthentication, you'll need to make sure you include a valid CSRF token for any \"unsafe\" HTTP method calls, such as PUT, PATCH, POST or DELETE requests. See the Django CSRF documentation for more details. Warning: Always use Django's standard login view when creating login pages. This will ensure your login views are properly protected. CSRF validation in REST framework works slightly differently from standard Django due to the need to support both session and non-session based authentication to the same views. This means that only authenticated requests require CSRF tokens, and anonymous requests may be sent without CSRF tokens. This behaviour is not suitable for login views, which should always have CSRF validation applied. RemoteUserAuthentication This authentication scheme allows you to delegate authentication to your web server, which sets the REMOTE_USER environment variable. To use it, you must have django.contrib.auth.backends.RemoteUserBackend (or a subclass) in your AUTHENTICATION_BACKENDS setting. By default, RemoteUserBackend creates User objects for usernames that don't already exist. To change this and other behaviour, consult the Django documentation. If successfully authenticated, RemoteUserAuthentication provides the following credentials:  \nrequest.user will be a Django User instance. \nrequest.auth will be None.  Consult your web server's documentation for information about configuring an authentication method, e.g.:  Apache Authentication How-To NGINX (Restricting Access)  Custom authentication To implement a custom authentication scheme, subclass BaseAuthentication and override the .authenticate(self, request) method. The method should return a two-tuple of (user, auth) if authentication succeeds, or None otherwise. In some circumstances instead of returning None, you may want to raise an AuthenticationFailed exception from the .authenticate() method. Typically the approach you should take is:  If authentication is not attempted, return None. Any other authentication schemes also in use will still be checked. If authentication is attempted but fails, raise an AuthenticationFailed exception. An error response will be returned immediately, regardless of any permissions checks, and without checking any other authentication schemes.  You may also override the .authenticate_header(self, request) method. If implemented, it should return a string that will be used as the value of the WWW-Authenticate header in a HTTP 401 Unauthorized response. If the .authenticate_header() method is not overridden, the authentication scheme will return HTTP 403 Forbidden responses when an unauthenticated request is denied access. Note: When your custom authenticator is invoked by the request object's .user or .auth properties, you may see an AttributeError re-raised as a WrappedAttributeError. This is necessary to prevent the original exception from being suppressed by the outer property access. Python will not recognize that the AttributeError originates from your custom authenticator and will instead assume that the request object does not have a .user or .auth property. These errors should be fixed or otherwise handled by your authenticator. Example The following example will authenticate any incoming request as the user given by the username in a custom request header named 'X-USERNAME'. from django.contrib.auth.models import User\nfrom rest_framework import authentication\nfrom rest_framework import exceptions\n\nclass ExampleAuthentication(authentication.BaseAuthentication):\n    def authenticate(self, request):\n        username = request.META.get('HTTP_X_USERNAME')\n        if not username:\n            return None\n\n        try:\n            user = User.objects.get(username=username)\n        except User.DoesNotExist:\n            raise exceptions.AuthenticationFailed('No such user')\n\n        return (user, None)\n Third party packages The following third-party packages are also available. Django OAuth Toolkit The Django OAuth Toolkit package provides OAuth 2.0 support and works with Python 3.4+. The package is maintained by jazzband and uses the excellent OAuthLib. The package is well documented, and well supported and is currently our recommended package for OAuth 2.0 support. Installation & configuration Install using pip. pip install django-oauth-toolkit\n Add the package to your INSTALLED_APPS and modify your REST framework settings. INSTALLED_APPS = [\n    ...\n    'oauth2_provider',\n]\n\nREST_FRAMEWORK = {\n    'DEFAULT_AUTHENTICATION_CLASSES': [\n        'oauth2_provider.contrib.rest_framework.OAuth2Authentication',\n    ]\n}\n For more details see the Django REST framework - Getting started documentation. Django REST framework OAuth The Django REST framework OAuth package provides both OAuth1 and OAuth2 support for REST framework. This package was previously included directly in the REST framework but is now supported and maintained as a third-party package. Installation & configuration Install the package using pip. pip install djangorestframework-oauth\n For details on configuration and usage see the Django REST framework OAuth documentation for authentication and permissions. JSON Web Token Authentication JSON Web Token is a fairly new standard which can be used for token-based authentication. Unlike the built-in TokenAuthentication scheme, JWT Authentication doesn't need to use a database to validate a token. A package for JWT authentication is djangorestframework-simplejwt which provides some features as well as a pluggable token blacklist app. Hawk HTTP Authentication The HawkREST library builds on the Mohawk library to let you work with Hawk signed requests and responses in your API. Hawk lets two parties securely communicate with each other using messages signed by a shared key. It is based on HTTP MAC access authentication (which was based on parts of OAuth 1.0). HTTP Signature Authentication HTTP Signature (currently a IETF draft) provides a way to achieve origin authentication and message integrity for HTTP messages. Similar to Amazon's HTTP Signature scheme, used by many of its services, it permits stateless, per-request authentication. Elvio Toccalino maintains the djangorestframework-httpsignature (outdated) package which provides an easy to use HTTP Signature Authentication mechanism. You can use the updated fork version of djangorestframework-httpsignature, which is drf-httpsig. Djoser Djoser library provides a set of views to handle basic actions such as registration, login, logout, password reset and account activation. The package works with a custom user model and uses token-based authentication. This is ready to use REST implementation of the Django authentication system. django-rest-auth \/ dj-rest-auth This library provides a set of REST API endpoints for registration, authentication (including social media authentication), password reset, retrieve and update user details, etc. By having these API endpoints, your client apps such as AngularJS, iOS, Android, and others can communicate to your Django backend site independently via REST APIs for user management. There are currently two forks of this project.  \nDjango-rest-auth is the original project, but is not currently receiving updates. \nDj-rest-auth is a newer fork of the project.  django-rest-framework-social-oauth2 Django-rest-framework-social-oauth2 library provides an easy way to integrate social plugins (facebook, twitter, google, etc.) to your authentication system and an easy oauth2 setup. With this library, you will be able to authenticate users based on external tokens (e.g. facebook access token), convert these tokens to \"in-house\" oauth2 tokens and use and generate oauth2 tokens to authenticate your users. django-rest-knox Django-rest-knox library provides models and views to handle token-based authentication in a more secure and extensible way than the built-in TokenAuthentication scheme - with Single Page Applications and Mobile clients in mind. It provides per-client tokens, and views to generate them when provided some other authentication (usually basic authentication), to delete the token (providing a server enforced logout) and to delete all tokens (logs out all clients that a user is logged into). drfpasswordless drfpasswordless adds (Medium, Square Cash inspired) passwordless support to Django REST Framework's TokenAuthentication scheme. Users log in and sign up with a token sent to a contact point like an email address or a mobile number. django-rest-authemail django-rest-authemail provides a RESTful API interface for user signup and authentication. Email addresses are used for authentication, rather than usernames. API endpoints are available for signup, signup email verification, login, logout, password reset, password reset verification, email change, email change verification, password change, and user detail. A fully functional example project and detailed instructions are included. Django-Rest-Durin Django-Rest-Durin is built with the idea to have one library that does token auth for multiple Web\/CLI\/Mobile API clients via one interface but allows different token configuration for each API Client that consumes the API. It provides support for multiple tokens per user via custom models, views, permissions that work with Django-Rest-Framework. The token expiration time can be different per API client and is customizable via the Django Admin Interface. More information can be found in the Documentation. authentication.py","title":"django_rest_framework.api-guide.authentication.index#basicauthentication"}]}
{"task_id":13598363,"prompt":"def f_13598363():\n\treturn ","suffix":"","canonical_solution":"Flask('test', template_folder='wherever')","test_start":"\nfrom flask import Flask\n\ndef check(candidate):","test":["\n    __name__ == \"test\"\n    assert candidate().template_folder == \"wherever\"\n"],"entry_point":"f_13598363","intent":"Flask set folder 'wherever' as the default template folder","library":["flask"],"docs":[{"text":"template_folder  \nThe path to the templates folder, relative to root_path, to add to the template loader. None if templates should not be added.","title":"flask.api.index#flask.Flask.template_folder"},{"text":"template_folder  \nThe path to the templates folder, relative to root_path, to add to the template loader. None if templates should not be added.","title":"flask.api.index#flask.Blueprint.template_folder"},{"text":"class filesystem.Loader  \nLoads templates from the filesystem, according to DIRS. This loader is enabled by default. However it won\u2019t find any templates until you set DIRS to a non-empty list: TEMPLATES = [{\n    'BACKEND': 'django.template.backends.django.DjangoTemplates',\n    'DIRS': [BASE_DIR \/ 'templates'],\n}]\n You can also override 'DIRS' and specify specific directories for a particular filesystem loader: TEMPLATES = [{\n    'BACKEND': 'django.template.backends.django.DjangoTemplates',\n    'OPTIONS': {\n        'loaders': [\n            (\n                'django.template.loaders.filesystem.Loader',\n                [BASE_DIR \/ 'templates'],\n            ),\n        ],\n    },\n}]","title":"django.ref.templates.api#django.template.loaders.filesystem.Loader"},{"text":"jinja_environment  \nalias of flask.templating.Environment","title":"flask.api.index#flask.Flask.jinja_environment"},{"text":"Foreword Read this before you get started with Flask. This hopefully answers some questions about the purpose and goals of the project, and when you should or should not be using it. What does \u201cmicro\u201d mean? \u201cMicro\u201d does not mean that your whole web application has to fit into a single Python file (although it certainly can), nor does it mean that Flask is lacking in functionality. The \u201cmicro\u201d in microframework means Flask aims to keep the core simple but extensible. Flask won\u2019t make many decisions for you, such as what database to use. Those decisions that it does make, such as what templating engine to use, are easy to change. Everything else is up to you, so that Flask can be everything you need and nothing you don\u2019t. By default, Flask does not include a database abstraction layer, form validation or anything else where different libraries already exist that can handle that. Instead, Flask supports extensions to add such functionality to your application as if it was implemented in Flask itself. Numerous extensions provide database integration, form validation, upload handling, various open authentication technologies, and more. Flask may be \u201cmicro\u201d, but it\u2019s ready for production use on a variety of needs. Configuration and Conventions Flask has many configuration values, with sensible defaults, and a few conventions when getting started. By convention, templates and static files are stored in subdirectories within the application\u2019s Python source tree, with the names templates and static respectively. While this can be changed, you usually don\u2019t have to, especially when getting started. Growing with Flask Once you have Flask up and running, you\u2019ll find a variety of extensions available in the community to integrate your project for production. As your codebase grows, you are free to make the design decisions appropriate for your project. Flask will continue to provide a very simple glue layer to the best that Python has to offer. You can implement advanced patterns in SQLAlchemy or another database tool, introduce non-relational data persistence as appropriate, and take advantage of framework-agnostic tools built for WSGI, the Python web interface. Flask includes many hooks to customize its behavior. Should you need more customization, the Flask class is built for subclassing. If you are interested in that, check out the Becoming Big chapter. If you are curious about the Flask design principles, head over to the section about Design Decisions in Flask.","title":"flask.foreword.index"},{"text":"class locmem.Loader  \nLoads templates from a Python dictionary. This is useful for testing. This loader takes a dictionary of templates as its first argument: TEMPLATES = [{\n    'BACKEND': 'django.template.backends.django.DjangoTemplates',\n    'OPTIONS': {\n        'loaders': [\n            ('django.template.loaders.locmem.Loader', {\n                'index.html': 'content here',\n            }),\n        ],\n    },\n}]\n This loader is disabled by default.","title":"django.ref.templates.api#django.template.loaders.locmem.Loader"},{"text":"Request.unverifiable  \nboolean, indicates whether the request is unverifiable as defined by RFC 2965.","title":"python.library.urllib.request#urllib.request.Request.unverifiable"},{"text":"site.main()  \nAdds all the standard site-specific directories to the module search path. This function is called automatically when this module is imported, unless the Python interpreter was started with the -S flag.  Changed in version 3.3: This function used to be called unconditionally.","title":"python.library.site#site.main"},{"text":"numpy.linalg.LinAlgError   exception linalg.LinAlgError[source]\n \nGeneric Python-exception-derived object raised by linalg functions. General purpose exception class, derived from Python\u2019s exception.Exception class, programmatically raised in linalg functions when a Linear Algebra-related condition would prevent further correct execution of the function.  Parameters \n None\n   Examples >>> from numpy import linalg as LA\n>>> LA.inv(np.zeros((2,2)))\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"...linalg.py\", line 350,\n    in inv return wrap(solve(a, identity(a.shape[0], dtype=a.dtype)))\n  File \"...linalg.py\", line 249,\n    in solve\n    raise LinAlgError('Singular matrix')\nnumpy.linalg.LinAlgError: Singular matrix","title":"numpy.reference.generated.numpy.linalg.linalgerror"},{"text":"Installation  Python Version We recommend using the latest version of Python. Werkzeug supports Python 3.6 and newer.   Dependencies Werkzeug does not have any direct dependencies.  Optional dependencies These distributions will not be installed automatically. Werkzeug will detect and use them if you install them.  \nColorama provides request log highlighting when using the development server on Windows. This works automatically on other systems. \nWatchdog provides a faster, more efficient reloader for the development server.     Virtual environments Use a virtual environment to manage the dependencies for your project, both in development and in production. What problem does a virtual environment solve? The more Python projects you have, the more likely it is that you need to work with different versions of Python libraries, or even Python itself. Newer versions of libraries for one project can break compatibility in another project. Virtual environments are independent groups of Python libraries, one for each project. Packages installed for one project will not affect other projects or the operating system\u2019s packages. Python comes bundled with the venv module to create virtual environments.  Create an environment Create a project folder and a venv folder within: mkdir myproject\ncd myproject\npython3 -m venv venv\n On Windows: py -3 -m venv venv\n   Activate the environment Before you work on your project, activate the corresponding environment: . venv\/bin\/activate\n On Windows: venv\\Scripts\\activate\n Your shell prompt will change to show the name of the activated environment.    Install Werkzeug Within the activated environment, use the following command to install Werkzeug: pip install Werkzeug","title":"werkzeug.installation.index"}]}
{"task_id":3398589,"prompt":"def f_3398589(c2):\n\t","suffix":"\n\treturn c2","canonical_solution":"c2.sort(key=lambda row: row[2])","test_start":"\ndef check(candidate):","test":["\n    c2 = [[14, 25, 46], [1, 22, 53], [7, 8, 9]]\n    candidate(c2)\n    assert c2[0] == [7,8,9]\n","\n    c2 = [[14.343, 25.24, 46], [1, 22, 53.45], [7, 8.65, 9]]\n    candidate(c2)\n    assert c2[0] == [7,8.65,9]\n"],"entry_point":"f_3398589","intent":"sort a list of lists 'c2' such that third row comes first","library":[],"docs":[]}
{"task_id":3398589,"prompt":"def f_3398589(c2):\n\t","suffix":"\n\treturn c2","canonical_solution":"c2.sort(key=lambda row: (row[2], row[1], row[0]))","test_start":"\ndef check(candidate):","test":["\n    c2 = [[14, 25, 46], [1, 22, 53], [7, 8, 9]]\n    candidate(c2)\n    assert c2[0] == [7,8,9]\n","\n    c2 = [[14.343, 25.24, 46], [1, 22, 53.45], [7, 8.65, 9]]\n    candidate(c2)\n    assert c2[0] == [7,8.65,9]\n"],"entry_point":"f_3398589","intent":"sort a list of lists 'c2' in reversed row order","library":[],"docs":[]}
{"task_id":3398589,"prompt":"def f_3398589(c2):\n\t","suffix":"\n\treturn c2","canonical_solution":"c2.sort(key=lambda row: (row[2], row[1]))","test_start":"\ndef check(candidate):","test":["\n    c2 = [[14, 25, 46], [1, 22, 53], [7, 8, 9]]\n    candidate(c2)\n    assert c2[0] == [7,8,9]\n","\n    c2 = [[14.343, 25.24, 46], [1, 22, 53.45], [7, 8.65, 9]]\n    candidate(c2)\n    assert c2[0] == [7,8.65,9]\n"],"entry_point":"f_3398589","intent":"Sorting a list of lists `c2`, each by the third and second row","library":[],"docs":[]}
{"task_id":10960463,"prompt":"def f_10960463():\n\treturn ","suffix":"","canonical_solution":"matplotlib.rc('font', **{'sans-serif': 'Arial', 'family': 'sans-serif'})","test_start":"\nimport matplotlib\n\ndef check(candidate):","test":["\n    try:\n        candidate()\n    except:\n        assert False\n"],"entry_point":"f_10960463","intent":"set font `Arial` to display non-ascii characters in matplotlib","library":["matplotlib"],"docs":[{"text":"matplotlib.testing.set_font_settings_for_testing()[source]","title":"matplotlib.testing_api#matplotlib.testing.set_font_settings_for_testing"},{"text":"tkinter.font.NORMAL  \ntkinter.font.BOLD  \ntkinter.font.ITALIC  \ntkinter.font.ROMAN","title":"python.library.tkinter.font#tkinter.font.NORMAL"},{"text":"tkinter.font.NORMAL  \ntkinter.font.BOLD  \ntkinter.font.ITALIC  \ntkinter.font.ROMAN","title":"python.library.tkinter.font#tkinter.font.ITALIC"},{"text":"tkinter.font.NORMAL  \ntkinter.font.BOLD  \ntkinter.font.ITALIC  \ntkinter.font.ROMAN","title":"python.library.tkinter.font#tkinter.font.ROMAN"},{"text":"tkinter.font.NORMAL  \ntkinter.font.BOLD  \ntkinter.font.ITALIC  \ntkinter.font.ROMAN","title":"python.library.tkinter.font#tkinter.font.BOLD"},{"text":"set_font(fontname, fontsize, store=True)[source]","title":"matplotlib.backend_ps_api#matplotlib.backends.backend_ps.RendererPS.set_font"},{"text":"FONTSIZE=10","title":"matplotlib.table_api#matplotlib.table.Table.FONTSIZE"},{"text":"selectfont=b'Tf'[source]","title":"matplotlib.backend_pdf_api#matplotlib.backends.backend_pdf.Op.selectfont"},{"text":"decrypted","title":"matplotlib.type1font#matplotlib.type1font.Type1Font.decrypted"},{"text":"set_fontname(fontname)[source]\n \nAlias for set_family. One-way alias only: the getter differs.  Parameters \n \nfontname{FONTNAME, 'serif', 'sans-serif', 'cursive', 'fantasy', 'monospace'}\n\n    See also  font_manager.FontProperties.set_family","title":"matplotlib.text_api#matplotlib.text.Text.set_fontname"}]}
{"task_id":20576618,"prompt":"def f_20576618(df):\n\treturn ","suffix":"","canonical_solution":"df['date'].apply(lambda x: x.toordinal())","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    df = pd.DataFrame(\n        {\n            \"group\": [\"A\", \"A\", \"A\", \"A\", \"A\"],\n            \"date\": pd.to_datetime([\"2020-01-02\", \"2020-01-13\", \"2020-02-01\", \"2020-02-23\", \"2020-03-05\"]),\n            \"value\": [10, 20, 16, 31, 56],\n        })    \n    data_series = candidate(df).tolist()\n    assert data_series[1] == 737437\n","\n    df = pd.DataFrame(\n        {\n            \"group\": [\"A\", \"A\", \"A\", \"A\", \"A\"],\n            \"date\": pd.to_datetime([\"2020-01-02\", \"2020-01-13\", \"2020-02-01\", \"2020-02-23\", \"2020-03-05\"]),\n            \"value\": [10, 20, 16, 31, 56],\n        })    \n    data_series = candidate(df).tolist()\n    assert data_series[1] == 737437\n"],"entry_point":"f_20576618","intent":"Convert  DateTime column 'date' of pandas dataframe 'df' to ordinal","library":["pandas"],"docs":[{"text":"date.toordinal()  \nReturn the proleptic Gregorian ordinal of the date, where January 1 of year 1 has ordinal 1. For any date object d, date.fromordinal(d.toordinal()) == d.","title":"python.library.datetime#datetime.date.toordinal"},{"text":"pandas.Timestamp.toordinal   Timestamp.toordinal()\n \nReturn proleptic Gregorian ordinal. January 1 of year 1 is day 1.","title":"pandas.reference.api.pandas.timestamp.toordinal"},{"text":"datetime.toordinal()  \nReturn the proleptic Gregorian ordinal of the date. The same as self.date().toordinal().","title":"python.library.datetime#datetime.datetime.toordinal"},{"text":"pandas.Period.ordinal   Period.ordinal","title":"pandas.reference.api.pandas.period.ordinal"},{"text":"pandas.Series.dt.day   Series.dt.day\n \nThe day of the datetime. Examples \n>>> datetime_series = pd.Series(\n...     pd.date_range(\"2000-01-01\", periods=3, freq=\"D\")\n... )\n>>> datetime_series\n0   2000-01-01\n1   2000-01-02\n2   2000-01-03\ndtype: datetime64[ns]\n>>> datetime_series.dt.day\n0    1\n1    2\n2    3\ndtype: int64","title":"pandas.reference.api.pandas.series.dt.day"},{"text":"pandas.tseries.offsets.DateOffset.n   DateOffset.n","title":"pandas.reference.api.pandas.tseries.offsets.dateoffset.n"},{"text":"pandas.DatetimeIndex.day   propertyDatetimeIndex.day\n \nThe day of the datetime. Examples \n>>> datetime_series = pd.Series(\n...     pd.date_range(\"2000-01-01\", periods=3, freq=\"D\")\n... )\n>>> datetime_series\n0   2000-01-01\n1   2000-01-02\n2   2000-01-03\ndtype: datetime64[ns]\n>>> datetime_series.dt.day\n0    1\n1    2\n2    3\ndtype: int64","title":"pandas.reference.api.pandas.datetimeindex.day"},{"text":"pandas.tseries.offsets.DateOffset.name   DateOffset.name","title":"pandas.reference.api.pandas.tseries.offsets.dateoffset.name"},{"text":"staticconvert(value, unit, axis)[source]\n \nIf value is not already a number or sequence of numbers, convert it with date2num. The unit and axis arguments are not used.","title":"matplotlib.dates_api#matplotlib.dates.DateConverter.convert"},{"text":"pandas.Series.dt.hour   Series.dt.hour\n \nThe hours of the datetime. Examples \n>>> datetime_series = pd.Series(\n...     pd.date_range(\"2000-01-01\", periods=3, freq=\"h\")\n... )\n>>> datetime_series\n0   2000-01-01 00:00:00\n1   2000-01-01 01:00:00\n2   2000-01-01 02:00:00\ndtype: datetime64[ns]\n>>> datetime_series.dt.hour\n0    0\n1    1\n2    2\ndtype: int64","title":"pandas.reference.api.pandas.series.dt.hour"}]}
{"task_id":31793195,"prompt":"def f_31793195(df):\n\treturn ","suffix":"","canonical_solution":"df.index.get_loc('bob')","test_start":"\nimport pandas as pd\nimport numpy as np\n\ndef check(candidate):","test":["\n    df = pd.DataFrame(data=np.asarray([[1,2,3],[4,5,6],[7,8,9]]), index=['alice', 'bob', 'charlie'])\n    index = candidate(df)\n    assert index == 1\n"],"entry_point":"f_31793195","intent":"Get the integer location of a key `bob` in a pandas data frame `df`","library":["numpy","pandas"],"docs":[{"text":"pandas.DataFrame.at   propertyDataFrame.at\n \nAccess a single value for a row\/column label pair. Similar to loc, in that both provide label-based lookups. Use at if you only need to get or set a single value in a DataFrame or Series.  Raises \n KeyError\n\nIf \u2018label\u2019 does not exist in DataFrame.      See also  DataFrame.iat\n\nAccess a single value for a row\/column pair by integer position.  DataFrame.loc\n\nAccess a group of rows and columns by label(s).  Series.at\n\nAccess a single value using a label.    Examples \n>>> df = pd.DataFrame([[0, 2, 3], [0, 4, 1], [10, 20, 30]],\n...                   index=[4, 5, 6], columns=['A', 'B', 'C'])\n>>> df\n    A   B   C\n4   0   2   3\n5   0   4   1\n6  10  20  30\n  Get value at specified row\/column pair \n>>> df.at[4, 'B']\n2\n  Set value at specified row\/column pair \n>>> df.at[4, 'B'] = 10\n>>> df.at[4, 'B']\n10\n  Get value within a Series \n>>> df.loc[5].at['B']\n4","title":"pandas.reference.api.pandas.dataframe.at"},{"text":"pandas.Series.at   propertySeries.at\n \nAccess a single value for a row\/column label pair. Similar to loc, in that both provide label-based lookups. Use at if you only need to get or set a single value in a DataFrame or Series.  Raises \n KeyError\n\nIf \u2018label\u2019 does not exist in DataFrame.      See also  DataFrame.iat\n\nAccess a single value for a row\/column pair by integer position.  DataFrame.loc\n\nAccess a group of rows and columns by label(s).  Series.at\n\nAccess a single value using a label.    Examples \n>>> df = pd.DataFrame([[0, 2, 3], [0, 4, 1], [10, 20, 30]],\n...                   index=[4, 5, 6], columns=['A', 'B', 'C'])\n>>> df\n    A   B   C\n4   0   2   3\n5   0   4   1\n6  10  20  30\n  Get value at specified row\/column pair \n>>> df.at[4, 'B']\n2\n  Set value at specified row\/column pair \n>>> df.at[4, 'B'] = 10\n>>> df.at[4, 'B']\n10\n  Get value within a Series \n>>> df.loc[5].at['B']\n4","title":"pandas.reference.api.pandas.series.at"},{"text":"pandas.Index.asof   finalIndex.asof(label)[source]\n \nReturn the label from the index, or, if not present, the previous one. Assuming that the index is sorted, return the passed index label if it is in the index, or return the previous index label if the passed one is not in the index.  Parameters \n \nlabel:object\n\n\nThe label up to which the method returns the latest index label.    Returns \n object\n\nThe passed label if it is in the index. The previous label if the passed label is not in the sorted index or NaN if there is no such label.      See also  Series.asof\n\nReturn the latest value in a Series up to the passed index.  merge_asof\n\nPerform an asof merge (similar to left join but it matches on nearest key rather than equal key).  Index.get_loc\n\nAn asof is a thin wrapper around get_loc with method=\u2019pad\u2019.    Examples Index.asof returns the latest index label up to the passed label. \n>>> idx = pd.Index(['2013-12-31', '2014-01-02', '2014-01-03'])\n>>> idx.asof('2014-01-01')\n'2013-12-31'\n  If the label is in the index, the method returns the passed label. \n>>> idx.asof('2014-01-02')\n'2014-01-02'\n  If all of the labels in the index are later than the passed label, NaN is returned. \n>>> idx.asof('1999-01-02')\nnan\n  If the index is not sorted, an error is raised. \n>>> idx_not_sorted = pd.Index(['2013-12-31', '2015-01-02',\n...                            '2014-01-03'])\n>>> idx_not_sorted.asof('2013-12-31')\nTraceback (most recent call last):\nValueError: index must be monotonic increasing or decreasing","title":"pandas.reference.api.pandas.index.asof"},{"text":"pandas.Index.asof_locs   Index.asof_locs(where, mask)[source]\n \nReturn the locations (indices) of labels in the index. As in the asof function, if the label (a particular entry in where) is not in the index, the latest index label up to the passed label is chosen and its index returned. If all of the labels in the index are later than a label in where, -1 is returned. mask is used to ignore NA values in the index during calculation.  Parameters \n \nwhere:Index\n\n\nAn Index consisting of an array of timestamps.  \nmask:np.ndarray[bool]\n\n\nArray of booleans denoting where values in the original data are not NA.    Returns \n np.ndarray[np.intp]\n\nAn array of locations (indices) of the labels from the Index which correspond to the return values of the asof function for every element in where.","title":"pandas.reference.api.pandas.index.asof_locs"},{"text":"pandas.DataFrame.iat   propertyDataFrame.iat\n \nAccess a single value for a row\/column pair by integer position. Similar to iloc, in that both provide integer-based lookups. Use iat if you only need to get or set a single value in a DataFrame or Series.  Raises \n IndexError\n\nWhen integer position is out of bounds.      See also  DataFrame.at\n\nAccess a single value for a row\/column label pair.  DataFrame.loc\n\nAccess a group of rows and columns by label(s).  DataFrame.iloc\n\nAccess a group of rows and columns by integer position(s).    Examples \n>>> df = pd.DataFrame([[0, 2, 3], [0, 4, 1], [10, 20, 30]],\n...                   columns=['A', 'B', 'C'])\n>>> df\n    A   B   C\n0   0   2   3\n1   0   4   1\n2  10  20  30\n  Get value at specified row\/column pair \n>>> df.iat[1, 2]\n1\n  Set value at specified row\/column pair \n>>> df.iat[1, 2] = 10\n>>> df.iat[1, 2]\n10\n  Get value within a series \n>>> df.loc[0].iat[1]\n2","title":"pandas.reference.api.pandas.dataframe.iat"},{"text":"pandas.IntervalIndex.get_loc   IntervalIndex.get_loc(key, method=None, tolerance=None)[source]\n \nGet integer location, slice or boolean mask for requested label.  Parameters \n \nkey:label\n\n\nmethod:{None}, optional\n\n\n default: matches where the label is within an interval only.     Returns \n int if unique index, slice if monotonic index, else mask\n   Examples \n>>> i1, i2 = pd.Interval(0, 1), pd.Interval(1, 2)\n>>> index = pd.IntervalIndex([i1, i2])\n>>> index.get_loc(1)\n0\n  You can also supply a point inside an interval. \n>>> index.get_loc(1.5)\n1\n  If a label is in several intervals, you get the locations of all the relevant intervals. \n>>> i3 = pd.Interval(0, 2)\n>>> overlapping_index = pd.IntervalIndex([i1, i2, i3])\n>>> overlapping_index.get_loc(0.5)\narray([ True, False,  True])\n  Only exact matches will be returned if an interval is provided. \n>>> index.get_loc(pd.Interval(0, 1))\n0","title":"pandas.reference.api.pandas.intervalindex.get_loc"},{"text":"pandas.DataFrame.loc   propertyDataFrame.loc\n \nAccess a group of rows and columns by label(s) or a boolean array. .loc[] is primarily label based, but may also be used with a boolean array. Allowed inputs are:  A single label, e.g. 5 or 'a', (note that 5 is interpreted as a label of the index, and never as an integer position along the index). A list or array of labels, e.g. ['a', 'b', 'c']. \nA slice object with labels, e.g. 'a':'f'.  Warning Note that contrary to usual python slices, both the start and the stop are included   A boolean array of the same length as the axis being sliced, e.g. [True, False, True]. An alignable boolean Series. The index of the key will be aligned before masking. An alignable Index. The Index of the returned selection will be the input. A callable function with one argument (the calling Series or DataFrame) and that returns valid output for indexing (one of the above)  See more at Selection by Label.  Raises \n KeyError\n\nIf any items are not found.  IndexingError\n\nIf an indexed key is passed and its index is unalignable to the frame index.      See also  DataFrame.at\n\nAccess a single value for a row\/column label pair.  DataFrame.iloc\n\nAccess group of rows and columns by integer position(s).  DataFrame.xs\n\nReturns a cross-section (row(s) or column(s)) from the Series\/DataFrame.  Series.loc\n\nAccess group of values using labels.    Examples Getting values \n>>> df = pd.DataFrame([[1, 2], [4, 5], [7, 8]],\n...      index=['cobra', 'viper', 'sidewinder'],\n...      columns=['max_speed', 'shield'])\n>>> df\n            max_speed  shield\ncobra               1       2\nviper               4       5\nsidewinder          7       8\n  Single label. Note this returns the row as a Series. \n>>> df.loc['viper']\nmax_speed    4\nshield       5\nName: viper, dtype: int64\n  List of labels. Note using [[]] returns a DataFrame. \n>>> df.loc[['viper', 'sidewinder']]\n            max_speed  shield\nviper               4       5\nsidewinder          7       8\n  Single label for row and column \n>>> df.loc['cobra', 'shield']\n2\n  Slice with labels for row and single label for column. As mentioned above, note that both the start and stop of the slice are included. \n>>> df.loc['cobra':'viper', 'max_speed']\ncobra    1\nviper    4\nName: max_speed, dtype: int64\n  Boolean list with the same length as the row axis \n>>> df.loc[[False, False, True]]\n            max_speed  shield\nsidewinder          7       8\n  Alignable boolean Series: \n>>> df.loc[pd.Series([False, True, False],\n...        index=['viper', 'sidewinder', 'cobra'])]\n            max_speed  shield\nsidewinder          7       8\n  Index (same behavior as df.reindex) \n>>> df.loc[pd.Index([\"cobra\", \"viper\"], name=\"foo\")]\n       max_speed  shield\nfoo\ncobra          1       2\nviper          4       5\n  Conditional that returns a boolean Series \n>>> df.loc[df['shield'] > 6]\n            max_speed  shield\nsidewinder          7       8\n  Conditional that returns a boolean Series with column labels specified \n>>> df.loc[df['shield'] > 6, ['max_speed']]\n            max_speed\nsidewinder          7\n  Callable that returns a boolean Series \n>>> df.loc[lambda df: df['shield'] == 8]\n            max_speed  shield\nsidewinder          7       8\n  Setting values Set value for all items matching the list of labels \n>>> df.loc[['viper', 'sidewinder'], ['shield']] = 50\n>>> df\n            max_speed  shield\ncobra               1       2\nviper               4      50\nsidewinder          7      50\n  Set value for an entire row \n>>> df.loc['cobra'] = 10\n>>> df\n            max_speed  shield\ncobra              10      10\nviper               4      50\nsidewinder          7      50\n  Set value for an entire column \n>>> df.loc[:, 'max_speed'] = 30\n>>> df\n            max_speed  shield\ncobra              30      10\nviper              30      50\nsidewinder         30      50\n  Set value for rows matching callable condition \n>>> df.loc[df['shield'] > 35] = 0\n>>> df\n            max_speed  shield\ncobra              30      10\nviper               0       0\nsidewinder          0       0\n  Getting values on a DataFrame with an index that has integer labels Another example using integers for the index \n>>> df = pd.DataFrame([[1, 2], [4, 5], [7, 8]],\n...      index=[7, 8, 9], columns=['max_speed', 'shield'])\n>>> df\n   max_speed  shield\n7          1       2\n8          4       5\n9          7       8\n  Slice with integer labels for rows. As mentioned above, note that both the start and stop of the slice are included. \n>>> df.loc[7:9]\n   max_speed  shield\n7          1       2\n8          4       5\n9          7       8\n  Getting values with a MultiIndex A number of examples using a DataFrame with a MultiIndex \n>>> tuples = [\n...    ('cobra', 'mark i'), ('cobra', 'mark ii'),\n...    ('sidewinder', 'mark i'), ('sidewinder', 'mark ii'),\n...    ('viper', 'mark ii'), ('viper', 'mark iii')\n... ]\n>>> index = pd.MultiIndex.from_tuples(tuples)\n>>> values = [[12, 2], [0, 4], [10, 20],\n...         [1, 4], [7, 1], [16, 36]]\n>>> df = pd.DataFrame(values, columns=['max_speed', 'shield'], index=index)\n>>> df\n                     max_speed  shield\ncobra      mark i           12       2\n           mark ii           0       4\nsidewinder mark i           10      20\n           mark ii           1       4\nviper      mark ii           7       1\n           mark iii         16      36\n  Single label. Note this returns a DataFrame with a single index. \n>>> df.loc['cobra']\n         max_speed  shield\nmark i          12       2\nmark ii          0       4\n  Single index tuple. Note this returns a Series. \n>>> df.loc[('cobra', 'mark ii')]\nmax_speed    0\nshield       4\nName: (cobra, mark ii), dtype: int64\n  Single label for row and column. Similar to passing in a tuple, this returns a Series. \n>>> df.loc['cobra', 'mark i']\nmax_speed    12\nshield        2\nName: (cobra, mark i), dtype: int64\n  Single tuple. Note using [[]] returns a DataFrame. \n>>> df.loc[[('cobra', 'mark ii')]]\n               max_speed  shield\ncobra mark ii          0       4\n  Single tuple for the index with a single label for the column \n>>> df.loc[('cobra', 'mark i'), 'shield']\n2\n  Slice from index tuple to single label \n>>> df.loc[('cobra', 'mark i'):'viper']\n                     max_speed  shield\ncobra      mark i           12       2\n           mark ii           0       4\nsidewinder mark i           10      20\n           mark ii           1       4\nviper      mark ii           7       1\n           mark iii         16      36\n  Slice from index tuple to index tuple \n>>> df.loc[('cobra', 'mark i'):('viper', 'mark ii')]\n                    max_speed  shield\ncobra      mark i          12       2\n           mark ii          0       4\nsidewinder mark i          10      20\n           mark ii          1       4\nviper      mark ii          7       1","title":"pandas.reference.api.pandas.dataframe.loc"},{"text":"Cookbook This is a repository for short and sweet examples and links for useful pandas recipes. We encourage users to add to this documentation. Adding interesting links and\/or inline examples to this section is a great First Pull Request. Simplified, condensed, new-user friendly, in-line examples have been inserted where possible to augment the Stack-Overflow and GitHub links. Many of the links contain expanded information, above what the in-line examples offer. pandas (pd) and NumPy (np) are the only two abbreviated imported modules. The rest are kept explicitly imported for newer users.  Idioms These are some neat pandas idioms if-then\/if-then-else on one column, and assignment to another one or more columns: \nIn [1]: df = pd.DataFrame(\n   ...:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ...: )\n   ...: \n\nIn [2]: df\nOut[2]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n   if-then\u2026 An if-then on one column \nIn [3]: df.loc[df.AAA >= 5, \"BBB\"] = -1\n\nIn [4]: df\nOut[4]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   -1   50\n2    6   -1  -30\n3    7   -1  -50\n  An if-then with assignment to 2 columns: \nIn [5]: df.loc[df.AAA >= 5, [\"BBB\", \"CCC\"]] = 555\n\nIn [6]: df\nOut[6]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5  555  555\n2    6  555  555\n3    7  555  555\n  Add another line with different logic, to do the -else \nIn [7]: df.loc[df.AAA < 5, [\"BBB\", \"CCC\"]] = 2000\n\nIn [8]: df\nOut[8]: \n   AAA   BBB   CCC\n0    4  2000  2000\n1    5   555   555\n2    6   555   555\n3    7   555   555\n  Or use pandas where after you\u2019ve set up a mask \nIn [9]: df_mask = pd.DataFrame(\n   ...:     {\"AAA\": [True] * 4, \"BBB\": [False] * 4, \"CCC\": [True, False] * 2}\n   ...: )\n   ...: \n\nIn [10]: df.where(df_mask, -1000)\nOut[10]: \n   AAA   BBB   CCC\n0    4 -1000  2000\n1    5 -1000 -1000\n2    6 -1000   555\n3    7 -1000 -1000\n  if-then-else using NumPy\u2019s where() \nIn [11]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [12]: df\nOut[12]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [13]: df[\"logic\"] = np.where(df[\"AAA\"] > 5, \"high\", \"low\")\n\nIn [14]: df\nOut[14]: \n   AAA  BBB  CCC logic\n0    4   10  100   low\n1    5   20   50   low\n2    6   30  -30  high\n3    7   40  -50  high\n    Splitting Split a frame with a boolean criterion \nIn [15]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [16]: df\nOut[16]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [17]: df[df.AAA <= 5]\nOut[17]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n\nIn [18]: df[df.AAA > 5]\nOut[18]: \n   AAA  BBB  CCC\n2    6   30  -30\n3    7   40  -50\n    Building criteria Select with multi-column criteria \nIn [19]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [20]: df\nOut[20]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n  \u2026and (without assignment returns a Series) \nIn [21]: df.loc[(df[\"BBB\"] < 25) & (df[\"CCC\"] >= -40), \"AAA\"]\nOut[21]: \n0    4\n1    5\nName: AAA, dtype: int64\n  \u2026or (without assignment returns a Series) \nIn [22]: df.loc[(df[\"BBB\"] > 25) | (df[\"CCC\"] >= -40), \"AAA\"]\nOut[22]: \n0    4\n1    5\n2    6\n3    7\nName: AAA, dtype: int64\n  \u2026or (with assignment modifies the DataFrame.) \nIn [23]: df.loc[(df[\"BBB\"] > 25) | (df[\"CCC\"] >= 75), \"AAA\"] = 0.1\n\nIn [24]: df\nOut[24]: \n   AAA  BBB  CCC\n0  0.1   10  100\n1  5.0   20   50\n2  0.1   30  -30\n3  0.1   40  -50\n  Select rows with data closest to certain value using argsort \nIn [25]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [26]: df\nOut[26]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [27]: aValue = 43.0\n\nIn [28]: df.loc[(df.CCC - aValue).abs().argsort()]\nOut[28]: \n   AAA  BBB  CCC\n1    5   20   50\n0    4   10  100\n2    6   30  -30\n3    7   40  -50\n  Dynamically reduce a list of criteria using a binary operators \nIn [29]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [30]: df\nOut[30]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [31]: Crit1 = df.AAA <= 5.5\n\nIn [32]: Crit2 = df.BBB == 10.0\n\nIn [33]: Crit3 = df.CCC > -40.0\n  One could hard code: \nIn [34]: AllCrit = Crit1 & Crit2 & Crit3\n  \u2026Or it can be done with a list of dynamically built criteria \nIn [35]: import functools\n\nIn [36]: CritList = [Crit1, Crit2, Crit3]\n\nIn [37]: AllCrit = functools.reduce(lambda x, y: x & y, CritList)\n\nIn [38]: df[AllCrit]\nOut[38]: \n   AAA  BBB  CCC\n0    4   10  100\n     Selection  Dataframes The indexing docs. Using both row labels and value conditionals \nIn [39]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [40]: df\nOut[40]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [41]: df[(df.AAA <= 6) & (df.index.isin([0, 2, 4]))]\nOut[41]: \n   AAA  BBB  CCC\n0    4   10  100\n2    6   30  -30\n  Use loc for label-oriented slicing and iloc positional slicing GH2904 \nIn [42]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]},\n   ....:     index=[\"foo\", \"bar\", \"boo\", \"kar\"],\n   ....: )\n   ....: \n  There are 2 explicit slicing methods, with a third general case  Positional-oriented (Python slicing style : exclusive of end) Label-oriented (Non-Python slicing style : inclusive of end) General (Either slicing style : depends on if the slice contains labels or positions)  \nIn [43]: df.loc[\"bar\":\"kar\"]  # Label\nOut[43]: \n     AAA  BBB  CCC\nbar    5   20   50\nboo    6   30  -30\nkar    7   40  -50\n\n# Generic\nIn [44]: df[0:3]\nOut[44]: \n     AAA  BBB  CCC\nfoo    4   10  100\nbar    5   20   50\nboo    6   30  -30\n\nIn [45]: df[\"bar\":\"kar\"]\nOut[45]: \n     AAA  BBB  CCC\nbar    5   20   50\nboo    6   30  -30\nkar    7   40  -50\n  Ambiguity arises when an index consists of integers with a non-zero start or non-unit increment. \nIn [46]: data = {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n\nIn [47]: df2 = pd.DataFrame(data=data, index=[1, 2, 3, 4])  # Note index starts at 1.\n\nIn [48]: df2.iloc[1:3]  # Position-oriented\nOut[48]: \n   AAA  BBB  CCC\n2    5   20   50\n3    6   30  -30\n\nIn [49]: df2.loc[1:3]  # Label-oriented\nOut[49]: \n   AAA  BBB  CCC\n1    4   10  100\n2    5   20   50\n3    6   30  -30\n  Using inverse operator (~) to take the complement of a mask \nIn [50]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [51]: df\nOut[51]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [52]: df[~((df.AAA <= 6) & (df.index.isin([0, 2, 4])))]\nOut[52]: \n   AAA  BBB  CCC\n1    5   20   50\n3    7   40  -50\n    New columns Efficiently and dynamically creating new columns using applymap \nIn [53]: df = pd.DataFrame({\"AAA\": [1, 2, 1, 3], \"BBB\": [1, 1, 2, 2], \"CCC\": [2, 1, 3, 1]})\n\nIn [54]: df\nOut[54]: \n   AAA  BBB  CCC\n0    1    1    2\n1    2    1    1\n2    1    2    3\n3    3    2    1\n\nIn [55]: source_cols = df.columns  # Or some subset would work too\n\nIn [56]: new_cols = [str(x) + \"_cat\" for x in source_cols]\n\nIn [57]: categories = {1: \"Alpha\", 2: \"Beta\", 3: \"Charlie\"}\n\nIn [58]: df[new_cols] = df[source_cols].applymap(categories.get)\n\nIn [59]: df\nOut[59]: \n   AAA  BBB  CCC  AAA_cat BBB_cat  CCC_cat\n0    1    1    2    Alpha   Alpha     Beta\n1    2    1    1     Beta   Alpha    Alpha\n2    1    2    3    Alpha    Beta  Charlie\n3    3    2    1  Charlie    Beta    Alpha\n  Keep other columns when using min() with groupby \nIn [60]: df = pd.DataFrame(\n   ....:     {\"AAA\": [1, 1, 1, 2, 2, 2, 3, 3], \"BBB\": [2, 1, 3, 4, 5, 1, 2, 3]}\n   ....: )\n   ....: \n\nIn [61]: df\nOut[61]: \n   AAA  BBB\n0    1    2\n1    1    1\n2    1    3\n3    2    4\n4    2    5\n5    2    1\n6    3    2\n7    3    3\n  Method 1 : idxmin() to get the index of the minimums \nIn [62]: df.loc[df.groupby(\"AAA\")[\"BBB\"].idxmin()]\nOut[62]: \n   AAA  BBB\n1    1    1\n5    2    1\n6    3    2\n  Method 2 : sort then take first of each \nIn [63]: df.sort_values(by=\"BBB\").groupby(\"AAA\", as_index=False).first()\nOut[63]: \n   AAA  BBB\n0    1    1\n1    2    1\n2    3    2\n  Notice the same results, with the exception of the index.    Multiindexing The multindexing docs. Creating a MultiIndex from a labeled frame \nIn [64]: df = pd.DataFrame(\n   ....:     {\n   ....:         \"row\": [0, 1, 2],\n   ....:         \"One_X\": [1.1, 1.1, 1.1],\n   ....:         \"One_Y\": [1.2, 1.2, 1.2],\n   ....:         \"Two_X\": [1.11, 1.11, 1.11],\n   ....:         \"Two_Y\": [1.22, 1.22, 1.22],\n   ....:     }\n   ....: )\n   ....: \n\nIn [65]: df\nOut[65]: \n   row  One_X  One_Y  Two_X  Two_Y\n0    0    1.1    1.2   1.11   1.22\n1    1    1.1    1.2   1.11   1.22\n2    2    1.1    1.2   1.11   1.22\n\n# As Labelled Index\nIn [66]: df = df.set_index(\"row\")\n\nIn [67]: df\nOut[67]: \n     One_X  One_Y  Two_X  Two_Y\nrow                            \n0      1.1    1.2   1.11   1.22\n1      1.1    1.2   1.11   1.22\n2      1.1    1.2   1.11   1.22\n\n# With Hierarchical Columns\nIn [68]: df.columns = pd.MultiIndex.from_tuples([tuple(c.split(\"_\")) for c in df.columns])\n\nIn [69]: df\nOut[69]: \n     One        Two      \n       X    Y     X     Y\nrow                      \n0    1.1  1.2  1.11  1.22\n1    1.1  1.2  1.11  1.22\n2    1.1  1.2  1.11  1.22\n\n# Now stack & Reset\nIn [70]: df = df.stack(0).reset_index(1)\n\nIn [71]: df\nOut[71]: \n    level_1     X     Y\nrow                    \n0       One  1.10  1.20\n0       Two  1.11  1.22\n1       One  1.10  1.20\n1       Two  1.11  1.22\n2       One  1.10  1.20\n2       Two  1.11  1.22\n\n# And fix the labels (Notice the label 'level_1' got added automatically)\nIn [72]: df.columns = [\"Sample\", \"All_X\", \"All_Y\"]\n\nIn [73]: df\nOut[73]: \n    Sample  All_X  All_Y\nrow                     \n0      One   1.10   1.20\n0      Two   1.11   1.22\n1      One   1.10   1.20\n1      Two   1.11   1.22\n2      One   1.10   1.20\n2      Two   1.11   1.22\n   Arithmetic Performing arithmetic with a MultiIndex that needs broadcasting \nIn [74]: cols = pd.MultiIndex.from_tuples(\n   ....:     [(x, y) for x in [\"A\", \"B\", \"C\"] for y in [\"O\", \"I\"]]\n   ....: )\n   ....: \n\nIn [75]: df = pd.DataFrame(np.random.randn(2, 6), index=[\"n\", \"m\"], columns=cols)\n\nIn [76]: df\nOut[76]: \n          A                   B                   C          \n          O         I         O         I         O         I\nn  0.469112 -0.282863 -1.509059 -1.135632  1.212112 -0.173215\nm  0.119209 -1.044236 -0.861849 -2.104569 -0.494929  1.071804\n\nIn [77]: df = df.div(df[\"C\"], level=1)\n\nIn [78]: df\nOut[78]: \n          A                   B              C     \n          O         I         O         I    O    I\nn  0.387021  1.633022 -1.244983  6.556214  1.0  1.0\nm -0.240860 -0.974279  1.741358 -1.963577  1.0  1.0\n    Slicing Slicing a MultiIndex with xs \nIn [79]: coords = [(\"AA\", \"one\"), (\"AA\", \"six\"), (\"BB\", \"one\"), (\"BB\", \"two\"), (\"BB\", \"six\")]\n\nIn [80]: index = pd.MultiIndex.from_tuples(coords)\n\nIn [81]: df = pd.DataFrame([11, 22, 33, 44, 55], index, [\"MyData\"])\n\nIn [82]: df\nOut[82]: \n        MyData\nAA one      11\n   six      22\nBB one      33\n   two      44\n   six      55\n  To take the cross section of the 1st level and 1st axis the index: \n# Note : level and axis are optional, and default to zero\nIn [83]: df.xs(\"BB\", level=0, axis=0)\nOut[83]: \n     MyData\none      33\ntwo      44\nsix      55\n  \u2026and now the 2nd level of the 1st axis. \nIn [84]: df.xs(\"six\", level=1, axis=0)\nOut[84]: \n    MyData\nAA      22\nBB      55\n  Slicing a MultiIndex with xs, method #2 \nIn [85]: import itertools\n\nIn [86]: index = list(itertools.product([\"Ada\", \"Quinn\", \"Violet\"], [\"Comp\", \"Math\", \"Sci\"]))\n\nIn [87]: headr = list(itertools.product([\"Exams\", \"Labs\"], [\"I\", \"II\"]))\n\nIn [88]: indx = pd.MultiIndex.from_tuples(index, names=[\"Student\", \"Course\"])\n\nIn [89]: cols = pd.MultiIndex.from_tuples(headr)  # Notice these are un-named\n\nIn [90]: data = [[70 + x + y + (x * y) % 3 for x in range(4)] for y in range(9)]\n\nIn [91]: df = pd.DataFrame(data, indx, cols)\n\nIn [92]: df\nOut[92]: \n               Exams     Labs    \n                   I  II    I  II\nStudent Course                   \nAda     Comp      70  71   72  73\n        Math      71  73   75  74\n        Sci       72  75   75  75\nQuinn   Comp      73  74   75  76\n        Math      74  76   78  77\n        Sci       75  78   78  78\nViolet  Comp      76  77   78  79\n        Math      77  79   81  80\n        Sci       78  81   81  81\n\nIn [93]: All = slice(None)\n\nIn [94]: df.loc[\"Violet\"]\nOut[94]: \n       Exams     Labs    \n           I  II    I  II\nCourse                   \nComp      76  77   78  79\nMath      77  79   81  80\nSci       78  81   81  81\n\nIn [95]: df.loc[(All, \"Math\"), All]\nOut[95]: \n               Exams     Labs    \n                   I  II    I  II\nStudent Course                   \nAda     Math      71  73   75  74\nQuinn   Math      74  76   78  77\nViolet  Math      77  79   81  80\n\nIn [96]: df.loc[(slice(\"Ada\", \"Quinn\"), \"Math\"), All]\nOut[96]: \n               Exams     Labs    \n                   I  II    I  II\nStudent Course                   \nAda     Math      71  73   75  74\nQuinn   Math      74  76   78  77\n\nIn [97]: df.loc[(All, \"Math\"), (\"Exams\")]\nOut[97]: \n                 I  II\nStudent Course        \nAda     Math    71  73\nQuinn   Math    74  76\nViolet  Math    77  79\n\nIn [98]: df.loc[(All, \"Math\"), (All, \"II\")]\nOut[98]: \n               Exams Labs\n                  II   II\nStudent Course           \nAda     Math      73   74\nQuinn   Math      76   77\nViolet  Math      79   80\n  Setting portions of a MultiIndex with xs   Sorting Sort by specific column or an ordered list of columns, with a MultiIndex \nIn [99]: df.sort_values(by=(\"Labs\", \"II\"), ascending=False)\nOut[99]: \n               Exams     Labs    \n                   I  II    I  II\nStudent Course                   \nViolet  Sci       78  81   81  81\n        Math      77  79   81  80\n        Comp      76  77   78  79\nQuinn   Sci       75  78   78  78\n        Math      74  76   78  77\n        Comp      73  74   75  76\nAda     Sci       72  75   75  75\n        Math      71  73   75  74\n        Comp      70  71   72  73\n  Partial selection, the need for sortedness GH2995   Levels Prepending a level to a multiindex Flatten Hierarchical columns    Missing data The missing data docs. Fill forward a reversed timeseries \nIn [100]: df = pd.DataFrame(\n   .....:     np.random.randn(6, 1),\n   .....:     index=pd.date_range(\"2013-08-01\", periods=6, freq=\"B\"),\n   .....:     columns=list(\"A\"),\n   .....: )\n   .....: \n\nIn [101]: df.loc[df.index[3], \"A\"] = np.nan\n\nIn [102]: df\nOut[102]: \n                   A\n2013-08-01  0.721555\n2013-08-02 -0.706771\n2013-08-05 -1.039575\n2013-08-06       NaN\n2013-08-07 -0.424972\n2013-08-08  0.567020\n\nIn [103]: df.reindex(df.index[::-1]).ffill()\nOut[103]: \n                   A\n2013-08-08  0.567020\n2013-08-07 -0.424972\n2013-08-06 -0.424972\n2013-08-05 -1.039575\n2013-08-02 -0.706771\n2013-08-01  0.721555\n  cumsum reset at NaN values  Replace Using replace with backrefs    Grouping The grouping docs. Basic grouping with apply Unlike agg, apply\u2019s callable is passed a sub-DataFrame which gives you access to all the columns \nIn [104]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"animal\": \"cat dog cat fish dog cat cat\".split(),\n   .....:         \"size\": list(\"SSMMMLL\"),\n   .....:         \"weight\": [8, 10, 11, 1, 20, 12, 12],\n   .....:         \"adult\": [False] * 5 + [True] * 2,\n   .....:     }\n   .....: )\n   .....: \n\nIn [105]: df\nOut[105]: \n  animal size  weight  adult\n0    cat    S       8  False\n1    dog    S      10  False\n2    cat    M      11  False\n3   fish    M       1  False\n4    dog    M      20  False\n5    cat    L      12   True\n6    cat    L      12   True\n\n# List the size of the animals with the highest weight.\nIn [106]: df.groupby(\"animal\").apply(lambda subf: subf[\"size\"][subf[\"weight\"].idxmax()])\nOut[106]: \nanimal\ncat     L\ndog     M\nfish    M\ndtype: object\n  Using get_group \nIn [107]: gb = df.groupby([\"animal\"])\n\nIn [108]: gb.get_group(\"cat\")\nOut[108]: \n  animal size  weight  adult\n0    cat    S       8  False\n2    cat    M      11  False\n5    cat    L      12   True\n6    cat    L      12   True\n  Apply to different items in a group \nIn [109]: def GrowUp(x):\n   .....:     avg_weight = sum(x[x[\"size\"] == \"S\"].weight * 1.5)\n   .....:     avg_weight += sum(x[x[\"size\"] == \"M\"].weight * 1.25)\n   .....:     avg_weight += sum(x[x[\"size\"] == \"L\"].weight)\n   .....:     avg_weight \/= len(x)\n   .....:     return pd.Series([\"L\", avg_weight, True], index=[\"size\", \"weight\", \"adult\"])\n   .....: \n\nIn [110]: expected_df = gb.apply(GrowUp)\n\nIn [111]: expected_df\nOut[111]: \n       size   weight  adult\nanimal                     \ncat       L  12.4375   True\ndog       L  20.0000   True\nfish      L   1.2500   True\n  Expanding apply \nIn [112]: S = pd.Series([i \/ 100.0 for i in range(1, 11)])\n\nIn [113]: def cum_ret(x, y):\n   .....:     return x * (1 + y)\n   .....: \n\nIn [114]: def red(x):\n   .....:     return functools.reduce(cum_ret, x, 1.0)\n   .....: \n\nIn [115]: S.expanding().apply(red, raw=True)\nOut[115]: \n0    1.010000\n1    1.030200\n2    1.061106\n3    1.103550\n4    1.158728\n5    1.228251\n6    1.314229\n7    1.419367\n8    1.547110\n9    1.701821\ndtype: float64\n  Replacing some values with mean of the rest of a group \nIn [116]: df = pd.DataFrame({\"A\": [1, 1, 2, 2], \"B\": [1, -1, 1, 2]})\n\nIn [117]: gb = df.groupby(\"A\")\n\nIn [118]: def replace(g):\n   .....:     mask = g < 0\n   .....:     return g.where(mask, g[~mask].mean())\n   .....: \n\nIn [119]: gb.transform(replace)\nOut[119]: \n     B\n0  1.0\n1 -1.0\n2  1.5\n3  1.5\n  Sort groups by aggregated data \nIn [120]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"code\": [\"foo\", \"bar\", \"baz\"] * 2,\n   .....:         \"data\": [0.16, -0.21, 0.33, 0.45, -0.59, 0.62],\n   .....:         \"flag\": [False, True] * 3,\n   .....:     }\n   .....: )\n   .....: \n\nIn [121]: code_groups = df.groupby(\"code\")\n\nIn [122]: agg_n_sort_order = code_groups[[\"data\"]].transform(sum).sort_values(by=\"data\")\n\nIn [123]: sorted_df = df.loc[agg_n_sort_order.index]\n\nIn [124]: sorted_df\nOut[124]: \n  code  data   flag\n1  bar -0.21   True\n4  bar -0.59  False\n0  foo  0.16  False\n3  foo  0.45   True\n2  baz  0.33  False\n5  baz  0.62   True\n  Create multiple aggregated columns \nIn [125]: rng = pd.date_range(start=\"2014-10-07\", periods=10, freq=\"2min\")\n\nIn [126]: ts = pd.Series(data=list(range(10)), index=rng)\n\nIn [127]: def MyCust(x):\n   .....:     if len(x) > 2:\n   .....:         return x[1] * 1.234\n   .....:     return pd.NaT\n   .....: \n\nIn [128]: mhc = {\"Mean\": np.mean, \"Max\": np.max, \"Custom\": MyCust}\n\nIn [129]: ts.resample(\"5min\").apply(mhc)\nOut[129]: \n                     Mean  Max Custom\n2014-10-07 00:00:00   1.0    2  1.234\n2014-10-07 00:05:00   3.5    4    NaT\n2014-10-07 00:10:00   6.0    7  7.404\n2014-10-07 00:15:00   8.5    9    NaT\n\nIn [130]: ts\nOut[130]: \n2014-10-07 00:00:00    0\n2014-10-07 00:02:00    1\n2014-10-07 00:04:00    2\n2014-10-07 00:06:00    3\n2014-10-07 00:08:00    4\n2014-10-07 00:10:00    5\n2014-10-07 00:12:00    6\n2014-10-07 00:14:00    7\n2014-10-07 00:16:00    8\n2014-10-07 00:18:00    9\nFreq: 2T, dtype: int64\n  Create a value counts column and reassign back to the DataFrame \nIn [131]: df = pd.DataFrame(\n   .....:     {\"Color\": \"Red Red Red Blue\".split(), \"Value\": [100, 150, 50, 50]}\n   .....: )\n   .....: \n\nIn [132]: df\nOut[132]: \n  Color  Value\n0   Red    100\n1   Red    150\n2   Red     50\n3  Blue     50\n\nIn [133]: df[\"Counts\"] = df.groupby([\"Color\"]).transform(len)\n\nIn [134]: df\nOut[134]: \n  Color  Value  Counts\n0   Red    100       3\n1   Red    150       3\n2   Red     50       3\n3  Blue     50       1\n  Shift groups of the values in a column based on the index \nIn [135]: df = pd.DataFrame(\n   .....:     {\"line_race\": [10, 10, 8, 10, 10, 8], \"beyer\": [99, 102, 103, 103, 88, 100]},\n   .....:     index=[\n   .....:         \"Last Gunfighter\",\n   .....:         \"Last Gunfighter\",\n   .....:         \"Last Gunfighter\",\n   .....:         \"Paynter\",\n   .....:         \"Paynter\",\n   .....:         \"Paynter\",\n   .....:     ],\n   .....: )\n   .....: \n\nIn [136]: df\nOut[136]: \n                 line_race  beyer\nLast Gunfighter         10     99\nLast Gunfighter         10    102\nLast Gunfighter          8    103\nPaynter                 10    103\nPaynter                 10     88\nPaynter                  8    100\n\nIn [137]: df[\"beyer_shifted\"] = df.groupby(level=0)[\"beyer\"].shift(1)\n\nIn [138]: df\nOut[138]: \n                 line_race  beyer  beyer_shifted\nLast Gunfighter         10     99            NaN\nLast Gunfighter         10    102           99.0\nLast Gunfighter          8    103          102.0\nPaynter                 10    103            NaN\nPaynter                 10     88          103.0\nPaynter                  8    100           88.0\n  Select row with maximum value from each group \nIn [139]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"host\": [\"other\", \"other\", \"that\", \"this\", \"this\"],\n   .....:         \"service\": [\"mail\", \"web\", \"mail\", \"mail\", \"web\"],\n   .....:         \"no\": [1, 2, 1, 2, 1],\n   .....:     }\n   .....: ).set_index([\"host\", \"service\"])\n   .....: \n\nIn [140]: mask = df.groupby(level=0).agg(\"idxmax\")\n\nIn [141]: df_count = df.loc[mask[\"no\"]].reset_index()\n\nIn [142]: df_count\nOut[142]: \n    host service  no\n0  other     web   2\n1   that    mail   1\n2   this    mail   2\n  Grouping like Python\u2019s itertools.groupby \nIn [143]: df = pd.DataFrame([0, 1, 0, 1, 1, 1, 0, 1, 1], columns=[\"A\"])\n\nIn [144]: df[\"A\"].groupby((df[\"A\"] != df[\"A\"].shift()).cumsum()).groups\nOut[144]: {1: [0], 2: [1], 3: [2], 4: [3, 4, 5], 5: [6], 6: [7, 8]}\n\nIn [145]: df[\"A\"].groupby((df[\"A\"] != df[\"A\"].shift()).cumsum()).cumsum()\nOut[145]: \n0    0\n1    1\n2    0\n3    1\n4    2\n5    3\n6    0\n7    1\n8    2\nName: A, dtype: int64\n   Expanding data Alignment and to-date Rolling Computation window based on values instead of counts Rolling Mean by Time Interval   Splitting Splitting a frame Create a list of dataframes, split using a delineation based on logic included in rows. \nIn [146]: df = pd.DataFrame(\n   .....:     data={\n   .....:         \"Case\": [\"A\", \"A\", \"A\", \"B\", \"A\", \"A\", \"B\", \"A\", \"A\"],\n   .....:         \"Data\": np.random.randn(9),\n   .....:     }\n   .....: )\n   .....: \n\nIn [147]: dfs = list(\n   .....:     zip(\n   .....:         *df.groupby(\n   .....:             (1 * (df[\"Case\"] == \"B\"))\n   .....:             .cumsum()\n   .....:             .rolling(window=3, min_periods=1)\n   .....:             .median()\n   .....:         )\n   .....:     )\n   .....: )[-1]\n   .....: \n\nIn [148]: dfs[0]\nOut[148]: \n  Case      Data\n0    A  0.276232\n1    A -1.087401\n2    A -0.673690\n3    B  0.113648\n\nIn [149]: dfs[1]\nOut[149]: \n  Case      Data\n4    A -1.478427\n5    A  0.524988\n6    B  0.404705\n\nIn [150]: dfs[2]\nOut[150]: \n  Case      Data\n7    A  0.577046\n8    A -1.715002\n    Pivot The Pivot docs. Partial sums and subtotals \nIn [151]: df = pd.DataFrame(\n   .....:     data={\n   .....:         \"Province\": [\"ON\", \"QC\", \"BC\", \"AL\", \"AL\", \"MN\", \"ON\"],\n   .....:         \"City\": [\n   .....:             \"Toronto\",\n   .....:             \"Montreal\",\n   .....:             \"Vancouver\",\n   .....:             \"Calgary\",\n   .....:             \"Edmonton\",\n   .....:             \"Winnipeg\",\n   .....:             \"Windsor\",\n   .....:         ],\n   .....:         \"Sales\": [13, 6, 16, 8, 4, 3, 1],\n   .....:     }\n   .....: )\n   .....: \n\nIn [152]: table = pd.pivot_table(\n   .....:     df,\n   .....:     values=[\"Sales\"],\n   .....:     index=[\"Province\"],\n   .....:     columns=[\"City\"],\n   .....:     aggfunc=np.sum,\n   .....:     margins=True,\n   .....: )\n   .....: \n\nIn [153]: table.stack(\"City\")\nOut[153]: \n                    Sales\nProvince City            \nAL       All         12.0\n         Calgary      8.0\n         Edmonton     4.0\nBC       All         16.0\n         Vancouver   16.0\n...                   ...\nAll      Montreal     6.0\n         Toronto     13.0\n         Vancouver   16.0\n         Windsor      1.0\n         Winnipeg     3.0\n\n[20 rows x 1 columns]\n  Frequency table like plyr in R \nIn [154]: grades = [48, 99, 75, 80, 42, 80, 72, 68, 36, 78]\n\nIn [155]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"ID\": [\"x%d\" % r for r in range(10)],\n   .....:         \"Gender\": [\"F\", \"M\", \"F\", \"M\", \"F\", \"M\", \"F\", \"M\", \"M\", \"M\"],\n   .....:         \"ExamYear\": [\n   .....:             \"2007\",\n   .....:             \"2007\",\n   .....:             \"2007\",\n   .....:             \"2008\",\n   .....:             \"2008\",\n   .....:             \"2008\",\n   .....:             \"2008\",\n   .....:             \"2009\",\n   .....:             \"2009\",\n   .....:             \"2009\",\n   .....:         ],\n   .....:         \"Class\": [\n   .....:             \"algebra\",\n   .....:             \"stats\",\n   .....:             \"bio\",\n   .....:             \"algebra\",\n   .....:             \"algebra\",\n   .....:             \"stats\",\n   .....:             \"stats\",\n   .....:             \"algebra\",\n   .....:             \"bio\",\n   .....:             \"bio\",\n   .....:         ],\n   .....:         \"Participated\": [\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"no\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:         ],\n   .....:         \"Passed\": [\"yes\" if x > 50 else \"no\" for x in grades],\n   .....:         \"Employed\": [\n   .....:             True,\n   .....:             True,\n   .....:             True,\n   .....:             False,\n   .....:             False,\n   .....:             False,\n   .....:             False,\n   .....:             True,\n   .....:             True,\n   .....:             False,\n   .....:         ],\n   .....:         \"Grade\": grades,\n   .....:     }\n   .....: )\n   .....: \n\nIn [156]: df.groupby(\"ExamYear\").agg(\n   .....:     {\n   .....:         \"Participated\": lambda x: x.value_counts()[\"yes\"],\n   .....:         \"Passed\": lambda x: sum(x == \"yes\"),\n   .....:         \"Employed\": lambda x: sum(x),\n   .....:         \"Grade\": lambda x: sum(x) \/ len(x),\n   .....:     }\n   .....: )\n   .....: \nOut[156]: \n          Participated  Passed  Employed      Grade\nExamYear                                           \n2007                 3       2         3  74.000000\n2008                 3       3         0  68.500000\n2009                 3       2         2  60.666667\n  Plot pandas DataFrame with year over year data To create year and month cross tabulation: \nIn [157]: df = pd.DataFrame(\n   .....:     {\"value\": np.random.randn(36)},\n   .....:     index=pd.date_range(\"2011-01-01\", freq=\"M\", periods=36),\n   .....: )\n   .....: \n\nIn [158]: pd.pivot_table(\n   .....:     df, index=df.index.month, columns=df.index.year, values=\"value\", aggfunc=\"sum\"\n   .....: )\n   .....: \nOut[158]: \n        2011      2012      2013\n1  -1.039268 -0.968914  2.565646\n2  -0.370647 -1.294524  1.431256\n3  -1.157892  0.413738  1.340309\n4  -1.344312  0.276662 -1.170299\n5   0.844885 -0.472035 -0.226169\n6   1.075770 -0.013960  0.410835\n7  -0.109050 -0.362543  0.813850\n8   1.643563 -0.006154  0.132003\n9  -1.469388 -0.923061 -0.827317\n10  0.357021  0.895717 -0.076467\n11 -0.674600  0.805244 -1.187678\n12 -1.776904 -1.206412  1.130127\n    Apply Rolling apply to organize - Turning embedded lists into a MultiIndex frame \nIn [159]: df = pd.DataFrame(\n   .....:     data={\n   .....:         \"A\": [[2, 4, 8, 16], [100, 200], [10, 20, 30]],\n   .....:         \"B\": [[\"a\", \"b\", \"c\"], [\"jj\", \"kk\"], [\"ccc\"]],\n   .....:     },\n   .....:     index=[\"I\", \"II\", \"III\"],\n   .....: )\n   .....: \n\nIn [160]: def SeriesFromSubList(aList):\n   .....:     return pd.Series(aList)\n   .....: \n\nIn [161]: df_orgz = pd.concat(\n   .....:     {ind: row.apply(SeriesFromSubList) for ind, row in df.iterrows()}\n   .....: )\n   .....: \n\nIn [162]: df_orgz\nOut[162]: \n         0     1     2     3\nI   A    2     4     8  16.0\n    B    a     b     c   NaN\nII  A  100   200   NaN   NaN\n    B   jj    kk   NaN   NaN\nIII A   10  20.0  30.0   NaN\n    B  ccc   NaN   NaN   NaN\n  Rolling apply with a DataFrame returning a Series Rolling Apply to multiple columns where function calculates a Series before a Scalar from the Series is returned \nIn [163]: df = pd.DataFrame(\n   .....:     data=np.random.randn(2000, 2) \/ 10000,\n   .....:     index=pd.date_range(\"2001-01-01\", periods=2000),\n   .....:     columns=[\"A\", \"B\"],\n   .....: )\n   .....: \n\nIn [164]: df\nOut[164]: \n                   A         B\n2001-01-01 -0.000144 -0.000141\n2001-01-02  0.000161  0.000102\n2001-01-03  0.000057  0.000088\n2001-01-04 -0.000221  0.000097\n2001-01-05 -0.000201 -0.000041\n...              ...       ...\n2006-06-19  0.000040 -0.000235\n2006-06-20 -0.000123 -0.000021\n2006-06-21 -0.000113  0.000114\n2006-06-22  0.000136  0.000109\n2006-06-23  0.000027  0.000030\n\n[2000 rows x 2 columns]\n\nIn [165]: def gm(df, const):\n   .....:     v = ((((df[\"A\"] + df[\"B\"]) + 1).cumprod()) - 1) * const\n   .....:     return v.iloc[-1]\n   .....: \n\nIn [166]: s = pd.Series(\n   .....:     {\n   .....:         df.index[i]: gm(df.iloc[i: min(i + 51, len(df) - 1)], 5)\n   .....:         for i in range(len(df) - 50)\n   .....:     }\n   .....: )\n   .....: \n\nIn [167]: s\nOut[167]: \n2001-01-01    0.000930\n2001-01-02    0.002615\n2001-01-03    0.001281\n2001-01-04    0.001117\n2001-01-05    0.002772\n                ...   \n2006-04-30    0.003296\n2006-05-01    0.002629\n2006-05-02    0.002081\n2006-05-03    0.004247\n2006-05-04    0.003928\nLength: 1950, dtype: float64\n  Rolling apply with a DataFrame returning a Scalar Rolling Apply to multiple columns where function returns a Scalar (Volume Weighted Average Price) \nIn [168]: rng = pd.date_range(start=\"2014-01-01\", periods=100)\n\nIn [169]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"Open\": np.random.randn(len(rng)),\n   .....:         \"Close\": np.random.randn(len(rng)),\n   .....:         \"Volume\": np.random.randint(100, 2000, len(rng)),\n   .....:     },\n   .....:     index=rng,\n   .....: )\n   .....: \n\nIn [170]: df\nOut[170]: \n                Open     Close  Volume\n2014-01-01 -1.611353 -0.492885    1219\n2014-01-02 -3.000951  0.445794    1054\n2014-01-03 -0.138359 -0.076081    1381\n2014-01-04  0.301568  1.198259    1253\n2014-01-05  0.276381 -0.669831    1728\n...              ...       ...     ...\n2014-04-06 -0.040338  0.937843    1188\n2014-04-07  0.359661 -0.285908    1864\n2014-04-08  0.060978  1.714814     941\n2014-04-09  1.759055 -0.455942    1065\n2014-04-10  0.138185 -1.147008    1453\n\n[100 rows x 3 columns]\n\nIn [171]: def vwap(bars):\n   .....:     return (bars.Close * bars.Volume).sum() \/ bars.Volume.sum()\n   .....: \n\nIn [172]: window = 5\n\nIn [173]: s = pd.concat(\n   .....:     [\n   .....:         (pd.Series(vwap(df.iloc[i: i + window]), index=[df.index[i + window]]))\n   .....:         for i in range(len(df) - window)\n   .....:     ]\n   .....: )\n   .....: \n\nIn [174]: s.round(2)\nOut[174]: \n2014-01-06    0.02\n2014-01-07    0.11\n2014-01-08    0.10\n2014-01-09    0.07\n2014-01-10   -0.29\n              ... \n2014-04-06   -0.63\n2014-04-07   -0.02\n2014-04-08   -0.03\n2014-04-09    0.34\n2014-04-10    0.29\nLength: 95, dtype: float64\n     Timeseries Between times Using indexer between time Constructing a datetime range that excludes weekends and includes only certain times Vectorized Lookup Aggregation and plotting time series Turn a matrix with hours in columns and days in rows into a continuous row sequence in the form of a time series. How to rearrange a Python pandas DataFrame? Dealing with duplicates when reindexing a timeseries to a specified frequency Calculate the first day of the month for each entry in a DatetimeIndex \nIn [175]: dates = pd.date_range(\"2000-01-01\", periods=5)\n\nIn [176]: dates.to_period(freq=\"M\").to_timestamp()\nOut[176]: \nDatetimeIndex(['2000-01-01', '2000-01-01', '2000-01-01', '2000-01-01',\n               '2000-01-01'],\n              dtype='datetime64[ns]', freq=None)\n   Resampling The Resample docs. Using Grouper instead of TimeGrouper for time grouping of values Time grouping with some missing values Valid frequency arguments to Grouper Timeseries Grouping using a MultiIndex Using TimeGrouper and another grouping to create subgroups, then apply a custom function GH3791 Resampling with custom periods Resample intraday frame without adding new days Resample minute data Resample with groupby    Merge The Join docs. Concatenate two dataframes with overlapping index (emulate R rbind) \nIn [177]: rng = pd.date_range(\"2000-01-01\", periods=6)\n\nIn [178]: df1 = pd.DataFrame(np.random.randn(6, 3), index=rng, columns=[\"A\", \"B\", \"C\"])\n\nIn [179]: df2 = df1.copy()\n  Depending on df construction, ignore_index may be needed \nIn [180]: df = pd.concat([df1, df2], ignore_index=True)\n\nIn [181]: df\nOut[181]: \n           A         B         C\n0  -0.870117 -0.479265 -0.790855\n1   0.144817  1.726395 -0.464535\n2  -0.821906  1.597605  0.187307\n3  -0.128342 -1.511638 -0.289858\n4   0.399194 -1.430030 -0.639760\n5   1.115116 -2.012600  1.810662\n6  -0.870117 -0.479265 -0.790855\n7   0.144817  1.726395 -0.464535\n8  -0.821906  1.597605  0.187307\n9  -0.128342 -1.511638 -0.289858\n10  0.399194 -1.430030 -0.639760\n11  1.115116 -2.012600  1.810662\n  Self Join of a DataFrame GH2996 \nIn [182]: df = pd.DataFrame(\n   .....:     data={\n   .....:         \"Area\": [\"A\"] * 5 + [\"C\"] * 2,\n   .....:         \"Bins\": [110] * 2 + [160] * 3 + [40] * 2,\n   .....:         \"Test_0\": [0, 1, 0, 1, 2, 0, 1],\n   .....:         \"Data\": np.random.randn(7),\n   .....:     }\n   .....: )\n   .....: \n\nIn [183]: df\nOut[183]: \n  Area  Bins  Test_0      Data\n0    A   110       0 -0.433937\n1    A   110       1 -0.160552\n2    A   160       0  0.744434\n3    A   160       1  1.754213\n4    A   160       2  0.000850\n5    C    40       0  0.342243\n6    C    40       1  1.070599\n\nIn [184]: df[\"Test_1\"] = df[\"Test_0\"] - 1\n\nIn [185]: pd.merge(\n   .....:     df,\n   .....:     df,\n   .....:     left_on=[\"Bins\", \"Area\", \"Test_0\"],\n   .....:     right_on=[\"Bins\", \"Area\", \"Test_1\"],\n   .....:     suffixes=(\"_L\", \"_R\"),\n   .....: )\n   .....: \nOut[185]: \n  Area  Bins  Test_0_L    Data_L  Test_1_L  Test_0_R    Data_R  Test_1_R\n0    A   110         0 -0.433937        -1         1 -0.160552         0\n1    A   160         0  0.744434        -1         1  1.754213         0\n2    A   160         1  1.754213         0         2  0.000850         1\n3    C    40         0  0.342243        -1         1  1.070599         0\n  How to set the index and join KDB like asof join Join with a criteria based on the values Using searchsorted to merge based on values inside a range   Plotting The Plotting docs. Make Matplotlib look like R Setting x-axis major and minor labels Plotting multiple charts in an IPython Jupyter notebook Creating a multi-line plot Plotting a heatmap Annotate a time-series plot Annotate a time-series plot #2 Generate Embedded plots in excel files using Pandas, Vincent and xlsxwriter Boxplot for each quartile of a stratifying variable \nIn [186]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"stratifying_var\": np.random.uniform(0, 100, 20),\n   .....:         \"price\": np.random.normal(100, 5, 20),\n   .....:     }\n   .....: )\n   .....: \n\nIn [187]: df[\"quartiles\"] = pd.qcut(\n   .....:     df[\"stratifying_var\"], 4, labels=[\"0-25%\", \"25-50%\", \"50-75%\", \"75-100%\"]\n   .....: )\n   .....: \n\nIn [188]: df.boxplot(column=\"price\", by=\"quartiles\")\nOut[188]: <AxesSubplot:title={'center':'price'}, xlabel='quartiles'>\n     Data in\/out Performance comparison of SQL vs HDF5  CSV The CSV docs read_csv in action appending to a csv Reading a csv chunk-by-chunk Reading only certain rows of a csv chunk-by-chunk Reading the first few lines of a frame Reading a file that is compressed but not by gzip\/bz2 (the native compressed formats which read_csv understands). This example shows a WinZipped file, but is a general application of opening the file within a context manager and using that handle to read. See here Inferring dtypes from a file Dealing with bad lines GH2886 Write a multi-row index CSV without writing duplicates  Reading multiple files to create a single DataFrame The best way to combine multiple files into a single DataFrame is to read the individual frames one by one, put all of the individual frames into a list, and then combine the frames in the list using pd.concat(): \nIn [189]: for i in range(3):\n   .....:     data = pd.DataFrame(np.random.randn(10, 4))\n   .....:     data.to_csv(\"file_{}.csv\".format(i))\n   .....: \n\nIn [190]: files = [\"file_0.csv\", \"file_1.csv\", \"file_2.csv\"]\n\nIn [191]: result = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n  You can use the same approach to read all files matching a pattern. Here is an example using glob: \nIn [192]: import glob\n\nIn [193]: import os\n\nIn [194]: files = glob.glob(\"file_*.csv\")\n\nIn [195]: result = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n  Finally, this strategy will work with the other pd.read_*(...) functions described in the io docs.   Parsing date components in multi-columns Parsing date components in multi-columns is faster with a format \nIn [196]: i = pd.date_range(\"20000101\", periods=10000)\n\nIn [197]: df = pd.DataFrame({\"year\": i.year, \"month\": i.month, \"day\": i.day})\n\nIn [198]: df.head()\nOut[198]: \n   year  month  day\n0  2000      1    1\n1  2000      1    2\n2  2000      1    3\n3  2000      1    4\n4  2000      1    5\n\nIn [199]: %timeit pd.to_datetime(df.year * 10000 + df.month * 100 + df.day, format='%Y%m%d')\n   .....: ds = df.apply(lambda x: \"%04d%02d%02d\" % (x[\"year\"], x[\"month\"], x[\"day\"]), axis=1)\n   .....: ds.head()\n   .....: %timeit pd.to_datetime(ds)\n   .....: \n8.7 ms +- 765 us per loop (mean +- std. dev. of 7 runs, 100 loops each)\n2.1 ms +- 419 us per loop (mean +- std. dev. of 7 runs, 100 loops each)\n    Skip row between header and data \nIn [200]: data = \"\"\";;;;\n   .....:  ;;;;\n   .....:  ;;;;\n   .....:  ;;;;\n   .....:  ;;;;\n   .....:  ;;;;\n   .....: ;;;;\n   .....:  ;;;;\n   .....:  ;;;;\n   .....: ;;;;\n   .....: date;Param1;Param2;Param4;Param5\n   .....:     ;m\u00b2;\u00b0C;m\u00b2;m\n   .....: ;;;;\n   .....: 01.01.1990 00:00;1;1;2;3\n   .....: 01.01.1990 01:00;5;3;4;5\n   .....: 01.01.1990 02:00;9;5;6;7\n   .....: 01.01.1990 03:00;13;7;8;9\n   .....: 01.01.1990 04:00;17;9;10;11\n   .....: 01.01.1990 05:00;21;11;12;13\n   .....: \"\"\"\n   .....: \n   Option 1: pass rows explicitly to skip rows \nIn [201]: from io import StringIO\n\nIn [202]: pd.read_csv(\n   .....:     StringIO(data),\n   .....:     sep=\";\",\n   .....:     skiprows=[11, 12],\n   .....:     index_col=0,\n   .....:     parse_dates=True,\n   .....:     header=10,\n   .....: )\n   .....: \nOut[202]: \n                     Param1  Param2  Param4  Param5\ndate                                               \n1990-01-01 00:00:00       1       1       2       3\n1990-01-01 01:00:00       5       3       4       5\n1990-01-01 02:00:00       9       5       6       7\n1990-01-01 03:00:00      13       7       8       9\n1990-01-01 04:00:00      17       9      10      11\n1990-01-01 05:00:00      21      11      12      13\n    Option 2: read column names and then data \nIn [203]: pd.read_csv(StringIO(data), sep=\";\", header=10, nrows=10).columns\nOut[203]: Index(['date', 'Param1', 'Param2', 'Param4', 'Param5'], dtype='object')\n\nIn [204]: columns = pd.read_csv(StringIO(data), sep=\";\", header=10, nrows=10).columns\n\nIn [205]: pd.read_csv(\n   .....:     StringIO(data), sep=\";\", index_col=0, header=12, parse_dates=True, names=columns\n   .....: )\n   .....: \nOut[205]: \n                     Param1  Param2  Param4  Param5\ndate                                               \n1990-01-01 00:00:00       1       1       2       3\n1990-01-01 01:00:00       5       3       4       5\n1990-01-01 02:00:00       9       5       6       7\n1990-01-01 03:00:00      13       7       8       9\n1990-01-01 04:00:00      17       9      10      11\n1990-01-01 05:00:00      21      11      12      13\n      SQL The SQL docs Reading from databases with SQL   Excel The Excel docs Reading from a filelike handle Modifying formatting in XlsxWriter output Loading only visible sheets GH19842#issuecomment-892150745   HTML Reading HTML tables from a server that cannot handle the default request header   HDFStore The HDFStores docs Simple queries with a Timestamp Index Managing heterogeneous data using a linked multiple table hierarchy GH3032 Merging on-disk tables with millions of rows Avoiding inconsistencies when writing to a store from multiple processes\/threads De-duplicating a large store by chunks, essentially a recursive reduction operation. Shows a function for taking in data from csv file and creating a store by chunks, with date parsing as well. See here Creating a store chunk-by-chunk from a csv file Appending to a store, while creating a unique index Large Data work flows Reading in a sequence of files, then providing a global unique index to a store while appending Groupby on a HDFStore with low group density Groupby on a HDFStore with high group density Hierarchical queries on a HDFStore Counting with a HDFStore Troubleshoot HDFStore exceptions Setting min_itemsize with strings Using ptrepack to create a completely-sorted-index on a store Storing Attributes to a group node \nIn [206]: df = pd.DataFrame(np.random.randn(8, 3))\n\nIn [207]: store = pd.HDFStore(\"test.h5\")\n\nIn [208]: store.put(\"df\", df)\n\n# you can store an arbitrary Python object via pickle\nIn [209]: store.get_storer(\"df\").attrs.my_attribute = {\"A\": 10}\n\nIn [210]: store.get_storer(\"df\").attrs.my_attribute\nOut[210]: {'A': 10}\n  You can create or load a HDFStore in-memory by passing the driver parameter to PyTables. Changes are only written to disk when the HDFStore is closed. \nIn [211]: store = pd.HDFStore(\"test.h5\", \"w\", driver=\"H5FD_CORE\")\n\nIn [212]: df = pd.DataFrame(np.random.randn(8, 3))\n\nIn [213]: store[\"test\"] = df\n\n# only after closing the store, data is written to disk:\nIn [214]: store.close()\n    Binary files pandas readily accepts NumPy record arrays, if you need to read in a binary file consisting of an array of C structs. For example, given this C program in a file called main.c compiled with gcc main.c -std=gnu99 on a 64-bit machine, \n#include <stdio.h>\n#include <stdint.h>\n\ntypedef struct _Data\n{\n    int32_t count;\n    double avg;\n    float scale;\n} Data;\n\nint main(int argc, const char *argv[])\n{\n    size_t n = 10;\n    Data d[n];\n\n    for (int i = 0; i < n; ++i)\n    {\n        d[i].count = i;\n        d[i].avg = i + 1.0;\n        d[i].scale = (float) i + 2.0f;\n    }\n\n    FILE *file = fopen(\"binary.dat\", \"wb\");\n    fwrite(&d, sizeof(Data), n, file);\n    fclose(file);\n\n    return 0;\n}\n  the following Python code will read the binary file 'binary.dat' into a pandas DataFrame, where each element of the struct corresponds to a column in the frame: \nnames = \"count\", \"avg\", \"scale\"\n\n# note that the offsets are larger than the size of the type because of\n# struct padding\noffsets = 0, 8, 16\nformats = \"i4\", \"f8\", \"f4\"\ndt = np.dtype({\"names\": names, \"offsets\": offsets, \"formats\": formats}, align=True)\ndf = pd.DataFrame(np.fromfile(\"binary.dat\", dt))\n   Note The offsets of the structure elements may be different depending on the architecture of the machine on which the file was created. Using a raw binary file format like this for general data storage is not recommended, as it is not cross platform. We recommended either HDF5 or parquet, both of which are supported by pandas\u2019 IO facilities.     Computation Numerical integration (sample-based) of a time series  Correlation Often it\u2019s useful to obtain the lower (or upper) triangular form of a correlation matrix calculated from DataFrame.corr(). This can be achieved by passing a boolean mask to where as follows: \nIn [215]: df = pd.DataFrame(np.random.random(size=(100, 5)))\n\nIn [216]: corr_mat = df.corr()\n\nIn [217]: mask = np.tril(np.ones_like(corr_mat, dtype=np.bool_), k=-1)\n\nIn [218]: corr_mat.where(mask)\nOut[218]: \n          0         1         2        3   4\n0       NaN       NaN       NaN      NaN NaN\n1 -0.079861       NaN       NaN      NaN NaN\n2 -0.236573  0.183801       NaN      NaN NaN\n3 -0.013795 -0.051975  0.037235      NaN NaN\n4 -0.031974  0.118342 -0.073499 -0.02063 NaN\n  The method argument within DataFrame.corr can accept a callable in addition to the named correlation types. Here we compute the distance correlation matrix for a DataFrame object. \nIn [219]: def distcorr(x, y):\n   .....:     n = len(x)\n   .....:     a = np.zeros(shape=(n, n))\n   .....:     b = np.zeros(shape=(n, n))\n   .....:     for i in range(n):\n   .....:         for j in range(i + 1, n):\n   .....:             a[i, j] = abs(x[i] - x[j])\n   .....:             b[i, j] = abs(y[i] - y[j])\n   .....:     a += a.T\n   .....:     b += b.T\n   .....:     a_bar = np.vstack([np.nanmean(a, axis=0)] * n)\n   .....:     b_bar = np.vstack([np.nanmean(b, axis=0)] * n)\n   .....:     A = a - a_bar - a_bar.T + np.full(shape=(n, n), fill_value=a_bar.mean())\n   .....:     B = b - b_bar - b_bar.T + np.full(shape=(n, n), fill_value=b_bar.mean())\n   .....:     cov_ab = np.sqrt(np.nansum(A * B)) \/ n\n   .....:     std_a = np.sqrt(np.sqrt(np.nansum(A ** 2)) \/ n)\n   .....:     std_b = np.sqrt(np.sqrt(np.nansum(B ** 2)) \/ n)\n   .....:     return cov_ab \/ std_a \/ std_b\n   .....: \n\nIn [220]: df = pd.DataFrame(np.random.normal(size=(100, 3)))\n\nIn [221]: df.corr(method=distcorr)\nOut[221]: \n          0         1         2\n0  1.000000  0.197613  0.216328\n1  0.197613  1.000000  0.208749\n2  0.216328  0.208749  1.000000\n     Timedeltas The Timedeltas docs. Using timedeltas \nIn [222]: import datetime\n\nIn [223]: s = pd.Series(pd.date_range(\"2012-1-1\", periods=3, freq=\"D\"))\n\nIn [224]: s - s.max()\nOut[224]: \n0   -2 days\n1   -1 days\n2    0 days\ndtype: timedelta64[ns]\n\nIn [225]: s.max() - s\nOut[225]: \n0   2 days\n1   1 days\n2   0 days\ndtype: timedelta64[ns]\n\nIn [226]: s - datetime.datetime(2011, 1, 1, 3, 5)\nOut[226]: \n0   364 days 20:55:00\n1   365 days 20:55:00\n2   366 days 20:55:00\ndtype: timedelta64[ns]\n\nIn [227]: s + datetime.timedelta(minutes=5)\nOut[227]: \n0   2012-01-01 00:05:00\n1   2012-01-02 00:05:00\n2   2012-01-03 00:05:00\ndtype: datetime64[ns]\n\nIn [228]: datetime.datetime(2011, 1, 1, 3, 5) - s\nOut[228]: \n0   -365 days +03:05:00\n1   -366 days +03:05:00\n2   -367 days +03:05:00\ndtype: timedelta64[ns]\n\nIn [229]: datetime.timedelta(minutes=5) + s\nOut[229]: \n0   2012-01-01 00:05:00\n1   2012-01-02 00:05:00\n2   2012-01-03 00:05:00\ndtype: datetime64[ns]\n  Adding and subtracting deltas and dates \nIn [230]: deltas = pd.Series([datetime.timedelta(days=i) for i in range(3)])\n\nIn [231]: df = pd.DataFrame({\"A\": s, \"B\": deltas})\n\nIn [232]: df\nOut[232]: \n           A      B\n0 2012-01-01 0 days\n1 2012-01-02 1 days\n2 2012-01-03 2 days\n\nIn [233]: df[\"New Dates\"] = df[\"A\"] + df[\"B\"]\n\nIn [234]: df[\"Delta\"] = df[\"A\"] - df[\"New Dates\"]\n\nIn [235]: df\nOut[235]: \n           A      B  New Dates   Delta\n0 2012-01-01 0 days 2012-01-01  0 days\n1 2012-01-02 1 days 2012-01-03 -1 days\n2 2012-01-03 2 days 2012-01-05 -2 days\n\nIn [236]: df.dtypes\nOut[236]: \nA             datetime64[ns]\nB            timedelta64[ns]\nNew Dates     datetime64[ns]\nDelta        timedelta64[ns]\ndtype: object\n  Another example Values can be set to NaT using np.nan, similar to datetime \nIn [237]: y = s - s.shift()\n\nIn [238]: y\nOut[238]: \n0      NaT\n1   1 days\n2   1 days\ndtype: timedelta64[ns]\n\nIn [239]: y[1] = np.nan\n\nIn [240]: y\nOut[240]: \n0      NaT\n1      NaT\n2   1 days\ndtype: timedelta64[ns]\n    Creating example data To create a dataframe from every combination of some given values, like R\u2019s expand.grid() function, we can create a dict where the keys are column names and the values are lists of the data values: \nIn [241]: def expand_grid(data_dict):\n   .....:     rows = itertools.product(*data_dict.values())\n   .....:     return pd.DataFrame.from_records(rows, columns=data_dict.keys())\n   .....: \n\nIn [242]: df = expand_grid(\n   .....:     {\"height\": [60, 70], \"weight\": [100, 140, 180], \"sex\": [\"Male\", \"Female\"]}\n   .....: )\n   .....: \n\nIn [243]: df\nOut[243]: \n    height  weight     sex\n0       60     100    Male\n1       60     100  Female\n2       60     140    Male\n3       60     140  Female\n4       60     180    Male\n5       60     180  Female\n6       70     100    Male\n7       70     100  Female\n8       70     140    Male\n9       70     140  Female\n10      70     180    Male\n11      70     180  Female","title":"pandas.user_guide.cookbook"},{"text":"pandas.Series.loc   propertySeries.loc\n \nAccess a group of rows and columns by label(s) or a boolean array. .loc[] is primarily label based, but may also be used with a boolean array. Allowed inputs are:  A single label, e.g. 5 or 'a', (note that 5 is interpreted as a label of the index, and never as an integer position along the index). A list or array of labels, e.g. ['a', 'b', 'c']. \nA slice object with labels, e.g. 'a':'f'.  Warning Note that contrary to usual python slices, both the start and the stop are included   A boolean array of the same length as the axis being sliced, e.g. [True, False, True]. An alignable boolean Series. The index of the key will be aligned before masking. An alignable Index. The Index of the returned selection will be the input. A callable function with one argument (the calling Series or DataFrame) and that returns valid output for indexing (one of the above)  See more at Selection by Label.  Raises \n KeyError\n\nIf any items are not found.  IndexingError\n\nIf an indexed key is passed and its index is unalignable to the frame index.      See also  DataFrame.at\n\nAccess a single value for a row\/column label pair.  DataFrame.iloc\n\nAccess group of rows and columns by integer position(s).  DataFrame.xs\n\nReturns a cross-section (row(s) or column(s)) from the Series\/DataFrame.  Series.loc\n\nAccess group of values using labels.    Examples Getting values \n>>> df = pd.DataFrame([[1, 2], [4, 5], [7, 8]],\n...      index=['cobra', 'viper', 'sidewinder'],\n...      columns=['max_speed', 'shield'])\n>>> df\n            max_speed  shield\ncobra               1       2\nviper               4       5\nsidewinder          7       8\n  Single label. Note this returns the row as a Series. \n>>> df.loc['viper']\nmax_speed    4\nshield       5\nName: viper, dtype: int64\n  List of labels. Note using [[]] returns a DataFrame. \n>>> df.loc[['viper', 'sidewinder']]\n            max_speed  shield\nviper               4       5\nsidewinder          7       8\n  Single label for row and column \n>>> df.loc['cobra', 'shield']\n2\n  Slice with labels for row and single label for column. As mentioned above, note that both the start and stop of the slice are included. \n>>> df.loc['cobra':'viper', 'max_speed']\ncobra    1\nviper    4\nName: max_speed, dtype: int64\n  Boolean list with the same length as the row axis \n>>> df.loc[[False, False, True]]\n            max_speed  shield\nsidewinder          7       8\n  Alignable boolean Series: \n>>> df.loc[pd.Series([False, True, False],\n...        index=['viper', 'sidewinder', 'cobra'])]\n            max_speed  shield\nsidewinder          7       8\n  Index (same behavior as df.reindex) \n>>> df.loc[pd.Index([\"cobra\", \"viper\"], name=\"foo\")]\n       max_speed  shield\nfoo\ncobra          1       2\nviper          4       5\n  Conditional that returns a boolean Series \n>>> df.loc[df['shield'] > 6]\n            max_speed  shield\nsidewinder          7       8\n  Conditional that returns a boolean Series with column labels specified \n>>> df.loc[df['shield'] > 6, ['max_speed']]\n            max_speed\nsidewinder          7\n  Callable that returns a boolean Series \n>>> df.loc[lambda df: df['shield'] == 8]\n            max_speed  shield\nsidewinder          7       8\n  Setting values Set value for all items matching the list of labels \n>>> df.loc[['viper', 'sidewinder'], ['shield']] = 50\n>>> df\n            max_speed  shield\ncobra               1       2\nviper               4      50\nsidewinder          7      50\n  Set value for an entire row \n>>> df.loc['cobra'] = 10\n>>> df\n            max_speed  shield\ncobra              10      10\nviper               4      50\nsidewinder          7      50\n  Set value for an entire column \n>>> df.loc[:, 'max_speed'] = 30\n>>> df\n            max_speed  shield\ncobra              30      10\nviper              30      50\nsidewinder         30      50\n  Set value for rows matching callable condition \n>>> df.loc[df['shield'] > 35] = 0\n>>> df\n            max_speed  shield\ncobra              30      10\nviper               0       0\nsidewinder          0       0\n  Getting values on a DataFrame with an index that has integer labels Another example using integers for the index \n>>> df = pd.DataFrame([[1, 2], [4, 5], [7, 8]],\n...      index=[7, 8, 9], columns=['max_speed', 'shield'])\n>>> df\n   max_speed  shield\n7          1       2\n8          4       5\n9          7       8\n  Slice with integer labels for rows. As mentioned above, note that both the start and stop of the slice are included. \n>>> df.loc[7:9]\n   max_speed  shield\n7          1       2\n8          4       5\n9          7       8\n  Getting values with a MultiIndex A number of examples using a DataFrame with a MultiIndex \n>>> tuples = [\n...    ('cobra', 'mark i'), ('cobra', 'mark ii'),\n...    ('sidewinder', 'mark i'), ('sidewinder', 'mark ii'),\n...    ('viper', 'mark ii'), ('viper', 'mark iii')\n... ]\n>>> index = pd.MultiIndex.from_tuples(tuples)\n>>> values = [[12, 2], [0, 4], [10, 20],\n...         [1, 4], [7, 1], [16, 36]]\n>>> df = pd.DataFrame(values, columns=['max_speed', 'shield'], index=index)\n>>> df\n                     max_speed  shield\ncobra      mark i           12       2\n           mark ii           0       4\nsidewinder mark i           10      20\n           mark ii           1       4\nviper      mark ii           7       1\n           mark iii         16      36\n  Single label. Note this returns a DataFrame with a single index. \n>>> df.loc['cobra']\n         max_speed  shield\nmark i          12       2\nmark ii          0       4\n  Single index tuple. Note this returns a Series. \n>>> df.loc[('cobra', 'mark ii')]\nmax_speed    0\nshield       4\nName: (cobra, mark ii), dtype: int64\n  Single label for row and column. Similar to passing in a tuple, this returns a Series. \n>>> df.loc['cobra', 'mark i']\nmax_speed    12\nshield        2\nName: (cobra, mark i), dtype: int64\n  Single tuple. Note using [[]] returns a DataFrame. \n>>> df.loc[[('cobra', 'mark ii')]]\n               max_speed  shield\ncobra mark ii          0       4\n  Single tuple for the index with a single label for the column \n>>> df.loc[('cobra', 'mark i'), 'shield']\n2\n  Slice from index tuple to single label \n>>> df.loc[('cobra', 'mark i'):'viper']\n                     max_speed  shield\ncobra      mark i           12       2\n           mark ii           0       4\nsidewinder mark i           10      20\n           mark ii           1       4\nviper      mark ii           7       1\n           mark iii         16      36\n  Slice from index tuple to index tuple \n>>> df.loc[('cobra', 'mark i'):('viper', 'mark ii')]\n                    max_speed  shield\ncobra      mark i          12       2\n           mark ii          0       4\nsidewinder mark i          10      20\n           mark ii          1       4\nviper      mark ii          7       1","title":"pandas.reference.api.pandas.series.loc"},{"text":"pandas.core.resample.Resampler.get_group   Resampler.get_group(name, obj=None)[source]\n \nConstruct DataFrame from group with provided name.  Parameters \n \nname:object\n\n\nThe name of the group to get as a DataFrame.  \nobj:DataFrame, default None\n\n\nThe DataFrame to take the DataFrame out of. If it is None, the object groupby was called on will be used.    Returns \n \ngroup:same type as obj","title":"pandas.reference.api.pandas.core.resample.resampler.get_group"}]}
{"task_id":10487278,"prompt":"def f_10487278(my_dict):\n\t","suffix":"\n\treturn my_dict","canonical_solution":"my_dict.update({'third_key': 1})","test_start":"\ndef check(candidate):","test":["\n    my_dict = {'a':1, 'b':2}\n    assert candidate(my_dict) == {'a':1, 'b':2, 'third_key': 1}\n","\n    my_dict = {'c':1, 'd':2}\n    assert candidate(my_dict) == {'c':1, 'd':2, 'third_key': 1}\n"],"entry_point":"f_10487278","intent":"add an item with key 'third_key' and value 1 to an dictionary `my_dict`","library":[],"docs":[]}
{"task_id":10487278,"prompt":"def f_10487278():\n\t","suffix":"\n\treturn my_list","canonical_solution":"my_list = []","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == []\n"],"entry_point":"f_10487278","intent":"declare an array `my_list`","library":[],"docs":[]}
{"task_id":10487278,"prompt":"def f_10487278(my_list):\n\t","suffix":"\n\treturn my_list","canonical_solution":"my_list.append(12)","test_start":"\ndef check(candidate):","test":["\n    assert candidate([1,2]) == [1, 2, 12] \n","\n    assert candidate([5,6]) == [5, 6, 12]\n"],"entry_point":"f_10487278","intent":"Insert item `12` to a list `my_list`","library":[],"docs":[]}
{"task_id":10155684,"prompt":"def f_10155684(myList):\n\t","suffix":"\n\treturn myList","canonical_solution":"myList.insert(0, 'wuggah')","test_start":"\ndef check(candidate):","test":["\n    assert candidate([1,2]) == ['wuggah', 1, 2]\n","\n    assert candidate([]) == ['wuggah'] \n"],"entry_point":"f_10155684","intent":"add an entry 'wuggah' at the beginning of list `myList`","library":[],"docs":[]}
{"task_id":3519125,"prompt":"def f_3519125(hex_str):\n\treturn ","suffix":"","canonical_solution":"bytes.fromhex(hex_str.replace('\\\\x', ''))","test_start":"\ndef check(candidate):","test":["\n    assert candidate(\"\\\\xF3\\\\xBE\\\\x80\\\\x80\") == b'\\xf3\\xbe\\x80\\x80'\n"],"entry_point":"f_3519125","intent":"convert a hex-string representation `hex_str` to actual bytes","library":[],"docs":[]}
{"task_id":40144769,"prompt":"def f_40144769(df):\n\treturn ","suffix":"","canonical_solution":"df[df.columns[-1]]","test_start":"\nimport pandas as pd \n\ndef check(candidate):","test":["\n    df = pd.DataFrame([[1, 2, 3],[4,5,6]], columns=[\"a\", \"b\", \"c\"])\n    assert candidate(df).tolist() == [3,6]\n","\n    df = pd.DataFrame([[\"Hello\", \"world!\"],[\"Hi\", \"world!\"]], columns=[\"a\", \"b\"])\n    assert candidate(df).tolist() == [\"world!\", \"world!\"]\n"],"entry_point":"f_40144769","intent":"select the last column of dataframe `df`","library":["pandas"],"docs":[{"text":"class LastValue(expression, **extra)","title":"django.ref.models.database-functions#django.db.models.functions.LastValue"},{"text":"pandas.core.groupby.GroupBy.last   finalGroupBy.last(numeric_only=False, min_count=- 1)[source]\n \nCompute last of group values.  Parameters \n \nnumeric_only:bool, default False\n\n\nInclude only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data.  \nmin_count:int, default -1\n\n\nThe required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA.    Returns \n Series or DataFrame\n\nComputed last of values within each group.","title":"pandas.reference.api.pandas.core.groupby.groupby.last"},{"text":"last()","title":"django.ref.models.querysets#django.db.models.query.QuerySet.last"},{"text":"pandas.core.resample.Resampler.last   Resampler.last(_method='last', min_count=0, *args, **kwargs)[source]\n \nCompute last of group values.  Parameters \n \nnumeric_only:bool, default False\n\n\nInclude only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data.  \nmin_count:int, default -1\n\n\nThe required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA.    Returns \n Series or DataFrame\n\nComputed last of values within each group.","title":"pandas.reference.api.pandas.core.resample.resampler.last"},{"text":"pandas.DataFrame.last   DataFrame.last(offset)[source]\n \nSelect final periods of time series data based on a date offset. For a DataFrame with a sorted DatetimeIndex, this function selects the last few rows based on a date offset.  Parameters \n \noffset:str, DateOffset, dateutil.relativedelta\n\n\nThe offset length of the data that will be selected. For instance, \u20183D\u2019 will display all the rows having their index within the last 3 days.    Returns \n Series or DataFrame\n\nA subset of the caller.    Raises \n TypeError\n\nIf the index is not a DatetimeIndex      See also  first\n\nSelect initial periods of time series based on a date offset.  at_time\n\nSelect values at a particular time of the day.  between_time\n\nSelect values between particular times of the day.    Examples \n>>> i = pd.date_range('2018-04-09', periods=4, freq='2D')\n>>> ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i)\n>>> ts\n            A\n2018-04-09  1\n2018-04-11  2\n2018-04-13  3\n2018-04-15  4\n  Get the rows for the last 3 days: \n>>> ts.last('3D')\n            A\n2018-04-13  3\n2018-04-15  4\n  Notice the data for 3 last calendar days were returned, not the last 3 observed days in the dataset, and therefore data for 2018-04-11 was not returned.","title":"pandas.reference.api.pandas.dataframe.last"},{"text":"is_last_col()[source]\n \n[Deprecated] Notes  Deprecated since version 3.4:","title":"matplotlib._as_gen.matplotlib.axes.subplotbase#matplotlib.axes.SubplotBase.is_last_col"},{"text":"pandas.Series.last   Series.last(offset)[source]\n \nSelect final periods of time series data based on a date offset. For a DataFrame with a sorted DatetimeIndex, this function selects the last few rows based on a date offset.  Parameters \n \noffset:str, DateOffset, dateutil.relativedelta\n\n\nThe offset length of the data that will be selected. For instance, \u20183D\u2019 will display all the rows having their index within the last 3 days.    Returns \n Series or DataFrame\n\nA subset of the caller.    Raises \n TypeError\n\nIf the index is not a DatetimeIndex      See also  first\n\nSelect initial periods of time series based on a date offset.  at_time\n\nSelect values at a particular time of the day.  between_time\n\nSelect values between particular times of the day.    Examples \n>>> i = pd.date_range('2018-04-09', periods=4, freq='2D')\n>>> ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i)\n>>> ts\n            A\n2018-04-09  1\n2018-04-11  2\n2018-04-13  3\n2018-04-15  4\n  Get the rows for the last 3 days: \n>>> ts.last('3D')\n            A\n2018-04-13  3\n2018-04-15  4\n  Notice the data for 3 last calendar days were returned, not the last 3 observed days in the dataset, and therefore data for 2018-04-11 was not returned.","title":"pandas.reference.api.pandas.series.last"},{"text":"pandas.DataFrame.last_valid_index   DataFrame.last_valid_index()[source]\n \nReturn index for last non-NA value or None, if no NA value is found.  Returns \n \nscalar:type of index\n\n   Notes If all elements are non-NA\/null, returns None. Also returns None for empty Series\/DataFrame.","title":"pandas.reference.api.pandas.dataframe.last_valid_index"},{"text":"latest(*fields)","title":"django.ref.models.querysets#django.db.models.query.QuerySet.latest"},{"text":"pandas.core.groupby.GroupBy.tail   finalGroupBy.tail(n=5)[source]\n \nReturn last n rows of each group. Similar to .apply(lambda x: x.tail(n)), but it returns a subset of rows from the original DataFrame with original index and order preserved (as_index flag is ignored).  Parameters \n \nn:int\n\n\nIf positive: number of entries to include from end of each group. If negative: number of entries to exclude from start of each group.    Returns \n Series or DataFrame\n\nSubset of original Series or DataFrame as determined by n.      See also  Series.groupby\n\nApply a function groupby to a Series.  DataFrame.groupby\n\nApply a function groupby to each row or column of a DataFrame.    Examples \n>>> df = pd.DataFrame([['a', 1], ['a', 2], ['b', 1], ['b', 2]],\n...                   columns=['A', 'B'])\n>>> df.groupby('A').tail(1)\n   A  B\n1  a  2\n3  b  2\n>>> df.groupby('A').tail(-1)\n   A  B\n1  a  2\n3  b  2","title":"pandas.reference.api.pandas.core.groupby.groupby.tail"}]}
{"task_id":30787901,"prompt":"def f_30787901(df):\n\treturn ","suffix":"","canonical_solution":"df.loc[df['Letters'] == 'C', 'Letters'].values[0]","test_start":"\nimport pandas as pd \n\ndef check(candidate):","test":["\n    df = pd.DataFrame([[\"a\", 1],[\"C\", 6]], columns=[\"Letters\", \"Numbers\"])\n    assert candidate(df) == 'C'\n","\n    df = pd.DataFrame([[None, 1],[\"C\", 789]], columns=[\"Letters\", \"Names\"])\n    assert candidate(df) == 'C'\n"],"entry_point":"f_30787901","intent":"get the first value from dataframe `df` where column 'Letters' is equal to 'C'","library":["pandas"],"docs":[{"text":"stringprep.in_table_c11_c12(code)  \nDetermine whether code is in tableC.1 (Space characters, union of C.1.1 and C.1.2).","title":"python.library.stringprep#stringprep.in_table_c11_c12"},{"text":"stringprep.in_table_c4(code)  \nDetermine whether code is in tableC.4 (Non-character code points).","title":"python.library.stringprep#stringprep.in_table_c4"},{"text":"class torch.distributions.chi2.Chi2(df, validate_args=None) [source]\n \nBases: torch.distributions.gamma.Gamma Creates a Chi2 distribution parameterized by shape parameter df. This is exactly equivalent to Gamma(alpha=0.5*df, beta=0.5) Example: >>> m = Chi2(torch.tensor([1.0]))\n>>> m.sample()  # Chi2 distributed with shape df=1\ntensor([ 0.1046])\n  Parameters \ndf (float or Tensor) \u2013 shape parameter of the distribution    \narg_constraints = {'df': GreaterThan(lower_bound=0.0)} \n  \nproperty df \n  \nexpand(batch_shape, _instance=None) [source]","title":"torch.distributions#torch.distributions.chi2.Chi2"},{"text":"stringprep.in_table_c21_c22(code)  \nDetermine whether code is in tableC.2 (Control characters, union of C.2.1 and C.2.2).","title":"python.library.stringprep#stringprep.in_table_c21_c22"},{"text":"stringprep.in_table_c3(code)  \nDetermine whether code is in tableC.3 (Private use).","title":"python.library.stringprep#stringprep.in_table_c3"},{"text":"stringprep.in_table_c21(code)  \nDetermine whether code is in tableC.2.1 (ASCII control characters).","title":"python.library.stringprep#stringprep.in_table_c21"},{"text":"stringprep.in_table_c9(code)  \nDetermine whether code is in tableC.9 (Tagging characters).","title":"python.library.stringprep#stringprep.in_table_c9"},{"text":"arg_constraints = {'df': GreaterThan(lower_bound=0.0)}","title":"torch.distributions#torch.distributions.chi2.Chi2.arg_constraints"},{"text":"stringprep.map_table_b3(code)  \nReturn the mapped value for code according to tableB.3 (Mapping for case-folding used with no normalization).","title":"python.library.stringprep#stringprep.map_table_b3"},{"text":"stringprep.in_table_c11(code)  \nDetermine whether code is in tableC.1.1 (ASCII space characters).","title":"python.library.stringprep#stringprep.in_table_c11"}]}
{"task_id":18730044,"prompt":"def f_18730044():\n\treturn ","suffix":"","canonical_solution":"np.column_stack(([1, 2, 3], [4, 5, 6]))","test_start":"\nimport numpy as np \n\ndef check(candidate):","test":["\n    assert np.all(candidate() == np.array([[1, 4], [2, 5], [3, 6]]))\n"],"entry_point":"f_18730044","intent":"converting two lists `[1, 2, 3]` and `[4, 5, 6]` into a matrix","library":["numpy"],"docs":[{"text":"numpy.bmat   numpy.bmat(obj, ldict=None, gdict=None)[source]\n \nBuild a matrix object from a string, nested sequence, or array.  Parameters \n \nobjstr or array_like\n\n\nInput data. If a string, variables in the current scope may be referenced by name.  \nldictdict, optional\n\n\nA dictionary that replaces local operands in current frame. Ignored if obj is not a string or gdict is None.  \ngdictdict, optional\n\n\nA dictionary that replaces global operands in current frame. Ignored if obj is not a string.    Returns \n \noutmatrix\n\n\nReturns a matrix object, which is a specialized 2-D array.      See also  block\n\nA generalization of this function for N-d arrays, that returns normal ndarrays.    Examples >>> A = np.mat('1 1; 1 1')\n>>> B = np.mat('2 2; 2 2')\n>>> C = np.mat('3 4; 5 6')\n>>> D = np.mat('7 8; 9 0')\n All the following expressions construct the same block matrix: >>> np.bmat([[A, B], [C, D]])\nmatrix([[1, 1, 2, 2],\n        [1, 1, 2, 2],\n        [3, 4, 7, 8],\n        [5, 6, 9, 0]])\n>>> np.bmat(np.r_[np.c_[A, B], np.c_[C, D]])\nmatrix([[1, 1, 2, 2],\n        [1, 1, 2, 2],\n        [3, 4, 7, 8],\n        [5, 6, 9, 0]])\n>>> np.bmat('A,B; C,D')\nmatrix([[1, 1, 2, 2],\n        [1, 1, 2, 2],\n        [3, 4, 7, 8],\n        [5, 6, 9, 0]])","title":"numpy.reference.generated.numpy.bmat"},{"text":"tf.keras.utils.to_categorical     View source on GitHub    Converts a class vector (integers) to binary class matrix.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.keras.utils.to_categorical  \ntf.keras.utils.to_categorical(\n    y, num_classes=None, dtype='float32'\n)\n E.g. for use with categorical_crossentropy.\n \n\n\n Arguments\n  y   class vector to be converted into a matrix (integers from 0 to num_classes).  \n  num_classes   total number of classes. If None, this would be inferred as the (largest number in y) + 1.  \n  dtype   The data type expected by the input. Default: 'float32'.   \n \n\n\n Returns   A binary matrix representation of the input. The classes axis is placed last.  \n Example: \na = tf.keras.utils.to_categorical([0, 1, 2, 3], num_classes=4)\na = tf.constant(a, shape=[4, 4])\nprint(a)\ntf.Tensor(\n  [[1. 0. 0. 0.]\n   [0. 1. 0. 0.]\n   [0. 0. 1. 0.]\n   [0. 0. 0. 1.]], shape=(4, 4), dtype=float32)\n \nb = tf.constant([.9, .04, .03, .03,\n                 .3, .45, .15, .13,\n                 .04, .01, .94, .05,\n                 .12, .21, .5, .17],\n                shape=[4, 4])\nloss = tf.keras.backend.categorical_crossentropy(a, b)\nprint(np.around(loss, 5))\n[0.10536 0.82807 0.1011  1.77196]\n \nloss = tf.keras.backend.categorical_crossentropy(a, a)\nprint(np.around(loss, 5))\n[0. 0. 0. 0.]\n\n \n\n\n Raises   Value Error: If input contains string value","title":"tensorflow.keras.utils.to_categorical"},{"text":"curses.getsyx()  \nReturn the current coordinates of the virtual screen cursor as a tuple (y, x). If leaveok is currently True, then return (-1, -1).","title":"python.library.curses#curses.getsyx"},{"text":"get_canvas_width_height()[source]\n \nReturn the canvas width and height in display coords.","title":"matplotlib.backend_cairo_api#matplotlib.backends.backend_cairo.RendererCairo.get_canvas_width_height"},{"text":"tf.compat.v1.initialize_variables See tf.compat.v1.variables_initializer. (deprecated) \ntf.compat.v1.initialize_variables(\n    var_list, name='init'\n)\n Warning: THIS FUNCTION IS DEPRECATED. It will be removed after 2017-03-02. Instructions for updating: Use tf.variables_initializer instead.\nNote: The output of this function should be used. If it is not, a warning will be logged or an error may be raised. To mark the output as used, call its .mark_used() method.","title":"tensorflow.compat.v1.initialize_variables"},{"text":"now()  \nReturns a datetime that represents the current point in time. Exactly what\u2019s returned depends on the value of USE_TZ:  If USE_TZ is False, this will be a naive datetime (i.e. a datetime without an associated timezone) that represents the current time in the system\u2019s local timezone. If USE_TZ is True, this will be an aware datetime representing the current time in UTC. Note that now() will always return times in UTC regardless of the value of TIME_ZONE; you can use localtime() to get the time in the current time zone.","title":"django.ref.utils#django.utils.timezone.now"},{"text":"window.getbegyx()  \nReturn a tuple (y, x) of co-ordinates of upper-left corner.","title":"python.library.curses#curses.window.getbegyx"},{"text":"flask.has_app_context()  \nWorks like has_request_context() but for the application context. You can also just do a boolean check on the current_app object instead.  Changelog New in version 0.9.   Return type \nbool","title":"flask.api.index#flask.has_app_context"},{"text":"numpy.column_stack   numpy.column_stack(tup)[source]\n \nStack 1-D arrays as columns into a 2-D array. Take a sequence of 1-D arrays and stack them as columns to make a single 2-D array. 2-D arrays are stacked as-is, just like with hstack. 1-D arrays are turned into 2-D columns first.  Parameters \n \ntupsequence of 1-D or 2-D arrays.\n\n\nArrays to stack. All of them must have the same first dimension.    Returns \n \nstacked2-D array\n\n\nThe array formed by stacking the given arrays.      See also  \nstack, hstack, vstack, concatenate\n\n  Examples >>> a = np.array((1,2,3))\n>>> b = np.array((2,3,4))\n>>> np.column_stack((a,b))\narray([[1, 2],\n       [2, 3],\n       [3, 4]])","title":"numpy.reference.generated.numpy.column_stack"},{"text":"bmm(batch2) \u2192 Tensor  \nSee torch.bmm()","title":"torch.tensors#torch.Tensor.bmm"}]}
{"task_id":402504,"prompt":"def f_402504(i):\n\treturn ","suffix":"","canonical_solution":"type(i)","test_start":"\ndef check(candidate):","test":["\n    assert candidate(\"hello\") is str\n","\n    assert candidate(123) is int\n","\n    assert candidate(\"123\") is str\n","\n    assert candidate(123.4) is float\n"],"entry_point":"f_402504","intent":"get the type of `i`","library":[],"docs":[]}
{"task_id":402504,"prompt":"def f_402504(v):\n\treturn ","suffix":"","canonical_solution":"type(v)","test_start":"\ndef check(candidate):","test":["\n    assert candidate(\"hello\") is str\n","\n    assert candidate(123) is int\n","\n    assert candidate(\"123\") is str\n","\n    assert candidate(123.4) is float\n"],"entry_point":"f_402504","intent":"determine the type of variable `v`","library":[],"docs":[]}
{"task_id":402504,"prompt":"def f_402504(v):\n\treturn ","suffix":"","canonical_solution":"type(v)","test_start":"\ndef check(candidate):","test":["\n    assert candidate(\"hello\") is str\n","\n    assert candidate(123) is int\n","\n    assert candidate(\"123\") is str\n","\n    assert candidate(123.4) is float\n"],"entry_point":"f_402504","intent":"determine the type of variable `v`","library":[],"docs":[]}
{"task_id":402504,"prompt":"def f_402504(variable_name):\n\treturn ","suffix":"","canonical_solution":"type(variable_name)","test_start":"\ndef check(candidate):","test":["\n    assert candidate(\"hello\") is str\n","\n    assert candidate(123) is int\n","\n    assert candidate(\"123\") is str\n","\n    assert candidate(123.4) is float\n"],"entry_point":"f_402504","intent":"get the type of variable `variable_name`","library":[],"docs":[]}
{"task_id":2300756,"prompt":"def f_2300756(g):\n\treturn ","suffix":"","canonical_solution":"next(itertools.islice(g, 5, 5 + 1))","test_start":"\nimport itertools\n\ndef check(candidate):","test":["\n    test = [1, 2, 3, 4, 5, 6, 7]\n    assert(candidate(test) == 6)\n"],"entry_point":"f_2300756","intent":"get the 5th item of a generator `g`","library":["itertools"],"docs":[{"text":"inspect.getgeneratorstate(generator)  \nGet current state of a generator-iterator.  Possible states are:\n\n GEN_CREATED: Waiting to start execution. GEN_RUNNING: Currently being executed by the interpreter. GEN_SUSPENDED: Currently suspended at a yield expression. GEN_CLOSED: Execution has completed.     New in version 3.2.","title":"python.library.inspect#inspect.getgeneratorstate"},{"text":"flask.stream_with_context(generator_or_function)  \nRequest contexts disappear when the response is started on the server. This is done for efficiency reasons and to make it less likely to encounter memory leaks with badly written WSGI middlewares. The downside is that if you are using streamed responses, the generator cannot access request bound information any more. This function however can help you keep the context around for longer: from flask import stream_with_context, request, Response\n\n@app.route('\/stream')\ndef streamed_response():\n    @stream_with_context\n    def generate():\n        yield 'Hello '\n        yield request.args['name']\n        yield '!'\n    return Response(generate())\n Alternatively it can also be used around a specific generator: from flask import stream_with_context, request, Response\n\n@app.route('\/stream')\ndef streamed_response():\n    def generate():\n        yield 'Hello '\n        yield request.args['name']\n        yield '!'\n    return Response(stream_with_context(generate()))\n  Changelog New in version 0.9.   Parameters \ngenerator_or_function (Union[Generator, Callable]) \u2013   Return type \nGenerator","title":"flask.api.index#flask.stream_with_context"},{"text":"class typing.Generator(Iterator[T_co], Generic[T_co, T_contra, V_co])  \nA generator can be annotated by the generic type Generator[YieldType, SendType, ReturnType]. For example: def echo_round() -> Generator[int, float, str]:\n    sent = yield 0\n    while sent >= 0:\n        sent = yield round(sent)\n    return 'Done'\n Note that unlike many other generics in the typing module, the SendType of Generator behaves contravariantly, not covariantly or invariantly. If your generator will only yield values, set the SendType and ReturnType to None: def infinite_stream(start: int) -> Generator[int, None, None]:\n    while True:\n        yield start\n        start += 1\n Alternatively, annotate your generator as having a return type of either Iterable[YieldType] or Iterator[YieldType]: def infinite_stream(start: int) -> Iterator[int]:\n    while True:\n        yield start\n        start += 1\n  Deprecated since version 3.9: collections.abc.Generator now supports []. See PEP 585 and Generic Alias Type.","title":"python.library.typing#typing.Generator"},{"text":"Match.__getitem__(g)  \nThis is identical to m.group(g). This allows easier access to an individual group from a match: >>> m = re.match(r\"(\\w+) (\\w+)\", \"Isaac Newton, physicist\")\n>>> m[0]       # The entire match\n'Isaac Newton'\n>>> m[1]       # The first parenthesized subgroup.\n'Isaac'\n>>> m[2]       # The second parenthesized subgroup.\n'Newton'\n  New in version 3.6.","title":"python.library.re#re.Match.__getitem__"},{"text":"flask.appcontext_pushed  \nThis signal is sent when an application context is pushed. The sender is the application. This is usually useful for unittests in order to temporarily hook in information. For instance it can be used to set a resource early onto the g object. Example usage: from contextlib import contextmanager\nfrom flask import appcontext_pushed\n\n@contextmanager\ndef user_set(app, user):\n    def handler(sender, **kwargs):\n        g.user = user\n    with appcontext_pushed.connected_to(handler, app):\n        yield\n And in the testcode: def test_user_me(self):\n    with user_set(app, 'john'):\n        c = app.test_client()\n        resp = c.get('\/users\/me')\n        assert resp.data == 'username=john'\n  Changelog New in version 0.10.","title":"flask.api.index#flask.appcontext_pushed"},{"text":"decimal.ROUND_05UP  \nRound away from zero if last digit after rounding towards zero would have been 0 or 5; otherwise round towards zero.","title":"python.library.decimal#decimal.ROUND_05UP"},{"text":"g \n Gets or sets the green value of the Color. g -> int  The green value of the Color.","title":"pygame.ref.color#pygame.Color.g"},{"text":"inspect.getgeneratorlocals(generator)  \nGet the mapping of live local variables in generator to their current values. A dictionary is returned that maps from variable names to values. This is the equivalent of calling locals() in the body of the generator, and all the same caveats apply. If generator is a generator with no currently associated frame, then an empty dictionary is returned. TypeError is raised if generator is not a Python generator object.  CPython implementation detail: This function relies on the generator exposing a Python stack frame for introspection, which isn\u2019t guaranteed to be the case in all implementations of Python. In such cases, this function will always return an empty dictionary.   New in version 3.3.","title":"python.library.inspect#inspect.getgeneratorlocals"},{"text":"sum(iterable, \/, start=0)  \nSums start and the items of an iterable from left to right and returns the total. The iterable\u2019s items are normally numbers, and the start value is not allowed to be a string. For some use cases, there are good alternatives to sum(). The preferred, fast way to concatenate a sequence of strings is by calling ''.join(sequence). To add floating point values with extended precision, see math.fsum(). To concatenate a series of iterables, consider using itertools.chain().  Changed in version 3.8: The start parameter can be specified as a keyword argument.","title":"python.library.functions#sum"},{"text":"inspect.isgeneratorfunction(object)  \nReturn True if the object is a Python generator function.  Changed in version 3.8: Functions wrapped in functools.partial() now return True if the wrapped function is a Python generator function.","title":"python.library.inspect#inspect.isgeneratorfunction"}]}
{"task_id":20056548,"prompt":"def f_20056548(word):\n\treturn ","suffix":"","canonical_solution":"'\"{}\"'.format(word)","test_start":"\ndef check(candidate):","test":["\n    assert candidate('Some Random Word') == '\"Some Random Word\"'\n"],"entry_point":"f_20056548","intent":"return a string `word` with string format","library":[],"docs":[]}
{"task_id":8546245,"prompt":"def f_8546245(list):\n\treturn ","suffix":"","canonical_solution":"\"\"\" \"\"\".join(list)","test_start":"\ndef check(candidate):","test":["\n    test = ['hello', 'good', 'morning']\n    assert candidate(test) == \"hello good morning\"\n"],"entry_point":"f_8546245","intent":"join a list of strings `list` using a space ' '","library":[],"docs":[]}
{"task_id":2276416,"prompt":"def f_2276416():\n\t","suffix":"\n\treturn y","canonical_solution":"y = [[] for n in range(2)]","test_start":"\ndef check(candidate):","test":["\n    assert(candidate() == [[], []])\n"],"entry_point":"f_2276416","intent":"create list `y` containing two empty lists","library":[],"docs":[]}
{"task_id":3925614,"prompt":"def f_3925614(filename):\n\t","suffix":"\n\treturn data","canonical_solution":"data = [line.strip() for line in open(filename, 'r')]","test_start":"\ndef check(candidate):","test":["\n    file1 = open(\"myfile.txt\", \"w\")\n    L = [\"This is Delhi \\n\", \"This is Paris \\n\", \"This is London \\n\"]\n    file1.writelines(L)\n    file1.close()\n    assert candidate('myfile.txt') == ['This is Delhi', 'This is Paris', 'This is London']\n"],"entry_point":"f_3925614","intent":"read a file `filename` into a list `data`","library":[],"docs":[]}
{"task_id":22187233,"prompt":"def f_22187233():\n\treturn ","suffix":"","canonical_solution":"\"\"\"\"\"\".join([char for char in 'it is icy' if char != 'i'])","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == 't s cy'\n"],"entry_point":"f_22187233","intent":"delete all occurrences of character 'i' in string 'it is icy'","library":[],"docs":[]}
{"task_id":22187233,"prompt":"def f_22187233():\n\treturn ","suffix":"","canonical_solution":"re.sub('i', '', 'it is icy')","test_start":"\nimport re \n\ndef check(candidate):","test":["\n    assert candidate() == 't s cy'\n"],"entry_point":"f_22187233","intent":"delete all instances of a character 'i' in a string 'it is icy'","library":["re"],"docs":[{"text":"i1i2i3 \n Gets or sets the I1I2I3 representation of the Color. i1i2i3 -> tuple  The I1I2I3 representation of the Color. The I1I2I3 components are in the ranges I1 = [0, 1], I2 = [-0.5, 0.5], I3 = [-0.5, 0.5]. Note that this will not return the absolutely exact I1I2I3 values for the set RGB values in all cases. Due to the RGB mapping from 0-255 and the I1I2I3 mapping from 0-1 rounding errors may cause the I1I2I3 values to differ slightly from what you might expect.","title":"pygame.ref.color#pygame.Color.i1i2i3"},{"text":"tf.raw_ops.InplaceSub Subtracts v into specified rows of x.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.raw_ops.InplaceSub  \ntf.raw_ops.InplaceSub(\n    x, i, v, name=None\n)\n Computes y = x; y[i, :] -= v; return y.\n \n\n\n Args\n  x   A Tensor. A Tensor of type T.  \n  i   A Tensor of type int32. A vector. Indices into the left-most dimension of x.  \n  v   A Tensor. Must have the same type as x. A Tensor of type T. Same dimension sizes as x except the first dimension, which must be the same as i's size.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor. Has the same type as x.","title":"tensorflow.raw_ops.inplacesub"},{"text":"curses.has_ic()  \nReturn True if the terminal has insert- and delete-character capabilities. This function is included for historical reasons only, as all modern software terminal emulators have such capabilities.","title":"python.library.curses#curses.has_ic"},{"text":"set_rticks(*args, **kwargs)[source]","title":"matplotlib.projections_api#matplotlib.projections.polar.PolarAxes.set_rticks"},{"text":"colorsys.yiq_to_rgb(y, i, q)  \nConvert the color from YIQ coordinates to RGB coordinates.","title":"python.library.colorsys#colorsys.yiq_to_rgb"},{"text":"numpy.nditer.remove_axis method   nditer.remove_axis(i, \/)\n \nRemoves axis i from the iterator. Requires that the flag \u201cmulti_index\u201d be enabled.","title":"numpy.reference.generated.numpy.nditer.remove_axis"},{"text":"icdf(value) [source]","title":"torch.distributions#torch.distributions.normal.Normal.icdf"},{"text":"get_shape(i) [source]\n \nShape of the i\u2019th bicluster.  Parameters \n \niint \n\nThe index of the cluster.    Returns \n \nn_rowsint \n\nNumber of rows in the bicluster.  \nn_colsint \n\nNumber of columns in the bicluster.","title":"sklearn.modules.generated.sklearn.base.biclustermixin#sklearn.base.BiclusterMixin.get_shape"},{"text":"curses.has_il()  \nReturn True if the terminal has insert- and delete-line capabilities, or can simulate them using scrolling regions. This function is included for historical reasons only, as all modern software terminal emulators have such capabilities.","title":"python.library.curses#curses.has_il"},{"text":"get_shape(i) [source]\n \nShape of the i\u2019th bicluster.  Parameters \n \niint \n\nThe index of the cluster.    Returns \n \nn_rowsint \n\nNumber of rows in the bicluster.  \nn_colsint \n\nNumber of columns in the bicluster.","title":"sklearn.modules.generated.sklearn.cluster.spectralcoclustering#sklearn.cluster.SpectralCoclustering.get_shape"}]}
{"task_id":22187233,"prompt":"def f_22187233():\n\treturn ","suffix":"","canonical_solution":"\"\"\"it is icy\"\"\".replace('i', '')","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == 't s cy'\n"],"entry_point":"f_22187233","intent":"delete all characters \"i\" in string \"it is icy\"","library":[],"docs":[]}
{"task_id":13413590,"prompt":"def f_13413590(df):\n\treturn ","suffix":"","canonical_solution":"df.dropna(subset=[1])","test_start":"\nimport numpy as np\nimport pandas as pd\n\ndef check(candidate):","test":["\n    data = {0:[3.0, 4.0, 2.0], 1:[2.0, 3.0, np.nan], 2:[np.nan, 3.0, np.nan]}\n    df = pd.DataFrame(data)\n    d = {0:[3.0, 4.0], 1:[2.0, 3.0], 2:[np.nan, 3.0]}\n    res = pd.DataFrame(d)\n    assert candidate(df).equals(res)\n"],"entry_point":"f_13413590","intent":"Drop rows of pandas dataframe `df` having NaN in column at index \"1\"","library":["numpy","pandas"],"docs":[{"text":"pandas.DataFrame.dropna   DataFrame.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)[source]\n \nRemove missing values. See the User Guide for more on which values are considered missing, and how to work with missing data.  Parameters \n \naxis:{0 or \u2018index\u2019, 1 or \u2018columns\u2019}, default 0\n\n\nDetermine if rows or columns which contain missing values are removed.  0, or \u2018index\u2019 : Drop rows which contain missing values. 1, or \u2018columns\u2019 : Drop columns which contain missing value.   Changed in version 1.0.0: Pass tuple or list to drop on multiple axes. Only a single axis is allowed.   \nhow:{\u2018any\u2019, \u2018all\u2019}, default \u2018any\u2019\n\n\nDetermine if row or column is removed from DataFrame, when we have at least one NA or all NA.  \u2018any\u2019 : If any NA values are present, drop that row or column. \u2018all\u2019 : If all values are NA, drop that row or column.   \nthresh:int, optional\n\n\nRequire that many non-NA values.  \nsubset:column label or sequence of labels, optional\n\n\nLabels along other axis to consider, e.g. if you are dropping rows these would be a list of columns to include.  \ninplace:bool, default False\n\n\nIf True, do operation inplace and return None.    Returns \n DataFrame or None\n\nDataFrame with NA entries dropped from it or None if inplace=True.      See also  DataFrame.isna\n\nIndicate missing values.  DataFrame.notna\n\nIndicate existing (non-missing) values.  DataFrame.fillna\n\nReplace missing values.  Series.dropna\n\nDrop missing values.  Index.dropna\n\nDrop missing indices.    Examples \n>>> df = pd.DataFrame({\"name\": ['Alfred', 'Batman', 'Catwoman'],\n...                    \"toy\": [np.nan, 'Batmobile', 'Bullwhip'],\n...                    \"born\": [pd.NaT, pd.Timestamp(\"1940-04-25\"),\n...                             pd.NaT]})\n>>> df\n       name        toy       born\n0    Alfred        NaN        NaT\n1    Batman  Batmobile 1940-04-25\n2  Catwoman   Bullwhip        NaT\n  Drop the rows where at least one element is missing. \n>>> df.dropna()\n     name        toy       born\n1  Batman  Batmobile 1940-04-25\n  Drop the columns where at least one element is missing. \n>>> df.dropna(axis='columns')\n       name\n0    Alfred\n1    Batman\n2  Catwoman\n  Drop the rows where all elements are missing. \n>>> df.dropna(how='all')\n       name        toy       born\n0    Alfred        NaN        NaT\n1    Batman  Batmobile 1940-04-25\n2  Catwoman   Bullwhip        NaT\n  Keep only the rows with at least 2 non-NA values. \n>>> df.dropna(thresh=2)\n       name        toy       born\n1    Batman  Batmobile 1940-04-25\n2  Catwoman   Bullwhip        NaT\n  Define in which columns to look for missing values. \n>>> df.dropna(subset=['name', 'toy'])\n       name        toy       born\n1    Batman  Batmobile 1940-04-25\n2  Catwoman   Bullwhip        NaT\n  Keep the DataFrame with valid entries in the same variable. \n>>> df.dropna(inplace=True)\n>>> df\n     name        toy       born\n1  Batman  Batmobile 1940-04-25","title":"pandas.reference.api.pandas.dataframe.dropna"},{"text":"pandas.Index.dropna   Index.dropna(how='any')[source]\n \nReturn Index without NA\/NaN values.  Parameters \n \nhow:{\u2018any\u2019, \u2018all\u2019}, default \u2018any\u2019\n\n\nIf the Index is a MultiIndex, drop the value when any or all levels are NaN.    Returns \n Index","title":"pandas.reference.api.pandas.index.dropna"},{"text":"pandas.Series.dropna   Series.dropna(axis=0, inplace=False, how=None)[source]\n \nReturn a new Series with missing values removed. See the User Guide for more on which values are considered missing, and how to work with missing data.  Parameters \n \naxis:{0 or \u2018index\u2019}, default 0\n\n\nThere is only one axis to drop values from.  \ninplace:bool, default False\n\n\nIf True, do operation inplace and return None.  \nhow:str, optional\n\n\nNot in use. Kept for compatibility.    Returns \n Series or None\n\nSeries with NA entries dropped from it or None if inplace=True.      See also  Series.isna\n\nIndicate missing values.  Series.notna\n\nIndicate existing (non-missing) values.  Series.fillna\n\nReplace missing values.  DataFrame.dropna\n\nDrop rows or columns which contain NA values.  Index.dropna\n\nDrop missing indices.    Examples \n>>> ser = pd.Series([1., 2., np.nan])\n>>> ser\n0    1.0\n1    2.0\n2    NaN\ndtype: float64\n  Drop NA values from a Series. \n>>> ser.dropna()\n0    1.0\n1    2.0\ndtype: float64\n  Keep the Series with valid entries in the same variable. \n>>> ser.dropna(inplace=True)\n>>> ser\n0    1.0\n1    2.0\ndtype: float64\n  Empty strings are not considered NA values. None is considered an NA value. \n>>> ser = pd.Series([np.NaN, 2, pd.NaT, '', None, 'I stay'])\n>>> ser\n0       NaN\n1         2\n2       NaT\n3\n4      None\n5    I stay\ndtype: object\n>>> ser.dropna()\n1         2\n3\n5    I stay\ndtype: object","title":"pandas.reference.api.pandas.series.dropna"},{"text":"pandas.api.extensions.ExtensionArray.dropna   ExtensionArray.dropna()[source]\n \nReturn ExtensionArray without NA values.  Returns \n \nvalid:ExtensionArray","title":"pandas.reference.api.pandas.api.extensions.extensionarray.dropna"},{"text":"pandas.DataFrame.sparse.to_dense   DataFrame.sparse.to_dense()[source]\n \nConvert a DataFrame with sparse values to dense.  New in version 0.25.0.   Returns \n DataFrame\n\nA DataFrame with the same values stored as dense arrays.     Examples \n>>> df = pd.DataFrame({\"A\": pd.arrays.SparseArray([0, 1, 0])})\n>>> df.sparse.to_dense()\n   A\n0  0\n1  1\n2  0","title":"pandas.reference.api.pandas.dataframe.sparse.to_dense"},{"text":"pandas.DataFrame.to_numpy   DataFrame.to_numpy(dtype=None, copy=False, na_value=NoDefault.no_default)[source]\n \nConvert the DataFrame to a NumPy array. By default, the dtype of the returned array will be the common NumPy dtype of all types in the DataFrame. For example, if the dtypes are float16 and float32, the results dtype will be float32. This may require copying data and coercing values, which may be expensive.  Parameters \n \ndtype:str or numpy.dtype, optional\n\n\nThe dtype to pass to numpy.asarray().  \ncopy:bool, default False\n\n\nWhether to ensure that the returned value is not a view on another array. Note that copy=False does not ensure that to_numpy() is no-copy. Rather, copy=True ensure that a copy is made, even if not strictly necessary.  \nna_value:Any, optional\n\n\nThe value to use for missing values. The default value depends on dtype and the dtypes of the DataFrame columns.  New in version 1.1.0.     Returns \n numpy.ndarray\n    See also  Series.to_numpy\n\nSimilar method for Series.    Examples \n>>> pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]}).to_numpy()\narray([[1, 3],\n       [2, 4]])\n  With heterogeneous data, the lowest common type will have to be used. \n>>> df = pd.DataFrame({\"A\": [1, 2], \"B\": [3.0, 4.5]})\n>>> df.to_numpy()\narray([[1. , 3. ],\n       [2. , 4.5]])\n  For a mix of numeric and non-numeric types, the output array will have object dtype. \n>>> df['C'] = pd.date_range('2000', periods=2)\n>>> df.to_numpy()\narray([[1, 3.0, Timestamp('2000-01-01 00:00:00')],\n       [2, 4.5, Timestamp('2000-01-02 00:00:00')]], dtype=object)","title":"pandas.reference.api.pandas.dataframe.to_numpy"},{"text":"numpy.ma.mask_rows   ma.mask_rows(a, axis=<no value>)[source]\n \nMask rows of a 2D array that contain masked values. This function is a shortcut to mask_rowcols with axis equal to 0.  See also  mask_rowcols\n\nMask rows and\/or columns of a 2D array.  masked_where\n\nMask where a condition is met.    Examples >>> import numpy.ma as ma\n>>> a = np.zeros((3, 3), dtype=int)\n>>> a[1, 1] = 1\n>>> a\narray([[0, 0, 0],\n       [0, 1, 0],\n       [0, 0, 0]])\n>>> a = ma.masked_equal(a, 1)\n>>> a\nmasked_array(\n  data=[[0, 0, 0],\n        [0, --, 0],\n        [0, 0, 0]],\n  mask=[[False, False, False],\n        [False,  True, False],\n        [False, False, False]],\n  fill_value=1)\n >>> ma.mask_rows(a)\nmasked_array(\n  data=[[0, 0, 0],\n        [--, --, --],\n        [0, 0, 0]],\n  mask=[[False, False, False],\n        [ True,  True,  True],\n        [False, False, False]],\n  fill_value=1)","title":"numpy.reference.generated.numpy.ma.mask_rows"},{"text":"pandas.DataFrame.notnull   DataFrame.notnull()[source]\n \nDataFrame.notnull is an alias for DataFrame.notna. Detect existing (non-missing) values. Return a boolean same-sized object indicating if the values are not NA. Non-missing values get mapped to True. Characters such as empty strings '' or numpy.inf are not considered NA values (unless you set pandas.options.mode.use_inf_as_na = True). NA values, such as None or numpy.NaN, get mapped to False values.  Returns \n DataFrame\n\nMask of bool values for each element in DataFrame that indicates whether an element is not an NA value.      See also  DataFrame.notnull\n\nAlias of notna.  DataFrame.isna\n\nBoolean inverse of notna.  DataFrame.dropna\n\nOmit axes labels with missing values.  notna\n\nTop-level notna.    Examples Show which entries in a DataFrame are not NA. \n>>> df = pd.DataFrame(dict(age=[5, 6, np.NaN],\n...                    born=[pd.NaT, pd.Timestamp('1939-05-27'),\n...                          pd.Timestamp('1940-04-25')],\n...                    name=['Alfred', 'Batman', ''],\n...                    toy=[None, 'Batmobile', 'Joker']))\n>>> df\n   age       born    name        toy\n0  5.0        NaT  Alfred       None\n1  6.0 1939-05-27  Batman  Batmobile\n2  NaN 1940-04-25              Joker\n  \n>>> df.notna()\n     age   born  name    toy\n0   True  False  True  False\n1   True   True  True   True\n2  False   True  True   True\n  Show which entries in a Series are not NA. \n>>> ser = pd.Series([5, 6, np.NaN])\n>>> ser\n0    5.0\n1    6.0\n2    NaN\ndtype: float64\n  \n>>> ser.notna()\n0     True\n1     True\n2    False\ndtype: bool","title":"pandas.reference.api.pandas.dataframe.notnull"},{"text":"os.sysconf_names  \nDictionary mapping names accepted by sysconf() to the integer values defined for those names by the host operating system. This can be used to determine the set of names known to the system. Availability: Unix.","title":"python.library.os#os.sysconf_names"},{"text":"float.as_integer_ratio()  \nReturn a pair of integers whose ratio is exactly equal to the original float and with a positive denominator. Raises OverflowError on infinities and a ValueError on NaNs.","title":"python.library.stdtypes#float.as_integer_ratio"}]}
{"task_id":598398,"prompt":"def f_598398(myList):\n\treturn ","suffix":"","canonical_solution":"[x for x in myList if x.n == 30]","test_start":"\nimport numpy as np\nimport pandas as pd\n\ndef check(candidate):","test":["\n    class Data: \n        def __init__(self, a, n): \n            self.a = a\n            self.n = n\n    \n    myList = [Data(i, 10*(i%4)) for i in range(20)]\n    assert candidate(myList) == [myList[i] for i in [3, 7, 11, 15, 19]]\n"],"entry_point":"f_598398","intent":"get elements from list `myList`, that have a field `n` value 30","library":["numpy","pandas"],"docs":[]}
{"task_id":10351772,"prompt":"def f_10351772(intstringlist):\n\t","suffix":"\n\treturn nums","canonical_solution":"nums = [int(x) for x in intstringlist]","test_start":"\ndef check(candidate):","test":["\n    assert candidate(['1', '2', '3', '4', '5']) == [1, 2, 3, 4, 5]\n","\n    assert candidate(['001', '200', '3', '4', '5']) == [1, 200, 3, 4, 5]\n"],"entry_point":"f_10351772","intent":"converting list of strings `intstringlist` to list of integer `nums`","library":[],"docs":[]}
{"task_id":493386,"prompt":"def f_493386():\n\treturn ","suffix":"","canonical_solution":"sys.stdout.write('.')","test_start":"\nimport sys\n\ndef check(candidate):","test":["\n    assert candidate() == 1\n"],"entry_point":"f_493386","intent":"print \".\" without newline","library":["sys"],"docs":[{"text":"re.S  \nre.DOTALL  \nMake the '.' special character match any character at all, including a newline; without this flag, '.' will match anything except a newline. Corresponds to the inline flag (?s).","title":"python.library.re#re.DOTALL"},{"text":"re.S  \nre.DOTALL  \nMake the '.' special character match any character at all, including a newline; without this flag, '.' will match anything except a newline. Corresponds to the inline flag (?s).","title":"python.library.re#re.S"},{"text":"token.DOT  \nToken value for \".\".","title":"python.library.token#token.DOT"},{"text":"turtle.tracer(n=None, delay=None)  \n Parameters \n \nn \u2013 nonnegative integer \ndelay \u2013 nonnegative integer    Turn turtle animation on\/off and set delay for update drawings. If n is given, only each n-th regular screen update is really performed. (Can be used to accelerate the drawing of complex graphics.) When called without arguments, returns the currently stored value of n. Second argument sets delay value (see delay()). >>> screen.tracer(8, 25)\n>>> dist = 2\n>>> for i in range(200):\n...     fd(dist)\n...     rt(90)\n...     dist += 2","title":"python.library.turtle#turtle.tracer"},{"text":"Cmd.emptyline()  \nMethod called when an empty line is entered in response to the prompt. If this method is not overridden, it repeats the last nonempty command entered.","title":"python.library.cmd#cmd.Cmd.emptyline"},{"text":"turtle.dot(size=None, *color)  \n Parameters \n \nsize \u2013 an integer >= 1 (if given) \ncolor \u2013 a colorstring or a numeric color tuple    Draw a circular dot with diameter size, using color. If size is not given, the maximum of pensize+4 and 2*pensize is used. >>> turtle.home()\n>>> turtle.dot()\n>>> turtle.fd(50); turtle.dot(20, \"blue\"); turtle.fd(50)\n>>> turtle.position()\n(100.00,-0.00)\n>>> turtle.heading()\n0.0","title":"python.library.turtle#turtle.dot"},{"text":"DefaultCookiePolicy.DomainStrictNoDots  \nWhen setting cookies, the \u2018host prefix\u2019 must not contain a dot (eg. www.foo.bar.com can\u2019t set a cookie for .bar.com, because www.foo contains a dot).","title":"python.library.http.cookiejar#http.cookiejar.DefaultCookiePolicy.DomainStrictNoDots"},{"text":"linesep  \nThe string to be used to terminate lines in serialized output. The default is \\n because that\u2019s the internal end-of-line discipline used by Python, though \\r\\n is required by the RFCs.","title":"python.library.email.policy#email.policy.Policy.linesep"},{"text":"numpy.nditer.debug_print method   nditer.debug_print()\n \nPrint the current state of the nditer instance and debug info to stdout.","title":"numpy.reference.generated.numpy.nditer.debug_print"},{"text":"numpy.ma.MaskedArray.__deepcopy__ method   ma.MaskedArray.__deepcopy__(memo, \/) \u2192 Deep copy of array.[source]\n \nUsed if copy.deepcopy is called on an array.","title":"numpy.reference.generated.numpy.ma.maskedarray.__deepcopy__"}]}
{"task_id":6569528,"prompt":"def f_6569528():\n\treturn ","suffix":"","canonical_solution":"int(round(2.52 * 100))","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == 252\n"],"entry_point":"f_6569528","intent":"round off the float that is the product of `2.52 * 100` and convert it to an int","library":[],"docs":[]}
{"task_id":3964681,"prompt":"def f_3964681():\n\t","suffix":"\n\treturn files","canonical_solution":"\n\tos.chdir('\/mydir')\n\tfiles = [] \n\tfor file in glob.glob('*.txt'):\n\t\tfiles.append(file)\n","test_start":"\nimport os\nimport glob\nfrom unittest.mock import Mock\n\ndef check(candidate):","test":["\n    samples = ['abc.txt']\n    os.chdir = Mock()\n    glob.glob = Mock(return_value = samples)\n    assert candidate() == samples\n"],"entry_point":"f_3964681","intent":"Find all files `files` in directory '\/mydir' with extension '.txt'","library":["glob","os"],"docs":[{"text":"test.support.findfile(filename, subdir=None)  \nReturn the path to the file named filename. If no match is found filename is returned. This does not equal a failure since it could be the path to the file. Setting subdir indicates a relative path to use to find the file rather than looking directly in the path directories.","title":"python.library.test#test.support.findfile"},{"text":"numpy.distutils.misc_util.get_script_files(scripts)[source]","title":"numpy.reference.distutils.misc_util#numpy.distutils.misc_util.get_script_files"},{"text":"matplotlib.font_manager.list_fonts(directory, extensions)[source]\n \nReturn a list of all fonts matching any of the extensions, found recursively under the directory.","title":"matplotlib.font_manager_api#matplotlib.font_manager.list_fonts"},{"text":"tf.keras.preprocessing.text_dataset_from_directory Generates a tf.data.Dataset from text files in a directory. \ntf.keras.preprocessing.text_dataset_from_directory(\n    directory, labels='inferred', label_mode='int',\n    class_names=None, batch_size=32, max_length=None, shuffle=True, seed=None,\n    validation_split=None, subset=None, follow_links=False\n)\n If your directory structure is: main_directory\/\n...class_a\/\n......a_text_1.txt\n......a_text_2.txt\n...class_b\/\n......b_text_1.txt\n......b_text_2.txt\n Then calling text_dataset_from_directory(main_directory, labels='inferred') will return a tf.data.Dataset that yields batches of texts from the subdirectories class_a and class_b, together with labels 0 and 1 (0 corresponding to class_a and 1 corresponding to class_b). Only .txt files are supported at this time.\n \n\n\n Arguments\n  directory   Directory where the data is located. If labels is \"inferred\", it should contain subdirectories, each containing text files for a class. Otherwise, the directory structure is ignored.  \n  labels   Either \"inferred\" (labels are generated from the directory structure), or a list\/tuple of integer labels of the same size as the number of text files found in the directory. Labels should be sorted according to the alphanumeric order of the text file paths (obtained via os.walk(directory) in Python).  \n  label_mode    'int': means that the labels are encoded as integers (e.g. for sparse_categorical_crossentropy loss). 'categorical' means that the labels are encoded as a categorical vector (e.g. for categorical_crossentropy loss). 'binary' means that the labels (there can be only 2) are encoded as float32 scalars with values 0 or 1 (e.g. for binary_crossentropy). None (no labels). \n\n \n  class_names   Only valid if \"labels\" is \"inferred\". This is the explict list of class names (must match names of subdirectories). Used to control the order of the classes (otherwise alphanumerical order is used).  \n  batch_size   Size of the batches of data. Default: 32.  \n  max_length   Maximum size of a text string. Texts longer than this will be truncated to max_length.  \n  shuffle   Whether to shuffle the data. Default: True. If set to False, sorts the data in alphanumeric order.  \n  seed   Optional random seed for shuffling and transformations.  \n  validation_split   Optional float between 0 and 1, fraction of data to reserve for validation.  \n  subset   One of \"training\" or \"validation\". Only used if validation_split is set.  \n  follow_links   Whether to visits subdirectories pointed to by symlinks. Defaults to False.   \n \n\n\n Returns   A tf.data.Dataset object.  If label_mode is None, it yields string tensors of shape (batch_size,), containing the contents of a batch of text files. Otherwise, it yields a tuple (texts, labels), where texts has shape (batch_size,) and labels follows the format described below. \n\n \n Rules regarding labels format:  if label_mode is int, the labels are an int32 tensor of shape (batch_size,). if label_mode is binary, the labels are a float32 tensor of 1s and 0s of shape (batch_size, 1). if label_mode is categorial, the labels are a float32 tensor of shape (batch_size, num_classes), representing a one-hot encoding of the class index.","title":"tensorflow.keras.preprocessing.text_dataset_from_directory"},{"text":"torch.divide(input, other, *, rounding_mode=None, out=None) \u2192 Tensor  \nAlias for torch.div().","title":"torch.generated.torch.divide#torch.divide"},{"text":"class torch.utils.data.TensorDataset(*tensors) [source]\n \nDataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension.  Parameters \n*tensors (Tensor) \u2013 tensors that have the same size of the first dimension.","title":"torch.data#torch.utils.data.TensorDataset"},{"text":"fnmatch.fnmatch(filename, pattern)  \nTest whether the filename string matches the pattern string, returning True or False. Both parameters are case-normalized using os.path.normcase(). fnmatchcase() can be used to perform a case-sensitive comparison, regardless of whether that\u2019s standard for the operating system. This example will print all file names in the current directory with the extension .txt: import fnmatch\nimport os\n\nfor file in os.listdir('.'):\n    if fnmatch.fnmatch(file, '*.txt'):\n        print(file)","title":"python.library.fnmatch#fnmatch.fnmatch"},{"text":"torch.distributed.get_backend(group=None) [source]\n \nReturns the backend of the given process group.  Parameters \ngroup (ProcessGroup, optional) \u2013 The process group to work on. The default is the general main process group. If another specific group is specified, the calling process must be part of group.  Returns \nThe backend of the given process group as a lower case string.","title":"torch.distributed#torch.distributed.get_backend"},{"text":"numpy.record.astype method   record.astype()\n \nScalar method identical to the corresponding array attribute. Please see ndarray.astype.","title":"numpy.reference.generated.numpy.record.astype"},{"text":"tf.raw_ops.MatchingFiles Returns the set of files matching one or more glob patterns.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.raw_ops.MatchingFiles  \ntf.raw_ops.MatchingFiles(\n    pattern, name=None\n)\n Note that this routine only supports wildcard characters in the basename portion of the pattern, not in the directory portion. Note also that the order of filenames returned is deterministic.\n \n\n\n Args\n  pattern   A Tensor of type string. Shell wildcard pattern(s). Scalar or vector of type string.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor of type string.","title":"tensorflow.raw_ops.matchingfiles"}]}
{"task_id":3964681,"prompt":"def f_3964681():\n\treturn ","suffix":"","canonical_solution":"[file for file in os.listdir('\/mydir') if file.endswith('.txt')]","test_start":"\nimport os\nfrom unittest.mock import Mock\n\ndef check(candidate):","test":["\n    samples = ['abc.txt', 'f.csv']\n    os.listdir = Mock(return_value = samples)\n    assert candidate() == ['abc.txt']\n"],"entry_point":"f_3964681","intent":"Find all files in directory \"\/mydir\" with extension \".txt\"","library":["os"],"docs":[{"text":"fnmatch.fnmatch(filename, pattern)  \nTest whether the filename string matches the pattern string, returning True or False. Both parameters are case-normalized using os.path.normcase(). fnmatchcase() can be used to perform a case-sensitive comparison, regardless of whether that\u2019s standard for the operating system. This example will print all file names in the current directory with the extension .txt: import fnmatch\nimport os\n\nfor file in os.listdir('.'):\n    if fnmatch.fnmatch(file, '*.txt'):\n        print(file)","title":"python.library.fnmatch#fnmatch.fnmatch"},{"text":"findtext(match, default=None, namespaces=None)  \nSame as Element.findtext(), starting at the root of the tree.","title":"python.library.xml.etree.elementtree#xml.etree.ElementTree.ElementTree.findtext"},{"text":"test.support.findfile(filename, subdir=None)  \nReturn the path to the file named filename. If no match is found filename is returned. This does not equal a failure since it could be the path to the file. Setting subdir indicates a relative path to use to find the file rather than looking directly in the path directories.","title":"python.library.test#test.support.findfile"},{"text":"tf.keras.preprocessing.text_dataset_from_directory Generates a tf.data.Dataset from text files in a directory. \ntf.keras.preprocessing.text_dataset_from_directory(\n    directory, labels='inferred', label_mode='int',\n    class_names=None, batch_size=32, max_length=None, shuffle=True, seed=None,\n    validation_split=None, subset=None, follow_links=False\n)\n If your directory structure is: main_directory\/\n...class_a\/\n......a_text_1.txt\n......a_text_2.txt\n...class_b\/\n......b_text_1.txt\n......b_text_2.txt\n Then calling text_dataset_from_directory(main_directory, labels='inferred') will return a tf.data.Dataset that yields batches of texts from the subdirectories class_a and class_b, together with labels 0 and 1 (0 corresponding to class_a and 1 corresponding to class_b). Only .txt files are supported at this time.\n \n\n\n Arguments\n  directory   Directory where the data is located. If labels is \"inferred\", it should contain subdirectories, each containing text files for a class. Otherwise, the directory structure is ignored.  \n  labels   Either \"inferred\" (labels are generated from the directory structure), or a list\/tuple of integer labels of the same size as the number of text files found in the directory. Labels should be sorted according to the alphanumeric order of the text file paths (obtained via os.walk(directory) in Python).  \n  label_mode    'int': means that the labels are encoded as integers (e.g. for sparse_categorical_crossentropy loss). 'categorical' means that the labels are encoded as a categorical vector (e.g. for categorical_crossentropy loss). 'binary' means that the labels (there can be only 2) are encoded as float32 scalars with values 0 or 1 (e.g. for binary_crossentropy). None (no labels). \n\n \n  class_names   Only valid if \"labels\" is \"inferred\". This is the explict list of class names (must match names of subdirectories). Used to control the order of the classes (otherwise alphanumerical order is used).  \n  batch_size   Size of the batches of data. Default: 32.  \n  max_length   Maximum size of a text string. Texts longer than this will be truncated to max_length.  \n  shuffle   Whether to shuffle the data. Default: True. If set to False, sorts the data in alphanumeric order.  \n  seed   Optional random seed for shuffling and transformations.  \n  validation_split   Optional float between 0 and 1, fraction of data to reserve for validation.  \n  subset   One of \"training\" or \"validation\". Only used if validation_split is set.  \n  follow_links   Whether to visits subdirectories pointed to by symlinks. Defaults to False.   \n \n\n\n Returns   A tf.data.Dataset object.  If label_mode is None, it yields string tensors of shape (batch_size,), containing the contents of a batch of text files. Otherwise, it yields a tuple (texts, labels), where texts has shape (batch_size,) and labels follows the format described below. \n\n \n Rules regarding labels format:  if label_mode is int, the labels are an int32 tensor of shape (batch_size,). if label_mode is binary, the labels are a float32 tensor of 1s and 0s of shape (batch_size, 1). if label_mode is categorial, the labels are a float32 tensor of shape (batch_size, num_classes), representing a one-hot encoding of the class index.","title":"tensorflow.keras.preprocessing.text_dataset_from_directory"},{"text":"close()  \nMaildir instances do not keep any open files and the underlying mailboxes do not support locking, so this method does nothing.","title":"python.library.mailbox#mailbox.Maildir.close"},{"text":"kthvalue(k, dim=None, keepdim=False) -> (Tensor, LongTensor)  \nSee torch.kthvalue()","title":"torch.tensors#torch.Tensor.kthvalue"},{"text":"python_code(root_module) [source]\n \nTurn this Graph into valid Python code.  Parameters \nroot_module (str) \u2013 The name of the root module on which to look-up qualified name targets. This is usually \u2018self\u2019.  Returns \nThe string source code generated from this Graph.","title":"torch.fx#torch.fx.Graph.python_code"},{"text":"tempfile.tempdir  \nWhen set to a value other than None, this variable defines the default value for the dir argument to the functions defined in this module. If tempdir is None (the default) at any call to any of the above functions except gettempprefix() it is initialized following the algorithm described in gettempdir().","title":"python.library.tempfile#tempfile.tempdir"},{"text":"matplotlib.font_manager.list_fonts(directory, extensions)[source]\n \nReturn a list of all fonts matching any of the extensions, found recursively under the directory.","title":"matplotlib.font_manager_api#matplotlib.font_manager.list_fonts"},{"text":"findtext(match, default=None, namespaces=None)  \nFinds text for the first subelement matching match. match may be a tag name or a path. Returns the text content of the first matching element, or default if no element was found. Note that if the matching element has no text content an empty string is returned. namespaces is an optional mapping from namespace prefix to full name. Pass '' as prefix to move all unprefixed tag names in the expression into the given namespace.","title":"python.library.xml.etree.elementtree#xml.etree.ElementTree.Element.findtext"}]}
{"task_id":3964681,"prompt":"def f_3964681():\n\treturn ","suffix":"","canonical_solution":"[file for (root, dirs, files) in os.walk('\/mydir') for file in files if file.endswith('.txt')]","test_start":"\nimport os\nfrom unittest.mock import Mock\n\ndef check(candidate):","test":["\n    name = '\/mydir'\n    samples = [(name, [], ['abc.txt', 'f.csv'])]\n    os.walk = Mock(return_value = samples)\n    assert candidate() == ['abc.txt']\n"],"entry_point":"f_3964681","intent":"Find all files in directory \"\/mydir\" with extension \".txt\"","library":["os"],"docs":[{"text":"fnmatch.fnmatch(filename, pattern)  \nTest whether the filename string matches the pattern string, returning True or False. Both parameters are case-normalized using os.path.normcase(). fnmatchcase() can be used to perform a case-sensitive comparison, regardless of whether that\u2019s standard for the operating system. This example will print all file names in the current directory with the extension .txt: import fnmatch\nimport os\n\nfor file in os.listdir('.'):\n    if fnmatch.fnmatch(file, '*.txt'):\n        print(file)","title":"python.library.fnmatch#fnmatch.fnmatch"},{"text":"findtext(match, default=None, namespaces=None)  \nSame as Element.findtext(), starting at the root of the tree.","title":"python.library.xml.etree.elementtree#xml.etree.ElementTree.ElementTree.findtext"},{"text":"test.support.findfile(filename, subdir=None)  \nReturn the path to the file named filename. If no match is found filename is returned. This does not equal a failure since it could be the path to the file. Setting subdir indicates a relative path to use to find the file rather than looking directly in the path directories.","title":"python.library.test#test.support.findfile"},{"text":"tf.keras.preprocessing.text_dataset_from_directory Generates a tf.data.Dataset from text files in a directory. \ntf.keras.preprocessing.text_dataset_from_directory(\n    directory, labels='inferred', label_mode='int',\n    class_names=None, batch_size=32, max_length=None, shuffle=True, seed=None,\n    validation_split=None, subset=None, follow_links=False\n)\n If your directory structure is: main_directory\/\n...class_a\/\n......a_text_1.txt\n......a_text_2.txt\n...class_b\/\n......b_text_1.txt\n......b_text_2.txt\n Then calling text_dataset_from_directory(main_directory, labels='inferred') will return a tf.data.Dataset that yields batches of texts from the subdirectories class_a and class_b, together with labels 0 and 1 (0 corresponding to class_a and 1 corresponding to class_b). Only .txt files are supported at this time.\n \n\n\n Arguments\n  directory   Directory where the data is located. If labels is \"inferred\", it should contain subdirectories, each containing text files for a class. Otherwise, the directory structure is ignored.  \n  labels   Either \"inferred\" (labels are generated from the directory structure), or a list\/tuple of integer labels of the same size as the number of text files found in the directory. Labels should be sorted according to the alphanumeric order of the text file paths (obtained via os.walk(directory) in Python).  \n  label_mode    'int': means that the labels are encoded as integers (e.g. for sparse_categorical_crossentropy loss). 'categorical' means that the labels are encoded as a categorical vector (e.g. for categorical_crossentropy loss). 'binary' means that the labels (there can be only 2) are encoded as float32 scalars with values 0 or 1 (e.g. for binary_crossentropy). None (no labels). \n\n \n  class_names   Only valid if \"labels\" is \"inferred\". This is the explict list of class names (must match names of subdirectories). Used to control the order of the classes (otherwise alphanumerical order is used).  \n  batch_size   Size of the batches of data. Default: 32.  \n  max_length   Maximum size of a text string. Texts longer than this will be truncated to max_length.  \n  shuffle   Whether to shuffle the data. Default: True. If set to False, sorts the data in alphanumeric order.  \n  seed   Optional random seed for shuffling and transformations.  \n  validation_split   Optional float between 0 and 1, fraction of data to reserve for validation.  \n  subset   One of \"training\" or \"validation\". Only used if validation_split is set.  \n  follow_links   Whether to visits subdirectories pointed to by symlinks. Defaults to False.   \n \n\n\n Returns   A tf.data.Dataset object.  If label_mode is None, it yields string tensors of shape (batch_size,), containing the contents of a batch of text files. Otherwise, it yields a tuple (texts, labels), where texts has shape (batch_size,) and labels follows the format described below. \n\n \n Rules regarding labels format:  if label_mode is int, the labels are an int32 tensor of shape (batch_size,). if label_mode is binary, the labels are a float32 tensor of 1s and 0s of shape (batch_size, 1). if label_mode is categorial, the labels are a float32 tensor of shape (batch_size, num_classes), representing a one-hot encoding of the class index.","title":"tensorflow.keras.preprocessing.text_dataset_from_directory"},{"text":"close()  \nMaildir instances do not keep any open files and the underlying mailboxes do not support locking, so this method does nothing.","title":"python.library.mailbox#mailbox.Maildir.close"},{"text":"kthvalue(k, dim=None, keepdim=False) -> (Tensor, LongTensor)  \nSee torch.kthvalue()","title":"torch.tensors#torch.Tensor.kthvalue"},{"text":"python_code(root_module) [source]\n \nTurn this Graph into valid Python code.  Parameters \nroot_module (str) \u2013 The name of the root module on which to look-up qualified name targets. This is usually \u2018self\u2019.  Returns \nThe string source code generated from this Graph.","title":"torch.fx#torch.fx.Graph.python_code"},{"text":"tempfile.tempdir  \nWhen set to a value other than None, this variable defines the default value for the dir argument to the functions defined in this module. If tempdir is None (the default) at any call to any of the above functions except gettempprefix() it is initialized following the algorithm described in gettempdir().","title":"python.library.tempfile#tempfile.tempdir"},{"text":"matplotlib.font_manager.list_fonts(directory, extensions)[source]\n \nReturn a list of all fonts matching any of the extensions, found recursively under the directory.","title":"matplotlib.font_manager_api#matplotlib.font_manager.list_fonts"},{"text":"findtext(match, default=None, namespaces=None)  \nFinds text for the first subelement matching match. match may be a tag name or a path. Returns the text content of the first matching element, or default if no element was found. Note that if the matching element has no text content an empty string is returned. namespaces is an optional mapping from namespace prefix to full name. Pass '' as prefix to move all unprefixed tag names in the expression into the given namespace.","title":"python.library.xml.etree.elementtree#xml.etree.ElementTree.Element.findtext"}]}
{"task_id":20865487,"prompt":"def f_20865487(df):\n\treturn ","suffix":"","canonical_solution":"df.plot(legend=False)","test_start":"\nimport os \nimport pandas as pd\n\ndef check(candidate):","test":["\n    df = pd.DataFrame([1, 2, 3, 4, 5], columns = ['Vals'])\n    res = candidate(df)\n    assert 'AxesSubplot' in str(type(res))\n    assert res.legend_ is None\n"],"entry_point":"f_20865487","intent":"plot dataframe `df` without a legend","library":["os","pandas"],"docs":[{"text":"pandas.DataFrame.plot   DataFrame.plot(*args, **kwargs)[source]\n \nMake plots of Series or DataFrame. Uses the backend specified by the option plotting.backend. By default, matplotlib is used.  Parameters \n \ndata:Series or DataFrame\n\n\nThe object for which the method is called.  \nx:label or position, default None\n\n\nOnly used if data is a DataFrame.  \ny:label, position or list of label, positions, default None\n\n\nAllows plotting of one column versus another. Only used if data is a DataFrame.  \nkind:str\n\n\nThe kind of plot to produce:  \u2018line\u2019 : line plot (default) \u2018bar\u2019 : vertical bar plot \u2018barh\u2019 : horizontal bar plot \u2018hist\u2019 : histogram \u2018box\u2019 : boxplot \u2018kde\u2019 : Kernel Density Estimation plot \u2018density\u2019 : same as \u2018kde\u2019 \u2018area\u2019 : area plot \u2018pie\u2019 : pie plot \u2018scatter\u2019 : scatter plot (DataFrame only) \u2018hexbin\u2019 : hexbin plot (DataFrame only)   \nax:matplotlib axes object, default None\n\n\nAn axes of the current figure.  \nsubplots:bool, default False\n\n\nMake separate subplots for each column.  \nsharex:bool, default True if ax is None else False\n\n\nIn case subplots=True, share x axis and set some x axis labels to invisible; defaults to True if ax is None otherwise False if an ax is passed in; Be aware, that passing in both an ax and sharex=True will alter all x axis labels for all axis in a figure.  \nsharey:bool, default False\n\n\nIn case subplots=True, share y axis and set some y axis labels to invisible.  \nlayout:tuple, optional\n\n\n(rows, columns) for the layout of subplots.  \nfigsize:a tuple (width, height) in inches\n\n\nSize of a figure object.  \nuse_index:bool, default True\n\n\nUse index as ticks for x axis.  \ntitle:str or list\n\n\nTitle to use for the plot. If a string is passed, print the string at the top of the figure. If a list is passed and subplots is True, print each item in the list above the corresponding subplot.  \ngrid:bool, default None (matlab style default)\n\n\nAxis grid lines.  \nlegend:bool or {\u2018reverse\u2019}\n\n\nPlace legend on axis subplots.  \nstyle:list or dict\n\n\nThe matplotlib line style per column.  \nlogx:bool or \u2018sym\u2019, default False\n\n\nUse log scaling or symlog scaling on x axis. .. versionchanged:: 0.25.0  \nlogy:bool or \u2018sym\u2019 default False\n\n\nUse log scaling or symlog scaling on y axis. .. versionchanged:: 0.25.0  \nloglog:bool or \u2018sym\u2019, default False\n\n\nUse log scaling or symlog scaling on both x and y axes. .. versionchanged:: 0.25.0  \nxticks:sequence\n\n\nValues to use for the xticks.  \nyticks:sequence\n\n\nValues to use for the yticks.  \nxlim:2-tuple\/list\n\n\nSet the x limits of the current axes.  \nylim:2-tuple\/list\n\n\nSet the y limits of the current axes.  \nxlabel:label, optional\n\n\nName to use for the xlabel on x-axis. Default uses index name as xlabel, or the x-column name for planar plots.  New in version 1.1.0.   Changed in version 1.2.0: Now applicable to planar plots (scatter, hexbin).   \nylabel:label, optional\n\n\nName to use for the ylabel on y-axis. Default will show no ylabel, or the y-column name for planar plots.  New in version 1.1.0.   Changed in version 1.2.0: Now applicable to planar plots (scatter, hexbin).   \nrot:int, default None\n\n\nRotation for ticks (xticks for vertical, yticks for horizontal plots).  \nfontsize:int, default None\n\n\nFont size for xticks and yticks.  \ncolormap:str or matplotlib colormap object, default None\n\n\nColormap to select colors from. If string, load colormap with that name from matplotlib.  \ncolorbar:bool, optional\n\n\nIf True, plot colorbar (only relevant for \u2018scatter\u2019 and \u2018hexbin\u2019 plots).  \nposition:float\n\n\nSpecify relative alignments for bar plot layout. From 0 (left\/bottom-end) to 1 (right\/top-end). Default is 0.5 (center).  \ntable:bool, Series or DataFrame, default False\n\n\nIf True, draw a table using the data in the DataFrame and the data will be transposed to meet matplotlib\u2019s default layout. If a Series or DataFrame is passed, use passed data to draw a table.  \nyerr:DataFrame, Series, array-like, dict and str\n\n\nSee Plotting with Error Bars for detail.  \nxerr:DataFrame, Series, array-like, dict and str\n\n\nEquivalent to yerr.  \nstacked:bool, default False in line and bar plots, and True in area plot\n\n\nIf True, create stacked plot.  \nsort_columns:bool, default False\n\n\nSort column names to determine plot ordering.  \nsecondary_y:bool or sequence, default False\n\n\nWhether to plot on the secondary y-axis if a list\/tuple, which columns to plot on secondary y-axis.  \nmark_right:bool, default True\n\n\nWhen using a secondary_y axis, automatically mark the column labels with \u201c(right)\u201d in the legend.  \ninclude_bool:bool, default is False\n\n\nIf True, boolean values can be plotted.  \nbackend:str, default None\n\n\nBackend to use instead of the backend specified in the option plotting.backend. For instance, \u2018matplotlib\u2019. Alternatively, to specify the plotting.backend for the whole session, set pd.options.plotting.backend.  New in version 1.0.0.   **kwargs\n\nOptions to pass to matplotlib plotting method.    Returns \n \nmatplotlib.axes.Axes or numpy.ndarray of them\n\nIf the backend is not the default matplotlib one, the return value will be the object returned by the backend.     Notes  See matplotlib documentation online for more on this subject If kind = \u2018bar\u2019 or \u2018barh\u2019, you can specify relative alignments for bar plot layout by position keyword. From 0 (left\/bottom-end) to 1 (right\/top-end). Default is 0.5 (center)","title":"pandas.reference.api.pandas.dataframe.plot"},{"text":"pandas.Series.plot   Series.plot(*args, **kwargs)[source]\n \nMake plots of Series or DataFrame. Uses the backend specified by the option plotting.backend. By default, matplotlib is used.  Parameters \n \ndata:Series or DataFrame\n\n\nThe object for which the method is called.  \nx:label or position, default None\n\n\nOnly used if data is a DataFrame.  \ny:label, position or list of label, positions, default None\n\n\nAllows plotting of one column versus another. Only used if data is a DataFrame.  \nkind:str\n\n\nThe kind of plot to produce:  \u2018line\u2019 : line plot (default) \u2018bar\u2019 : vertical bar plot \u2018barh\u2019 : horizontal bar plot \u2018hist\u2019 : histogram \u2018box\u2019 : boxplot \u2018kde\u2019 : Kernel Density Estimation plot \u2018density\u2019 : same as \u2018kde\u2019 \u2018area\u2019 : area plot \u2018pie\u2019 : pie plot \u2018scatter\u2019 : scatter plot (DataFrame only) \u2018hexbin\u2019 : hexbin plot (DataFrame only)   \nax:matplotlib axes object, default None\n\n\nAn axes of the current figure.  \nsubplots:bool, default False\n\n\nMake separate subplots for each column.  \nsharex:bool, default True if ax is None else False\n\n\nIn case subplots=True, share x axis and set some x axis labels to invisible; defaults to True if ax is None otherwise False if an ax is passed in; Be aware, that passing in both an ax and sharex=True will alter all x axis labels for all axis in a figure.  \nsharey:bool, default False\n\n\nIn case subplots=True, share y axis and set some y axis labels to invisible.  \nlayout:tuple, optional\n\n\n(rows, columns) for the layout of subplots.  \nfigsize:a tuple (width, height) in inches\n\n\nSize of a figure object.  \nuse_index:bool, default True\n\n\nUse index as ticks for x axis.  \ntitle:str or list\n\n\nTitle to use for the plot. If a string is passed, print the string at the top of the figure. If a list is passed and subplots is True, print each item in the list above the corresponding subplot.  \ngrid:bool, default None (matlab style default)\n\n\nAxis grid lines.  \nlegend:bool or {\u2018reverse\u2019}\n\n\nPlace legend on axis subplots.  \nstyle:list or dict\n\n\nThe matplotlib line style per column.  \nlogx:bool or \u2018sym\u2019, default False\n\n\nUse log scaling or symlog scaling on x axis. .. versionchanged:: 0.25.0  \nlogy:bool or \u2018sym\u2019 default False\n\n\nUse log scaling or symlog scaling on y axis. .. versionchanged:: 0.25.0  \nloglog:bool or \u2018sym\u2019, default False\n\n\nUse log scaling or symlog scaling on both x and y axes. .. versionchanged:: 0.25.0  \nxticks:sequence\n\n\nValues to use for the xticks.  \nyticks:sequence\n\n\nValues to use for the yticks.  \nxlim:2-tuple\/list\n\n\nSet the x limits of the current axes.  \nylim:2-tuple\/list\n\n\nSet the y limits of the current axes.  \nxlabel:label, optional\n\n\nName to use for the xlabel on x-axis. Default uses index name as xlabel, or the x-column name for planar plots.  New in version 1.1.0.   Changed in version 1.2.0: Now applicable to planar plots (scatter, hexbin).   \nylabel:label, optional\n\n\nName to use for the ylabel on y-axis. Default will show no ylabel, or the y-column name for planar plots.  New in version 1.1.0.   Changed in version 1.2.0: Now applicable to planar plots (scatter, hexbin).   \nrot:int, default None\n\n\nRotation for ticks (xticks for vertical, yticks for horizontal plots).  \nfontsize:int, default None\n\n\nFont size for xticks and yticks.  \ncolormap:str or matplotlib colormap object, default None\n\n\nColormap to select colors from. If string, load colormap with that name from matplotlib.  \ncolorbar:bool, optional\n\n\nIf True, plot colorbar (only relevant for \u2018scatter\u2019 and \u2018hexbin\u2019 plots).  \nposition:float\n\n\nSpecify relative alignments for bar plot layout. From 0 (left\/bottom-end) to 1 (right\/top-end). Default is 0.5 (center).  \ntable:bool, Series or DataFrame, default False\n\n\nIf True, draw a table using the data in the DataFrame and the data will be transposed to meet matplotlib\u2019s default layout. If a Series or DataFrame is passed, use passed data to draw a table.  \nyerr:DataFrame, Series, array-like, dict and str\n\n\nSee Plotting with Error Bars for detail.  \nxerr:DataFrame, Series, array-like, dict and str\n\n\nEquivalent to yerr.  \nstacked:bool, default False in line and bar plots, and True in area plot\n\n\nIf True, create stacked plot.  \nsort_columns:bool, default False\n\n\nSort column names to determine plot ordering.  \nsecondary_y:bool or sequence, default False\n\n\nWhether to plot on the secondary y-axis if a list\/tuple, which columns to plot on secondary y-axis.  \nmark_right:bool, default True\n\n\nWhen using a secondary_y axis, automatically mark the column labels with \u201c(right)\u201d in the legend.  \ninclude_bool:bool, default is False\n\n\nIf True, boolean values can be plotted.  \nbackend:str, default None\n\n\nBackend to use instead of the backend specified in the option plotting.backend. For instance, \u2018matplotlib\u2019. Alternatively, to specify the plotting.backend for the whole session, set pd.options.plotting.backend.  New in version 1.0.0.   **kwargs\n\nOptions to pass to matplotlib plotting method.    Returns \n \nmatplotlib.axes.Axes or numpy.ndarray of them\n\nIf the backend is not the default matplotlib one, the return value will be the object returned by the backend.     Notes  See matplotlib documentation online for more on this subject If kind = \u2018bar\u2019 or \u2018barh\u2019, you can specify relative alignments for bar plot layout by position keyword. From 0 (left\/bottom-end) to 1 (right\/top-end). Default is 0.5 (center)","title":"pandas.reference.api.pandas.series.plot"},{"text":"Plotting The following functions are contained in the pandas.plotting module.       \nandrews_curves(frame, class_column[, ax, ...]) Generate a matplotlib plot of Andrews curves, for visualising clusters of multivariate data.  \nautocorrelation_plot(series[, ax]) Autocorrelation plot for time series.  \nbootstrap_plot(series[, fig, size, samples]) Bootstrap plot on mean, median and mid-range statistics.  \nboxplot(data[, column, by, ax, fontsize, ...]) Make a box plot from DataFrame columns.  \nderegister_matplotlib_converters() Remove pandas formatters and converters.  \nlag_plot(series[, lag, ax]) Lag plot for time series.  \nparallel_coordinates(frame, class_column[, ...]) Parallel coordinates plotting.  \nplot_params Stores pandas plotting options.  \nradviz(frame, class_column[, ax, color, ...]) Plot a multidimensional dataset in 2D.  \nregister_matplotlib_converters() Register pandas formatters and converters with matplotlib.  \nscatter_matrix(frame[, alpha, figsize, ax, ...]) Draw a matrix of scatter plots.  \ntable(ax, data[, rowLabels, colLabels]) Helper function to convert DataFrame and Series to matplotlib.table.","title":"pandas.reference.plotting"},{"text":"matplotlib.axes.Axes.plot   Axes.plot(*args, scalex=True, scaley=True, data=None, **kwargs)[source]\n \nPlot y versus x as lines and\/or markers. Call signatures: plot([x], y, [fmt], *, data=None, **kwargs)\nplot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n The coordinates of the points or line nodes are given by x, y. The optional parameter fmt is a convenient way for defining basic formatting like color, marker and linestyle. It's a shortcut string notation described in the Notes section below. >>> plot(x, y)        # plot x and y using default line style and color\n>>> plot(x, y, 'bo')  # plot x and y using blue circle markers\n>>> plot(y)           # plot y using x as index array 0..N-1\n>>> plot(y, 'r+')     # ditto, but with red plusses\n You can use Line2D properties as keyword arguments for more control on the appearance. Line properties and fmt can be mixed. The following two calls yield identical results: >>> plot(x, y, 'go--', linewidth=2, markersize=12)\n>>> plot(x, y, color='green', marker='o', linestyle='dashed',\n...      linewidth=2, markersize=12)\n When conflicting with fmt, keyword arguments take precedence. Plotting labelled data There's a convenient way for plotting objects with labelled data (i.e. data that can be accessed by index obj['y']). Instead of giving the data in x and y, you can provide the object in the data parameter and just give the labels for x and y: >>> plot('xlabel', 'ylabel', data=obj)\n All indexable objects are supported. This could e.g. be a dict, a pandas.DataFrame or a structured numpy array. Plotting multiple sets of data There are various ways to plot multiple sets of data.  \nThe most straight forward way is just to call plot multiple times. Example: >>> plot(x1, y1, 'bo')\n>>> plot(x2, y2, 'go')\n  \nIf x and\/or y are 2D arrays a separate data set will be drawn for every column. If both x and y are 2D, they must have the same shape. If only one of them is 2D with shape (N, m) the other must have length N and will be used for every data set m. Example: >>> x = [1, 2, 3]\n>>> y = np.array([[1, 2], [3, 4], [5, 6]])\n>>> plot(x, y)\n is equivalent to: >>> for col in range(y.shape[1]):\n...     plot(x, y[:, col])\n  \nThe third way is to specify multiple sets of [x], y, [fmt] groups: >>> plot(x1, y1, 'g^', x2, y2, 'g-')\n In this case, any additional keyword argument applies to all datasets. Also this syntax cannot be combined with the data parameter.   By default, each line is assigned a different style specified by a 'style cycle'. The fmt and line property parameters are only necessary if you want explicit deviations from these defaults. Alternatively, you can also change the style cycle using rcParams[\"axes.prop_cycle\"] (default: cycler('color', ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])).  Parameters \n \nx, yarray-like or scalar\n\n\nThe horizontal \/ vertical coordinates of the data points. x values are optional and default to range(len(y)). Commonly, these parameters are 1D arrays. They can also be scalars, or two-dimensional (in that case, the columns represent separate data sets). These arguments cannot be passed as keywords.  \nfmtstr, optional\n\n\nA format string, e.g. 'ro' for red circles. See the Notes section for a full description of the format strings. Format strings are just an abbreviation for quickly setting basic line properties. All of these and more can also be controlled by keyword arguments. This argument cannot be passed as keyword.  \ndataindexable object, optional\n\n\nAn object with labelled data. If given, provide the label names to plot in x and y.  Note Technically there's a slight ambiguity in calls where the second label is a valid fmt. plot('n', 'o', data=obj) could be plt(x, y) or plt(y, fmt). In such cases, the former interpretation is chosen, but a warning is issued. You may suppress the warning by adding an empty format string plot('n', 'o', '', data=obj).     Returns \n list of Line2D\n\n\nA list of lines representing the plotted data.    Other Parameters \n \nscalex, scaleybool, default: True\n\n\nThese parameters determine if the view limits are adapted to the data limits. The values are passed on to autoscale_view.  \n**kwargsLine2D properties, optional\n\n\nkwargs are used to specify properties like a line label (for auto legends), linewidth, antialiasing, marker face color. Example: >>> plot([1, 2, 3], [1, 2, 3], 'go-', label='line 1', linewidth=2)\n>>> plot([1, 2, 3], [1, 4, 9], 'rs', label='line 2')\n If you specify multiple lines with one plot call, the kwargs apply to all those lines. In case the label object is iterable, each element is used as labels for each set of data. Here is a list of available Line2D properties:   \nProperty Description   \nagg_filter a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array  \nalpha scalar or None  \nanimated bool  \nantialiased or aa bool  \nclip_box Bbox  \nclip_on bool  \nclip_path Patch or (Path, Transform) or None  \ncolor or c color  \ndash_capstyle CapStyle or {'butt', 'projecting', 'round'}  \ndash_joinstyle JoinStyle or {'miter', 'round', 'bevel'}  \ndashes sequence of floats (on\/off ink in points) or (None, None)  \ndata (2, N) array or two 1D arrays  \ndrawstyle or ds {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'  \nfigure Figure  \nfillstyle {'full', 'left', 'right', 'bottom', 'top', 'none'}  \ngid str  \nin_layout bool  \nlabel object  \nlinestyle or ls {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}  \nlinewidth or lw float  \nmarker marker style string, Path or MarkerStyle  \nmarkeredgecolor or mec color  \nmarkeredgewidth or mew float  \nmarkerfacecolor or mfc color  \nmarkerfacecoloralt or mfcalt color  \nmarkersize or ms float  \nmarkevery None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]  \npath_effects AbstractPathEffect  \npicker float or callable[[Artist, Event], tuple[bool, dict]]  \npickradius float  \nrasterized bool  \nsketch_params (scale: float, length: float, randomness: float)  \nsnap bool or None  \nsolid_capstyle CapStyle or {'butt', 'projecting', 'round'}  \nsolid_joinstyle JoinStyle or {'miter', 'round', 'bevel'}  \ntransform unknown  \nurl str  \nvisible bool  \nxdata 1D array  \nydata 1D array  \nzorder float        See also  scatter\n\nXY scatter plot with markers of varying size and\/or color ( sometimes also called bubble chart).    Notes Format Strings A format string consists of a part for color, marker and line: fmt = '[marker][line][color]'\n Each of them is optional. If not provided, the value from the style cycle is used. Exception: If line is given, but no marker, the data will be a line without markers. Other combinations such as [color][marker][line] are also supported, but note that their parsing may be ambiguous. Markers   \ncharacter description   \n'.' point marker  \n',' pixel marker  \n'o' circle marker  \n'v' triangle_down marker  \n'^' triangle_up marker  \n'<' triangle_left marker  \n'>' triangle_right marker  \n'1' tri_down marker  \n'2' tri_up marker  \n'3' tri_left marker  \n'4' tri_right marker  \n'8' octagon marker  \n's' square marker  \n'p' pentagon marker  \n'P' plus (filled) marker  \n'*' star marker  \n'h' hexagon1 marker  \n'H' hexagon2 marker  \n'+' plus marker  \n'x' x marker  \n'X' x (filled) marker  \n'D' diamond marker  \n'd' thin_diamond marker  \n'|' vline marker  \n'_' hline marker   Line Styles   \ncharacter description   \n'-' solid line style  \n'--' dashed line style  \n'-.' dash-dot line style  \n':' dotted line style   Example format strings: 'b'    # blue markers with default shape\n'or'   # red circles\n'-g'   # green solid line\n'--'   # dashed line with default color\n'^k:'  # black triangle_up markers connected by a dotted line\n Colors The supported color abbreviations are the single letter codes   \ncharacter color   \n'b' blue  \n'g' green  \n'r' red  \n'c' cyan  \n'm' magenta  \n'y' yellow  \n'k' black  \n'w' white   and the 'CN' colors that index into the default property cycle. If the color is the only part of the format string, you can additionally use any matplotlib.colors spec, e.g. full names ('green') or hex strings ('#008000'). \n  Examples using matplotlib.axes.Axes.plot\n \n   Plotting categorical variables   \n\n   CSD Demo   \n\n   Curve with error band   \n\n   EventCollection Demo   \n\n   Fill Between and Alpha   \n\n   Filling the area between lines   \n\n   Fill Betweenx Demo   \n\n   Customizing dashed line styles   \n\n   Lines with a ticked patheffect   \n\n   Marker reference   \n\n   Markevery Demo   \n\n   prop_cycle property markevery in rcParams   \n\n   Psd Demo   \n\n   Simple Plot   \n\n   Using span_where   \n\n   Creating a timeline with lines, dates, and text   \n\n   hlines and vlines   \n\n   Contour Corner Mask   \n\n   Contour plot of irregularly spaced data   \n\n   pcolormesh grids and shading   \n\n   Streamplot   \n\n   Spectrogram Demo   \n\n   Watermark image   \n\n   Aligning Labels   \n\n   Axes box aspect   \n\n   Axes Demo   \n\n   Controlling view limits using margins and sticky_edges   \n\n   Axes Props   \n\n   axhspan Demo   \n\n   Broken Axis   \n\n   Resizing axes with constrained layout   \n\n   Resizing axes with tight layout   \n\n   Figure labels: suptitle, supxlabel, supylabel   \n\n   Invert Axes   \n\n   Secondary Axis   \n\n   Sharing axis limits and views   \n\n   Figure subfigures   \n\n   Multiple subplots   \n\n   Creating multiple subplots using plt.subplots   \n\n   Plots with different scales   \n\n   Boxplots   \n\n   Using histograms to plot a cumulative distribution   \n\n   Some features of the histogram (hist) function   \n\n   Polar plot   \n\n   Polar Legend   \n\n   Using accented text in matplotlib   \n\n   Scale invariant angle label   \n\n   Annotating Plots   \n\n   Composing Custom Legends   \n\n   Date tick labels   \n\n   Custom tick formatter for time series   \n\n   AnnotationBbox demo   \n\n   Labeling ticks using engineering notation   \n\n   Annotation arrow style reference   \n\n   Legend using pre-defined labels   \n\n   Legend Demo   \n\n   Mathtext   \n\n   Math fontfamily   \n\n   Multiline   \n\n   Rendering math equations using TeX   \n\n   Text Rotation Relative To Line   \n\n   Title positioning   \n\n   Text watermark   \n\n   Annotate Transform   \n\n   Annotating a plot   \n\n   Annotation Polar   \n\n   Programmatically controlling subplot adjustment   \n\n   Dollar Ticks   \n\n   Simple axes labels   \n\n   Text Commands   \n\n   Color Demo   \n\n   Color by y-value   \n\n   PathPatch object   \n\n   Bezier Curve   \n\n   Dark background style sheet   \n\n   FiveThirtyEight style sheet   \n\n   ggplot style sheet   \n\n   Axes with a fixed physical size   \n\n   Parasite Simple   \n\n   Simple Axisline4   \n\n   Axis line styles   \n\n   Parasite Axes demo   \n\n   Parasite axis demo   \n\n   Custom spines with axisartist   \n\n   Simple Axisline   \n\n   Anatomy of a figure   \n\n   Bachelor's degrees by gender   \n\n   Integral as the area under a curve   \n\n   XKCD   \n\n   Decay   \n\n   The Bayes update   \n\n   The double pendulum problem   \n\n   Animated 3D random walk   \n\n   Animated line plot   \n\n   MATPLOTLIB UNCHAINED   \n\n   Mouse move and click events   \n\n   Data Browser   \n\n   Keypress event   \n\n   Legend Picking   \n\n   Looking Glass   \n\n   Path Editor   \n\n   Pick Event Demo2   \n\n   Resampling Data   \n\n   Timers   \n\n   Frontpage histogram example   \n\n   Frontpage plot example   \n\n   Changing colors of lines intersecting a box   \n\n   Cross hair cursor   \n\n   Custom projection   \n\n   Patheffect Demo   \n\n   Pythonic Matplotlib   \n\n   SVG Filter Line   \n\n   TickedStroke patheffect   \n\n   Zorder Demo   \n\n   Plot 2D data on 3D plot   \n\n   3D box surface plot   \n\n   Parametric Curve   \n\n   Lorenz Attractor   \n\n   2D and 3D Axes in same Figure   \n\n   Loglog Aspect   \n\n   Scales   \n\n   Symlog Demo   \n\n   Anscombe's quartet   \n\n   Radar chart (aka spider or star chart)   \n\n   Centered spines with arrows   \n\n   Multiple Yaxis With Spines   \n\n   Spine Placement   \n\n   Spines   \n\n   Custom spine bounds   \n\n   Centering labels between ticks   \n\n   Formatting date ticks using ConciseDateFormatter   \n\n   Date Demo Convert   \n\n   Date Index Formatter   \n\n   Date Precision and Epochs   \n\n   Major and minor ticks   \n\n   The default tick formatter   \n\n   Set default y-axis tick labels on the right   \n\n   Setting tick labels from a list of values   \n\n   Set default x-axis tick labels on the top   \n\n   Evans test   \n\n   CanvasAgg demo   \n\n   Annotate Explain   \n\n   Connect Simple01   \n\n   Connection styles for annotations   \n\n   Nested GridSpecs   \n\n   Pgf Fonts   \n\n   Pgf Texsystem   \n\n   Simple Annotate01   \n\n   Simple Legend01   \n\n   Simple Legend02   \n\n   Annotated Cursor   \n\n   Check Buttons   \n\n   Cursor   \n\n   Multicursor   \n\n   Radio Buttons   \n\n   Rectangle and ellipse selectors   \n\n   Span Selector   \n\n   Textbox   \n\n   Basic Usage   \n\n   Artist tutorial   \n\n   Legend guide   \n\n   Styling with cycler   \n\n   Constrained Layout Guide   \n\n   Tight Layout guide   \n\n   Arranging multiple Axes in a Figure   \n\n   Autoscaling   \n\n   Faster rendering by using blitting   \n\n   Path Tutorial   \n\n   Transformations Tutorial   \n\n   Specifying Colors   \n\n   Text in Matplotlib Plots   \n\n   plot(x, y)   \n\n   fill_between(x, y1, y2)   \n\n   tricontour(x, y, z)   \n\n   tricontourf(x, y, z)   \n\n   tripcolor(x, y, z)","title":"matplotlib._as_gen.matplotlib.axes.axes.plot"},{"text":"pandas.plotting.bootstrap_plot   pandas.plotting.bootstrap_plot(series, fig=None, size=50, samples=500, **kwds)[source]\n \nBootstrap plot on mean, median and mid-range statistics. The bootstrap plot is used to estimate the uncertainty of a statistic by relaying on random sampling with replacement [1]. This function will generate bootstrapping plots for mean, median and mid-range statistics for the given number of samples of the given size.  1 \n\u201cBootstrapping (statistics)\u201d in https:\/\/en.wikipedia.org\/wiki\/Bootstrapping_%28statistics%29    Parameters \n \nseries:pandas.Series\n\n\nSeries from where to get the samplings for the bootstrapping.  \nfig:matplotlib.figure.Figure, default None\n\n\nIf given, it will use the fig reference for plotting instead of creating a new one with default parameters.  \nsize:int, default 50\n\n\nNumber of data points to consider during each sampling. It must be less than or equal to the length of the series.  \nsamples:int, default 500\n\n\nNumber of times the bootstrap procedure is performed.  **kwds\n\nOptions to pass to matplotlib plotting method.    Returns \n matplotlib.figure.Figure\n\nMatplotlib figure.      See also  DataFrame.plot\n\nBasic plotting for DataFrame objects.  Series.plot\n\nBasic plotting for Series objects.    Examples This example draws a basic bootstrap plot for a Series. \n>>> s = pd.Series(np.random.uniform(size=100))\n>>> pd.plotting.bootstrap_plot(s)\n<Figure size 640x480 with 6 Axes>","title":"pandas.reference.api.pandas.plotting.bootstrap_plot"},{"text":"pandas.DataFrame.plot.scatter   DataFrame.plot.scatter(x, y, s=None, c=None, **kwargs)[source]\n \nCreate a scatter plot with varying marker point size and color. The coordinates of each point are defined by two dataframe columns and filled circles are used to represent each point. This kind of plot is useful to see complex correlations between two variables. Points could be for instance natural 2D coordinates like longitude and latitude in a map or, in general, any pair of metrics that can be plotted against each other.  Parameters \n \nx:int or str\n\n\nThe column name or column position to be used as horizontal coordinates for each point.  \ny:int or str\n\n\nThe column name or column position to be used as vertical coordinates for each point.  \ns:str, scalar or array-like, optional\n\n\nThe size of each point. Possible values are:  A string with the name of the column to be used for marker\u2019s size. A single scalar so all points have the same size. \nA sequence of scalars, which will be used for each point\u2019s size recursively. For instance, when passing [2,14] all points size will be either 2 or 14, alternatively.  Changed in version 1.1.0.     \nc:str, int or array-like, optional\n\n\nThe color of each point. Possible values are:  A single color string referred to by name, RGB or RGBA code, for instance \u2018red\u2019 or \u2018#a98d19\u2019. A sequence of color strings referred to by name, RGB or RGBA code, which will be used for each point\u2019s color recursively. For instance [\u2018green\u2019,\u2019yellow\u2019] all points will be filled in green or yellow, alternatively. A column name or position whose values will be used to color the marker points according to a colormap.   **kwargs\n\nKeyword arguments to pass on to DataFrame.plot().    Returns \n \nmatplotlib.axes.Axes or numpy.ndarray of them\n    See also  matplotlib.pyplot.scatter\n\nScatter plot using multiple input data formats.    Examples Let\u2019s see how to draw a scatter plot using coordinates from the values in a DataFrame\u2019s columns. \n>>> df = pd.DataFrame([[5.1, 3.5, 0], [4.9, 3.0, 0], [7.0, 3.2, 1],\n...                    [6.4, 3.2, 1], [5.9, 3.0, 2]],\n...                   columns=['length', 'width', 'species'])\n>>> ax1 = df.plot.scatter(x='length',\n...                       y='width',\n...                       c='DarkBlue')\n     And now with the color determined by a column as well. \n>>> ax2 = df.plot.scatter(x='length',\n...                       y='width',\n...                       c='species',\n...                       colormap='viridis')","title":"pandas.reference.api.pandas.dataframe.plot.scatter"},{"text":"pandas.plotting.autocorrelation_plot   pandas.plotting.autocorrelation_plot(series, ax=None, **kwargs)[source]\n \nAutocorrelation plot for time series.  Parameters \n \nseries:Time series\n\n\nax:Matplotlib axis object, optional\n\n**kwargs\n\nOptions to pass to matplotlib plotting method.    Returns \n class:matplotlib.axis.Axes\n\n   Examples The horizontal lines in the plot correspond to 95% and 99% confidence bands. The dashed line is 99% confidence band. \n>>> spacing = np.linspace(-9 * np.pi, 9 * np.pi, num=1000)\n>>> s = pd.Series(0.7 * np.random.rand(1000) + 0.3 * np.sin(spacing))\n>>> pd.plotting.autocorrelation_plot(s)\n<AxesSubplot:title={'center':'width'}, xlabel='Lag', ylabel='Autocorrelation'>","title":"pandas.reference.api.pandas.plotting.autocorrelation_plot"},{"text":"matplotlib.pyplot.plot   matplotlib.pyplot.plot(*args, scalex=True, scaley=True, data=None, **kwargs)[source]\n \nPlot y versus x as lines and\/or markers. Call signatures: plot([x], y, [fmt], *, data=None, **kwargs)\nplot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n The coordinates of the points or line nodes are given by x, y. The optional parameter fmt is a convenient way for defining basic formatting like color, marker and linestyle. It's a shortcut string notation described in the Notes section below. >>> plot(x, y)        # plot x and y using default line style and color\n>>> plot(x, y, 'bo')  # plot x and y using blue circle markers\n>>> plot(y)           # plot y using x as index array 0..N-1\n>>> plot(y, 'r+')     # ditto, but with red plusses\n You can use Line2D properties as keyword arguments for more control on the appearance. Line properties and fmt can be mixed. The following two calls yield identical results: >>> plot(x, y, 'go--', linewidth=2, markersize=12)\n>>> plot(x, y, color='green', marker='o', linestyle='dashed',\n...      linewidth=2, markersize=12)\n When conflicting with fmt, keyword arguments take precedence. Plotting labelled data There's a convenient way for plotting objects with labelled data (i.e. data that can be accessed by index obj['y']). Instead of giving the data in x and y, you can provide the object in the data parameter and just give the labels for x and y: >>> plot('xlabel', 'ylabel', data=obj)\n All indexable objects are supported. This could e.g. be a dict, a pandas.DataFrame or a structured numpy array. Plotting multiple sets of data There are various ways to plot multiple sets of data.  \nThe most straight forward way is just to call plot multiple times. Example: >>> plot(x1, y1, 'bo')\n>>> plot(x2, y2, 'go')\n  \nIf x and\/or y are 2D arrays a separate data set will be drawn for every column. If both x and y are 2D, they must have the same shape. If only one of them is 2D with shape (N, m) the other must have length N and will be used for every data set m. Example: >>> x = [1, 2, 3]\n>>> y = np.array([[1, 2], [3, 4], [5, 6]])\n>>> plot(x, y)\n is equivalent to: >>> for col in range(y.shape[1]):\n...     plot(x, y[:, col])\n  \nThe third way is to specify multiple sets of [x], y, [fmt] groups: >>> plot(x1, y1, 'g^', x2, y2, 'g-')\n In this case, any additional keyword argument applies to all datasets. Also this syntax cannot be combined with the data parameter.   By default, each line is assigned a different style specified by a 'style cycle'. The fmt and line property parameters are only necessary if you want explicit deviations from these defaults. Alternatively, you can also change the style cycle using rcParams[\"axes.prop_cycle\"] (default: cycler('color', ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])).  Parameters \n \nx, yarray-like or scalar\n\n\nThe horizontal \/ vertical coordinates of the data points. x values are optional and default to range(len(y)). Commonly, these parameters are 1D arrays. They can also be scalars, or two-dimensional (in that case, the columns represent separate data sets). These arguments cannot be passed as keywords.  \nfmtstr, optional\n\n\nA format string, e.g. 'ro' for red circles. See the Notes section for a full description of the format strings. Format strings are just an abbreviation for quickly setting basic line properties. All of these and more can also be controlled by keyword arguments. This argument cannot be passed as keyword.  \ndataindexable object, optional\n\n\nAn object with labelled data. If given, provide the label names to plot in x and y.  Note Technically there's a slight ambiguity in calls where the second label is a valid fmt. plot('n', 'o', data=obj) could be plt(x, y) or plt(y, fmt). In such cases, the former interpretation is chosen, but a warning is issued. You may suppress the warning by adding an empty format string plot('n', 'o', '', data=obj).     Returns \n list of Line2D\n\n\nA list of lines representing the plotted data.    Other Parameters \n \nscalex, scaleybool, default: True\n\n\nThese parameters determine if the view limits are adapted to the data limits. The values are passed on to autoscale_view.  \n**kwargsLine2D properties, optional\n\n\nkwargs are used to specify properties like a line label (for auto legends), linewidth, antialiasing, marker face color. Example: >>> plot([1, 2, 3], [1, 2, 3], 'go-', label='line 1', linewidth=2)\n>>> plot([1, 2, 3], [1, 4, 9], 'rs', label='line 2')\n If you specify multiple lines with one plot call, the kwargs apply to all those lines. In case the label object is iterable, each element is used as labels for each set of data. Here is a list of available Line2D properties:   \nProperty Description   \nagg_filter a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array  \nalpha scalar or None  \nanimated bool  \nantialiased or aa bool  \nclip_box Bbox  \nclip_on bool  \nclip_path Patch or (Path, Transform) or None  \ncolor or c color  \ndash_capstyle CapStyle or {'butt', 'projecting', 'round'}  \ndash_joinstyle JoinStyle or {'miter', 'round', 'bevel'}  \ndashes sequence of floats (on\/off ink in points) or (None, None)  \ndata (2, N) array or two 1D arrays  \ndrawstyle or ds {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'  \nfigure Figure  \nfillstyle {'full', 'left', 'right', 'bottom', 'top', 'none'}  \ngid str  \nin_layout bool  \nlabel object  \nlinestyle or ls {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}  \nlinewidth or lw float  \nmarker marker style string, Path or MarkerStyle  \nmarkeredgecolor or mec color  \nmarkeredgewidth or mew float  \nmarkerfacecolor or mfc color  \nmarkerfacecoloralt or mfcalt color  \nmarkersize or ms float  \nmarkevery None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]  \npath_effects AbstractPathEffect  \npicker float or callable[[Artist, Event], tuple[bool, dict]]  \npickradius float  \nrasterized bool  \nsketch_params (scale: float, length: float, randomness: float)  \nsnap bool or None  \nsolid_capstyle CapStyle or {'butt', 'projecting', 'round'}  \nsolid_joinstyle JoinStyle or {'miter', 'round', 'bevel'}  \ntransform unknown  \nurl str  \nvisible bool  \nxdata 1D array  \nydata 1D array  \nzorder float        See also  scatter\n\nXY scatter plot with markers of varying size and\/or color ( sometimes also called bubble chart).    Notes Format Strings A format string consists of a part for color, marker and line: fmt = '[marker][line][color]'\n Each of them is optional. If not provided, the value from the style cycle is used. Exception: If line is given, but no marker, the data will be a line without markers. Other combinations such as [color][marker][line] are also supported, but note that their parsing may be ambiguous. Markers   \ncharacter description   \n'.' point marker  \n',' pixel marker  \n'o' circle marker  \n'v' triangle_down marker  \n'^' triangle_up marker  \n'<' triangle_left marker  \n'>' triangle_right marker  \n'1' tri_down marker  \n'2' tri_up marker  \n'3' tri_left marker  \n'4' tri_right marker  \n'8' octagon marker  \n's' square marker  \n'p' pentagon marker  \n'P' plus (filled) marker  \n'*' star marker  \n'h' hexagon1 marker  \n'H' hexagon2 marker  \n'+' plus marker  \n'x' x marker  \n'X' x (filled) marker  \n'D' diamond marker  \n'd' thin_diamond marker  \n'|' vline marker  \n'_' hline marker   Line Styles   \ncharacter description   \n'-' solid line style  \n'--' dashed line style  \n'-.' dash-dot line style  \n':' dotted line style   Example format strings: 'b'    # blue markers with default shape\n'or'   # red circles\n'-g'   # green solid line\n'--'   # dashed line with default color\n'^k:'  # black triangle_up markers connected by a dotted line\n Colors The supported color abbreviations are the single letter codes   \ncharacter color   \n'b' blue  \n'g' green  \n'r' red  \n'c' cyan  \n'm' magenta  \n'y' yellow  \n'k' black  \n'w' white   and the 'CN' colors that index into the default property cycle. If the color is the only part of the format string, you can additionally use any matplotlib.colors spec, e.g. full names ('green') or hex strings ('#008000'). \n  Examples using matplotlib.pyplot.plot\n \n   Plotting masked and NaN values   \n\n   Scatter Masked   \n\n   Stairs Demo   \n\n   Step Demo   \n\n   Custom Figure subclasses   \n\n   Managing multiple figures in pyplot   \n\n   Shared Axis   \n\n   Multiple subplots   \n\n   Controlling style of text and labels using a dictionary   \n\n   Title positioning   \n\n   Infinite lines   \n\n   plot() format string   \n\n   Pyplot Mathtext   \n\n   Pyplot Simple   \n\n   Pyplot Three   \n\n   Pyplot Two Subplots   \n\n   Dolphins   \n\n   Solarized Light stylesheet   \n\n   Frame grabbing   \n\n   Coords Report   \n\n   Customize Rc   \n\n   Findobj Demo   \n\n   Multipage PDF   \n\n   Print Stdout   \n\n   Set and get properties   \n\n   transforms.offset_copy   \n\n   Zorder Demo   \n\n   Custom scale   \n\n   Placing date ticks using recurrence rules   \n\n   Rotating custom tick labels   \n\n   Tool Manager   \n\n   Buttons   \n\n   Slider   \n\n   Snapping Sliders to Discrete Values   \n\n   Basic Usage   \n\n   Pyplot tutorial   \n\n   Customizing Matplotlib with style sheets and rcParams   \n\n   Path effects guide","title":"matplotlib._as_gen.matplotlib.pyplot.plot"},{"text":"pandas.plotting.lag_plot   pandas.plotting.lag_plot(series, lag=1, ax=None, **kwds)[source]\n \nLag plot for time series.  Parameters \n \nseries:Time series\n\n\nlag:lag of the scatter plot, default 1\n\n\nax:Matplotlib axis object, optional\n\n**kwds\n\nMatplotlib scatter method keyword arguments.    Returns \n class:matplotlib.axis.Axes\n\n   Examples Lag plots are most commonly used to look for patterns in time series data. Given the following time series \n>>> np.random.seed(5)\n>>> x = np.cumsum(np.random.normal(loc=1, scale=5, size=50))\n>>> s = pd.Series(x)\n>>> s.plot()\n<AxesSubplot:xlabel='Midrange'>\n     A lag plot with lag=1 returns \n>>> pd.plotting.lag_plot(s, lag=1)\n<AxesSubplot:xlabel='y(t)', ylabel='y(t + 1)'>","title":"pandas.reference.api.pandas.plotting.lag_plot"},{"text":"pandas.DataFrame.plot.line   DataFrame.plot.line(x=None, y=None, **kwargs)[source]\n \nPlot Series or DataFrame as lines. This function is useful to plot lines using DataFrame\u2019s values as coordinates.  Parameters \n \nx:label or position, optional\n\n\nAllows plotting of one column versus another. If not specified, the index of the DataFrame is used.  \ny:label or position, optional\n\n\nAllows plotting of one column versus another. If not specified, all numerical columns are used.  \ncolor:str, array-like, or dict, optional\n\n\nThe color for each of the DataFrame\u2019s columns. Possible values are:  \n A single color string referred to by name, RGB or RGBA code,\n\nfor instance \u2018red\u2019 or \u2018#a98d19\u2019.    \n A sequence of color strings referred to by name, RGB or RGBA\n\ncode, which will be used for each column recursively. For instance [\u2018green\u2019,\u2019yellow\u2019] each column\u2019s line will be filled in green or yellow, alternatively. If there is only a single column to be plotted, then only the first color from the color list will be used.    \n A dict of the form {column name:color}, so that each column will be\n\n\ncolored accordingly. For example, if your columns are called a and b, then passing {\u2018a\u2019: \u2018green\u2019, \u2018b\u2019: \u2018red\u2019} will color lines for column a in green and lines for column b in red.      New in version 1.1.0.   **kwargs\n\nAdditional keyword arguments are documented in DataFrame.plot().    Returns \n matplotlib.axes.Axes or np.ndarray of them\n\nAn ndarray is returned with one matplotlib.axes.Axes per column when subplots=True.      See also  matplotlib.pyplot.plot\n\nPlot y versus x as lines and\/or markers.    Examples \n>>> s = pd.Series([1, 3, 2])\n>>> s.plot.line()\n<AxesSubplot:ylabel='Density'>\n     The following example shows the populations for some animals over the years. \n>>> df = pd.DataFrame({\n...    'pig': [20, 18, 489, 675, 1776],\n...    'horse': [4, 25, 281, 600, 1900]\n...    }, index=[1990, 1997, 2003, 2009, 2014])\n>>> lines = df.plot.line()\n     An example with subplots, so an array of axes is returned. \n>>> axes = df.plot.line(subplots=True)\n>>> type(axes)\n<class 'numpy.ndarray'>\n     Let\u2019s repeat the same example, but specifying colors for each column (in this case, for each animal). \n>>> axes = df.plot.line(\n...     subplots=True, color={\"pig\": \"pink\", \"horse\": \"#742802\"}\n... )\n     The following example shows the relationship between both populations. \n>>> lines = df.plot.line(x='pig', y='horse')","title":"pandas.reference.api.pandas.dataframe.plot.line"}]}
{"task_id":13368659,"prompt":"def f_13368659():\n\treturn ","suffix":"","canonical_solution":"['192.168.%d.%d'%(i, j) for i in range(256) for j in range(256)]","test_start":"\ndef check(candidate):","test":["\n    addrs = candidate()\n    assert len(addrs) == 256*256\n    assert addrs == [f'192.168.{i}.{j}' for i in range(256) for j in range(256)]\n"],"entry_point":"f_13368659","intent":"loop through the IP address range \"192.168.x.x\"","library":[],"docs":[]}
{"task_id":4065737,"prompt":"def f_4065737(x):\n\treturn ","suffix":"","canonical_solution":"sum(1 << i for i, b in enumerate(x) if b)","test_start":"\ndef check(candidate):","test":["\n    assert candidate([1,2,3]) == 7\n","\n    assert candidate([1,2,None,3,None]) == 11\n"],"entry_point":"f_4065737","intent":"Sum the corresponding decimal values for binary values of each boolean element in list `x`","library":[],"docs":[]}
{"task_id":8691311,"prompt":"def f_8691311(line1, line2, line3, target):\n\t","suffix":"\n\treturn ","canonical_solution":"target.write('%r\\n%r\\n%r\\n' % (line1, line2, line3))","test_start":"\ndef check(candidate):","test":["\n    file_name = 'abc.txt'\n    lines = ['fgh', 'ijk', 'mnop']\n    f = open(file_name, 'a')\n    candidate(lines[0], lines[1], lines[2], f)\n    f.close()\n    with open(file_name, 'r') as f:\n        f_lines = f.readlines()\n        for i in range (0, len(lines)):\n            assert lines[i] in f_lines[i]\n"],"entry_point":"f_8691311","intent":"write multiple strings `line1`, `line2` and `line3` in one line in a file `target`","library":[],"docs":[]}
{"task_id":10632111,"prompt":"def f_10632111(data):\n\treturn ","suffix":"","canonical_solution":"[y for x in data for y in (x if isinstance(x, list) else [x])]","test_start":"\ndef check(candidate):","test":["\n    data = [[1, 2], [3]]\n    assert candidate(data) == [1, 2, 3]\n","\n    data = [[1, 2], [3], []]\n    assert candidate(data) == [1, 2, 3]\n","\n    data = [1,2,3]\n    assert candidate(data) == [1, 2, 3]\n"],"entry_point":"f_10632111","intent":"Convert list of lists `data` into a flat list","library":[],"docs":[]}
{"task_id":15392730,"prompt":"def f_15392730():\n\treturn ","suffix":"","canonical_solution":"'foo\\nbar'.encode('unicode_escape')","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == b'foo\\\\nbar'\n"],"entry_point":"f_15392730","intent":"Print new line character as `\\n` in a string `foo\\nbar`","library":[],"docs":[]}
{"task_id":1010961,"prompt":"def f_1010961(s):\n\treturn ","suffix":"","canonical_solution":"\"\"\"\"\"\".join(s.rsplit(',', 1))","test_start":"\ndef check(candidate):","test":["\n    assert candidate('abc, def, klm') == 'abc, def klm'\n"],"entry_point":"f_1010961","intent":"remove last comma character ',' in string `s`","library":[],"docs":[]}
{"task_id":23855976,"prompt":"def f_23855976(x):\n\treturn ","suffix":"","canonical_solution":"(x[1:] + x[:-1]) \/ 2","test_start":"\nimport numpy as np\n\ndef check(candidate):","test":["\n    x = np.array([ 1230.,  1230.,  1227.,  1235.,  1217.,  1153.,  1170.])\n    xm = np.array([1230. , 1228.5, 1231. , 1226. , 1185. , 1161.5])\n    assert np.array_equal(candidate(x), xm)\n"],"entry_point":"f_23855976","intent":"calculate the mean of each element in array `x` with the element previous to it","library":["numpy"],"docs":[]}
{"task_id":23855976,"prompt":"def f_23855976(x):\n\treturn ","suffix":"","canonical_solution":"x[:-1] + (x[1:] - x[:-1]) \/ 2","test_start":"\nimport numpy as np\n\ndef check(candidate):","test":["\n    x = np.array([ 1230.,  1230.,  1227.,  1235.,  1217.,  1153.,  1170.])\n    xm = np.array([1230. , 1228.5, 1231. , 1226. , 1185. , 1161.5])\n    assert np.array_equal(candidate(x), xm)\n"],"entry_point":"f_23855976","intent":"get an array of the mean of each two consecutive values in numpy array `x`","library":["numpy"],"docs":[]}
{"task_id":6375343,"prompt":"def f_6375343():\n\t","suffix":"\n\treturn arr","canonical_solution":"arr = numpy.fromiter(codecs.open('new.txt', encoding='utf-8'), dtype='<U2')","test_start":"\nimport numpy\nimport codecs\nimport numpy as np\n\ndef check(candidate):","test":["\n    with open ('new.txt', 'a', encoding='utf-8') as f:\n        f.write('\u091f')\n        f.write('\u091c')\n    arr = candidate()\n    assert arr[0] == '\u091f\u091c'\n"],"entry_point":"f_6375343","intent":"load data containing `utf-8` from file `new.txt` into numpy array `arr`","library":["codecs","numpy"],"docs":[{"text":"POP3.utf8()  \nTry to switch to UTF-8 mode. Returns the server response if successful, raises error_proto if not. Specified in RFC 6856.  New in version 3.5.","title":"python.library.poplib#poplib.POP3.utf8"},{"text":"numpy.newaxis\n \nA convenient alias for None, useful for indexing arrays. Examples >>> newaxis is None\nTrue\n>>> x = np.arange(3)\n>>> x\narray([0, 1, 2])\n>>> x[:, newaxis]\narray([[0],\n[1],\n[2]])\n>>> x[:, newaxis, newaxis]\narray([[[0]],\n[[1]],\n[[2]]])\n>>> x[:, newaxis] * x\narray([[0, 0, 0],\n[0, 1, 2],\n[0, 2, 4]])\n Outer product, same as outer(x, y): >>> y = np.arange(3, 6)\n>>> x[:, newaxis] * y\narray([[ 0,  0,  0],\n[ 3,  4,  5],\n[ 6,  8, 10]])\n x[newaxis, :] is equivalent to x[newaxis] and x[None]: >>> x[newaxis, :].shape\n(1, 3)\n>>> x[newaxis].shape\n(1, 3)\n>>> x[None].shape\n(1, 3)\n>>> x[:, newaxis].shape\n(3, 1)","title":"numpy.reference.constants#numpy.newaxis"},{"text":"numpy.fromregex   numpy.fromregex(file, regexp, dtype, encoding=None)[source]\n \nConstruct an array from a text file, using regular expression parsing. The returned array is always a structured array, and is constructed from all matches of the regular expression in the file. Groups in the regular expression are converted to fields of the structured array.  Parameters \n \nfilepath or file\n\n\nFilename or file object to read.  Changed in version 1.22.0: Now accepts os.PathLike implementations.   \nregexpstr or regexp\n\n\nRegular expression used to parse the file. Groups in the regular expression correspond to fields in the dtype.  \ndtypedtype or list of dtypes\n\n\nDtype for the structured array; must be a structured datatype.  \nencodingstr, optional\n\n\nEncoding used to decode the inputfile. Does not apply to input streams.  New in version 1.14.0.     Returns \n \noutputndarray\n\n\nThe output array, containing the part of the content of file that was matched by regexp. output is always a structured array.    Raises \n TypeError\n\nWhen dtype is not a valid dtype for a structured array.      See also  \nfromstring, loadtxt\n\n  Notes Dtypes for structured arrays can be specified in several forms, but all forms specify at least the data type and field name. For details see basics.rec. Examples >>> from io import StringIO\n>>> text = StringIO(\"1312 foo\\n1534  bar\\n444   qux\")\n >>> regexp = r\"(\\d+)\\s+(...)\"  # match [digits, whitespace, anything]\n>>> output = np.fromregex(text, regexp,\n...                       [('num', np.int64), ('key', 'S3')])\n>>> output\narray([(1312, b'foo'), (1534, b'bar'), ( 444, b'qux')],\n      dtype=[('num', '<i8'), ('key', 'S3')])\n>>> output['num']\narray([1312, 1534,  444])","title":"numpy.reference.generated.numpy.fromregex"},{"text":"new  \nTrue if the session is new, False otherwise.","title":"flask.api.index#flask.session.new"},{"text":"numpy.loadtxt   numpy.loadtxt(fname, dtype=<class 'float'>, comments='#', delimiter=None, converters=None, skiprows=0, usecols=None, unpack=False, ndmin=0, encoding='bytes', max_rows=None, *, like=None)[source]\n \nLoad data from a text file. Each row in the text file must have the same number of values.  Parameters \n \nfnamefile, str, pathlib.Path, list of str, generator\n\n\nFile, filename, list, or generator to read. If the filename extension is .gz or .bz2, the file is first decompressed. Note that generators must return bytes or strings. The strings in a list or produced by a generator are treated as lines.  \ndtypedata-type, optional\n\n\nData-type of the resulting array; default: float. If this is a structured data-type, the resulting array will be 1-dimensional, and each row will be interpreted as an element of the array. In this case, the number of columns used must match the number of fields in the data-type.  \ncommentsstr or sequence of str, optional\n\n\nThe characters or list of characters used to indicate the start of a comment. None implies no comments. For backwards compatibility, byte strings will be decoded as \u2018latin1\u2019. The default is \u2018#\u2019.  \ndelimiterstr, optional\n\n\nThe string used to separate values. For backwards compatibility, byte strings will be decoded as \u2018latin1\u2019. The default is whitespace.  \nconvertersdict, optional\n\n\nA dictionary mapping column number to a function that will parse the column string into the desired value. E.g., if column 0 is a date string: converters = {0: datestr2num}. Converters can also be used to provide a default value for missing data (but see also genfromtxt): converters = {3: lambda s: float(s.strip() or 0)}. Default: None.  \nskiprowsint, optional\n\n\nSkip the first skiprows lines, including comments; default: 0.  \nusecolsint or sequence, optional\n\n\nWhich columns to read, with 0 being the first. For example, usecols = (1,4,5) will extract the 2nd, 5th and 6th columns. The default, None, results in all columns being read.  Changed in version 1.11.0: When a single column has to be read it is possible to use an integer instead of a tuple. E.g usecols = 3 reads the fourth column the same way as usecols = (3,) would.   \nunpackbool, optional\n\n\nIf True, the returned array is transposed, so that arguments may be unpacked using x, y, z = loadtxt(...). When used with a structured data-type, arrays are returned for each field. Default is False.  \nndminint, optional\n\n\nThe returned array will have at least ndmin dimensions. Otherwise mono-dimensional axes will be squeezed. Legal values: 0 (default), 1 or 2.  New in version 1.6.0.   \nencodingstr, optional\n\n\nEncoding used to decode the inputfile. Does not apply to input streams. The special value \u2018bytes\u2019 enables backward compatibility workarounds that ensures you receive byte arrays as results if possible and passes \u2018latin1\u2019 encoded strings to converters. Override this value to receive unicode arrays and pass strings as input to converters. If set to None the system default is used. The default value is \u2018bytes\u2019.  New in version 1.14.0.   \nmax_rowsint, optional\n\n\nRead max_rows lines of content after skiprows lines. The default is to read all the lines.  New in version 1.16.0.   \nlikearray_like\n\n\nReference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as like supports the __array_function__ protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.  New in version 1.20.0.     Returns \n \noutndarray\n\n\nData read from the text file.      See also  \nload, fromstring, fromregex\n\ngenfromtxt\n\nLoad data with missing values handled as specified.  scipy.io.loadmat\n\nreads MATLAB data files    Notes This function aims to be a fast reader for simply formatted files. The genfromtxt function provides more sophisticated handling of, e.g., lines with missing values.  New in version 1.10.0.  The strings produced by the Python float.hex method can be used as input for floats. Examples >>> from io import StringIO   # StringIO behaves like a file object\n>>> c = StringIO(\"0 1\\n2 3\")\n>>> np.loadtxt(c)\narray([[0., 1.],\n       [2., 3.]])\n >>> d = StringIO(\"M 21 72\\nF 35 58\")\n>>> np.loadtxt(d, dtype={'names': ('gender', 'age', 'weight'),\n...                      'formats': ('S1', 'i4', 'f4')})\narray([(b'M', 21, 72.), (b'F', 35, 58.)],\n      dtype=[('gender', 'S1'), ('age', '<i4'), ('weight', '<f4')])\n >>> c = StringIO(\"1,0,2\\n3,0,4\")\n>>> x, y = np.loadtxt(c, delimiter=',', usecols=(0, 2), unpack=True)\n>>> x\narray([1., 3.])\n>>> y\narray([2., 4.])\n This example shows how converters can be used to convert a field with a trailing minus sign into a negative number. >>> s = StringIO('10.01 31.25-\\n19.22 64.31\\n17.57- 63.94')\n>>> def conv(fld):\n...     return -float(fld[:-1]) if fld.endswith(b'-') else float(fld)\n...\n>>> np.loadtxt(s, converters={0: conv, 1: conv})\narray([[ 10.01, -31.25],\n       [ 19.22,  64.31],\n       [-17.57,  63.94]])","title":"numpy.reference.generated.numpy.loadtxt"},{"text":"newPage(width, height)[source]","title":"matplotlib.backend_pdf_api#matplotlib.backends.backend_pdf.PdfFile.newPage"},{"text":"numpy.record.newbyteorder method   record.newbyteorder(new_order='S', \/)\n \nReturn a new dtype with a different byte order. Changes are also made in all fields and sub-arrays of the data type. The new_order code can be any from the following:  \u2018S\u2019 - swap dtype from current to opposite endian {\u2018<\u2019, \u2018little\u2019} - little endian {\u2018>\u2019, \u2018big\u2019} - big endian {\u2018=\u2019, \u2018native\u2019} - native order {\u2018|\u2019, \u2018I\u2019} - ignore (no change to byte order)   Parameters \n \nnew_orderstr, optional\n\n\nByte order to force; a value from the byte order specifications above. The default value (\u2018S\u2019) results in swapping the current byte order.    Returns \n \nnew_dtypedtype\n\n\nNew dtype object with the given change to the byte order.","title":"numpy.reference.generated.numpy.record.newbyteorder"},{"text":"tf.raw_ops.Empty Creates a tensor with the given shape.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.raw_ops.Empty  \ntf.raw_ops.Empty(\n    shape, dtype, init=False, name=None\n)\n This operation creates a tensor of shape and dtype. Args: shape: A Tensor of type int32. 1-D. Represents the shape of the output tensor. dtype: A tf.DType. init: An optional bool. Defaults to False. If True, initialize the returned tensor with the default value of dtype. Otherwise, the implementation is free not to initializethe tensor's content. name: A name for the operation (optional). Returns: A Tensor of type dtype.","title":"tensorflow.raw_ops.empty"},{"text":"array.tounicode()  \nConvert the array to a unicode string. The array must be a type 'u' array; otherwise a ValueError is raised. Use array.tobytes().decode(enc) to obtain a unicode string from an array of some other type.","title":"python.library.array#array.array.tounicode"},{"text":"new_fixed_axis(loc, nth_coord=None, axis_direction=None, offset=None, axes=None)[source]","title":"matplotlib._as_gen.mpl_toolkits.axisartist.axislines.gridhelperrectlinear#mpl_toolkits.axisartist.axislines.GridHelperRectlinear.new_fixed_axis"}]}
{"task_id":1547733,"prompt":"def f_1547733(l):\n\t","suffix":"\n\treturn l","canonical_solution":"l = sorted(l, key=itemgetter('time'), reverse=True)","test_start":"\nfrom operator import itemgetter\n\ndef check(candidate):","test":["\n    l = [ {'time':33}, {'time':11}, {'time':66} ]\n    assert candidate(l) == [{'time':66}, {'time':33}, {'time':11}]\n"],"entry_point":"f_1547733","intent":"reverse sort list of dicts `l` by value for key `time`","library":["operator"],"docs":[{"text":"datetimes(field_name, kind, order='ASC', tzinfo=None, is_dst=None)","title":"django.ref.models.querysets#django.db.models.query.QuerySet.datetimes"},{"text":"reverse_order()  \nThis method for the Stats class reverses the ordering of the basic list within the object. Note that by default ascending vs descending order is properly selected based on the sort key of choice.","title":"python.library.profile#pstats.Stats.reverse_order"},{"text":"dates(field, kind, order='ASC')","title":"django.ref.models.querysets#django.db.models.query.QuerySet.dates"},{"text":"pandas.DatetimeIndex.indexer_at_time   DatetimeIndex.indexer_at_time(time, asof=False)[source]\n \nReturn index locations of values at particular time of day (e.g. 9:30AM).  Parameters \n \ntime:datetime.time or str\n\n\nTime passed in either as object (datetime.time) or as string in appropriate format (\u201c%H:%M\u201d, \u201c%H%M\u201d, \u201c%I:%M%p\u201d, \u201c%I%M%p\u201d, \u201c%H:%M:%S\u201d, \u201c%H%M%S\u201d, \u201c%I:%M:%S%p\u201d, \u201c%I%M%S%p\u201d).    Returns \n np.ndarray[np.intp]\n    See also  indexer_between_time\n\nGet index locations of values between particular times of day.  DataFrame.at_time\n\nSelect values at particular time of day.","title":"pandas.reference.api.pandas.datetimeindex.indexer_at_time"},{"text":"pandas.Series.at_time   Series.at_time(time, asof=False, axis=None)[source]\n \nSelect values at particular time of day (e.g., 9:30AM).  Parameters \n \ntime:datetime.time or str\n\n\naxis:{0 or \u2018index\u2019, 1 or \u2018columns\u2019}, default 0\n\n  Returns \n Series or DataFrame\n  Raises \n TypeError\n\nIf the index is not a DatetimeIndex      See also  between_time\n\nSelect values between particular times of the day.  first\n\nSelect initial periods of time series based on a date offset.  last\n\nSelect final periods of time series based on a date offset.  DatetimeIndex.indexer_at_time\n\nGet just the index locations for values at particular time of the day.    Examples \n>>> i = pd.date_range('2018-04-09', periods=4, freq='12H')\n>>> ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i)\n>>> ts\n                     A\n2018-04-09 00:00:00  1\n2018-04-09 12:00:00  2\n2018-04-10 00:00:00  3\n2018-04-10 12:00:00  4\n  \n>>> ts.at_time('12:00')\n                     A\n2018-04-09 12:00:00  2\n2018-04-10 12:00:00  4","title":"pandas.reference.api.pandas.series.at_time"},{"text":"werkzeug.urls.url_encode(obj, charset='utf-8', encode_keys=None, sort=False, key=None, separator='&')  \nURL encode a dict\/MultiDict. If a value is None it will not appear in the result string. Per default only values are encoded into the target charset strings.  Parameters \n \nobj (Union[Mapping[str, str], Iterable[Tuple[str, str]]]) \u2013 the object to encode into a query string. \ncharset (str) \u2013 the charset of the query string. \nsort (bool) \u2013 set to True if you want parameters to be sorted by key. \nseparator (str) \u2013 the separator to be used for the pairs. \nkey (Optional[Callable[[Tuple[str, str]], Any]]) \u2013 an optional function to be used for sorting. For more details check out the sorted() documentation. \nencode_keys (None) \u2013    Return type \nstr    Changed in version 2.0: The encode_keys parameter is deprecated and will be removed in Werkzeug 2.1.   Changelog Changed in version 0.5: Added the sort, key, and separator parameters.","title":"werkzeug.urls.index#werkzeug.urls.url_encode"},{"text":"pandas.DataFrame.at_time   DataFrame.at_time(time, asof=False, axis=None)[source]\n \nSelect values at particular time of day (e.g., 9:30AM).  Parameters \n \ntime:datetime.time or str\n\n\naxis:{0 or \u2018index\u2019, 1 or \u2018columns\u2019}, default 0\n\n  Returns \n Series or DataFrame\n  Raises \n TypeError\n\nIf the index is not a DatetimeIndex      See also  between_time\n\nSelect values between particular times of the day.  first\n\nSelect initial periods of time series based on a date offset.  last\n\nSelect final periods of time series based on a date offset.  DatetimeIndex.indexer_at_time\n\nGet just the index locations for values at particular time of the day.    Examples \n>>> i = pd.date_range('2018-04-09', periods=4, freq='12H')\n>>> ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i)\n>>> ts\n                     A\n2018-04-09 00:00:00  1\n2018-04-09 12:00:00  2\n2018-04-10 00:00:00  3\n2018-04-10 12:00:00  4\n  \n>>> ts.at_time('12:00')\n                     A\n2018-04-09 12:00:00  2\n2018-04-10 12:00:00  4","title":"pandas.reference.api.pandas.dataframe.at_time"},{"text":"pandas.DatetimeIndex.indexer_between_time   DatetimeIndex.indexer_between_time(start_time, end_time, include_start=True, include_end=True)[source]\n \nReturn index locations of values between particular times of day (e.g., 9:00-9:30AM).  Parameters \n \nstart_time, end_time:datetime.time, str\n\n\nTime passed either as object (datetime.time) or as string in appropriate format (\u201c%H:%M\u201d, \u201c%H%M\u201d, \u201c%I:%M%p\u201d, \u201c%I%M%p\u201d, \u201c%H:%M:%S\u201d, \u201c%H%M%S\u201d, \u201c%I:%M:%S%p\u201d,\u201d%I%M%S%p\u201d).  \ninclude_start:bool, default True\n\n\ninclude_end:bool, default True\n\n  Returns \n np.ndarray[np.intp]\n    See also  indexer_at_time\n\nGet index locations of values at particular time of day.  DataFrame.between_time\n\nSelect values between particular times of day.","title":"pandas.reference.api.pandas.datetimeindex.indexer_between_time"},{"text":"get_date_list(queryset, date_type=None, ordering='ASC')  \nReturns the list of dates of type date_type for which queryset contains entries. For example, get_date_list(qs, 'year') will return the list of years for which qs has entries. If date_type isn\u2019t provided, the result of get_date_list_period() is used. date_type and ordering are passed to QuerySet.dates().","title":"django.ref.class-based-views.mixins-date-based#django.views.generic.dates.BaseDateListView.get_date_list"},{"text":"classmethod datetime.combine(date, time, tzinfo=self.tzinfo)  \nReturn a new datetime object whose date components are equal to the given date object\u2019s, and whose time components are equal to the given time object\u2019s. If the tzinfo argument is provided, its value is used to set the tzinfo attribute of the result, otherwise the tzinfo attribute of the time argument is used. For any datetime object d, d == datetime.combine(d.date(), d.time(), d.tzinfo). If date is a datetime object, its time components and tzinfo attributes are ignored.  Changed in version 3.6: Added the tzinfo argument.","title":"python.library.datetime#datetime.datetime.combine"}]}
{"task_id":1547733,"prompt":"def f_1547733(l):\n\t","suffix":"\n\treturn l","canonical_solution":"l = sorted(l, key=lambda a: a['time'], reverse=True)","test_start":"\ndef check(candidate):","test":["\n    l = [ {'time':33}, {'time':11}, {'time':66} ]\n    assert candidate(l) == [{'time':66}, {'time':33}, {'time':11}]\n"],"entry_point":"f_1547733","intent":"Sort a list of dictionary `l` based on key `time` in descending order","library":[],"docs":[]}
{"task_id":37080612,"prompt":"def f_37080612(df):\n\treturn ","suffix":"","canonical_solution":"df.loc[df[0].str.contains('(Hel|Just)')]","test_start":"\nimport pandas as pd \n\ndef check(candidate):","test":["\n    df = pd.DataFrame([['Hello', 'World'], ['Just', 'Wanted'], ['To', 'Say'], ['I\\'m', 'Tired']])\n    df1 = candidate(df)\n    assert df1[0][0] == 'Hello'\n    assert df1[0][1] == 'Just'\n"],"entry_point":"f_37080612","intent":"get rows of dataframe `df` that match regex '(Hel|Just)'","library":["pandas"],"docs":[{"text":"pandas.DataFrame.filter   DataFrame.filter(items=None, like=None, regex=None, axis=None)[source]\n \nSubset the dataframe rows or columns according to the specified index labels. Note that this routine does not filter a dataframe on its contents. The filter is applied to the labels of the index.  Parameters \n \nitems:list-like\n\n\nKeep labels from axis which are in items.  \nlike:str\n\n\nKeep labels from axis for which \u201clike in label == True\u201d.  \nregex:str (regular expression)\n\n\nKeep labels from axis for which re.search(regex, label) == True.  \naxis:{0 or \u2018index\u2019, 1 or \u2018columns\u2019, None}, default None\n\n\nThe axis to filter on, expressed either as an index (int) or axis name (str). By default this is the info axis, \u2018index\u2019 for Series, \u2018columns\u2019 for DataFrame.    Returns \n same type as input object\n    See also  DataFrame.loc\n\nAccess a group of rows and columns by label(s) or a boolean array.    Notes The items, like, and regex parameters are enforced to be mutually exclusive. axis defaults to the info axis that is used when indexing with []. Examples \n>>> df = pd.DataFrame(np.array(([1, 2, 3], [4, 5, 6])),\n...                   index=['mouse', 'rabbit'],\n...                   columns=['one', 'two', 'three'])\n>>> df\n        one  two  three\nmouse     1    2      3\nrabbit    4    5      6\n  \n>>> # select columns by name\n>>> df.filter(items=['one', 'three'])\n         one  three\nmouse     1      3\nrabbit    4      6\n  \n>>> # select columns by regular expression\n>>> df.filter(regex='e$', axis=1)\n         one  three\nmouse     1      3\nrabbit    4      6\n  \n>>> # select rows containing 'bbi'\n>>> df.filter(like='bbi', axis=0)\n         one  two  three\nrabbit    4    5      6","title":"pandas.reference.api.pandas.dataframe.filter"},{"text":"pandas.Series.filter   Series.filter(items=None, like=None, regex=None, axis=None)[source]\n \nSubset the dataframe rows or columns according to the specified index labels. Note that this routine does not filter a dataframe on its contents. The filter is applied to the labels of the index.  Parameters \n \nitems:list-like\n\n\nKeep labels from axis which are in items.  \nlike:str\n\n\nKeep labels from axis for which \u201clike in label == True\u201d.  \nregex:str (regular expression)\n\n\nKeep labels from axis for which re.search(regex, label) == True.  \naxis:{0 or \u2018index\u2019, 1 or \u2018columns\u2019, None}, default None\n\n\nThe axis to filter on, expressed either as an index (int) or axis name (str). By default this is the info axis, \u2018index\u2019 for Series, \u2018columns\u2019 for DataFrame.    Returns \n same type as input object\n    See also  DataFrame.loc\n\nAccess a group of rows and columns by label(s) or a boolean array.    Notes The items, like, and regex parameters are enforced to be mutually exclusive. axis defaults to the info axis that is used when indexing with []. Examples \n>>> df = pd.DataFrame(np.array(([1, 2, 3], [4, 5, 6])),\n...                   index=['mouse', 'rabbit'],\n...                   columns=['one', 'two', 'three'])\n>>> df\n        one  two  three\nmouse     1    2      3\nrabbit    4    5      6\n  \n>>> # select columns by name\n>>> df.filter(items=['one', 'three'])\n         one  three\nmouse     1      3\nrabbit    4      6\n  \n>>> # select columns by regular expression\n>>> df.filter(regex='e$', axis=1)\n         one  three\nmouse     1      3\nrabbit    4      6\n  \n>>> # select rows containing 'bbi'\n>>> df.filter(like='bbi', axis=0)\n         one  two  three\nrabbit    4    5      6","title":"pandas.reference.api.pandas.series.filter"},{"text":"pandas.Series.str.extractall   Series.str.extractall(pat, flags=0)[source]\n \nExtract capture groups in the regex pat as columns in DataFrame. For each subject string in the Series, extract groups from all matches of regular expression pat. When each subject string in the Series has exactly one match, extractall(pat).xs(0, level=\u2019match\u2019) is the same as extract(pat).  Parameters \n \npat:str\n\n\nRegular expression pattern with capturing groups.  \nflags:int, default 0 (no flags)\n\n\nA re module flag, for example re.IGNORECASE. These allow to modify regular expression matching for things like case, spaces, etc. Multiple flags can be combined with the bitwise OR operator, for example re.IGNORECASE | re.MULTILINE.    Returns \n DataFrame\n\nA DataFrame with one row for each match, and one column for each group. Its rows have a MultiIndex with first levels that come from the subject Series. The last level is named \u2018match\u2019 and indexes the matches in each item of the Series. Any capture group names in regular expression pat will be used for column names; otherwise capture group numbers will be used.      See also  extract\n\nReturns first match only (not all matches).    Examples A pattern with one group will return a DataFrame with one column. Indices with no matches will not appear in the result. \n>>> s = pd.Series([\"a1a2\", \"b1\", \"c1\"], index=[\"A\", \"B\", \"C\"])\n>>> s.str.extractall(r\"[ab](\\d)\")\n        0\nmatch\nA 0      1\n  1      2\nB 0      1\n  Capture group names are used for column names of the result. \n>>> s.str.extractall(r\"[ab](?P<digit>\\d)\")\n        digit\nmatch\nA 0         1\n  1         2\nB 0         1\n  A pattern with two groups will return a DataFrame with two columns. \n>>> s.str.extractall(r\"(?P<letter>[ab])(?P<digit>\\d)\")\n        letter digit\nmatch\nA 0          a     1\n  1          a     2\nB 0          b     1\n  Optional groups that do not match are NaN in the result. \n>>> s.str.extractall(r\"(?P<letter>[ab])?(?P<digit>\\d)\")\n        letter digit\nmatch\nA 0          a     1\n  1          a     2\nB 0          b     1\nC 0        NaN     1","title":"pandas.reference.api.pandas.series.str.extractall"},{"text":"pandas.Series.str.extract   Series.str.extract(pat, flags=0, expand=True)[source]\n \nExtract capture groups in the regex pat as columns in a DataFrame. For each subject string in the Series, extract groups from the first match of regular expression pat.  Parameters \n \npat:str\n\n\nRegular expression pattern with capturing groups.  \nflags:int, default 0 (no flags)\n\n\nFlags from the re module, e.g. re.IGNORECASE, that modify regular expression matching for things like case, spaces, etc. For more details, see re.  \nexpand:bool, default True\n\n\nIf True, return DataFrame with one column per capture group. If False, return a Series\/Index if there is one capture group or DataFrame if there are multiple capture groups.    Returns \n DataFrame or Series or Index\n\nA DataFrame with one row for each subject string, and one column for each group. Any capture group names in regular expression pat will be used for column names; otherwise capture group numbers will be used. The dtype of each result column is always object, even when no match is found. If expand=False and pat has only one capture group, then return a Series (if subject is a Series) or Index (if subject is an Index).      See also  extractall\n\nReturns all matches (not just the first match).    Examples A pattern with two groups will return a DataFrame with two columns. Non-matches will be NaN. \n>>> s = pd.Series(['a1', 'b2', 'c3'])\n>>> s.str.extract(r'([ab])(\\d)')\n    0    1\n0    a    1\n1    b    2\n2  NaN  NaN\n  A pattern may contain optional groups. \n>>> s.str.extract(r'([ab])?(\\d)')\n    0  1\n0    a  1\n1    b  2\n2  NaN  3\n  Named groups will become column names in the result. \n>>> s.str.extract(r'(?P<letter>[ab])(?P<digit>\\d)')\nletter digit\n0      a     1\n1      b     2\n2    NaN   NaN\n  A pattern with one group will return a DataFrame with one column if expand=True. \n>>> s.str.extract(r'[ab](\\d)', expand=True)\n    0\n0    1\n1    2\n2  NaN\n  A pattern with one group will return a Series if expand=False. \n>>> s.str.extract(r'[ab](\\d)', expand=False)\n0      1\n1      2\n2    NaN\ndtype: object","title":"pandas.reference.api.pandas.series.str.extract"},{"text":"Cookbook This is a repository for short and sweet examples and links for useful pandas recipes. We encourage users to add to this documentation. Adding interesting links and\/or inline examples to this section is a great First Pull Request. Simplified, condensed, new-user friendly, in-line examples have been inserted where possible to augment the Stack-Overflow and GitHub links. Many of the links contain expanded information, above what the in-line examples offer. pandas (pd) and NumPy (np) are the only two abbreviated imported modules. The rest are kept explicitly imported for newer users.  Idioms These are some neat pandas idioms if-then\/if-then-else on one column, and assignment to another one or more columns: \nIn [1]: df = pd.DataFrame(\n   ...:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ...: )\n   ...: \n\nIn [2]: df\nOut[2]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n   if-then\u2026 An if-then on one column \nIn [3]: df.loc[df.AAA >= 5, \"BBB\"] = -1\n\nIn [4]: df\nOut[4]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   -1   50\n2    6   -1  -30\n3    7   -1  -50\n  An if-then with assignment to 2 columns: \nIn [5]: df.loc[df.AAA >= 5, [\"BBB\", \"CCC\"]] = 555\n\nIn [6]: df\nOut[6]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5  555  555\n2    6  555  555\n3    7  555  555\n  Add another line with different logic, to do the -else \nIn [7]: df.loc[df.AAA < 5, [\"BBB\", \"CCC\"]] = 2000\n\nIn [8]: df\nOut[8]: \n   AAA   BBB   CCC\n0    4  2000  2000\n1    5   555   555\n2    6   555   555\n3    7   555   555\n  Or use pandas where after you\u2019ve set up a mask \nIn [9]: df_mask = pd.DataFrame(\n   ...:     {\"AAA\": [True] * 4, \"BBB\": [False] * 4, \"CCC\": [True, False] * 2}\n   ...: )\n   ...: \n\nIn [10]: df.where(df_mask, -1000)\nOut[10]: \n   AAA   BBB   CCC\n0    4 -1000  2000\n1    5 -1000 -1000\n2    6 -1000   555\n3    7 -1000 -1000\n  if-then-else using NumPy\u2019s where() \nIn [11]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [12]: df\nOut[12]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [13]: df[\"logic\"] = np.where(df[\"AAA\"] > 5, \"high\", \"low\")\n\nIn [14]: df\nOut[14]: \n   AAA  BBB  CCC logic\n0    4   10  100   low\n1    5   20   50   low\n2    6   30  -30  high\n3    7   40  -50  high\n    Splitting Split a frame with a boolean criterion \nIn [15]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [16]: df\nOut[16]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [17]: df[df.AAA <= 5]\nOut[17]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n\nIn [18]: df[df.AAA > 5]\nOut[18]: \n   AAA  BBB  CCC\n2    6   30  -30\n3    7   40  -50\n    Building criteria Select with multi-column criteria \nIn [19]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [20]: df\nOut[20]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n  \u2026and (without assignment returns a Series) \nIn [21]: df.loc[(df[\"BBB\"] < 25) & (df[\"CCC\"] >= -40), \"AAA\"]\nOut[21]: \n0    4\n1    5\nName: AAA, dtype: int64\n  \u2026or (without assignment returns a Series) \nIn [22]: df.loc[(df[\"BBB\"] > 25) | (df[\"CCC\"] >= -40), \"AAA\"]\nOut[22]: \n0    4\n1    5\n2    6\n3    7\nName: AAA, dtype: int64\n  \u2026or (with assignment modifies the DataFrame.) \nIn [23]: df.loc[(df[\"BBB\"] > 25) | (df[\"CCC\"] >= 75), \"AAA\"] = 0.1\n\nIn [24]: df\nOut[24]: \n   AAA  BBB  CCC\n0  0.1   10  100\n1  5.0   20   50\n2  0.1   30  -30\n3  0.1   40  -50\n  Select rows with data closest to certain value using argsort \nIn [25]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [26]: df\nOut[26]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [27]: aValue = 43.0\n\nIn [28]: df.loc[(df.CCC - aValue).abs().argsort()]\nOut[28]: \n   AAA  BBB  CCC\n1    5   20   50\n0    4   10  100\n2    6   30  -30\n3    7   40  -50\n  Dynamically reduce a list of criteria using a binary operators \nIn [29]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [30]: df\nOut[30]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [31]: Crit1 = df.AAA <= 5.5\n\nIn [32]: Crit2 = df.BBB == 10.0\n\nIn [33]: Crit3 = df.CCC > -40.0\n  One could hard code: \nIn [34]: AllCrit = Crit1 & Crit2 & Crit3\n  \u2026Or it can be done with a list of dynamically built criteria \nIn [35]: import functools\n\nIn [36]: CritList = [Crit1, Crit2, Crit3]\n\nIn [37]: AllCrit = functools.reduce(lambda x, y: x & y, CritList)\n\nIn [38]: df[AllCrit]\nOut[38]: \n   AAA  BBB  CCC\n0    4   10  100\n     Selection  Dataframes The indexing docs. Using both row labels and value conditionals \nIn [39]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [40]: df\nOut[40]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [41]: df[(df.AAA <= 6) & (df.index.isin([0, 2, 4]))]\nOut[41]: \n   AAA  BBB  CCC\n0    4   10  100\n2    6   30  -30\n  Use loc for label-oriented slicing and iloc positional slicing GH2904 \nIn [42]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]},\n   ....:     index=[\"foo\", \"bar\", \"boo\", \"kar\"],\n   ....: )\n   ....: \n  There are 2 explicit slicing methods, with a third general case  Positional-oriented (Python slicing style : exclusive of end) Label-oriented (Non-Python slicing style : inclusive of end) General (Either slicing style : depends on if the slice contains labels or positions)  \nIn [43]: df.loc[\"bar\":\"kar\"]  # Label\nOut[43]: \n     AAA  BBB  CCC\nbar    5   20   50\nboo    6   30  -30\nkar    7   40  -50\n\n# Generic\nIn [44]: df[0:3]\nOut[44]: \n     AAA  BBB  CCC\nfoo    4   10  100\nbar    5   20   50\nboo    6   30  -30\n\nIn [45]: df[\"bar\":\"kar\"]\nOut[45]: \n     AAA  BBB  CCC\nbar    5   20   50\nboo    6   30  -30\nkar    7   40  -50\n  Ambiguity arises when an index consists of integers with a non-zero start or non-unit increment. \nIn [46]: data = {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n\nIn [47]: df2 = pd.DataFrame(data=data, index=[1, 2, 3, 4])  # Note index starts at 1.\n\nIn [48]: df2.iloc[1:3]  # Position-oriented\nOut[48]: \n   AAA  BBB  CCC\n2    5   20   50\n3    6   30  -30\n\nIn [49]: df2.loc[1:3]  # Label-oriented\nOut[49]: \n   AAA  BBB  CCC\n1    4   10  100\n2    5   20   50\n3    6   30  -30\n  Using inverse operator (~) to take the complement of a mask \nIn [50]: df = pd.DataFrame(\n   ....:     {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n   ....: )\n   ....: \n\nIn [51]: df\nOut[51]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\nIn [52]: df[~((df.AAA <= 6) & (df.index.isin([0, 2, 4])))]\nOut[52]: \n   AAA  BBB  CCC\n1    5   20   50\n3    7   40  -50\n    New columns Efficiently and dynamically creating new columns using applymap \nIn [53]: df = pd.DataFrame({\"AAA\": [1, 2, 1, 3], \"BBB\": [1, 1, 2, 2], \"CCC\": [2, 1, 3, 1]})\n\nIn [54]: df\nOut[54]: \n   AAA  BBB  CCC\n0    1    1    2\n1    2    1    1\n2    1    2    3\n3    3    2    1\n\nIn [55]: source_cols = df.columns  # Or some subset would work too\n\nIn [56]: new_cols = [str(x) + \"_cat\" for x in source_cols]\n\nIn [57]: categories = {1: \"Alpha\", 2: \"Beta\", 3: \"Charlie\"}\n\nIn [58]: df[new_cols] = df[source_cols].applymap(categories.get)\n\nIn [59]: df\nOut[59]: \n   AAA  BBB  CCC  AAA_cat BBB_cat  CCC_cat\n0    1    1    2    Alpha   Alpha     Beta\n1    2    1    1     Beta   Alpha    Alpha\n2    1    2    3    Alpha    Beta  Charlie\n3    3    2    1  Charlie    Beta    Alpha\n  Keep other columns when using min() with groupby \nIn [60]: df = pd.DataFrame(\n   ....:     {\"AAA\": [1, 1, 1, 2, 2, 2, 3, 3], \"BBB\": [2, 1, 3, 4, 5, 1, 2, 3]}\n   ....: )\n   ....: \n\nIn [61]: df\nOut[61]: \n   AAA  BBB\n0    1    2\n1    1    1\n2    1    3\n3    2    4\n4    2    5\n5    2    1\n6    3    2\n7    3    3\n  Method 1 : idxmin() to get the index of the minimums \nIn [62]: df.loc[df.groupby(\"AAA\")[\"BBB\"].idxmin()]\nOut[62]: \n   AAA  BBB\n1    1    1\n5    2    1\n6    3    2\n  Method 2 : sort then take first of each \nIn [63]: df.sort_values(by=\"BBB\").groupby(\"AAA\", as_index=False).first()\nOut[63]: \n   AAA  BBB\n0    1    1\n1    2    1\n2    3    2\n  Notice the same results, with the exception of the index.    Multiindexing The multindexing docs. Creating a MultiIndex from a labeled frame \nIn [64]: df = pd.DataFrame(\n   ....:     {\n   ....:         \"row\": [0, 1, 2],\n   ....:         \"One_X\": [1.1, 1.1, 1.1],\n   ....:         \"One_Y\": [1.2, 1.2, 1.2],\n   ....:         \"Two_X\": [1.11, 1.11, 1.11],\n   ....:         \"Two_Y\": [1.22, 1.22, 1.22],\n   ....:     }\n   ....: )\n   ....: \n\nIn [65]: df\nOut[65]: \n   row  One_X  One_Y  Two_X  Two_Y\n0    0    1.1    1.2   1.11   1.22\n1    1    1.1    1.2   1.11   1.22\n2    2    1.1    1.2   1.11   1.22\n\n# As Labelled Index\nIn [66]: df = df.set_index(\"row\")\n\nIn [67]: df\nOut[67]: \n     One_X  One_Y  Two_X  Two_Y\nrow                            \n0      1.1    1.2   1.11   1.22\n1      1.1    1.2   1.11   1.22\n2      1.1    1.2   1.11   1.22\n\n# With Hierarchical Columns\nIn [68]: df.columns = pd.MultiIndex.from_tuples([tuple(c.split(\"_\")) for c in df.columns])\n\nIn [69]: df\nOut[69]: \n     One        Two      \n       X    Y     X     Y\nrow                      \n0    1.1  1.2  1.11  1.22\n1    1.1  1.2  1.11  1.22\n2    1.1  1.2  1.11  1.22\n\n# Now stack & Reset\nIn [70]: df = df.stack(0).reset_index(1)\n\nIn [71]: df\nOut[71]: \n    level_1     X     Y\nrow                    \n0       One  1.10  1.20\n0       Two  1.11  1.22\n1       One  1.10  1.20\n1       Two  1.11  1.22\n2       One  1.10  1.20\n2       Two  1.11  1.22\n\n# And fix the labels (Notice the label 'level_1' got added automatically)\nIn [72]: df.columns = [\"Sample\", \"All_X\", \"All_Y\"]\n\nIn [73]: df\nOut[73]: \n    Sample  All_X  All_Y\nrow                     \n0      One   1.10   1.20\n0      Two   1.11   1.22\n1      One   1.10   1.20\n1      Two   1.11   1.22\n2      One   1.10   1.20\n2      Two   1.11   1.22\n   Arithmetic Performing arithmetic with a MultiIndex that needs broadcasting \nIn [74]: cols = pd.MultiIndex.from_tuples(\n   ....:     [(x, y) for x in [\"A\", \"B\", \"C\"] for y in [\"O\", \"I\"]]\n   ....: )\n   ....: \n\nIn [75]: df = pd.DataFrame(np.random.randn(2, 6), index=[\"n\", \"m\"], columns=cols)\n\nIn [76]: df\nOut[76]: \n          A                   B                   C          \n          O         I         O         I         O         I\nn  0.469112 -0.282863 -1.509059 -1.135632  1.212112 -0.173215\nm  0.119209 -1.044236 -0.861849 -2.104569 -0.494929  1.071804\n\nIn [77]: df = df.div(df[\"C\"], level=1)\n\nIn [78]: df\nOut[78]: \n          A                   B              C     \n          O         I         O         I    O    I\nn  0.387021  1.633022 -1.244983  6.556214  1.0  1.0\nm -0.240860 -0.974279  1.741358 -1.963577  1.0  1.0\n    Slicing Slicing a MultiIndex with xs \nIn [79]: coords = [(\"AA\", \"one\"), (\"AA\", \"six\"), (\"BB\", \"one\"), (\"BB\", \"two\"), (\"BB\", \"six\")]\n\nIn [80]: index = pd.MultiIndex.from_tuples(coords)\n\nIn [81]: df = pd.DataFrame([11, 22, 33, 44, 55], index, [\"MyData\"])\n\nIn [82]: df\nOut[82]: \n        MyData\nAA one      11\n   six      22\nBB one      33\n   two      44\n   six      55\n  To take the cross section of the 1st level and 1st axis the index: \n# Note : level and axis are optional, and default to zero\nIn [83]: df.xs(\"BB\", level=0, axis=0)\nOut[83]: \n     MyData\none      33\ntwo      44\nsix      55\n  \u2026and now the 2nd level of the 1st axis. \nIn [84]: df.xs(\"six\", level=1, axis=0)\nOut[84]: \n    MyData\nAA      22\nBB      55\n  Slicing a MultiIndex with xs, method #2 \nIn [85]: import itertools\n\nIn [86]: index = list(itertools.product([\"Ada\", \"Quinn\", \"Violet\"], [\"Comp\", \"Math\", \"Sci\"]))\n\nIn [87]: headr = list(itertools.product([\"Exams\", \"Labs\"], [\"I\", \"II\"]))\n\nIn [88]: indx = pd.MultiIndex.from_tuples(index, names=[\"Student\", \"Course\"])\n\nIn [89]: cols = pd.MultiIndex.from_tuples(headr)  # Notice these are un-named\n\nIn [90]: data = [[70 + x + y + (x * y) % 3 for x in range(4)] for y in range(9)]\n\nIn [91]: df = pd.DataFrame(data, indx, cols)\n\nIn [92]: df\nOut[92]: \n               Exams     Labs    \n                   I  II    I  II\nStudent Course                   \nAda     Comp      70  71   72  73\n        Math      71  73   75  74\n        Sci       72  75   75  75\nQuinn   Comp      73  74   75  76\n        Math      74  76   78  77\n        Sci       75  78   78  78\nViolet  Comp      76  77   78  79\n        Math      77  79   81  80\n        Sci       78  81   81  81\n\nIn [93]: All = slice(None)\n\nIn [94]: df.loc[\"Violet\"]\nOut[94]: \n       Exams     Labs    \n           I  II    I  II\nCourse                   \nComp      76  77   78  79\nMath      77  79   81  80\nSci       78  81   81  81\n\nIn [95]: df.loc[(All, \"Math\"), All]\nOut[95]: \n               Exams     Labs    \n                   I  II    I  II\nStudent Course                   \nAda     Math      71  73   75  74\nQuinn   Math      74  76   78  77\nViolet  Math      77  79   81  80\n\nIn [96]: df.loc[(slice(\"Ada\", \"Quinn\"), \"Math\"), All]\nOut[96]: \n               Exams     Labs    \n                   I  II    I  II\nStudent Course                   \nAda     Math      71  73   75  74\nQuinn   Math      74  76   78  77\n\nIn [97]: df.loc[(All, \"Math\"), (\"Exams\")]\nOut[97]: \n                 I  II\nStudent Course        \nAda     Math    71  73\nQuinn   Math    74  76\nViolet  Math    77  79\n\nIn [98]: df.loc[(All, \"Math\"), (All, \"II\")]\nOut[98]: \n               Exams Labs\n                  II   II\nStudent Course           \nAda     Math      73   74\nQuinn   Math      76   77\nViolet  Math      79   80\n  Setting portions of a MultiIndex with xs   Sorting Sort by specific column or an ordered list of columns, with a MultiIndex \nIn [99]: df.sort_values(by=(\"Labs\", \"II\"), ascending=False)\nOut[99]: \n               Exams     Labs    \n                   I  II    I  II\nStudent Course                   \nViolet  Sci       78  81   81  81\n        Math      77  79   81  80\n        Comp      76  77   78  79\nQuinn   Sci       75  78   78  78\n        Math      74  76   78  77\n        Comp      73  74   75  76\nAda     Sci       72  75   75  75\n        Math      71  73   75  74\n        Comp      70  71   72  73\n  Partial selection, the need for sortedness GH2995   Levels Prepending a level to a multiindex Flatten Hierarchical columns    Missing data The missing data docs. Fill forward a reversed timeseries \nIn [100]: df = pd.DataFrame(\n   .....:     np.random.randn(6, 1),\n   .....:     index=pd.date_range(\"2013-08-01\", periods=6, freq=\"B\"),\n   .....:     columns=list(\"A\"),\n   .....: )\n   .....: \n\nIn [101]: df.loc[df.index[3], \"A\"] = np.nan\n\nIn [102]: df\nOut[102]: \n                   A\n2013-08-01  0.721555\n2013-08-02 -0.706771\n2013-08-05 -1.039575\n2013-08-06       NaN\n2013-08-07 -0.424972\n2013-08-08  0.567020\n\nIn [103]: df.reindex(df.index[::-1]).ffill()\nOut[103]: \n                   A\n2013-08-08  0.567020\n2013-08-07 -0.424972\n2013-08-06 -0.424972\n2013-08-05 -1.039575\n2013-08-02 -0.706771\n2013-08-01  0.721555\n  cumsum reset at NaN values  Replace Using replace with backrefs    Grouping The grouping docs. Basic grouping with apply Unlike agg, apply\u2019s callable is passed a sub-DataFrame which gives you access to all the columns \nIn [104]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"animal\": \"cat dog cat fish dog cat cat\".split(),\n   .....:         \"size\": list(\"SSMMMLL\"),\n   .....:         \"weight\": [8, 10, 11, 1, 20, 12, 12],\n   .....:         \"adult\": [False] * 5 + [True] * 2,\n   .....:     }\n   .....: )\n   .....: \n\nIn [105]: df\nOut[105]: \n  animal size  weight  adult\n0    cat    S       8  False\n1    dog    S      10  False\n2    cat    M      11  False\n3   fish    M       1  False\n4    dog    M      20  False\n5    cat    L      12   True\n6    cat    L      12   True\n\n# List the size of the animals with the highest weight.\nIn [106]: df.groupby(\"animal\").apply(lambda subf: subf[\"size\"][subf[\"weight\"].idxmax()])\nOut[106]: \nanimal\ncat     L\ndog     M\nfish    M\ndtype: object\n  Using get_group \nIn [107]: gb = df.groupby([\"animal\"])\n\nIn [108]: gb.get_group(\"cat\")\nOut[108]: \n  animal size  weight  adult\n0    cat    S       8  False\n2    cat    M      11  False\n5    cat    L      12   True\n6    cat    L      12   True\n  Apply to different items in a group \nIn [109]: def GrowUp(x):\n   .....:     avg_weight = sum(x[x[\"size\"] == \"S\"].weight * 1.5)\n   .....:     avg_weight += sum(x[x[\"size\"] == \"M\"].weight * 1.25)\n   .....:     avg_weight += sum(x[x[\"size\"] == \"L\"].weight)\n   .....:     avg_weight \/= len(x)\n   .....:     return pd.Series([\"L\", avg_weight, True], index=[\"size\", \"weight\", \"adult\"])\n   .....: \n\nIn [110]: expected_df = gb.apply(GrowUp)\n\nIn [111]: expected_df\nOut[111]: \n       size   weight  adult\nanimal                     \ncat       L  12.4375   True\ndog       L  20.0000   True\nfish      L   1.2500   True\n  Expanding apply \nIn [112]: S = pd.Series([i \/ 100.0 for i in range(1, 11)])\n\nIn [113]: def cum_ret(x, y):\n   .....:     return x * (1 + y)\n   .....: \n\nIn [114]: def red(x):\n   .....:     return functools.reduce(cum_ret, x, 1.0)\n   .....: \n\nIn [115]: S.expanding().apply(red, raw=True)\nOut[115]: \n0    1.010000\n1    1.030200\n2    1.061106\n3    1.103550\n4    1.158728\n5    1.228251\n6    1.314229\n7    1.419367\n8    1.547110\n9    1.701821\ndtype: float64\n  Replacing some values with mean of the rest of a group \nIn [116]: df = pd.DataFrame({\"A\": [1, 1, 2, 2], \"B\": [1, -1, 1, 2]})\n\nIn [117]: gb = df.groupby(\"A\")\n\nIn [118]: def replace(g):\n   .....:     mask = g < 0\n   .....:     return g.where(mask, g[~mask].mean())\n   .....: \n\nIn [119]: gb.transform(replace)\nOut[119]: \n     B\n0  1.0\n1 -1.0\n2  1.5\n3  1.5\n  Sort groups by aggregated data \nIn [120]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"code\": [\"foo\", \"bar\", \"baz\"] * 2,\n   .....:         \"data\": [0.16, -0.21, 0.33, 0.45, -0.59, 0.62],\n   .....:         \"flag\": [False, True] * 3,\n   .....:     }\n   .....: )\n   .....: \n\nIn [121]: code_groups = df.groupby(\"code\")\n\nIn [122]: agg_n_sort_order = code_groups[[\"data\"]].transform(sum).sort_values(by=\"data\")\n\nIn [123]: sorted_df = df.loc[agg_n_sort_order.index]\n\nIn [124]: sorted_df\nOut[124]: \n  code  data   flag\n1  bar -0.21   True\n4  bar -0.59  False\n0  foo  0.16  False\n3  foo  0.45   True\n2  baz  0.33  False\n5  baz  0.62   True\n  Create multiple aggregated columns \nIn [125]: rng = pd.date_range(start=\"2014-10-07\", periods=10, freq=\"2min\")\n\nIn [126]: ts = pd.Series(data=list(range(10)), index=rng)\n\nIn [127]: def MyCust(x):\n   .....:     if len(x) > 2:\n   .....:         return x[1] * 1.234\n   .....:     return pd.NaT\n   .....: \n\nIn [128]: mhc = {\"Mean\": np.mean, \"Max\": np.max, \"Custom\": MyCust}\n\nIn [129]: ts.resample(\"5min\").apply(mhc)\nOut[129]: \n                     Mean  Max Custom\n2014-10-07 00:00:00   1.0    2  1.234\n2014-10-07 00:05:00   3.5    4    NaT\n2014-10-07 00:10:00   6.0    7  7.404\n2014-10-07 00:15:00   8.5    9    NaT\n\nIn [130]: ts\nOut[130]: \n2014-10-07 00:00:00    0\n2014-10-07 00:02:00    1\n2014-10-07 00:04:00    2\n2014-10-07 00:06:00    3\n2014-10-07 00:08:00    4\n2014-10-07 00:10:00    5\n2014-10-07 00:12:00    6\n2014-10-07 00:14:00    7\n2014-10-07 00:16:00    8\n2014-10-07 00:18:00    9\nFreq: 2T, dtype: int64\n  Create a value counts column and reassign back to the DataFrame \nIn [131]: df = pd.DataFrame(\n   .....:     {\"Color\": \"Red Red Red Blue\".split(), \"Value\": [100, 150, 50, 50]}\n   .....: )\n   .....: \n\nIn [132]: df\nOut[132]: \n  Color  Value\n0   Red    100\n1   Red    150\n2   Red     50\n3  Blue     50\n\nIn [133]: df[\"Counts\"] = df.groupby([\"Color\"]).transform(len)\n\nIn [134]: df\nOut[134]: \n  Color  Value  Counts\n0   Red    100       3\n1   Red    150       3\n2   Red     50       3\n3  Blue     50       1\n  Shift groups of the values in a column based on the index \nIn [135]: df = pd.DataFrame(\n   .....:     {\"line_race\": [10, 10, 8, 10, 10, 8], \"beyer\": [99, 102, 103, 103, 88, 100]},\n   .....:     index=[\n   .....:         \"Last Gunfighter\",\n   .....:         \"Last Gunfighter\",\n   .....:         \"Last Gunfighter\",\n   .....:         \"Paynter\",\n   .....:         \"Paynter\",\n   .....:         \"Paynter\",\n   .....:     ],\n   .....: )\n   .....: \n\nIn [136]: df\nOut[136]: \n                 line_race  beyer\nLast Gunfighter         10     99\nLast Gunfighter         10    102\nLast Gunfighter          8    103\nPaynter                 10    103\nPaynter                 10     88\nPaynter                  8    100\n\nIn [137]: df[\"beyer_shifted\"] = df.groupby(level=0)[\"beyer\"].shift(1)\n\nIn [138]: df\nOut[138]: \n                 line_race  beyer  beyer_shifted\nLast Gunfighter         10     99            NaN\nLast Gunfighter         10    102           99.0\nLast Gunfighter          8    103          102.0\nPaynter                 10    103            NaN\nPaynter                 10     88          103.0\nPaynter                  8    100           88.0\n  Select row with maximum value from each group \nIn [139]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"host\": [\"other\", \"other\", \"that\", \"this\", \"this\"],\n   .....:         \"service\": [\"mail\", \"web\", \"mail\", \"mail\", \"web\"],\n   .....:         \"no\": [1, 2, 1, 2, 1],\n   .....:     }\n   .....: ).set_index([\"host\", \"service\"])\n   .....: \n\nIn [140]: mask = df.groupby(level=0).agg(\"idxmax\")\n\nIn [141]: df_count = df.loc[mask[\"no\"]].reset_index()\n\nIn [142]: df_count\nOut[142]: \n    host service  no\n0  other     web   2\n1   that    mail   1\n2   this    mail   2\n  Grouping like Python\u2019s itertools.groupby \nIn [143]: df = pd.DataFrame([0, 1, 0, 1, 1, 1, 0, 1, 1], columns=[\"A\"])\n\nIn [144]: df[\"A\"].groupby((df[\"A\"] != df[\"A\"].shift()).cumsum()).groups\nOut[144]: {1: [0], 2: [1], 3: [2], 4: [3, 4, 5], 5: [6], 6: [7, 8]}\n\nIn [145]: df[\"A\"].groupby((df[\"A\"] != df[\"A\"].shift()).cumsum()).cumsum()\nOut[145]: \n0    0\n1    1\n2    0\n3    1\n4    2\n5    3\n6    0\n7    1\n8    2\nName: A, dtype: int64\n   Expanding data Alignment and to-date Rolling Computation window based on values instead of counts Rolling Mean by Time Interval   Splitting Splitting a frame Create a list of dataframes, split using a delineation based on logic included in rows. \nIn [146]: df = pd.DataFrame(\n   .....:     data={\n   .....:         \"Case\": [\"A\", \"A\", \"A\", \"B\", \"A\", \"A\", \"B\", \"A\", \"A\"],\n   .....:         \"Data\": np.random.randn(9),\n   .....:     }\n   .....: )\n   .....: \n\nIn [147]: dfs = list(\n   .....:     zip(\n   .....:         *df.groupby(\n   .....:             (1 * (df[\"Case\"] == \"B\"))\n   .....:             .cumsum()\n   .....:             .rolling(window=3, min_periods=1)\n   .....:             .median()\n   .....:         )\n   .....:     )\n   .....: )[-1]\n   .....: \n\nIn [148]: dfs[0]\nOut[148]: \n  Case      Data\n0    A  0.276232\n1    A -1.087401\n2    A -0.673690\n3    B  0.113648\n\nIn [149]: dfs[1]\nOut[149]: \n  Case      Data\n4    A -1.478427\n5    A  0.524988\n6    B  0.404705\n\nIn [150]: dfs[2]\nOut[150]: \n  Case      Data\n7    A  0.577046\n8    A -1.715002\n    Pivot The Pivot docs. Partial sums and subtotals \nIn [151]: df = pd.DataFrame(\n   .....:     data={\n   .....:         \"Province\": [\"ON\", \"QC\", \"BC\", \"AL\", \"AL\", \"MN\", \"ON\"],\n   .....:         \"City\": [\n   .....:             \"Toronto\",\n   .....:             \"Montreal\",\n   .....:             \"Vancouver\",\n   .....:             \"Calgary\",\n   .....:             \"Edmonton\",\n   .....:             \"Winnipeg\",\n   .....:             \"Windsor\",\n   .....:         ],\n   .....:         \"Sales\": [13, 6, 16, 8, 4, 3, 1],\n   .....:     }\n   .....: )\n   .....: \n\nIn [152]: table = pd.pivot_table(\n   .....:     df,\n   .....:     values=[\"Sales\"],\n   .....:     index=[\"Province\"],\n   .....:     columns=[\"City\"],\n   .....:     aggfunc=np.sum,\n   .....:     margins=True,\n   .....: )\n   .....: \n\nIn [153]: table.stack(\"City\")\nOut[153]: \n                    Sales\nProvince City            \nAL       All         12.0\n         Calgary      8.0\n         Edmonton     4.0\nBC       All         16.0\n         Vancouver   16.0\n...                   ...\nAll      Montreal     6.0\n         Toronto     13.0\n         Vancouver   16.0\n         Windsor      1.0\n         Winnipeg     3.0\n\n[20 rows x 1 columns]\n  Frequency table like plyr in R \nIn [154]: grades = [48, 99, 75, 80, 42, 80, 72, 68, 36, 78]\n\nIn [155]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"ID\": [\"x%d\" % r for r in range(10)],\n   .....:         \"Gender\": [\"F\", \"M\", \"F\", \"M\", \"F\", \"M\", \"F\", \"M\", \"M\", \"M\"],\n   .....:         \"ExamYear\": [\n   .....:             \"2007\",\n   .....:             \"2007\",\n   .....:             \"2007\",\n   .....:             \"2008\",\n   .....:             \"2008\",\n   .....:             \"2008\",\n   .....:             \"2008\",\n   .....:             \"2009\",\n   .....:             \"2009\",\n   .....:             \"2009\",\n   .....:         ],\n   .....:         \"Class\": [\n   .....:             \"algebra\",\n   .....:             \"stats\",\n   .....:             \"bio\",\n   .....:             \"algebra\",\n   .....:             \"algebra\",\n   .....:             \"stats\",\n   .....:             \"stats\",\n   .....:             \"algebra\",\n   .....:             \"bio\",\n   .....:             \"bio\",\n   .....:         ],\n   .....:         \"Participated\": [\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"no\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:             \"yes\",\n   .....:         ],\n   .....:         \"Passed\": [\"yes\" if x > 50 else \"no\" for x in grades],\n   .....:         \"Employed\": [\n   .....:             True,\n   .....:             True,\n   .....:             True,\n   .....:             False,\n   .....:             False,\n   .....:             False,\n   .....:             False,\n   .....:             True,\n   .....:             True,\n   .....:             False,\n   .....:         ],\n   .....:         \"Grade\": grades,\n   .....:     }\n   .....: )\n   .....: \n\nIn [156]: df.groupby(\"ExamYear\").agg(\n   .....:     {\n   .....:         \"Participated\": lambda x: x.value_counts()[\"yes\"],\n   .....:         \"Passed\": lambda x: sum(x == \"yes\"),\n   .....:         \"Employed\": lambda x: sum(x),\n   .....:         \"Grade\": lambda x: sum(x) \/ len(x),\n   .....:     }\n   .....: )\n   .....: \nOut[156]: \n          Participated  Passed  Employed      Grade\nExamYear                                           \n2007                 3       2         3  74.000000\n2008                 3       3         0  68.500000\n2009                 3       2         2  60.666667\n  Plot pandas DataFrame with year over year data To create year and month cross tabulation: \nIn [157]: df = pd.DataFrame(\n   .....:     {\"value\": np.random.randn(36)},\n   .....:     index=pd.date_range(\"2011-01-01\", freq=\"M\", periods=36),\n   .....: )\n   .....: \n\nIn [158]: pd.pivot_table(\n   .....:     df, index=df.index.month, columns=df.index.year, values=\"value\", aggfunc=\"sum\"\n   .....: )\n   .....: \nOut[158]: \n        2011      2012      2013\n1  -1.039268 -0.968914  2.565646\n2  -0.370647 -1.294524  1.431256\n3  -1.157892  0.413738  1.340309\n4  -1.344312  0.276662 -1.170299\n5   0.844885 -0.472035 -0.226169\n6   1.075770 -0.013960  0.410835\n7  -0.109050 -0.362543  0.813850\n8   1.643563 -0.006154  0.132003\n9  -1.469388 -0.923061 -0.827317\n10  0.357021  0.895717 -0.076467\n11 -0.674600  0.805244 -1.187678\n12 -1.776904 -1.206412  1.130127\n    Apply Rolling apply to organize - Turning embedded lists into a MultiIndex frame \nIn [159]: df = pd.DataFrame(\n   .....:     data={\n   .....:         \"A\": [[2, 4, 8, 16], [100, 200], [10, 20, 30]],\n   .....:         \"B\": [[\"a\", \"b\", \"c\"], [\"jj\", \"kk\"], [\"ccc\"]],\n   .....:     },\n   .....:     index=[\"I\", \"II\", \"III\"],\n   .....: )\n   .....: \n\nIn [160]: def SeriesFromSubList(aList):\n   .....:     return pd.Series(aList)\n   .....: \n\nIn [161]: df_orgz = pd.concat(\n   .....:     {ind: row.apply(SeriesFromSubList) for ind, row in df.iterrows()}\n   .....: )\n   .....: \n\nIn [162]: df_orgz\nOut[162]: \n         0     1     2     3\nI   A    2     4     8  16.0\n    B    a     b     c   NaN\nII  A  100   200   NaN   NaN\n    B   jj    kk   NaN   NaN\nIII A   10  20.0  30.0   NaN\n    B  ccc   NaN   NaN   NaN\n  Rolling apply with a DataFrame returning a Series Rolling Apply to multiple columns where function calculates a Series before a Scalar from the Series is returned \nIn [163]: df = pd.DataFrame(\n   .....:     data=np.random.randn(2000, 2) \/ 10000,\n   .....:     index=pd.date_range(\"2001-01-01\", periods=2000),\n   .....:     columns=[\"A\", \"B\"],\n   .....: )\n   .....: \n\nIn [164]: df\nOut[164]: \n                   A         B\n2001-01-01 -0.000144 -0.000141\n2001-01-02  0.000161  0.000102\n2001-01-03  0.000057  0.000088\n2001-01-04 -0.000221  0.000097\n2001-01-05 -0.000201 -0.000041\n...              ...       ...\n2006-06-19  0.000040 -0.000235\n2006-06-20 -0.000123 -0.000021\n2006-06-21 -0.000113  0.000114\n2006-06-22  0.000136  0.000109\n2006-06-23  0.000027  0.000030\n\n[2000 rows x 2 columns]\n\nIn [165]: def gm(df, const):\n   .....:     v = ((((df[\"A\"] + df[\"B\"]) + 1).cumprod()) - 1) * const\n   .....:     return v.iloc[-1]\n   .....: \n\nIn [166]: s = pd.Series(\n   .....:     {\n   .....:         df.index[i]: gm(df.iloc[i: min(i + 51, len(df) - 1)], 5)\n   .....:         for i in range(len(df) - 50)\n   .....:     }\n   .....: )\n   .....: \n\nIn [167]: s\nOut[167]: \n2001-01-01    0.000930\n2001-01-02    0.002615\n2001-01-03    0.001281\n2001-01-04    0.001117\n2001-01-05    0.002772\n                ...   \n2006-04-30    0.003296\n2006-05-01    0.002629\n2006-05-02    0.002081\n2006-05-03    0.004247\n2006-05-04    0.003928\nLength: 1950, dtype: float64\n  Rolling apply with a DataFrame returning a Scalar Rolling Apply to multiple columns where function returns a Scalar (Volume Weighted Average Price) \nIn [168]: rng = pd.date_range(start=\"2014-01-01\", periods=100)\n\nIn [169]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"Open\": np.random.randn(len(rng)),\n   .....:         \"Close\": np.random.randn(len(rng)),\n   .....:         \"Volume\": np.random.randint(100, 2000, len(rng)),\n   .....:     },\n   .....:     index=rng,\n   .....: )\n   .....: \n\nIn [170]: df\nOut[170]: \n                Open     Close  Volume\n2014-01-01 -1.611353 -0.492885    1219\n2014-01-02 -3.000951  0.445794    1054\n2014-01-03 -0.138359 -0.076081    1381\n2014-01-04  0.301568  1.198259    1253\n2014-01-05  0.276381 -0.669831    1728\n...              ...       ...     ...\n2014-04-06 -0.040338  0.937843    1188\n2014-04-07  0.359661 -0.285908    1864\n2014-04-08  0.060978  1.714814     941\n2014-04-09  1.759055 -0.455942    1065\n2014-04-10  0.138185 -1.147008    1453\n\n[100 rows x 3 columns]\n\nIn [171]: def vwap(bars):\n   .....:     return (bars.Close * bars.Volume).sum() \/ bars.Volume.sum()\n   .....: \n\nIn [172]: window = 5\n\nIn [173]: s = pd.concat(\n   .....:     [\n   .....:         (pd.Series(vwap(df.iloc[i: i + window]), index=[df.index[i + window]]))\n   .....:         for i in range(len(df) - window)\n   .....:     ]\n   .....: )\n   .....: \n\nIn [174]: s.round(2)\nOut[174]: \n2014-01-06    0.02\n2014-01-07    0.11\n2014-01-08    0.10\n2014-01-09    0.07\n2014-01-10   -0.29\n              ... \n2014-04-06   -0.63\n2014-04-07   -0.02\n2014-04-08   -0.03\n2014-04-09    0.34\n2014-04-10    0.29\nLength: 95, dtype: float64\n     Timeseries Between times Using indexer between time Constructing a datetime range that excludes weekends and includes only certain times Vectorized Lookup Aggregation and plotting time series Turn a matrix with hours in columns and days in rows into a continuous row sequence in the form of a time series. How to rearrange a Python pandas DataFrame? Dealing with duplicates when reindexing a timeseries to a specified frequency Calculate the first day of the month for each entry in a DatetimeIndex \nIn [175]: dates = pd.date_range(\"2000-01-01\", periods=5)\n\nIn [176]: dates.to_period(freq=\"M\").to_timestamp()\nOut[176]: \nDatetimeIndex(['2000-01-01', '2000-01-01', '2000-01-01', '2000-01-01',\n               '2000-01-01'],\n              dtype='datetime64[ns]', freq=None)\n   Resampling The Resample docs. Using Grouper instead of TimeGrouper for time grouping of values Time grouping with some missing values Valid frequency arguments to Grouper Timeseries Grouping using a MultiIndex Using TimeGrouper and another grouping to create subgroups, then apply a custom function GH3791 Resampling with custom periods Resample intraday frame without adding new days Resample minute data Resample with groupby    Merge The Join docs. Concatenate two dataframes with overlapping index (emulate R rbind) \nIn [177]: rng = pd.date_range(\"2000-01-01\", periods=6)\n\nIn [178]: df1 = pd.DataFrame(np.random.randn(6, 3), index=rng, columns=[\"A\", \"B\", \"C\"])\n\nIn [179]: df2 = df1.copy()\n  Depending on df construction, ignore_index may be needed \nIn [180]: df = pd.concat([df1, df2], ignore_index=True)\n\nIn [181]: df\nOut[181]: \n           A         B         C\n0  -0.870117 -0.479265 -0.790855\n1   0.144817  1.726395 -0.464535\n2  -0.821906  1.597605  0.187307\n3  -0.128342 -1.511638 -0.289858\n4   0.399194 -1.430030 -0.639760\n5   1.115116 -2.012600  1.810662\n6  -0.870117 -0.479265 -0.790855\n7   0.144817  1.726395 -0.464535\n8  -0.821906  1.597605  0.187307\n9  -0.128342 -1.511638 -0.289858\n10  0.399194 -1.430030 -0.639760\n11  1.115116 -2.012600  1.810662\n  Self Join of a DataFrame GH2996 \nIn [182]: df = pd.DataFrame(\n   .....:     data={\n   .....:         \"Area\": [\"A\"] * 5 + [\"C\"] * 2,\n   .....:         \"Bins\": [110] * 2 + [160] * 3 + [40] * 2,\n   .....:         \"Test_0\": [0, 1, 0, 1, 2, 0, 1],\n   .....:         \"Data\": np.random.randn(7),\n   .....:     }\n   .....: )\n   .....: \n\nIn [183]: df\nOut[183]: \n  Area  Bins  Test_0      Data\n0    A   110       0 -0.433937\n1    A   110       1 -0.160552\n2    A   160       0  0.744434\n3    A   160       1  1.754213\n4    A   160       2  0.000850\n5    C    40       0  0.342243\n6    C    40       1  1.070599\n\nIn [184]: df[\"Test_1\"] = df[\"Test_0\"] - 1\n\nIn [185]: pd.merge(\n   .....:     df,\n   .....:     df,\n   .....:     left_on=[\"Bins\", \"Area\", \"Test_0\"],\n   .....:     right_on=[\"Bins\", \"Area\", \"Test_1\"],\n   .....:     suffixes=(\"_L\", \"_R\"),\n   .....: )\n   .....: \nOut[185]: \n  Area  Bins  Test_0_L    Data_L  Test_1_L  Test_0_R    Data_R  Test_1_R\n0    A   110         0 -0.433937        -1         1 -0.160552         0\n1    A   160         0  0.744434        -1         1  1.754213         0\n2    A   160         1  1.754213         0         2  0.000850         1\n3    C    40         0  0.342243        -1         1  1.070599         0\n  How to set the index and join KDB like asof join Join with a criteria based on the values Using searchsorted to merge based on values inside a range   Plotting The Plotting docs. Make Matplotlib look like R Setting x-axis major and minor labels Plotting multiple charts in an IPython Jupyter notebook Creating a multi-line plot Plotting a heatmap Annotate a time-series plot Annotate a time-series plot #2 Generate Embedded plots in excel files using Pandas, Vincent and xlsxwriter Boxplot for each quartile of a stratifying variable \nIn [186]: df = pd.DataFrame(\n   .....:     {\n   .....:         \"stratifying_var\": np.random.uniform(0, 100, 20),\n   .....:         \"price\": np.random.normal(100, 5, 20),\n   .....:     }\n   .....: )\n   .....: \n\nIn [187]: df[\"quartiles\"] = pd.qcut(\n   .....:     df[\"stratifying_var\"], 4, labels=[\"0-25%\", \"25-50%\", \"50-75%\", \"75-100%\"]\n   .....: )\n   .....: \n\nIn [188]: df.boxplot(column=\"price\", by=\"quartiles\")\nOut[188]: <AxesSubplot:title={'center':'price'}, xlabel='quartiles'>\n     Data in\/out Performance comparison of SQL vs HDF5  CSV The CSV docs read_csv in action appending to a csv Reading a csv chunk-by-chunk Reading only certain rows of a csv chunk-by-chunk Reading the first few lines of a frame Reading a file that is compressed but not by gzip\/bz2 (the native compressed formats which read_csv understands). This example shows a WinZipped file, but is a general application of opening the file within a context manager and using that handle to read. See here Inferring dtypes from a file Dealing with bad lines GH2886 Write a multi-row index CSV without writing duplicates  Reading multiple files to create a single DataFrame The best way to combine multiple files into a single DataFrame is to read the individual frames one by one, put all of the individual frames into a list, and then combine the frames in the list using pd.concat(): \nIn [189]: for i in range(3):\n   .....:     data = pd.DataFrame(np.random.randn(10, 4))\n   .....:     data.to_csv(\"file_{}.csv\".format(i))\n   .....: \n\nIn [190]: files = [\"file_0.csv\", \"file_1.csv\", \"file_2.csv\"]\n\nIn [191]: result = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n  You can use the same approach to read all files matching a pattern. Here is an example using glob: \nIn [192]: import glob\n\nIn [193]: import os\n\nIn [194]: files = glob.glob(\"file_*.csv\")\n\nIn [195]: result = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n  Finally, this strategy will work with the other pd.read_*(...) functions described in the io docs.   Parsing date components in multi-columns Parsing date components in multi-columns is faster with a format \nIn [196]: i = pd.date_range(\"20000101\", periods=10000)\n\nIn [197]: df = pd.DataFrame({\"year\": i.year, \"month\": i.month, \"day\": i.day})\n\nIn [198]: df.head()\nOut[198]: \n   year  month  day\n0  2000      1    1\n1  2000      1    2\n2  2000      1    3\n3  2000      1    4\n4  2000      1    5\n\nIn [199]: %timeit pd.to_datetime(df.year * 10000 + df.month * 100 + df.day, format='%Y%m%d')\n   .....: ds = df.apply(lambda x: \"%04d%02d%02d\" % (x[\"year\"], x[\"month\"], x[\"day\"]), axis=1)\n   .....: ds.head()\n   .....: %timeit pd.to_datetime(ds)\n   .....: \n8.7 ms +- 765 us per loop (mean +- std. dev. of 7 runs, 100 loops each)\n2.1 ms +- 419 us per loop (mean +- std. dev. of 7 runs, 100 loops each)\n    Skip row between header and data \nIn [200]: data = \"\"\";;;;\n   .....:  ;;;;\n   .....:  ;;;;\n   .....:  ;;;;\n   .....:  ;;;;\n   .....:  ;;;;\n   .....: ;;;;\n   .....:  ;;;;\n   .....:  ;;;;\n   .....: ;;;;\n   .....: date;Param1;Param2;Param4;Param5\n   .....:     ;m\u00b2;\u00b0C;m\u00b2;m\n   .....: ;;;;\n   .....: 01.01.1990 00:00;1;1;2;3\n   .....: 01.01.1990 01:00;5;3;4;5\n   .....: 01.01.1990 02:00;9;5;6;7\n   .....: 01.01.1990 03:00;13;7;8;9\n   .....: 01.01.1990 04:00;17;9;10;11\n   .....: 01.01.1990 05:00;21;11;12;13\n   .....: \"\"\"\n   .....: \n   Option 1: pass rows explicitly to skip rows \nIn [201]: from io import StringIO\n\nIn [202]: pd.read_csv(\n   .....:     StringIO(data),\n   .....:     sep=\";\",\n   .....:     skiprows=[11, 12],\n   .....:     index_col=0,\n   .....:     parse_dates=True,\n   .....:     header=10,\n   .....: )\n   .....: \nOut[202]: \n                     Param1  Param2  Param4  Param5\ndate                                               \n1990-01-01 00:00:00       1       1       2       3\n1990-01-01 01:00:00       5       3       4       5\n1990-01-01 02:00:00       9       5       6       7\n1990-01-01 03:00:00      13       7       8       9\n1990-01-01 04:00:00      17       9      10      11\n1990-01-01 05:00:00      21      11      12      13\n    Option 2: read column names and then data \nIn [203]: pd.read_csv(StringIO(data), sep=\";\", header=10, nrows=10).columns\nOut[203]: Index(['date', 'Param1', 'Param2', 'Param4', 'Param5'], dtype='object')\n\nIn [204]: columns = pd.read_csv(StringIO(data), sep=\";\", header=10, nrows=10).columns\n\nIn [205]: pd.read_csv(\n   .....:     StringIO(data), sep=\";\", index_col=0, header=12, parse_dates=True, names=columns\n   .....: )\n   .....: \nOut[205]: \n                     Param1  Param2  Param4  Param5\ndate                                               \n1990-01-01 00:00:00       1       1       2       3\n1990-01-01 01:00:00       5       3       4       5\n1990-01-01 02:00:00       9       5       6       7\n1990-01-01 03:00:00      13       7       8       9\n1990-01-01 04:00:00      17       9      10      11\n1990-01-01 05:00:00      21      11      12      13\n      SQL The SQL docs Reading from databases with SQL   Excel The Excel docs Reading from a filelike handle Modifying formatting in XlsxWriter output Loading only visible sheets GH19842#issuecomment-892150745   HTML Reading HTML tables from a server that cannot handle the default request header   HDFStore The HDFStores docs Simple queries with a Timestamp Index Managing heterogeneous data using a linked multiple table hierarchy GH3032 Merging on-disk tables with millions of rows Avoiding inconsistencies when writing to a store from multiple processes\/threads De-duplicating a large store by chunks, essentially a recursive reduction operation. Shows a function for taking in data from csv file and creating a store by chunks, with date parsing as well. See here Creating a store chunk-by-chunk from a csv file Appending to a store, while creating a unique index Large Data work flows Reading in a sequence of files, then providing a global unique index to a store while appending Groupby on a HDFStore with low group density Groupby on a HDFStore with high group density Hierarchical queries on a HDFStore Counting with a HDFStore Troubleshoot HDFStore exceptions Setting min_itemsize with strings Using ptrepack to create a completely-sorted-index on a store Storing Attributes to a group node \nIn [206]: df = pd.DataFrame(np.random.randn(8, 3))\n\nIn [207]: store = pd.HDFStore(\"test.h5\")\n\nIn [208]: store.put(\"df\", df)\n\n# you can store an arbitrary Python object via pickle\nIn [209]: store.get_storer(\"df\").attrs.my_attribute = {\"A\": 10}\n\nIn [210]: store.get_storer(\"df\").attrs.my_attribute\nOut[210]: {'A': 10}\n  You can create or load a HDFStore in-memory by passing the driver parameter to PyTables. Changes are only written to disk when the HDFStore is closed. \nIn [211]: store = pd.HDFStore(\"test.h5\", \"w\", driver=\"H5FD_CORE\")\n\nIn [212]: df = pd.DataFrame(np.random.randn(8, 3))\n\nIn [213]: store[\"test\"] = df\n\n# only after closing the store, data is written to disk:\nIn [214]: store.close()\n    Binary files pandas readily accepts NumPy record arrays, if you need to read in a binary file consisting of an array of C structs. For example, given this C program in a file called main.c compiled with gcc main.c -std=gnu99 on a 64-bit machine, \n#include <stdio.h>\n#include <stdint.h>\n\ntypedef struct _Data\n{\n    int32_t count;\n    double avg;\n    float scale;\n} Data;\n\nint main(int argc, const char *argv[])\n{\n    size_t n = 10;\n    Data d[n];\n\n    for (int i = 0; i < n; ++i)\n    {\n        d[i].count = i;\n        d[i].avg = i + 1.0;\n        d[i].scale = (float) i + 2.0f;\n    }\n\n    FILE *file = fopen(\"binary.dat\", \"wb\");\n    fwrite(&d, sizeof(Data), n, file);\n    fclose(file);\n\n    return 0;\n}\n  the following Python code will read the binary file 'binary.dat' into a pandas DataFrame, where each element of the struct corresponds to a column in the frame: \nnames = \"count\", \"avg\", \"scale\"\n\n# note that the offsets are larger than the size of the type because of\n# struct padding\noffsets = 0, 8, 16\nformats = \"i4\", \"f8\", \"f4\"\ndt = np.dtype({\"names\": names, \"offsets\": offsets, \"formats\": formats}, align=True)\ndf = pd.DataFrame(np.fromfile(\"binary.dat\", dt))\n   Note The offsets of the structure elements may be different depending on the architecture of the machine on which the file was created. Using a raw binary file format like this for general data storage is not recommended, as it is not cross platform. We recommended either HDF5 or parquet, both of which are supported by pandas\u2019 IO facilities.     Computation Numerical integration (sample-based) of a time series  Correlation Often it\u2019s useful to obtain the lower (or upper) triangular form of a correlation matrix calculated from DataFrame.corr(). This can be achieved by passing a boolean mask to where as follows: \nIn [215]: df = pd.DataFrame(np.random.random(size=(100, 5)))\n\nIn [216]: corr_mat = df.corr()\n\nIn [217]: mask = np.tril(np.ones_like(corr_mat, dtype=np.bool_), k=-1)\n\nIn [218]: corr_mat.where(mask)\nOut[218]: \n          0         1         2        3   4\n0       NaN       NaN       NaN      NaN NaN\n1 -0.079861       NaN       NaN      NaN NaN\n2 -0.236573  0.183801       NaN      NaN NaN\n3 -0.013795 -0.051975  0.037235      NaN NaN\n4 -0.031974  0.118342 -0.073499 -0.02063 NaN\n  The method argument within DataFrame.corr can accept a callable in addition to the named correlation types. Here we compute the distance correlation matrix for a DataFrame object. \nIn [219]: def distcorr(x, y):\n   .....:     n = len(x)\n   .....:     a = np.zeros(shape=(n, n))\n   .....:     b = np.zeros(shape=(n, n))\n   .....:     for i in range(n):\n   .....:         for j in range(i + 1, n):\n   .....:             a[i, j] = abs(x[i] - x[j])\n   .....:             b[i, j] = abs(y[i] - y[j])\n   .....:     a += a.T\n   .....:     b += b.T\n   .....:     a_bar = np.vstack([np.nanmean(a, axis=0)] * n)\n   .....:     b_bar = np.vstack([np.nanmean(b, axis=0)] * n)\n   .....:     A = a - a_bar - a_bar.T + np.full(shape=(n, n), fill_value=a_bar.mean())\n   .....:     B = b - b_bar - b_bar.T + np.full(shape=(n, n), fill_value=b_bar.mean())\n   .....:     cov_ab = np.sqrt(np.nansum(A * B)) \/ n\n   .....:     std_a = np.sqrt(np.sqrt(np.nansum(A ** 2)) \/ n)\n   .....:     std_b = np.sqrt(np.sqrt(np.nansum(B ** 2)) \/ n)\n   .....:     return cov_ab \/ std_a \/ std_b\n   .....: \n\nIn [220]: df = pd.DataFrame(np.random.normal(size=(100, 3)))\n\nIn [221]: df.corr(method=distcorr)\nOut[221]: \n          0         1         2\n0  1.000000  0.197613  0.216328\n1  0.197613  1.000000  0.208749\n2  0.216328  0.208749  1.000000\n     Timedeltas The Timedeltas docs. Using timedeltas \nIn [222]: import datetime\n\nIn [223]: s = pd.Series(pd.date_range(\"2012-1-1\", periods=3, freq=\"D\"))\n\nIn [224]: s - s.max()\nOut[224]: \n0   -2 days\n1   -1 days\n2    0 days\ndtype: timedelta64[ns]\n\nIn [225]: s.max() - s\nOut[225]: \n0   2 days\n1   1 days\n2   0 days\ndtype: timedelta64[ns]\n\nIn [226]: s - datetime.datetime(2011, 1, 1, 3, 5)\nOut[226]: \n0   364 days 20:55:00\n1   365 days 20:55:00\n2   366 days 20:55:00\ndtype: timedelta64[ns]\n\nIn [227]: s + datetime.timedelta(minutes=5)\nOut[227]: \n0   2012-01-01 00:05:00\n1   2012-01-02 00:05:00\n2   2012-01-03 00:05:00\ndtype: datetime64[ns]\n\nIn [228]: datetime.datetime(2011, 1, 1, 3, 5) - s\nOut[228]: \n0   -365 days +03:05:00\n1   -366 days +03:05:00\n2   -367 days +03:05:00\ndtype: timedelta64[ns]\n\nIn [229]: datetime.timedelta(minutes=5) + s\nOut[229]: \n0   2012-01-01 00:05:00\n1   2012-01-02 00:05:00\n2   2012-01-03 00:05:00\ndtype: datetime64[ns]\n  Adding and subtracting deltas and dates \nIn [230]: deltas = pd.Series([datetime.timedelta(days=i) for i in range(3)])\n\nIn [231]: df = pd.DataFrame({\"A\": s, \"B\": deltas})\n\nIn [232]: df\nOut[232]: \n           A      B\n0 2012-01-01 0 days\n1 2012-01-02 1 days\n2 2012-01-03 2 days\n\nIn [233]: df[\"New Dates\"] = df[\"A\"] + df[\"B\"]\n\nIn [234]: df[\"Delta\"] = df[\"A\"] - df[\"New Dates\"]\n\nIn [235]: df\nOut[235]: \n           A      B  New Dates   Delta\n0 2012-01-01 0 days 2012-01-01  0 days\n1 2012-01-02 1 days 2012-01-03 -1 days\n2 2012-01-03 2 days 2012-01-05 -2 days\n\nIn [236]: df.dtypes\nOut[236]: \nA             datetime64[ns]\nB            timedelta64[ns]\nNew Dates     datetime64[ns]\nDelta        timedelta64[ns]\ndtype: object\n  Another example Values can be set to NaT using np.nan, similar to datetime \nIn [237]: y = s - s.shift()\n\nIn [238]: y\nOut[238]: \n0      NaT\n1   1 days\n2   1 days\ndtype: timedelta64[ns]\n\nIn [239]: y[1] = np.nan\n\nIn [240]: y\nOut[240]: \n0      NaT\n1      NaT\n2   1 days\ndtype: timedelta64[ns]\n    Creating example data To create a dataframe from every combination of some given values, like R\u2019s expand.grid() function, we can create a dict where the keys are column names and the values are lists of the data values: \nIn [241]: def expand_grid(data_dict):\n   .....:     rows = itertools.product(*data_dict.values())\n   .....:     return pd.DataFrame.from_records(rows, columns=data_dict.keys())\n   .....: \n\nIn [242]: df = expand_grid(\n   .....:     {\"height\": [60, 70], \"weight\": [100, 140, 180], \"sex\": [\"Male\", \"Female\"]}\n   .....: )\n   .....: \n\nIn [243]: df\nOut[243]: \n    height  weight     sex\n0       60     100    Male\n1       60     100  Female\n2       60     140    Male\n3       60     140  Female\n4       60     180    Male\n5       60     180  Female\n6       70     100    Male\n7       70     100  Female\n8       70     140    Male\n9       70     140  Female\n10      70     180    Male\n11      70     180  Female","title":"pandas.user_guide.cookbook"},{"text":"tf.keras.initializers.HeUniform He uniform variance scaling initializer. Inherits From: VarianceScaling, Initializer  View aliases  Main aliases \ntf.initializers.HeUniform, tf.initializers.he_uniform, tf.keras.initializers.he_uniform  \ntf.keras.initializers.HeUniform(\n    seed=None\n)\n Also available via the shortcut function tf.keras.initializers.he_uniform. Draws samples from a uniform distribution within [-limit, limit], where limit = sqrt(6 \/ fan_in) (fan_in is the number of input units in the weight tensor). Examples: \n# Standalone usage:\ninitializer = tf.keras.initializers.HeUniform()\nvalues = initializer(shape=(2, 2))\n \n# Usage in a Keras layer:\ninitializer = tf.keras.initializers.HeUniform()\nlayer = tf.keras.layers.Dense(3, kernel_initializer=initializer)\n\n \n\n\n Arguments\n  seed   A Python integer. An initializer created with a given seed will always produce the same random tensor for a given shape and dtype.    References: He et al., 2015 (pdf) Methods from_config View source \n@classmethod\nfrom_config(\n    config\n)\n Instantiates an initializer from a configuration dictionary. Example: initializer = RandomUniform(-1, 1)\nconfig = initializer.get_config()\ninitializer = RandomUniform.from_config(config)\n\n \n\n\n Args\n  config   A Python dictionary. It will typically be the output of get_config.   \n \n\n\n Returns   An Initializer instance.  \n get_config View source \nget_config()\n Returns the configuration of the initializer as a JSON-serializable dict.\n \n\n\n Returns   A JSON-serializable Python dict.  \n __call__ View source \n__call__(\n    shape, dtype=None, **kwargs\n)\n Returns a tensor object initialized as specified by the initializer.\n \n\n\n Args\n  shape   Shape of the tensor.  \n  dtype   Optional dtype of the tensor. Only floating point types are supported. If not specified, tf.keras.backend.floatx() is used, which default to float32 unless you configured it otherwise (via tf.keras.backend.set_floatx(float_dtype))  \n  **kwargs   Additional keyword arguments.","title":"tensorflow.keras.initializers.heuniform"},{"text":"tf.compat.v1.initializers.he_uniform He uniform variance scaling initializer. \ntf.compat.v1.initializers.he_uniform(\n    seed=None\n)\n It draws samples from a uniform distribution within [-limit, limit] where limit is sqrt(6 \/ fan_in) where fan_in is the number of input units in the weight tensor.\n \n\n\n Arguments\n  seed   A Python integer. Used to seed the random generator.   \n \n\n\n Returns   An initializer.  \n References: He et al., 2015 (pdf)","title":"tensorflow.compat.v1.initializers.he_uniform"},{"text":"arg_constraints = {'df': GreaterThan(lower_bound=0.0)}","title":"torch.distributions#torch.distributions.chi2.Chi2.arg_constraints"},{"text":"pandas.core.resample.Resampler.get_group   Resampler.get_group(name, obj=None)[source]\n \nConstruct DataFrame from group with provided name.  Parameters \n \nname:object\n\n\nThe name of the group to get as a DataFrame.  \nobj:DataFrame, default None\n\n\nThe DataFrame to take the DataFrame out of. If it is None, the object groupby was called on will be used.    Returns \n \ngroup:same type as obj","title":"pandas.reference.api.pandas.core.resample.resampler.get_group"},{"text":"ticket_lifetime_hint","title":"python.library.ssl#ssl.SSLSession.ticket_lifetime_hint"}]}
{"task_id":14716342,"prompt":"def f_14716342(your_string):\n\treturn ","suffix":"","canonical_solution":"re.search('\\\\[(.*)\\\\]', your_string).group(1)","test_start":"\nimport re\n\ndef check(candidate):","test":["\n    assert candidate('[uranus]') == 'uranus'\n","\n    assert candidate('hello[world] !') == 'world'\n"],"entry_point":"f_14716342","intent":"find the string in `your_string` between two special characters \"[\" and \"]\"","library":["re"],"docs":[{"text":"email.utils.unquote(str)  \nReturn a new string which is an unquoted version of str. If str ends and begins with double quotes, they are stripped off. Likewise if str ends and begins with angle brackets, they are stripped off.","title":"python.library.email.utils#email.utils.unquote"},{"text":"token.LSQB  \nToken value for \"[\".","title":"python.library.token#token.LSQB"},{"text":"token.RSQB  \nToken value for \"]\".","title":"python.library.token#token.RSQB"},{"text":"difflib.restore(sequence, which)  \nReturn one of the two sequences that generated a delta. Given a sequence produced by Differ.compare() or ndiff(), extract lines originating from file 1 or 2 (parameter which), stripping off line prefixes. Example: >>> diff = ndiff('one\\ntwo\\nthree\\n'.splitlines(keepends=True),\n...              'ore\\ntree\\nemu\\n'.splitlines(keepends=True))\n>>> diff = list(diff) # materialize the generated delta into a list\n>>> print(''.join(restore(diff, 1)), end=\"\")\none\ntwo\nthree\n>>> print(''.join(restore(diff, 2)), end=\"\")\nore\ntree\nemu","title":"python.library.difflib#difflib.restore"},{"text":"get_topmost_subplotspec()[source]\n \nReturn the topmost SubplotSpec instance associated with the subplot.","title":"matplotlib._as_gen.matplotlib.gridspec.gridspecfromsubplotspec#matplotlib.gridspec.GridSpecFromSubplotSpec.get_topmost_subplotspec"},{"text":"get_parent() \n find the parent of a subsurface get_parent() -> Surface  Returns the parent Surface of a subsurface. If this is not a subsurface then None will be returned.","title":"pygame.ref.surface#pygame.Surface.get_parent"},{"text":"read_string(string, source='<string>')  \nParse configuration data from a string. Optional argument source specifies a context-specific name of the string passed. If not given, '<string>' is used. This should commonly be a filesystem path or a URL.  New in version 3.2.","title":"python.library.configparser#configparser.ConfigParser.read_string"},{"text":"contains() \n test if one rectangle is inside another contains(Rect) -> bool  Returns true when the argument is completely inside the Rect.","title":"pygame.ref.rect#pygame.Rect.contains"},{"text":"matplotlib.rcsetup.validate_string(s)[source]","title":"matplotlib.rcsetup_api#matplotlib.rcsetup.validate_string"},{"text":"classmatplotlib.tri.TriFinder(triangulation)[source]\n \nAbstract base class for classes used to find the triangles of a Triangulation in which (x, y) points lie. Rather than instantiate an object of a class derived from TriFinder, it is usually better to use the function Triangulation.get_trifinder. Derived classes implement __call__(x, y) where x and y are array-like point coordinates of the same shape.","title":"matplotlib.tri_api#matplotlib.tri.TriFinder"}]}
{"task_id":18684076,"prompt":"def f_18684076():\n\treturn ","suffix":"","canonical_solution":"[d.strftime('%Y%m%d') for d in pandas.date_range('20130226', '20130302')]","test_start":"\nimport pandas \n\ndef check(candidate):","test":["\n    assert candidate() == ['20130226', '20130227', '20130228', '20130301', '20130302']\n"],"entry_point":"f_18684076","intent":"create a list of date string in 'yyyymmdd' format with Python Pandas from '20130226' to '20130302'","library":["pandas"],"docs":[{"text":"pandas.Series.tolist   Series.tolist()[source]\n \nReturn a list of the values. These are each a scalar type, which is a Python scalar (for str, int, float) or a pandas scalar (for Timestamp\/Timedelta\/Interval\/Period)  Returns \n list\n    See also  numpy.ndarray.tolist\n\nReturn the array as an a.ndim-levels deep nested list of Python scalars.","title":"pandas.reference.api.pandas.series.tolist"},{"text":"pandas.StringDtype   classpandas.StringDtype(storage=None)[source]\n \nExtension dtype for string data.  New in version 1.0.0.   Warning StringDtype is considered experimental. The implementation and parts of the API may change without warning. In particular, StringDtype.na_value may change to no longer be numpy.nan.   Parameters \n \nstorage:{\u201cpython\u201d, \u201cpyarrow\u201d}, optional\n\n\nIf not given, the value of pd.options.mode.string_storage.     Examples \n>>> pd.StringDtype()\nstring[python]\n  \n>>> pd.StringDtype(storage=\"pyarrow\")\nstring[pyarrow]\n  Attributes       \nNone     Methods       \nNone","title":"pandas.reference.api.pandas.stringdtype"},{"text":"pandas.Series.to_list   Series.to_list()[source]\n \nReturn a list of the values. These are each a scalar type, which is a Python scalar (for str, int, float) or a pandas scalar (for Timestamp\/Timedelta\/Interval\/Period)  Returns \n list\n    See also  numpy.ndarray.tolist\n\nReturn the array as an a.ndim-levels deep nested list of Python scalars.","title":"pandas.reference.api.pandas.series.to_list"},{"text":"matplotlib.dates.drange(dstart, dend, delta)[source]\n \nReturn a sequence of equally spaced Matplotlib dates. The dates start at dstart and reach up to, but not including dend. They are spaced by delta.  Parameters \n \ndstart, denddatetime\n\n\nThe date limits.  \ndeltadatetime.timedelta\n\n\nSpacing of the dates.    Returns \n numpy.array\n\nA list floats representing Matplotlib dates.","title":"matplotlib.dates_api#matplotlib.dates.drange"},{"text":"pandas.Index.to_list   Index.to_list()[source]\n \nReturn a list of the values. These are each a scalar type, which is a Python scalar (for str, int, float) or a pandas scalar (for Timestamp\/Timedelta\/Interval\/Period)  Returns \n list\n    See also  numpy.ndarray.tolist\n\nReturn the array as an a.ndim-levels deep nested list of Python scalars.","title":"pandas.reference.api.pandas.index.to_list"},{"text":"pandas.Index.tolist   Index.tolist()[source]\n \nReturn a list of the values. These are each a scalar type, which is a Python scalar (for str, int, float) or a pandas scalar (for Timestamp\/Timedelta\/Interval\/Period)  Returns \n list\n    See also  numpy.ndarray.tolist\n\nReturn the array as an a.ndim-levels deep nested list of Python scalars.","title":"pandas.reference.api.pandas.index.tolist"},{"text":"class tracemalloc.Trace  \nTrace of a memory block. The Snapshot.traces attribute is a sequence of Trace instances.  Changed in version 3.6: Added the domain attribute.   \ndomain  \nAddress space of a memory block (int). Read-only property. tracemalloc uses the domain 0 to trace memory allocations made by Python. C extensions can use other domains to trace other resources. \n  \nsize  \nSize of the memory block in bytes (int). \n  \ntraceback  \nTraceback where the memory block was allocated, Traceback instance.","title":"python.library.tracemalloc#tracemalloc.Trace"},{"text":"torch.gcd(input, other, *, out=None) \u2192 Tensor  \nComputes the element-wise greatest common divisor (GCD) of input and other. Both input and other must have integer types.  Note This defines gcd(0,0)=0gcd(0, 0) = 0 .   Parameters \n \ninput (Tensor) \u2013 the input tensor. \nother (Tensor) \u2013 the second input tensor   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.tensor([5, 10, 15])\n>>> b = torch.tensor([3, 4, 5])\n>>> torch.gcd(a, b)\ntensor([1, 2, 5])\n>>> c = torch.tensor([3])\n>>> torch.gcd(a, c)\ntensor([1, 1, 3])","title":"torch.generated.torch.gcd#torch.gcd"},{"text":"pandas.Series.dt.date   Series.dt.date\n \nReturns numpy array of python datetime.date objects. Namely, the date part of Timestamps without time and timezone information.","title":"pandas.reference.api.pandas.series.dt.date"},{"text":"pandas.PeriodIndex.strftime   PeriodIndex.strftime(*args, **kwargs)[source]\n \nConvert to Index using specified date_format. Return an Index of formatted strings specified by date_format, which supports the same string format as the python standard library. Details of the string format can be found in python string format doc.  Parameters \n \ndate_format:str\n\n\nDate format string (e.g. \u201c%Y-%m-%d\u201d).    Returns \n ndarray[object]\n\nNumPy ndarray of formatted strings.      See also  to_datetime\n\nConvert the given argument to datetime.  DatetimeIndex.normalize\n\nReturn DatetimeIndex with times to midnight.  DatetimeIndex.round\n\nRound the DatetimeIndex to the specified freq.  DatetimeIndex.floor\n\nFloor the DatetimeIndex to the specified freq.    Examples \n>>> rng = pd.date_range(pd.Timestamp(\"2018-03-10 09:00\"),\n...                     periods=3, freq='s')\n>>> rng.strftime('%B %d, %Y, %r')\nIndex(['March 10, 2018, 09:00:00 AM', 'March 10, 2018, 09:00:01 AM',\n       'March 10, 2018, 09:00:02 AM'],\n      dtype='object')","title":"pandas.reference.api.pandas.periodindex.strftime"}]}
{"task_id":1666700,"prompt":"def f_1666700():\n\treturn ","suffix":"","canonical_solution":"\"\"\"The big brown fox is brown\"\"\".count('brown')","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == 2\n"],"entry_point":"f_1666700","intent":"count number of times string 'brown' occurred in string 'The big brown fox is brown'","library":[],"docs":[]}
{"task_id":18979111,"prompt":"def f_18979111(request_body):\n\treturn ","suffix":"","canonical_solution":"json.loads(request_body)","test_start":"\nimport json \n\ndef check(candidate):","test":["\n    x = \"\"\"{\n    \"Name\": \"Jennifer Smith\",\n    \"Contact Number\": 7867567898,\n    \"Email\": \"jen123@gmail.com\",\n    \"Hobbies\":[\"Reading\", \"Sketching\", \"Horse Riding\"]\n    }\"\"\"\n    assert candidate(x) == {'Hobbies': ['Reading', 'Sketching', 'Horse Riding'], 'Name': 'Jennifer Smith', 'Email': 'jen123@gmail.com', 'Contact Number': 7867567898}\n"],"entry_point":"f_18979111","intent":"decode json string `request_body` to python dict","library":["json"],"docs":[{"text":"json_decoder  \nalias of flask.json.JSONDecoder","title":"flask.api.index#flask.Flask.json_decoder"},{"text":"HttpRequest.body  \nThe raw HTTP request body as a bytestring. This is useful for processing data in different ways than conventional HTML forms: binary images, XML payload etc. For processing conventional form data, use HttpRequest.POST. You can also read from an HttpRequest using a file-like interface with HttpRequest.read() or HttpRequest.readline(). Accessing the body attribute after reading the request with either of these I\/O stream methods will produce a RawPostDataException.","title":"django.ref.request-response#django.http.HttpRequest.body"},{"text":"tf.compat.v1.estimator.classifier_parse_example_spec Generates parsing spec for tf.parse_example to be used with classifiers. \ntf.compat.v1.estimator.classifier_parse_example_spec(\n    feature_columns, label_key, label_dtype=tf.dtypes.int64, label_default=None,\n    weight_column=None\n)\n If users keep data in tf.Example format, they need to call tf.parse_example with a proper feature spec. There are two main things that this utility helps:  Users need to combine parsing spec of features with labels and weights (if any) since they are all parsed from same tf.Example instance. This utility combines these specs. It is difficult to map expected label by a classifier such as DNNClassifier to corresponding tf.parse_example spec. This utility encodes it by getting related information from users (key, dtype).  Example output of parsing spec: # Define features and transformations\nfeature_b = tf.feature_column.numeric_column(...)\nfeature_c_bucketized = tf.feature_column.bucketized_column(\n  tf.feature_column.numeric_column(\"feature_c\"), ...)\nfeature_a_x_feature_c = tf.feature_column.crossed_column(\n    columns=[\"feature_a\", feature_c_bucketized], ...)\n\nfeature_columns = [feature_b, feature_c_bucketized, feature_a_x_feature_c]\nparsing_spec = tf.estimator.classifier_parse_example_spec(\n    feature_columns, label_key='my-label', label_dtype=tf.string)\n\n# For the above example, classifier_parse_example_spec would return the dict:\nassert parsing_spec == {\n  \"feature_a\": parsing_ops.VarLenFeature(tf.string),\n  \"feature_b\": parsing_ops.FixedLenFeature([1], dtype=tf.float32),\n  \"feature_c\": parsing_ops.FixedLenFeature([1], dtype=tf.float32)\n  \"my-label\" : parsing_ops.FixedLenFeature([1], dtype=tf.string)\n}\n Example usage with a classifier: feature_columns = # define features via tf.feature_column\nestimator = DNNClassifier(\n    n_classes=1000,\n    feature_columns=feature_columns,\n    weight_column='example-weight',\n    label_vocabulary=['photos', 'keep', ...],\n    hidden_units=[256, 64, 16])\n# This label configuration tells the classifier the following:\n# * weights are retrieved with key 'example-weight'\n# * label is string and can be one of the following ['photos', 'keep', ...]\n# * integer id for label 'photos' is 0, 'keep' is 1, ...\n\n\n# Input builders\ndef input_fn_train():  # Returns a tuple of features and labels.\n  features = tf.contrib.learn.read_keyed_batch_features(\n      file_pattern=train_files,\n      batch_size=batch_size,\n      # creates parsing configuration for tf.parse_example\n      features=tf.estimator.classifier_parse_example_spec(\n          feature_columns,\n          label_key='my-label',\n          label_dtype=tf.string,\n          weight_column='example-weight'),\n      reader=tf.RecordIOReader)\n   labels = features.pop('my-label')\n   return features, labels\n\nestimator.train(input_fn=input_fn_train)\n\n \n\n\n Args\n  feature_columns   An iterable containing all feature columns. All items should be instances of classes derived from FeatureColumn.  \n  label_key   A string identifying the label. It means tf.Example stores labels with this key.  \n  label_dtype   A tf.dtype identifies the type of labels. By default it is tf.int64. If user defines a label_vocabulary, this should be set as tf.string. tf.float32 labels are only supported for binary classification.  \n  label_default   used as label if label_key does not exist in given tf.Example. An example usage: let's say label_key is 'clicked' and tf.Example contains clicked data only for positive examples in following format key:clicked, value:1. This means that if there is no data with key 'clicked' it should count as negative example by setting label_deafault=0. Type of this value should be compatible with label_dtype.  \n  weight_column   A string or a NumericColumn created by tf.feature_column.numeric_column defining feature column representing weights. It is used to down weight or boost examples during training. It will be multiplied by the loss of the example. If it is a string, it is used as a key to fetch weight tensor from the features. If it is a NumericColumn, raw tensor is fetched by key weight_column.key, then weight_column.normalizer_fn is applied on it to get weight tensor.   \n \n\n\n Returns   A dict mapping each feature key to a FixedLenFeature or VarLenFeature value.  \n\n \n\n\n Raises\n  ValueError   If label is used in feature_columns.  \n  ValueError   If weight_column is used in feature_columns.  \n  ValueError   If any of the given feature_columns is not a _FeatureColumn instance.  \n  ValueError   If weight_column is not a NumericColumn instance.  \n  ValueError   if label_key is None.","title":"tensorflow.compat.v1.estimator.classifier_parse_example_spec"},{"text":"tf.compat.v1.estimator.regressor_parse_example_spec Generates parsing spec for tf.parse_example to be used with regressors. \ntf.compat.v1.estimator.regressor_parse_example_spec(\n    feature_columns, label_key, label_dtype=tf.dtypes.float32, label_default=None,\n    label_dimension=1, weight_column=None\n)\n If users keep data in tf.Example format, they need to call tf.parse_example with a proper feature spec. There are two main things that this utility helps:  Users need to combine parsing spec of features with labels and weights (if any) since they are all parsed from same tf.Example instance. This utility combines these specs. It is difficult to map expected label by a regressor such as DNNRegressor to corresponding tf.parse_example spec. This utility encodes it by getting related information from users (key, dtype).  Example output of parsing spec: # Define features and transformations\nfeature_b = tf.feature_column.numeric_column(...)\nfeature_c_bucketized = tf.feature_column.bucketized_column(\n  tf.feature_column.numeric_column(\"feature_c\"), ...)\nfeature_a_x_feature_c = tf.feature_column.crossed_column(\n    columns=[\"feature_a\", feature_c_bucketized], ...)\n\nfeature_columns = [feature_b, feature_c_bucketized, feature_a_x_feature_c]\nparsing_spec = tf.estimator.regressor_parse_example_spec(\n    feature_columns, label_key='my-label')\n\n# For the above example, regressor_parse_example_spec would return the dict:\nassert parsing_spec == {\n  \"feature_a\": parsing_ops.VarLenFeature(tf.string),\n  \"feature_b\": parsing_ops.FixedLenFeature([1], dtype=tf.float32),\n  \"feature_c\": parsing_ops.FixedLenFeature([1], dtype=tf.float32)\n  \"my-label\" : parsing_ops.FixedLenFeature([1], dtype=tf.float32)\n}\n Example usage with a regressor: feature_columns = # define features via tf.feature_column\nestimator = DNNRegressor(\n    hidden_units=[256, 64, 16],\n    feature_columns=feature_columns,\n    weight_column='example-weight',\n    label_dimension=3)\n# This label configuration tells the regressor the following:\n# * weights are retrieved with key 'example-weight'\n# * label is a 3 dimension tensor with float32 dtype.\n\n\n# Input builders\ndef input_fn_train():  # Returns a tuple of features and labels.\n  features = tf.contrib.learn.read_keyed_batch_features(\n      file_pattern=train_files,\n      batch_size=batch_size,\n      # creates parsing configuration for tf.parse_example\n      features=tf.estimator.classifier_parse_example_spec(\n          feature_columns,\n          label_key='my-label',\n          label_dimension=3,\n          weight_column='example-weight'),\n      reader=tf.RecordIOReader)\n   labels = features.pop('my-label')\n   return features, labels\n\nestimator.train(input_fn=input_fn_train)\n\n \n\n\n Args\n  feature_columns   An iterable containing all feature columns. All items should be instances of classes derived from _FeatureColumn.  \n  label_key   A string identifying the label. It means tf.Example stores labels with this key.  \n  label_dtype   A tf.dtype identifies the type of labels. By default it is tf.float32.  \n  label_default   used as label if label_key does not exist in given tf.Example. By default default_value is none, which means tf.parse_example will error out if there is any missing label.  \n  label_dimension   Number of regression targets per example. This is the size of the last dimension of the labels and logits Tensor objects (typically, these have shape [batch_size, label_dimension]).  \n  weight_column   A string or a NumericColumn created by tf.feature_column.numeric_column defining feature column representing weights. It is used to down weight or boost examples during training. It will be multiplied by the loss of the example. If it is a string, it is used as a key to fetch weight tensor from the features. If it is a NumericColumn, raw tensor is fetched by key weight_column.key, then weight_column.normalizer_fn is applied on it to get weight tensor.   \n \n\n\n Returns   A dict mapping each feature key to a FixedLenFeature or VarLenFeature value.  \n\n \n\n\n Raises\n  ValueError   If label is used in feature_columns.  \n  ValueError   If weight_column is used in feature_columns.  \n  ValueError   If any of the given feature_columns is not a _FeatureColumn instance.  \n  ValueError   If weight_column is not a NumericColumn instance.  \n  ValueError   if label_key is None.","title":"tensorflow.compat.v1.estimator.regressor_parse_example_spec"},{"text":"CGIXMLRPCRequestHandler.handle_request(request_text=None)  \nHandle an XML-RPC request. If request_text is given, it should be the POST data provided by the HTTP server, otherwise the contents of stdin will be used.","title":"python.library.xmlrpc.server#xmlrpc.server.CGIXMLRPCRequestHandler.handle_request"},{"text":"tf.compat.v1.feature_column.make_parse_example_spec Creates parsing spec dictionary from input feature_columns. \ntf.compat.v1.feature_column.make_parse_example_spec(\n    feature_columns\n)\n The returned dictionary can be used as arg 'features' in tf.io.parse_example. Typical usage example: # Define features and transformations\nfeature_a = categorical_column_with_vocabulary_file(...)\nfeature_b = numeric_column(...)\nfeature_c_bucketized = bucketized_column(numeric_column(\"feature_c\"), ...)\nfeature_a_x_feature_c = crossed_column(\n    columns=[\"feature_a\", feature_c_bucketized], ...)\n\nfeature_columns = set(\n    [feature_b, feature_c_bucketized, feature_a_x_feature_c])\nfeatures = tf.io.parse_example(\n    serialized=serialized_examples,\n    features=make_parse_example_spec(feature_columns))\n For the above example, make_parse_example_spec would return the dict: {\n    \"feature_a\": parsing_ops.VarLenFeature(tf.string),\n    \"feature_b\": parsing_ops.FixedLenFeature([1], dtype=tf.float32),\n    \"feature_c\": parsing_ops.FixedLenFeature([1], dtype=tf.float32)\n}\n\n \n\n\n Args\n  feature_columns   An iterable containing all feature columns. All items should be instances of classes derived from _FeatureColumn.   \n \n\n\n Returns   A dict mapping each feature key to a FixedLenFeature or VarLenFeature value.  \n\n \n\n\n Raises\n  ValueError   If any of the given feature_columns is not a _FeatureColumn instance.","title":"tensorflow.compat.v1.feature_column.make_parse_example_spec"},{"text":"class socketserver.UnixStreamServer(server_address, RequestHandlerClass, bind_and_activate=True)  \nclass socketserver.UnixDatagramServer(server_address, RequestHandlerClass, bind_and_activate=True)  \nThese more infrequently used classes are similar to the TCP and UDP classes, but use Unix domain sockets; they\u2019re not available on non-Unix platforms. The parameters are the same as for TCPServer.","title":"python.library.socketserver#socketserver.UnixStreamServer"},{"text":"tf.feature_column.make_parse_example_spec     View source on GitHub    Creates parsing spec dictionary from input feature_columns. \ntf.feature_column.make_parse_example_spec(\n    feature_columns\n)\n The returned dictionary can be used as arg 'features' in tf.io.parse_example. Typical usage example: # Define features and transformations\nfeature_a = tf.feature_column.categorical_column_with_vocabulary_file(...)\nfeature_b = tf.feature_column.numeric_column(...)\nfeature_c_bucketized = tf.feature_column.bucketized_column(\n    tf.feature_column.numeric_column(\"feature_c\"), ...)\nfeature_a_x_feature_c = tf.feature_column.crossed_column(\n    columns=[\"feature_a\", feature_c_bucketized], ...)\n\nfeature_columns = set(\n    [feature_b, feature_c_bucketized, feature_a_x_feature_c])\nfeatures = tf.io.parse_example(\n    serialized=serialized_examples,\n    features=tf.feature_column.make_parse_example_spec(feature_columns))\n For the above example, make_parse_example_spec would return the dict: {\n    \"feature_a\": parsing_ops.VarLenFeature(tf.string),\n    \"feature_b\": parsing_ops.FixedLenFeature([1], dtype=tf.float32),\n    \"feature_c\": parsing_ops.FixedLenFeature([1], dtype=tf.float32)\n}\n\n \n\n\n Args\n  feature_columns   An iterable containing all feature columns. All items should be instances of classes derived from FeatureColumn.   \n \n\n\n Returns   A dict mapping each feature key to a FixedLenFeature or VarLenFeature value.  \n\n \n\n\n Raises\n  ValueError   If any of the given feature_columns is not a FeatureColumn instance.","title":"tensorflow.feature_column.make_parse_example_spec"},{"text":"AttributesNS.getValueByQName(name)  \nReturn the value for a qualified name.","title":"python.library.xml.sax.reader#xml.sax.xmlreader.AttributesNS.getValueByQName"},{"text":"tf.estimator.classifier_parse_example_spec     View source on GitHub    Generates parsing spec for tf.parse_example to be used with classifiers. \ntf.estimator.classifier_parse_example_spec(\n    feature_columns, label_key, label_dtype=tf.dtypes.int64, label_default=None,\n    weight_column=None\n)\n If users keep data in tf.Example format, they need to call tf.parse_example with a proper feature spec. There are two main things that this utility helps:  Users need to combine parsing spec of features with labels and weights (if any) since they are all parsed from same tf.Example instance. This utility combines these specs. It is difficult to map expected label by a classifier such as DNNClassifier to corresponding tf.parse_example spec. This utility encodes it by getting related information from users (key, dtype).  Example output of parsing spec: # Define features and transformations\nfeature_b = tf.feature_column.numeric_column(...)\nfeature_c_bucketized = tf.feature_column.bucketized_column(\n  tf.feature_column.numeric_column(\"feature_c\"), ...)\nfeature_a_x_feature_c = tf.feature_column.crossed_column(\n    columns=[\"feature_a\", feature_c_bucketized], ...)\n\nfeature_columns = [feature_b, feature_c_bucketized, feature_a_x_feature_c]\nparsing_spec = tf.estimator.classifier_parse_example_spec(\n    feature_columns, label_key='my-label', label_dtype=tf.string)\n\n# For the above example, classifier_parse_example_spec would return the dict:\nassert parsing_spec == {\n  \"feature_a\": parsing_ops.VarLenFeature(tf.string),\n  \"feature_b\": parsing_ops.FixedLenFeature([1], dtype=tf.float32),\n  \"feature_c\": parsing_ops.FixedLenFeature([1], dtype=tf.float32)\n  \"my-label\" : parsing_ops.FixedLenFeature([1], dtype=tf.string)\n}\n Example usage with a classifier: feature_columns = # define features via tf.feature_column\nestimator = DNNClassifier(\n    n_classes=1000,\n    feature_columns=feature_columns,\n    weight_column='example-weight',\n    label_vocabulary=['photos', 'keep', ...],\n    hidden_units=[256, 64, 16])\n# This label configuration tells the classifier the following:\n# * weights are retrieved with key 'example-weight'\n# * label is string and can be one of the following ['photos', 'keep', ...]\n# * integer id for label 'photos' is 0, 'keep' is 1, ...\n\n\n# Input builders\ndef input_fn_train():  # Returns a tuple of features and labels.\n  features = tf.contrib.learn.read_keyed_batch_features(\n      file_pattern=train_files,\n      batch_size=batch_size,\n      # creates parsing configuration for tf.parse_example\n      features=tf.estimator.classifier_parse_example_spec(\n          feature_columns,\n          label_key='my-label',\n          label_dtype=tf.string,\n          weight_column='example-weight'),\n      reader=tf.RecordIOReader)\n   labels = features.pop('my-label')\n   return features, labels\n\nestimator.train(input_fn=input_fn_train)\n\n \n\n\n Args\n  feature_columns   An iterable containing all feature columns. All items should be instances of classes derived from FeatureColumn.  \n  label_key   A string identifying the label. It means tf.Example stores labels with this key.  \n  label_dtype   A tf.dtype identifies the type of labels. By default it is tf.int64. If user defines a label_vocabulary, this should be set as tf.string. tf.float32 labels are only supported for binary classification.  \n  label_default   used as label if label_key does not exist in given tf.Example. An example usage: let's say label_key is 'clicked' and tf.Example contains clicked data only for positive examples in following format key:clicked, value:1. This means that if there is no data with key 'clicked' it should count as negative example by setting label_deafault=0. Type of this value should be compatible with label_dtype.  \n  weight_column   A string or a NumericColumn created by tf.feature_column.numeric_column defining feature column representing weights. It is used to down weight or boost examples during training. It will be multiplied by the loss of the example. If it is a string, it is used as a key to fetch weight tensor from the features. If it is a NumericColumn, raw tensor is fetched by key weight_column.key, then weight_column.normalizer_fn is applied on it to get weight tensor.   \n \n\n\n Returns   A dict mapping each feature key to a FixedLenFeature or VarLenFeature value.  \n\n \n\n\n Raises\n  ValueError   If label is used in feature_columns.  \n  ValueError   If weight_column is used in feature_columns.  \n  ValueError   If any of the given feature_columns is not a _FeatureColumn instance.  \n  ValueError   If weight_column is not a NumericColumn instance.  \n  ValueError   if label_key is None.","title":"tensorflow.estimator.classifier_parse_example_spec"}]}
{"task_id":7243750,"prompt":"def f_7243750(url, file_name):\n\treturn ","suffix":"","canonical_solution":"urllib.request.urlretrieve(url, file_name)","test_start":"\nimport urllib \n\ndef check(candidate):","test":["\n    file_name = 'g.html'\n    candidate('https:\/\/asia.nikkei.com\/Business\/Tech\/Semiconductors\/U.S.-chip-tool-maker-Synopsys-expands-in-Vietnam-amid-China-tech-war', file_name)\n    with open (file_name, 'r') as f:\n        lines = f.readlines()\n        if len(lines) == 0: assert False\n        else: assert True\n"],"entry_point":"f_7243750","intent":"download the file from url `url` and save it under file `file_name`","library":["urllib"],"docs":[{"text":"winreg.SaveKey(key, file_name)  \nSaves the specified key, and all its subkeys to the specified file. key is an already open key, or one of the predefined HKEY_* constants. file_name is the name of the file to save registry data to. This file cannot already exist. If this filename includes an extension, it cannot be used on file allocation table (FAT) file systems by the LoadKey() method. If key represents a key on a remote computer, the path described by file_name is relative to the remote computer. The caller of this method must possess the SeBackupPrivilege security privilege. Note that privileges are different than permissions \u2013 see the Conflicts Between User Rights and Permissions documentation for more details. This function passes NULL for security_attributes to the API. Raises an auditing event winreg.SaveKey with arguments key, file_name.","title":"python.library.winreg#winreg.SaveKey"},{"text":"torch.hub.download_url_to_file(url, dst, hash_prefix=None, progress=True) [source]\n \nDownload object at the given URL to a local path.  Parameters \n \nurl (string) \u2013 URL of the object to download \ndst (string) \u2013 Full path where object will be saved, e.g. \/tmp\/temporary_file\n \nhash_prefix (string, optional) \u2013 If not None, the SHA256 downloaded file should start with hash_prefix. Default: None \nprogress (bool, optional) \u2013 whether or not to display a progress bar to stderr Default: True    Example >>> torch.hub.download_url_to_file('https:\/\/s3.amazonaws.com\/pytorch\/models\/resnet18-5c106cde.pth', '\/tmp\/temporary_file')","title":"torch.hub#torch.hub.download_url_to_file"},{"text":"Engine.select_template(template_name_list)  \nLike get_template(), except it takes a list of names and returns the first template that was found.","title":"django.ref.templates.api#django.template.Engine.select_template"},{"text":"select_template(template_name_list, using=None)  \nselect_template() is just like get_template(), except it takes a list of template names. It tries each name in order and returns the first template that exists.","title":"django.topics.templates#django.template.loader.select_template"},{"text":"tf.raw_ops.BatchMatrixInverse  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.raw_ops.BatchMatrixInverse  \ntf.raw_ops.BatchMatrixInverse(\n    input, adjoint=False, name=None\n)\n\n \n\n\n Args\n  input   A Tensor. Must be one of the following types: float64, float32.  \n  adjoint   An optional bool. Defaults to False.  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A Tensor. Has the same type as input.","title":"tensorflow.raw_ops.batchmatrixinverse"},{"text":"class torch.distributed.FileStore  \nA store implementation that uses a file to store the underlying key-value pairs.  Parameters \n \nfile_name (str) \u2013 path of the file in which to store the key-value pairs \nworld_size (int) \u2013 The total number of processes using the store     Example::\n\n>>> import torch.distributed as dist\n>>> store1 = dist.FileStore(\"\/tmp\/filestore\", 2)\n>>> store2 = dist.FileStore(\"\/tmp\/filestore\", 2)\n>>> # Use any of the store methods from either the client or server after initialization\n>>> store1.set(\"first_key\", \"first_value\")\n>>> store2.get(\"first_key\")","title":"torch.distributed#torch.distributed.FileStore"},{"text":"set_name(fontname)[source]\n \nAlias for set_fontname.","title":"matplotlib.text_api#matplotlib.text.Text.set_name"},{"text":"class sklearn.linear_model.GammaRegressor(*, alpha=1.0, fit_intercept=True, max_iter=100, tol=0.0001, warm_start=False, verbose=0) [source]\n \nGeneralized Linear Model with a Gamma distribution. Read more in the User Guide.  New in version 0.23.   Parameters \n \nalphafloat, default=1 \n\nConstant that multiplies the penalty term and thus determines the regularization strength. alpha = 0 is equivalent to unpenalized GLMs. In this case, the design matrix X must have full column rank (no collinearities).  \nfit_interceptbool, default=True \n\nSpecifies if a constant (a.k.a. bias or intercept) should be added to the linear predictor (X @ coef + intercept).  \nmax_iterint, default=100 \n\nThe maximal number of iterations for the solver.  \ntolfloat, default=1e-4 \n\nStopping criterion. For the lbfgs solver, the iteration will stop when max{|g_j|, j = 1, ..., d} <= tol where g_j is the j-th component of the gradient (derivative) of the objective function.  \nwarm_startbool, default=False \n\nIf set to True, reuse the solution of the previous call to fit as initialization for coef_ and intercept_ .  \nverboseint, default=0 \n\nFor the lbfgs solver set verbose to any positive number for verbosity.    Attributes \n \ncoef_array of shape (n_features,) \n\nEstimated coefficients for the linear predictor (X * coef_ +\nintercept_) in the GLM.  \nintercept_float \n\nIntercept (a.k.a. bias) added to linear predictor.  \nn_iter_int \n\nActual number of iterations used in the solver.     Examples >>> from sklearn import linear_model\n>>> clf = linear_model.GammaRegressor()\n>>> X = [[1, 2], [2, 3], [3, 4], [4, 3]]\n>>> y = [19, 26, 33, 30]\n>>> clf.fit(X, y)\nGammaRegressor()\n>>> clf.score(X, y)\n0.773...\n>>> clf.coef_\narray([0.072..., 0.066...])\n>>> clf.intercept_\n2.896...\n>>> clf.predict([[1, 0], [2, 8]])\narray([19.483..., 35.795...])\n Methods  \nfit(X, y[, sample_weight]) Fit a Generalized Linear Model.  \nget_params([deep]) Get parameters for this estimator.  \npredict(X) Predict using GLM with feature matrix X.  \nscore(X, y[, sample_weight]) Compute D^2, the percentage of deviance explained.  \nset_params(**params) Set the parameters of this estimator.    \nfit(X, y, sample_weight=None) [source]\n \nFit a Generalized Linear Model.  Parameters \n \nX{array-like, sparse matrix} of shape (n_samples, n_features) \n\nTraining data.  \nyarray-like of shape (n_samples,) \n\nTarget values.  \nsample_weightarray-like of shape (n_samples,), default=None \n\nSample weights.    Returns \n \nselfreturns an instance of self. \n   \n  \nget_params(deep=True) [source]\n \nGet parameters for this estimator.  Parameters \n \ndeepbool, default=True \n\nIf True, will return the parameters for this estimator and contained subobjects that are estimators.    Returns \n \nparamsdict \n\nParameter names mapped to their values.     \n  \npredict(X) [source]\n \nPredict using GLM with feature matrix X.  Parameters \n \nX{array-like, sparse matrix} of shape (n_samples, n_features) \n\nSamples.    Returns \n \ny_predarray of shape (n_samples,) \n\nReturns predicted values.     \n  \nscore(X, y, sample_weight=None) [source]\n \nCompute D^2, the percentage of deviance explained. D^2 is a generalization of the coefficient of determination R^2. R^2 uses squared error and D^2 deviance. Note that those two are equal for family='normal'. D^2 is defined as \\(D^2 = 1-\\frac{D(y_{true},y_{pred})}{D_{null}}\\), \\(D_{null}\\) is the null deviance, i.e. the deviance of a model with intercept alone, which corresponds to \\(y_{pred} = \\bar{y}\\). The mean \\(\\bar{y}\\) is averaged by sample_weight. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse).  Parameters \n \nX{array-like, sparse matrix} of shape (n_samples, n_features) \n\nTest samples.  \nyarray-like of shape (n_samples,) \n\nTrue values of target.  \nsample_weightarray-like of shape (n_samples,), default=None \n\nSample weights.    Returns \n \nscorefloat \n\nD^2 of self.predict(X) w.r.t. y.     \n  \nset_params(**params) [source]\n \nSet the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as Pipeline). The latter have parameters of the form <component>__<parameter> so that it\u2019s possible to update each component of a nested object.  Parameters \n \n**paramsdict \n\nEstimator parameters.    Returns \n \nselfestimator instance \n\nEstimator instance.","title":"sklearn.modules.generated.sklearn.linear_model.gammaregressor#sklearn.linear_model.GammaRegressor"},{"text":"winreg.LoadKey(key, sub_key, file_name)  \nCreates a subkey under the specified key and stores registration information from a specified file into that subkey. key is a handle returned by ConnectRegistry() or one of the constants HKEY_USERS or HKEY_LOCAL_MACHINE. sub_key is a string that identifies the subkey to load. file_name is the name of the file to load registry data from. This file must have been created with the SaveKey() function. Under the file allocation table (FAT) file system, the filename may not have an extension. A call to LoadKey() fails if the calling process does not have the SE_RESTORE_PRIVILEGE privilege. Note that privileges are different from permissions \u2013 see the RegLoadKey documentation for more details. If key is a handle returned by ConnectRegistry(), then the path specified in file_name is relative to the remote computer. Raises an auditing event winreg.LoadKey with arguments key, sub_key, file_name.","title":"python.library.winreg#winreg.LoadKey"},{"text":"sklearn.linear_model.GammaRegressor  \nclass sklearn.linear_model.GammaRegressor(*, alpha=1.0, fit_intercept=True, max_iter=100, tol=0.0001, warm_start=False, verbose=0) [source]\n \nGeneralized Linear Model with a Gamma distribution. Read more in the User Guide.  New in version 0.23.   Parameters \n \nalphafloat, default=1 \n\nConstant that multiplies the penalty term and thus determines the regularization strength. alpha = 0 is equivalent to unpenalized GLMs. In this case, the design matrix X must have full column rank (no collinearities).  \nfit_interceptbool, default=True \n\nSpecifies if a constant (a.k.a. bias or intercept) should be added to the linear predictor (X @ coef + intercept).  \nmax_iterint, default=100 \n\nThe maximal number of iterations for the solver.  \ntolfloat, default=1e-4 \n\nStopping criterion. For the lbfgs solver, the iteration will stop when max{|g_j|, j = 1, ..., d} <= tol where g_j is the j-th component of the gradient (derivative) of the objective function.  \nwarm_startbool, default=False \n\nIf set to True, reuse the solution of the previous call to fit as initialization for coef_ and intercept_ .  \nverboseint, default=0 \n\nFor the lbfgs solver set verbose to any positive number for verbosity.    Attributes \n \ncoef_array of shape (n_features,) \n\nEstimated coefficients for the linear predictor (X * coef_ +\nintercept_) in the GLM.  \nintercept_float \n\nIntercept (a.k.a. bias) added to linear predictor.  \nn_iter_int \n\nActual number of iterations used in the solver.     Examples >>> from sklearn import linear_model\n>>> clf = linear_model.GammaRegressor()\n>>> X = [[1, 2], [2, 3], [3, 4], [4, 3]]\n>>> y = [19, 26, 33, 30]\n>>> clf.fit(X, y)\nGammaRegressor()\n>>> clf.score(X, y)\n0.773...\n>>> clf.coef_\narray([0.072..., 0.066...])\n>>> clf.intercept_\n2.896...\n>>> clf.predict([[1, 0], [2, 8]])\narray([19.483..., 35.795...])\n Methods  \nfit(X, y[, sample_weight]) Fit a Generalized Linear Model.  \nget_params([deep]) Get parameters for this estimator.  \npredict(X) Predict using GLM with feature matrix X.  \nscore(X, y[, sample_weight]) Compute D^2, the percentage of deviance explained.  \nset_params(**params) Set the parameters of this estimator.    \nfit(X, y, sample_weight=None) [source]\n \nFit a Generalized Linear Model.  Parameters \n \nX{array-like, sparse matrix} of shape (n_samples, n_features) \n\nTraining data.  \nyarray-like of shape (n_samples,) \n\nTarget values.  \nsample_weightarray-like of shape (n_samples,), default=None \n\nSample weights.    Returns \n \nselfreturns an instance of self. \n   \n  \nget_params(deep=True) [source]\n \nGet parameters for this estimator.  Parameters \n \ndeepbool, default=True \n\nIf True, will return the parameters for this estimator and contained subobjects that are estimators.    Returns \n \nparamsdict \n\nParameter names mapped to their values.     \n  \npredict(X) [source]\n \nPredict using GLM with feature matrix X.  Parameters \n \nX{array-like, sparse matrix} of shape (n_samples, n_features) \n\nSamples.    Returns \n \ny_predarray of shape (n_samples,) \n\nReturns predicted values.     \n  \nscore(X, y, sample_weight=None) [source]\n \nCompute D^2, the percentage of deviance explained. D^2 is a generalization of the coefficient of determination R^2. R^2 uses squared error and D^2 deviance. Note that those two are equal for family='normal'. D^2 is defined as \\(D^2 = 1-\\frac{D(y_{true},y_{pred})}{D_{null}}\\), \\(D_{null}\\) is the null deviance, i.e. the deviance of a model with intercept alone, which corresponds to \\(y_{pred} = \\bar{y}\\). The mean \\(\\bar{y}\\) is averaged by sample_weight. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse).  Parameters \n \nX{array-like, sparse matrix} of shape (n_samples, n_features) \n\nTest samples.  \nyarray-like of shape (n_samples,) \n\nTrue values of target.  \nsample_weightarray-like of shape (n_samples,), default=None \n\nSample weights.    Returns \n \nscorefloat \n\nD^2 of self.predict(X) w.r.t. y.     \n  \nset_params(**params) [source]\n \nSet the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as Pipeline). The latter have parameters of the form <component>__<parameter> so that it\u2019s possible to update each component of a nested object.  Parameters \n \n**paramsdict \n\nEstimator parameters.    Returns \n \nselfestimator instance \n\nEstimator instance.     \n \n Examples using sklearn.linear_model.GammaRegressor\n \n  Release Highlights for scikit-learn 0.23  \n\n  Tweedie regression on insurance claims","title":"sklearn.modules.generated.sklearn.linear_model.gammaregressor"}]}
{"task_id":743806,"prompt":"def f_743806(text):\n\treturn ","suffix":"","canonical_solution":"text.split()","test_start":"\ndef check(candidate):","test":["\n    assert candidate('The quick brown fox') == ['The', 'quick', 'brown', 'fox']\n","\n    assert candidate('hello!') == ['hello!']\n","\n    assert candidate('hello world !') == ['hello', 'world', '!']\n"],"entry_point":"f_743806","intent":"split string `text` by space","library":[],"docs":[]}
{"task_id":743806,"prompt":"def f_743806(text):\n\treturn ","suffix":"","canonical_solution":"text.split(',')","test_start":"\ndef check(candidate):","test":["\n    assert candidate('The quick brown fox') == ['The quick brown fox']\n","\n    assert candidate('The,quick,brown,fox') == ['The', 'quick', 'brown', 'fox']\n"],"entry_point":"f_743806","intent":"split string `text` by \",\"","library":[],"docs":[]}
{"task_id":743806,"prompt":"def f_743806(line):\n\treturn ","suffix":"","canonical_solution":"line.split()","test_start":"\ndef check(candidate):","test":["\n    assert candidate('The  quick brown  fox') == ['The', 'quick', 'brown', 'fox']\n"],"entry_point":"f_743806","intent":"Split string `line` into a list by whitespace","library":[],"docs":[]}
{"task_id":35044115,"prompt":"def f_35044115(s):\n\treturn ","suffix":"","canonical_solution":"[re.sub('(?<!\\\\d)\\\\.(?!\\\\d)', ' ', i) for i in s]","test_start":"\nimport re \n\ndef check(candidate):","test":["\n    assert candidate('h.j.k') == ['h', ' ', 'j', ' ', 'k']\n"],"entry_point":"f_35044115","intent":"replace dot characters  '.' associated with ascii letters in list `s` with space ' '","library":["re"],"docs":[{"text":"re.S  \nre.DOTALL  \nMake the '.' special character match any character at all, including a newline; without this flag, '.' will match anything except a newline. Corresponds to the inline flag (?s).","title":"python.library.re#re.DOTALL"},{"text":"re.S  \nre.DOTALL  \nMake the '.' special character match any character at all, including a newline; without this flag, '.' will match anything except a newline. Corresponds to the inline flag (?s).","title":"python.library.re#re.S"},{"text":"token.DOT  \nToken value for \".\".","title":"python.library.token#token.DOT"},{"text":"DefaultCookiePolicy.DomainStrictNoDots  \nWhen setting cookies, the \u2018host prefix\u2019 must not contain a dot (eg. www.foo.bar.com can\u2019t set a cookie for .bar.com, because www.foo contains a dot).","title":"python.library.http.cookiejar#http.cookiejar.DefaultCookiePolicy.DomainStrictNoDots"},{"text":"tf.keras.layers.Dot     View source on GitHub    Layer that computes a dot product between samples in two tensors. Inherits From: Layer, Module  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.keras.layers.Dot  \ntf.keras.layers.Dot(\n    axes, normalize=False, **kwargs\n)\n E.g. if applied to a list of two tensors a and b of shape (batch_size, n), the output will be a tensor of shape (batch_size, 1) where each entry i will be the dot product between a[i] and b[i]. \nx = np.arange(10).reshape(1, 5, 2)\nprint(x)\n[[[0 1]\n  [2 3]\n  [4 5]\n  [6 7]\n  [8 9]]]\ny = np.arange(10, 20).reshape(1, 2, 5)\nprint(y)\n[[[10 11 12 13 14]\n  [15 16 17 18 19]]]\ntf.keras.layers.Dot(axes=(1, 2))([x, y])\n<tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy=\narray([[[260, 360],\n        [320, 445]]])>\n \nx1 = tf.keras.layers.Dense(8)(np.arange(10).reshape(5, 2))\nx2 = tf.keras.layers.Dense(8)(np.arange(10, 20).reshape(5, 2))\ndotted = tf.keras.layers.Dot(axes=1)([x1, x2])\ndotted.shape\nTensorShape([5, 1])\n\n \n\n\n Arguments\n  axes   Integer or tuple of integers, axis or axes along which to take the dot product. If a tuple, should be two integers corresponding to the desired axis from the first input and the desired axis from the second input, respectively. Note that the size of the two selected axes must match.  \n  normalize   Whether to L2-normalize samples along the dot product axis before taking the dot product. If set to True, then the output of the dot product is the cosine proximity between the two samples.  \n  **kwargs   Standard layer keyword arguments.","title":"tensorflow.keras.layers.dot"},{"text":"matplotlib.fontconfig_pattern.family_escape(\/, repl, string, count=0)\n \nReturn the string obtained by replacing the leftmost non-overlapping occurrences of pattern in string by the replacement repl.","title":"matplotlib.fontconfig_pattern_api#matplotlib.fontconfig_pattern.family_escape"},{"text":"numpy.distutils.misc_util.minrelpath(path)[source]\n \nResolve  and \u2018.\u2019 from path.","title":"numpy.reference.distutils.misc_util#numpy.distutils.misc_util.minrelpath"},{"text":"app_context_processor(f)  \nLike Flask.context_processor() but for a blueprint. Such a function is executed each request, even if outside of the blueprint.  Parameters \nf (Callable[[], Dict[str, Any]]) \u2013   Return type \nCallable[[], Dict[str, Any]]","title":"flask.api.index#flask.Blueprint.app_context_processor"},{"text":"re.A  \nre.ASCII  \nMake \\w, \\W, \\b, \\B, \\d, \\D, \\s and \\S perform ASCII-only matching instead of full Unicode matching. This is only meaningful for Unicode patterns, and is ignored for byte patterns. Corresponds to the inline flag (?a). Note that for backward compatibility, the re.U flag still exists (as well as its synonym re.UNICODE and its embedded counterpart (?u)), but these are redundant in Python 3 since matches are Unicode by default for strings (and Unicode matching isn\u2019t allowed for bytes).","title":"python.library.re#re.ASCII"},{"text":"doctest.script_from_examples(s)  \nConvert text with examples to a script. Argument s is a string containing doctest examples. The string is converted to a Python script, where doctest examples in s are converted to regular code, and everything else is converted to Python comments. The generated script is returned as a string. For example, import doctest\nprint(doctest.script_from_examples(r\"\"\"\n    Set x and y to 1 and 2.\n    >>> x, y = 1, 2\n\n    Print their sum:\n    >>> print(x+y)\n    3\n\"\"\"))\n displays: # Set x and y to 1 and 2.\nx, y = 1, 2\n#\n# Print their sum:\nprint(x+y)\n# Expected:\n## 3\n This function is used internally by other functions (see below), but can also be useful when you want to transform an interactive Python session into a Python script.","title":"python.library.doctest#doctest.script_from_examples"}]}
{"task_id":38388799,"prompt":"def f_38388799(list_of_strings):\n\treturn ","suffix":"","canonical_solution":"sorted(list_of_strings, key=lambda s: s.split(',')[1])","test_start":"\ndef check(candidate):","test":["\n    assert candidate(['parrot, medicine', 'abott, kangaroo', 'sriracha, coriander', 'phone, bottle']) == ['phone, bottle', 'sriracha, coriander', 'abott, kangaroo', 'parrot, medicine']\n","\n    assert candidate(['abott, kangaroo', 'parrot, medicine', 'sriracha, coriander', 'phone, bottle']) == ['phone, bottle', 'sriracha, coriander', 'abott, kangaroo', 'parrot, medicine']\n"],"entry_point":"f_38388799","intent":"sort list `list_of_strings` based on second index of each string `s`","library":[],"docs":[]}
{"task_id":37004138,"prompt":"def f_37004138(lst):\n\treturn ","suffix":"","canonical_solution":"[element for element in lst if isinstance(element, int)]","test_start":"\ndef check(candidate):","test":["\n    lst = [1, \"hello\", \"string\", 2, 4.46]\n    assert candidate(lst) == [1, 2]\n","\n    lst = [\"hello\", \"string\"]\n    assert candidate(lst) == []\n"],"entry_point":"f_37004138","intent":"eliminate non-integer items from list `lst`","library":[],"docs":[]}
{"task_id":37004138,"prompt":"def f_37004138(lst):\n\treturn ","suffix":"","canonical_solution":"[element for element in lst if not isinstance(element, str)]","test_start":"\ndef check(candidate):","test":["\n    lst = [1, \"hello\", \"string\", 2, 4.46]\n    assert candidate(lst) == [1, 2, 4.46]\n","\n    lst = [\"hello\", \"string\"]\n    assert candidate(lst) == []\n"],"entry_point":"f_37004138","intent":"get all the elements except strings from the list 'lst'.","library":[],"docs":[]}
{"task_id":72899,"prompt":"def f_72899(list_to_be_sorted):\n\treturn ","suffix":"","canonical_solution":"sorted(list_to_be_sorted, key=lambda k: k['name'])","test_start":"\ndef check(candidate):","test":["\n    list_to_be_sorted = [{'name': 'Homer', 'age': 39}, {'name': 'Bart', 'age': 10}]\n    assert candidate(list_to_be_sorted) == [{'name': 'Bart', 'age': 10}, {'name': 'Homer', 'age': 39}]\n","\n    list_to_be_sorted = [{'name': 'ABCD'}, {'name': 'AABCD'}]\n    assert candidate(list_to_be_sorted) == [{'name': 'AABCD'}, {'name': 'ABCD'}]\n"],"entry_point":"f_72899","intent":"Sort a list of dictionaries `list_to_be_sorted` by the value of the dictionary key `name`","library":[],"docs":[]}
{"task_id":72899,"prompt":"def f_72899(l):\n\treturn ","suffix":"","canonical_solution":"sorted(l, key=itemgetter('name'), reverse=True)","test_start":"\nfrom operator import itemgetter\n\ndef check(candidate):","test":["\n    list_to_be_sorted = [{'name': 'Homer', 'age': 39}, {'name': 'Bart', 'age': 10}]\n    assert candidate(list_to_be_sorted) == [{'name': 'Homer', 'age': 39}, {'name': 'Bart', 'age': 10}]\n","\n    list_to_be_sorted = [{'name': 'ABCD'}, {'name': 'AABCD'}]\n    assert candidate(list_to_be_sorted) == [{'name': 'ABCD'}, {'name': 'AABCD'}]\n"],"entry_point":"f_72899","intent":"sort a list of dictionaries `l` by values in key `name` in descending order","library":["operator"],"docs":[{"text":"numpy.distutils.ccompiler_opt.CCompilerOpt.feature_sorted method   distutils.ccompiler_opt.CCompilerOpt.feature_sorted(names, reverse=False)[source]\n \nSort a list of CPU features ordered by the lowest interest.  Parameters \n \u2018names\u2019: sequence\n\nsequence of supported feature names in uppercase.  \u2018reverse\u2019: bool, optional\n\nIf true, the sorted features is reversed. (highest interest)    Returns \n list, sorted CPU features","title":"numpy.reference.generated.numpy.distutils.ccompiler_opt.ccompileropt.feature_sorted"},{"text":"get_all(name)  \nReturn a list of all the values for the named header. The returned list will be sorted in the order they appeared in the original header list or were added to this instance, and may contain duplicates. Any fields deleted and re-inserted are always appended to the header list. If no fields exist with the given name, returns an empty list.","title":"python.library.wsgiref#wsgiref.headers.Headers.get_all"},{"text":"sort(*, key=None, reverse=False)  \nThis method sorts the list in place, using only < comparisons between items. Exceptions are not suppressed - if any comparison operations fail, the entire sort operation will fail (and the list will likely be left in a partially modified state). sort() accepts two arguments that can only be passed by keyword (keyword-only arguments): key specifies a function of one argument that is used to extract a comparison key from each list element (for example, key=str.lower). The key corresponding to each item in the list is calculated once and then used for the entire sorting process. The default value of None means that list items are sorted directly without calculating a separate key value. The functools.cmp_to_key() utility is available to convert a 2.x style cmp function to a key function. reverse is a boolean value. If set to True, then the list elements are sorted as if each comparison were reversed. This method modifies the sequence in place for economy of space when sorting a large sequence. To remind users that it operates by side effect, it does not return the sorted sequence (use sorted() to explicitly request a new sorted list instance). The sort() method is guaranteed to be stable. A sort is stable if it guarantees not to change the relative order of elements that compare equal \u2014 this is helpful for sorting in multiple passes (for example, sort by department, then by salary grade). For sorting examples and a brief sorting tutorial, see Sorting HOW TO.  CPython implementation detail: While a list is being sorted, the effect of attempting to mutate, or even inspect, the list is undefined. The C implementation of Python makes the list appear empty for the duration, and raises ValueError if it can detect that the list has been mutated during a sort.","title":"python.library.stdtypes#list.sort"},{"text":"get_all(name, failobj=None)  \nReturn a list of all the values for the field named name. If there are no such named headers in the message, failobj is returned (defaults to None).","title":"python.library.email.compat32-message#email.message.Message.get_all"},{"text":"get_all(name, failobj=None)  \nReturn a list of all the values for the field named name. If there are no such named headers in the message, failobj is returned (defaults to None).","title":"python.library.email.message#email.message.EmailMessage.get_all"},{"text":"pkgutil.extend_path(path, name)  \nExtend the search path for the modules which comprise a package. Intended use is to place the following code in a package\u2019s __init__.py: from pkgutil import extend_path\n__path__ = extend_path(__path__, __name__)\n This will add to the package\u2019s __path__ all subdirectories of directories on sys.path named after the package. This is useful if one wants to distribute different parts of a single logical package as multiple directories. It also looks for *.pkg files beginning where * matches the name argument. This feature is similar to *.pth files (see the site module for more information), except that it doesn\u2019t special-case lines starting with import. A *.pkg file is trusted at face value: apart from checking for duplicates, all entries found in a *.pkg file are added to the path, regardless of whether they exist on the filesystem. (This is a feature.) If the input path is not a list (as is the case for frozen packages) it is returned unchanged. The input path is not modified; an extended copy is returned. Items are only appended to the copy at the end. It is assumed that sys.path is a sequence. Items of sys.path that are not strings referring to existing directories are ignored. Unicode items on sys.path that cause errors when used as filenames may cause this function to raise an exception (in line with os.path.isdir() behavior).","title":"python.library.pkgutil#pkgutil.extend_path"},{"text":"SSLContext.set_ecdh_curve(curve_name)  \nSet the curve name for Elliptic Curve-based Diffie-Hellman (ECDH) key exchange. ECDH is significantly faster than regular DH while arguably as secure. The curve_name parameter should be a string describing a well-known elliptic curve, for example prime256v1 for a widely supported curve. This setting doesn\u2019t apply to client sockets. You can also use the OP_SINGLE_ECDH_USE option to further improve security. This method is not available if HAS_ECDH is False.  New in version 3.3.   See also  SSL\/TLS & Perfect Forward Secrecy\n\nVincent Bernat.","title":"python.library.ssl#ssl.SSLContext.set_ecdh_curve"},{"text":"FieldStorage.getlist(name)  \nThis method always returns a list of values associated with form field name. The method returns an empty list if no such form field or value exists for name. It returns a list consisting of one item if only one such value exists.","title":"python.library.cgi#cgi.FieldStorage.getlist"},{"text":"codecs.register_error(name, error_handler)  \nRegister the error handling function error_handler under the name name. The error_handler argument will be called during encoding and decoding in case of an error, when name is specified as the errors parameter. For encoding, error_handler will be called with a UnicodeEncodeError instance, which contains information about the location of the error. The error handler must either raise this or a different exception, or return a tuple with a replacement for the unencodable part of the input and a position where encoding should continue. The replacement may be either str or bytes. If the replacement is bytes, the encoder will simply copy them into the output buffer. If the replacement is a string, the encoder will encode the replacement. Encoding continues on original input at the specified position. Negative position values will be treated as being relative to the end of the input string. If the resulting position is out of bound an IndexError will be raised. Decoding and translating works similarly, except UnicodeDecodeError or UnicodeTranslateError will be passed to the handler and that the replacement from the error handler will be put into the output directly.","title":"python.library.codecs#codecs.register_error"},{"text":"get_width_from_char_name(name)[source]\n \nGet the width of the character from a type1 character name.","title":"matplotlib.afm_api#matplotlib.afm.AFM.get_width_from_char_name"}]}
{"task_id":72899,"prompt":"def f_72899(list_of_dicts):\n\t","suffix":"\n\treturn list_of_dicts","canonical_solution":"list_of_dicts.sort(key=operator.itemgetter('name'))","test_start":"\nimport operator\n\ndef check(candidate):","test":["\n    list_to_be_sorted = [{'name': 'Homer', 'age': 39}, {'name': 'Bart', 'age': 10}]\n    assert candidate(list_to_be_sorted) == [{'name': 'Bart', 'age': 10}, {'name': 'Homer', 'age': 39}]\n","\n    list_to_be_sorted = [{'name': 'ABCD'}, {'name': 'AABCD'}]\n    assert candidate(list_to_be_sorted) == [{'name': 'AABCD'}, {'name': 'ABCD'}]\n"],"entry_point":"f_72899","intent":"sort a list of dictionaries `list_of_dicts` by `name` values of the dictionary","library":["operator"],"docs":[{"text":"sortTestMethodsUsing  \nFunction to be used to compare method names when sorting them in getTestCaseNames() and all the loadTestsFrom*() methods.","title":"python.library.unittest#unittest.TestLoader.sortTestMethodsUsing"},{"text":"JSON_SORT_KEYS  \nSort the keys of JSON objects alphabetically. This is useful for caching because it ensures the data is serialized the same way no matter what Python\u2019s hash seed is. While not recommended, you can disable this for a possible performance improvement at the cost of caching. Default: True","title":"flask.config.index#JSON_SORT_KEYS"},{"text":"class werkzeug.datastructures.CombinedMultiDict(dicts=None)  \nA read only MultiDict that you can pass multiple MultiDict instances as sequence and it will combine the return values of all wrapped dicts: >>> from werkzeug.datastructures import CombinedMultiDict, MultiDict\n>>> post = MultiDict([('foo', 'bar')])\n>>> get = MultiDict([('blub', 'blah')])\n>>> combined = CombinedMultiDict([get, post])\n>>> combined['foo']\n'bar'\n>>> combined['blub']\n'blah'\n This works for all read operations and will raise a TypeError for methods that usually change data which isn\u2019t possible. From Werkzeug 0.3 onwards, the KeyError raised by this class is also a subclass of the BadRequest HTTP exception and will render a page for a 400 BAD REQUEST if caught in a catch-all for HTTP exceptions.","title":"werkzeug.datastructures.index#werkzeug.datastructures.CombinedMultiDict"},{"text":"loadTestsFromNames(names, module=None)  \nSimilar to loadTestsFromName(), but takes a sequence of names rather than a single name. The return value is a test suite which supports all the tests defined for each name.","title":"python.library.unittest#unittest.TestLoader.loadTestsFromNames"},{"text":"pprint.pp(object, *args, sort_dicts=False, **kwargs)  \nPrints the formatted representation of object followed by a newline. If sort_dicts is false (the default), dictionaries will be displayed with their keys in insertion order, otherwise the dict keys will be sorted. args and kwargs will be passed to pprint() as formatting parameters.  New in version 3.8.","title":"python.library.pprint#pprint.pp"},{"text":"get_all(name)  \nReturn a list of all the values for the named header. The returned list will be sorted in the order they appeared in the original header list or were added to this instance, and may contain duplicates. Any fields deleted and re-inserted are always appended to the header list. If no fields exist with the given name, returns an empty list.","title":"python.library.wsgiref#wsgiref.headers.Headers.get_all"},{"text":"get_all(name, failobj=None)  \nReturn a list of all the values for the field named name. If there are no such named headers in the message, failobj is returned (defaults to None).","title":"python.library.email.message#email.message.EmailMessage.get_all"},{"text":"addTests(tests)  \nAdd all the tests from an iterable of TestCase and TestSuite instances to this test suite. This is equivalent to iterating over tests, calling addTest() for each element.","title":"python.library.unittest#unittest.TestSuite.addTests"},{"text":"test.support.sortdict(dict)  \nReturn a repr of dict with keys sorted.","title":"python.library.test#test.support.sortdict"},{"text":"get_all(name, failobj=None)  \nReturn a list of all the values for the field named name. If there are no such named headers in the message, failobj is returned (defaults to None).","title":"python.library.email.compat32-message#email.message.Message.get_all"}]}
{"task_id":72899,"prompt":"def f_72899(list_of_dicts):\n\t","suffix":"\n\treturn list_of_dicts","canonical_solution":"list_of_dicts.sort(key=operator.itemgetter('age'))","test_start":"\nimport operator\n\ndef check(candidate):","test":["\n    list_to_be_sorted = [{'name': 'Homer', 'age': 39}, {'name': 'Bart', 'age': 10}]\n    assert candidate(list_to_be_sorted) == [{'name': 'Bart', 'age': 10}, {'name': 'Homer', 'age': 39}]\n","\n    list_to_be_sorted = [{'name': 'ABCD', 'age': 10}, {'name': 'AABCD', 'age': 9}]\n    assert candidate(list_to_be_sorted) == [{'name': 'AABCD', 'age': 9}, {'name': 'ABCD', 'age': 10}]\n"],"entry_point":"f_72899","intent":"sort a list of dictionaries `list_of_dicts` by `age` values of the dictionary","library":["operator"],"docs":[{"text":"sort_stats(*keys)  \nThis method modifies the Stats object by sorting it according to the supplied criteria. The argument can be either a string or a SortKey enum identifying the basis of a sort (example: 'time', 'name', SortKey.TIME or SortKey.NAME). The SortKey enums argument have advantage over the string argument in that it is more robust and less error prone. When more than one key is provided, then additional keys are used as secondary criteria when there is equality in all keys selected before them. For example, sort_stats(SortKey.NAME, SortKey.FILE) will sort all the entries according to their function name, and resolve all ties (identical function names) by sorting by file name. For the string argument, abbreviations can be used for any key names, as long as the abbreviation is unambiguous. The following are the valid string and SortKey:   \nValid String Arg Valid enum Arg Meaning   \n'calls' SortKey.CALLS call count  \n'cumulative' SortKey.CUMULATIVE cumulative time  \n'cumtime' N\/A cumulative time  \n'file' N\/A file name  \n'filename' SortKey.FILENAME file name  \n'module' N\/A file name  \n'ncalls' N\/A call count  \n'pcalls' SortKey.PCALLS primitive call count  \n'line' SortKey.LINE line number  \n'name' SortKey.NAME function name  \n'nfl' SortKey.NFL name\/file\/line  \n'stdname' SortKey.STDNAME standard name  \n'time' SortKey.TIME internal time  \n'tottime' N\/A internal time   Note that all sorts on statistics are in descending order (placing most time consuming items first), where as name, file, and line number searches are in ascending order (alphabetical). The subtle distinction between SortKey.NFL and SortKey.STDNAME is that the standard name is a sort of the name as printed, which means that the embedded line numbers get compared in an odd way. For example, lines 3, 20, and 40 would (if the file names were the same) appear in the string order 20, 3 and 40. In contrast, SortKey.NFL does a numeric compare of the line numbers. In fact, sort_stats(SortKey.NFL) is the same as sort_stats(SortKey.NAME, SortKey.FILENAME, SortKey.LINE). For backward-compatibility reasons, the numeric arguments -1, 0, 1, and 2 are permitted. They are interpreted as 'stdname', 'calls', 'time', and 'cumulative' respectively. If this old style format (numeric) is used, only one sort key (the numeric key) will be used, and additional arguments will be silently ignored.  New in version 3.7: Added the SortKey enum.","title":"python.library.profile#pstats.Stats.sort_stats"},{"text":"set_3d_properties(path, zs=0, zdir='z')[source]","title":"matplotlib._as_gen.mpl_toolkits.mplot3d.art3d.pathpatch3d#mpl_toolkits.mplot3d.art3d.PathPatch3D.set_3d_properties"},{"text":"numpy.sign   numpy.sign(x, \/, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj]) = <ufunc 'sign'>\n \nReturns an element-wise indication of the sign of a number. The sign function returns -1 if x < 0, 0 if x==0, 1 if x > 0. nan is returned for nan inputs. For complex inputs, the sign function returns sign(x.real) + 0j if x.real != 0 else sign(x.imag) + 0j. complex(nan, 0) is returned for complex nan inputs.  Parameters \n \nxarray_like\n\n\nInput values.  \noutndarray, None, or tuple of ndarray and None, optional\n\n\nA location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.  \nwherearray_like, optional\n\n\nThis condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.  **kwargs\n\nFor other keyword-only arguments, see the ufunc docs.    Returns \n \nyndarray\n\n\nThe sign of x. This is a scalar if x is a scalar.     Notes There is more than one definition of sign in common use for complex numbers. The definition used here is equivalent to \\(x\/\\sqrt{x*x}\\) which is different from a common alternative, \\(x\/|x|\\). Examples >>> np.sign([-5., 4.5])\narray([-1.,  1.])\n>>> np.sign(0)\n0\n>>> np.sign(5-2j)\n(1+0j)","title":"numpy.reference.generated.numpy.sign"},{"text":"class werkzeug.datastructures.CombinedMultiDict(dicts=None)  \nA read only MultiDict that you can pass multiple MultiDict instances as sequence and it will combine the return values of all wrapped dicts: >>> from werkzeug.datastructures import CombinedMultiDict, MultiDict\n>>> post = MultiDict([('foo', 'bar')])\n>>> get = MultiDict([('blub', 'blah')])\n>>> combined = CombinedMultiDict([get, post])\n>>> combined['foo']\n'bar'\n>>> combined['blub']\n'blah'\n This works for all read operations and will raise a TypeError for methods that usually change data which isn\u2019t possible. From Werkzeug 0.3 onwards, the KeyError raised by this class is also a subclass of the BadRequest HTTP exception and will render a page for a 400 BAD REQUEST if caught in a catch-all for HTTP exceptions.","title":"werkzeug.datastructures.index#werkzeug.datastructures.CombinedMultiDict"},{"text":"pygame.image\n  \n pygame module for image transfer  The image module contains functions for loading and saving pictures, as well as transferring Surfaces to formats usable by other packages. Note that there is no Image class; an image is loaded as a Surface object. The Surface class allows manipulation (drawing lines, setting pixels, capturing regions, etc.). The image module is a required dependency of pygame, but it only optionally supports any extended file formats. By default it can only load uncompressed BMP images. When built with full image support, the pygame.image.load() function can support the following formats.  \n JPG PNG \nGIF (non-animated) BMP PCX \nTGA (uncompressed) TIF \nLBM (and PBM) \nPBM (and PGM, PPM) XPM  \n Saving images only supports a limited set of formats. You can save to the following formats.  \n BMP TGA PNG JPEG  \n JPEG and JPG refer to the same file format  New in pygame 1.8: Saving PNG and JPEG files.    pygame.image.load_basic() \n load new BMP image from a file (or file-like object) load_basic(file) -> Surface  Load an image from a file source. You can pass either a filename or a Python file-like object. This function only supports loading \"basic\" image format, ie BMP format. This function is always available, no matter how pygame was built. \n   pygame.image.load() \n load new image from a file (or file-like object) load(filename) -> Surface load(fileobj, namehint=\"\") -> Surface  Load an image from a file source. You can pass either a filename or a Python file-like object. Pygame will automatically determine the image type (e.g., GIF or bitmap) and create a new Surface object from the data. In some cases it will need to know the file extension (e.g., GIF images should end in \".gif\"). If you pass a raw file-like object, you may also want to pass the original filename as the namehint argument. The returned Surface will contain the same color format, colorkey and alpha transparency as the file it came from. You will often want to call Surface.convert() with no arguments, to create a copy that will draw more quickly on the screen. For alpha transparency, like in .png images, use the convert_alpha() method after loading so that the image has per pixel transparency. pygame may not always be built to support all image formats. At minimum it will support uncompressed BMP. If pygame.image.get_extended() returns 'True', you should be able to load most images (including PNG, JPG and GIF). You should use os.path.join() for compatibility. eg. asurf = pygame.image.load(os.path.join('data', 'bla.png')) \n   pygame.image.load_extended() \n load an image from a file (or file-like object) load_extended(filename) -> Surface load_extended(fileobj, namehint=\"\") -> Surface  This function is similar to pygame.image.load(), except that this function can only be used if pygame was built with extended image format support. From version 2.0.1, this function is always available, but raises an error if extended image formats are not supported. Previously, this function may or may not be available, depending on the state of extended image format support.  Changed in pygame 2.0.1.  \n   pygame.image.save() \n save an image to file (or file-like object) save(Surface, filename) -> None save(Surface, fileobj, namehint=\"\") -> None  This will save your Surface as either a BMP, TGA, PNG, or JPEG image. If the filename extension is unrecognized it will default to TGA. Both TGA, and BMP file formats create uncompressed files. You can pass a filename or a Python file-like object. For file-like object, the image is saved to TGA format unless a namehint with a recognizable extension is passed in.  Note To be able to save the JPEG file format to a file-like object, SDL2_Image version 2.0.2 or newer is needed.   Note When saving to a file-like object, it seems that for most formats, the object needs to be flushed after saving to it to make loading from it possible.   Changed in pygame 1.8: Saving PNG and JPEG files.   Changed in pygame 2.0.0.dev11: The namehint parameter was added to make it possible to save other formats than TGA to a file-like object.  \n   pygame.image.save_extended() \n save a png\/jpg image to file (or file-like object) save_extended(Surface, filename) -> None save_extended(Surface, fileobj, namehint=\"\") -> None  This will save your Surface as either a PNG or JPEG image. Incase the image is being saved to a file-like object, this function uses the namehint argument to determine the format of the file being saved. Saves to JPEG incase the namehint was not specified while saving to file-like object. From version 2.0.1, this function is always available, but raises an error if extended image formats are not supported. Previously, this function may or may not be available, depending on the state of extended image format support.  Changed in pygame 2.0.1.  \n   pygame.image.get_sdl_image_version() \n get version number of the SDL_Image library being used get_sdl_image_version() -> None get_sdl_image_version() -> (major, minor, patch)  If pygame is built with extended image formats, then this function will return the SDL_Image library's version number as a tuple of 3 integers (major, minor, patch). If not, then it will return None.  New in pygame 2.0.0.dev11.  \n   pygame.image.get_extended() \n test if extended image formats can be loaded get_extended() -> bool  If pygame is built with extended image formats this function will return True. It is still not possible to determine which formats will be available, but generally you will be able to load them all. \n   pygame.image.tostring() \n transfer image to string buffer tostring(Surface, format, flipped=False) -> string  Creates a string that can be transferred with the 'fromstring' method in other Python imaging packages. Some Python image packages prefer their images in bottom-to-top format (PyOpenGL for example). If you pass True for the flipped argument, the string buffer will be vertically flipped. The format argument is a string of one of the following values. Note that only 8-bit Surfaces can use the \"P\" format. The other formats will work for any Surface. Also note that other Python image packages support more formats than pygame.  \n \nP, 8-bit palettized Surfaces \nRGB, 24-bit image \nRGBX, 32-bit image with unused space \nRGBA, 32-bit image with an alpha channel \nARGB, 32-bit image with alpha channel first \nRGBA_PREMULT, 32-bit image with colors scaled by alpha channel \nARGB_PREMULT, 32-bit image with colors scaled by alpha channel, alpha channel first  \n \n   pygame.image.fromstring() \n create new Surface from a string buffer fromstring(string, size, format, flipped=False) -> Surface  This function takes arguments similar to pygame.image.tostring(). The size argument is a pair of numbers representing the width and height. Once the new Surface is created you can destroy the string buffer. The size and format image must compute the exact same size as the passed string buffer. Otherwise an exception will be raised. See the pygame.image.frombuffer() method for a potentially faster way to transfer images into pygame. \n   pygame.image.frombuffer() \n create a new Surface that shares data inside a bytes buffer frombuffer(bytes, size, format) -> Surface  Create a new Surface that shares pixel data directly from a bytes buffer. This method takes similar arguments to pygame.image.fromstring(), but is unable to vertically flip the source data. This will run much faster than pygame.image.fromstring(), since no pixel data must be allocated and copied. It accepts the following 'format' arguments:  \n \nP, 8-bit palettized Surfaces \nRGB, 24-bit image \nBGR, 24-bit image, red and blue channels swapped. \nRGBX, 32-bit image with unused space \nRGBA, 32-bit image with an alpha channel \nARGB, 32-bit image with alpha channel first","title":"pygame.ref.image"},{"text":"sklearn.decomposition.dict_learning  \nsklearn.decomposition.dict_learning(X, n_components, *, alpha, max_iter=100, tol=1e-08, method='lars', n_jobs=None, dict_init=None, code_init=None, callback=None, verbose=False, random_state=None, return_n_iter=False, positive_dict=False, positive_code=False, method_max_iter=1000) [source]\n \nSolves a dictionary learning matrix factorization problem. Finds the best dictionary and the corresponding sparse code for approximating the data matrix X by solving: (U^*, V^*) = argmin 0.5 || X - U V ||_2^2 + alpha * || U ||_1\n             (U,V)\n            with || V_k ||_2 = 1 for all  0 <= k < n_components\n where V is the dictionary and U is the sparse code. Read more in the User Guide.  Parameters \n \nXndarray of shape (n_samples, n_features) \n\nData matrix.  \nn_componentsint \n\nNumber of dictionary atoms to extract.  \nalphaint \n\nSparsity controlling parameter.  \nmax_iterint, default=100 \n\nMaximum number of iterations to perform.  \ntolfloat, default=1e-8 \n\nTolerance for the stopping condition.  \nmethod{\u2018lars\u2019, \u2018cd\u2019}, default=\u2019lars\u2019 \n\nThe method used:  \n \n'lars': uses the least angle regression method to solve the lasso \n\nproblem (linear_model.lars_path);    \n'cd': uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). Lars will be faster if the estimated components are sparse.   \nn_jobsint, default=None \n\nNumber of parallel jobs to run. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details.  \ndict_initndarray of shape (n_components, n_features), default=None \n\nInitial value for the dictionary for warm restart scenarios.  \ncode_initndarray of shape (n_samples, n_components), default=None \n\nInitial value for the sparse code for warm restart scenarios.  \ncallbackcallable, default=None \n\nCallable that gets invoked every five iterations  \nverbosebool, default=False \n\nTo control the verbosity of the procedure.  \nrandom_stateint, RandomState instance or None, default=None \n\nUsed for randomly initializing the dictionary. Pass an int for reproducible results across multiple function calls. See Glossary.  \nreturn_n_iterbool, default=False \n\nWhether or not to return the number of iterations.  \npositive_dictbool, default=False \n\nWhether to enforce positivity when finding the dictionary.  New in version 0.20.   \npositive_codebool, default=False \n\nWhether to enforce positivity when finding the code.  New in version 0.20.   \nmethod_max_iterint, default=1000 \n\nMaximum number of iterations to perform.  New in version 0.22.     Returns \n \ncodendarray of shape (n_samples, n_components) \n\nThe sparse code factor in the matrix factorization.  \ndictionaryndarray of shape (n_components, n_features), \n\nThe dictionary factor in the matrix factorization.  \nerrorsarray \n\nVector of errors at each iteration.  \nn_iterint \n\nNumber of iterations run. Returned only if return_n_iter is set to True.      See also  \ndict_learning_online\n\n\nDictionaryLearning\n\n\nMiniBatchDictionaryLearning\n\n\nSparsePCA\n\n\nMiniBatchSparsePCA","title":"sklearn.modules.generated.sklearn.decomposition.dict_learning"},{"text":"sklearn.decomposition.dict_learning(X, n_components, *, alpha, max_iter=100, tol=1e-08, method='lars', n_jobs=None, dict_init=None, code_init=None, callback=None, verbose=False, random_state=None, return_n_iter=False, positive_dict=False, positive_code=False, method_max_iter=1000) [source]\n \nSolves a dictionary learning matrix factorization problem. Finds the best dictionary and the corresponding sparse code for approximating the data matrix X by solving: (U^*, V^*) = argmin 0.5 || X - U V ||_2^2 + alpha * || U ||_1\n             (U,V)\n            with || V_k ||_2 = 1 for all  0 <= k < n_components\n where V is the dictionary and U is the sparse code. Read more in the User Guide.  Parameters \n \nXndarray of shape (n_samples, n_features) \n\nData matrix.  \nn_componentsint \n\nNumber of dictionary atoms to extract.  \nalphaint \n\nSparsity controlling parameter.  \nmax_iterint, default=100 \n\nMaximum number of iterations to perform.  \ntolfloat, default=1e-8 \n\nTolerance for the stopping condition.  \nmethod{\u2018lars\u2019, \u2018cd\u2019}, default=\u2019lars\u2019 \n\nThe method used:  \n \n'lars': uses the least angle regression method to solve the lasso \n\nproblem (linear_model.lars_path);    \n'cd': uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). Lars will be faster if the estimated components are sparse.   \nn_jobsint, default=None \n\nNumber of parallel jobs to run. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details.  \ndict_initndarray of shape (n_components, n_features), default=None \n\nInitial value for the dictionary for warm restart scenarios.  \ncode_initndarray of shape (n_samples, n_components), default=None \n\nInitial value for the sparse code for warm restart scenarios.  \ncallbackcallable, default=None \n\nCallable that gets invoked every five iterations  \nverbosebool, default=False \n\nTo control the verbosity of the procedure.  \nrandom_stateint, RandomState instance or None, default=None \n\nUsed for randomly initializing the dictionary. Pass an int for reproducible results across multiple function calls. See Glossary.  \nreturn_n_iterbool, default=False \n\nWhether or not to return the number of iterations.  \npositive_dictbool, default=False \n\nWhether to enforce positivity when finding the dictionary.  New in version 0.20.   \npositive_codebool, default=False \n\nWhether to enforce positivity when finding the code.  New in version 0.20.   \nmethod_max_iterint, default=1000 \n\nMaximum number of iterations to perform.  New in version 0.22.     Returns \n \ncodendarray of shape (n_samples, n_components) \n\nThe sparse code factor in the matrix factorization.  \ndictionaryndarray of shape (n_components, n_features), \n\nThe dictionary factor in the matrix factorization.  \nerrorsarray \n\nVector of errors at each iteration.  \nn_iterint \n\nNumber of iterations run. Returned only if return_n_iter is set to True.      See also  \ndict_learning_online\n\n\nDictionaryLearning\n\n\nMiniBatchDictionaryLearning\n\n\nSparsePCA\n\n\nMiniBatchSparsePCA","title":"sklearn.modules.generated.sklearn.decomposition.dict_learning#sklearn.decomposition.dict_learning"},{"text":"@functools.total_ordering  \nGiven a class defining one or more rich comparison ordering methods, this class decorator supplies the rest. This simplifies the effort involved in specifying all of the possible rich comparison operations: The class must define one of __lt__(), __le__(), __gt__(), or __ge__(). In addition, the class should supply an __eq__() method. For example: @total_ordering\nclass Student:\n    def _is_valid_operand(self, other):\n        return (hasattr(other, \"lastname\") and\n                hasattr(other, \"firstname\"))\n    def __eq__(self, other):\n        if not self._is_valid_operand(other):\n            return NotImplemented\n        return ((self.lastname.lower(), self.firstname.lower()) ==\n                (other.lastname.lower(), other.firstname.lower()))\n    def __lt__(self, other):\n        if not self._is_valid_operand(other):\n            return NotImplemented\n        return ((self.lastname.lower(), self.firstname.lower()) <\n                (other.lastname.lower(), other.firstname.lower()))\n  Note While this decorator makes it easy to create well behaved totally ordered types, it does come at the cost of slower execution and more complex stack traces for the derived comparison methods. If performance benchmarking indicates this is a bottleneck for a given application, implementing all six rich comparison methods instead is likely to provide an easy speed boost.   New in version 3.2.   Changed in version 3.4: Returning NotImplemented from the underlying comparison function for unrecognised types is now supported.","title":"python.library.functools#functools.total_ordering"},{"text":"sort(*, key=None, reverse=False)  \nThis method sorts the list in place, using only < comparisons between items. Exceptions are not suppressed - if any comparison operations fail, the entire sort operation will fail (and the list will likely be left in a partially modified state). sort() accepts two arguments that can only be passed by keyword (keyword-only arguments): key specifies a function of one argument that is used to extract a comparison key from each list element (for example, key=str.lower). The key corresponding to each item in the list is calculated once and then used for the entire sorting process. The default value of None means that list items are sorted directly without calculating a separate key value. The functools.cmp_to_key() utility is available to convert a 2.x style cmp function to a key function. reverse is a boolean value. If set to True, then the list elements are sorted as if each comparison were reversed. This method modifies the sequence in place for economy of space when sorting a large sequence. To remind users that it operates by side effect, it does not return the sorted sequence (use sorted() to explicitly request a new sorted list instance). The sort() method is guaranteed to be stable. A sort is stable if it guarantees not to change the relative order of elements that compare equal \u2014 this is helpful for sorting in multiple passes (for example, sort by department, then by salary grade). For sorting examples and a brief sorting tutorial, see Sorting HOW TO.  CPython implementation detail: While a list is being sorted, the effect of attempting to mutate, or even inspect, the list is undefined. The C implementation of Python makes the list appear empty for the duration, and raises ValueError if it can detect that the list has been mutated during a sort.","title":"python.library.stdtypes#list.sort"},{"text":"heapq.merge(*iterables, key=None, reverse=False)  \nMerge multiple sorted inputs into a single sorted output (for example, merge timestamped entries from multiple log files). Returns an iterator over the sorted values. Similar to sorted(itertools.chain(*iterables)) but returns an iterable, does not pull the data into memory all at once, and assumes that each of the input streams is already sorted (smallest to largest). Has two optional arguments which must be specified as keyword arguments. key specifies a key function of one argument that is used to extract a comparison key from each input element. The default value is None (compare the elements directly). reverse is a boolean value. If set to True, then the input elements are merged as if each comparison were reversed. To achieve behavior similar to sorted(itertools.chain(*iterables), reverse=True), all iterables must be sorted from largest to smallest.  Changed in version 3.5: Added the optional key and reverse parameters.","title":"python.library.heapq#heapq.merge"}]}
{"task_id":36402748,"prompt":"def f_36402748(df):\n\treturn ","suffix":"","canonical_solution":"df.groupby('prots').sum().sort_values('scores', ascending=False)","test_start":"\nimport pandas as pd \n\ndef check(candidate):","test":["\n    COLUMN_NAMES = [\"chemicals\", \"prots\", \"scores\"]\n    data = [[\"chemical1\", \"prot1\", 100],[\"chemical2\", \"prot2\", 50],[\"chemical3\", \"prot1\", 120]]\n    df = pd.DataFrame(data, columns = COLUMN_NAMES)\n    assert candidate(df).to_dict() == {'scores': {'prot1': 220, 'prot2': 50}}\n"],"entry_point":"f_36402748","intent":"sort a Dataframe `df` by the total ocurrences in a column 'scores' group by 'prots'","library":["pandas"],"docs":[{"text":"pandas.core.resample.Resampler.prod   Resampler.prod(_method='prod', min_count=0, *args, **kwargs)[source]\n \nCompute prod of group values.  Parameters \n \nnumeric_only:bool, default True\n\n\nInclude only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data.  \nmin_count:int, default 0\n\n\nThe required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA.    Returns \n Series or DataFrame\n\nComputed prod of values within each group.","title":"pandas.reference.api.pandas.core.resample.resampler.prod"},{"text":"pandas.core.groupby.GroupBy.prod   finalGroupBy.prod(numeric_only=NoDefault.no_default, min_count=0)[source]\n \nCompute prod of group values.  Parameters \n \nnumeric_only:bool, default True\n\n\nInclude only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data.  \nmin_count:int, default 0\n\n\nThe required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA.    Returns \n Series or DataFrame\n\nComputed prod of values within each group.","title":"pandas.reference.api.pandas.core.groupby.groupby.prod"},{"text":"tf.compat.v1.train.RMSPropOptimizer Optimizer that implements the RMSProp algorithm (Tielemans et al. Inherits From: Optimizer \ntf.compat.v1.train.RMSPropOptimizer(\n    learning_rate, decay=0.9, momentum=0.0, epsilon=1e-10, use_locking=False,\n    centered=False, name='RMSProp'\n)\n 2012). References: Coursera slide 29: Hinton, 2012 (pdf)\n \n\n\n Args\n  learning_rate   A Tensor or a floating point value. The learning rate.  \n  decay   Discounting factor for the history\/coming gradient  \n  momentum   A scalar tensor.  \n  epsilon   Small value to avoid zero denominator.  \n  use_locking   If True use locks for update operation.  \n  centered   If True, gradients are normalized by the estimated variance of the gradient; if False, by the uncentered second moment. Setting this to True may help with training, but is slightly more expensive in terms of computation and memory. Defaults to False.  \n  name   Optional name prefix for the operations created when applying gradients. Defaults to \"RMSProp\".    Methods apply_gradients View source \napply_gradients(\n    grads_and_vars, global_step=None, name=None\n)\n Apply gradients to variables. This is the second part of minimize(). It returns an Operation that applies gradients.\n \n\n\n Args\n  grads_and_vars   List of (gradient, variable) pairs as returned by compute_gradients().  \n  global_step   Optional Variable to increment by one after the variables have been updated.  \n  name   Optional name for the returned operation. Default to the name passed to the Optimizer constructor.   \n \n\n\n Returns   An Operation that applies the specified gradients. If global_step was not None, that operation also increments global_step.  \n\n \n\n\n Raises\n  TypeError   If grads_and_vars is malformed.  \n  ValueError   If none of the variables have gradients.  \n  RuntimeError   If you should use _distributed_apply() instead.    compute_gradients View source \ncompute_gradients(\n    loss, var_list=None, gate_gradients=GATE_OP, aggregation_method=None,\n    colocate_gradients_with_ops=False, grad_loss=None\n)\n Compute gradients of loss for the variables in var_list. This is the first part of minimize(). It returns a list of (gradient, variable) pairs where \"gradient\" is the gradient for \"variable\". Note that \"gradient\" can be a Tensor, an IndexedSlices, or None if there is no gradient for the given variable.\n \n\n\n Args\n  loss   A Tensor containing the value to minimize or a callable taking no arguments which returns the value to minimize. When eager execution is enabled it must be a callable.  \n  var_list   Optional list or tuple of tf.Variable to update to minimize loss. Defaults to the list of variables collected in the graph under the key GraphKeys.TRAINABLE_VARIABLES.  \n  gate_gradients   How to gate the computation of gradients. Can be GATE_NONE, GATE_OP, or GATE_GRAPH.  \n  aggregation_method   Specifies the method used to combine gradient terms. Valid values are defined in the class AggregationMethod.  \n  colocate_gradients_with_ops   If True, try colocating gradients with the corresponding op.  \n  grad_loss   Optional. A Tensor holding the gradient computed for loss.   \n \n\n\n Returns   A list of (gradient, variable) pairs. Variable is always present, but gradient can be None.  \n\n \n\n\n Raises\n  TypeError   If var_list contains anything else than Variable objects.  \n  ValueError   If some arguments are invalid.  \n  RuntimeError   If called with eager execution enabled and loss is not callable.    Eager Compatibility When eager execution is enabled, gate_gradients, aggregation_method, and colocate_gradients_with_ops are ignored. get_name View source \nget_name()\n get_slot View source \nget_slot(\n    var, name\n)\n Return a slot named name created for var by the Optimizer. Some Optimizer subclasses use additional variables. For example Momentum and Adagrad use variables to accumulate updates. This method gives access to these Variable objects if for some reason you need them. Use get_slot_names() to get the list of slot names created by the Optimizer.\n \n\n\n Args\n  var   A variable passed to minimize() or apply_gradients().  \n  name   A string.   \n \n\n\n Returns   The Variable for the slot if it was created, None otherwise.  \n get_slot_names View source \nget_slot_names()\n Return a list of the names of slots created by the Optimizer. See get_slot().\n \n\n\n Returns   A list of strings.  \n minimize View source \nminimize(\n    loss, global_step=None, var_list=None, gate_gradients=GATE_OP,\n    aggregation_method=None, colocate_gradients_with_ops=False, name=None,\n    grad_loss=None\n)\n Add operations to minimize loss by updating var_list. This method simply combines calls compute_gradients() and apply_gradients(). If you want to process the gradient before applying them call compute_gradients() and apply_gradients() explicitly instead of using this function.\n \n\n\n Args\n  loss   A Tensor containing the value to minimize.  \n  global_step   Optional Variable to increment by one after the variables have been updated.  \n  var_list   Optional list or tuple of Variable objects to update to minimize loss. Defaults to the list of variables collected in the graph under the key GraphKeys.TRAINABLE_VARIABLES.  \n  gate_gradients   How to gate the computation of gradients. Can be GATE_NONE, GATE_OP, or GATE_GRAPH.  \n  aggregation_method   Specifies the method used to combine gradient terms. Valid values are defined in the class AggregationMethod.  \n  colocate_gradients_with_ops   If True, try colocating gradients with the corresponding op.  \n  name   Optional name for the returned operation.  \n  grad_loss   Optional. A Tensor holding the gradient computed for loss.   \n \n\n\n Returns   An Operation that updates the variables in var_list. If global_step was not None, that operation also increments global_step.  \n\n \n\n\n Raises\n  ValueError   If some of the variables are not Variable objects.    Eager Compatibility When eager execution is enabled, loss should be a Python function that takes no arguments and computes the value to be minimized. Minimization (and gradient computation) is done with respect to the elements of var_list if not None, else with respect to any trainable variables created during the execution of the loss function. gate_gradients, aggregation_method, colocate_gradients_with_ops and grad_loss are ignored when eager execution is enabled. variables View source \nvariables()\n A list of variables which encode the current state of Optimizer. Includes slot variables and additional global variables created by the optimizer in the current default graph.\n \n\n\n Returns   A list of variables.  \n\n \n\n\n Class Variables\n  GATE_GRAPH   2  \n  GATE_NONE   0  \n  GATE_OP   1","title":"tensorflow.compat.v1.train.rmspropoptimizer"},{"text":"mpl_toolkits.mplot3d.proj3d.proj_points   mpl_toolkits.mplot3d.proj3d.proj_points(points, M)[source]","title":"matplotlib._as_gen.mpl_toolkits.mplot3d.proj3d.proj_points"},{"text":"class flask.sessions.NullSession(initial=None)  \nClass used to generate nicer error messages if sessions are not available. Will still allow read-only access to the empty session but fail on setting.  Parameters \ninitial (Any) \u2013   Return type \nNone","title":"flask.api.index#flask.sessions.NullSession"},{"text":"pandas.DataFrame.prod   DataFrame.prod(axis=None, skipna=True, level=None, numeric_only=None, min_count=0, **kwargs)[source]\n \nReturn the product of the values over the requested axis.  Parameters \n \naxis:{index (0), columns (1)}\n\n\nAxis for the function to be applied on.  \nskipna:bool, default True\n\n\nExclude NA\/null values when computing the result.  \nlevel:int or level name, default None\n\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series.  \nnumeric_only:bool, default None\n\n\nInclude only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series.  \nmin_count:int, default 0\n\n\nThe required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA.  **kwargs\n\nAdditional keyword arguments to be passed to the function.    Returns \n Series or DataFrame (if level specified)\n    See also  Series.sum\n\nReturn the sum.  Series.min\n\nReturn the minimum.  Series.max\n\nReturn the maximum.  Series.idxmin\n\nReturn the index of the minimum.  Series.idxmax\n\nReturn the index of the maximum.  DataFrame.sum\n\nReturn the sum over the requested axis.  DataFrame.min\n\nReturn the minimum over the requested axis.  DataFrame.max\n\nReturn the maximum over the requested axis.  DataFrame.idxmin\n\nReturn the index of the minimum over the requested axis.  DataFrame.idxmax\n\nReturn the index of the maximum over the requested axis.    Examples By default, the product of an empty or all-NA Series is 1 \n>>> pd.Series([], dtype=\"float64\").prod()\n1.0\n  This can be controlled with the min_count parameter \n>>> pd.Series([], dtype=\"float64\").prod(min_count=1)\nnan\n  Thanks to the skipna parameter, min_count handles all-NA and empty series identically. \n>>> pd.Series([np.nan]).prod()\n1.0\n  \n>>> pd.Series([np.nan]).prod(min_count=1)\nnan","title":"pandas.reference.api.pandas.dataframe.prod"},{"text":"HttpRequest.session  \nFrom the SessionMiddleware: A readable and writable, dictionary-like object that represents the current session.","title":"django.ref.request-response#django.http.HttpRequest.session"},{"text":"tf.keras.optimizers.RMSprop     View source on GitHub    Optimizer that implements the RMSprop algorithm. Inherits From: Optimizer  View aliases  Main aliases \ntf.optimizers.RMSprop Compat aliases for migration See Migration guide for more details. tf.compat.v1.keras.optimizers.RMSprop  \ntf.keras.optimizers.RMSprop(\n    learning_rate=0.001, rho=0.9, momentum=0.0, epsilon=1e-07, centered=False,\n    name='RMSprop', **kwargs\n)\n The gist of RMSprop is to:  Maintain a moving (discounted) average of the square of gradients Divide the gradient by the root of this average  This implementation of RMSprop uses plain momentum, not Nesterov momentum. The centered version additionally maintains a moving average of the gradients, and uses that average to estimate the variance.\n \n\n\n Args\n  learning_rate   A Tensor, floating point value, or a schedule that is a tf.keras.optimizers.schedules.LearningRateSchedule, or a callable that takes no arguments and returns the actual value to use. The learning rate. Defaults to 0.001.  \n  rho   Discounting factor for the history\/coming gradient. Defaults to 0.9.  \n  momentum   A scalar or a scalar Tensor. Defaults to 0.0.  \n  epsilon   A small constant for numerical stability. This epsilon is \"epsilon hat\" in the Kingma and Ba paper (in the formula just before Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to 1e-7.  \n  centered   Boolean. If True, gradients are normalized by the estimated variance of the gradient; if False, by the uncentered second moment. Setting this to True may help with training, but is slightly more expensive in terms of computation and memory. Defaults to False.  \n  name   Optional name prefix for the operations created when applying gradients. Defaults to \"RMSprop\".  \n  **kwargs   Keyword arguments. Allowed to be one of \"clipnorm\" or \"clipvalue\". \"clipnorm\" (float) clips gradients by norm; \"clipvalue\" (float) clips gradients by value.    Note that in the dense implementation of this algorithm, variables and their corresponding accumulators (momentum, gradient moving average, square gradient moving average) will be updated even if the gradient is zero (i.e. accumulators will decay, momentum will be applied). The sparse implementation (used when the gradient is an IndexedSlices object, typically because of tf.gather or an embedding lookup in the forward pass) will not update variable slices or their accumulators unless those slices were used in the forward pass (nor is there an \"eventual\" correction to account for these omitted updates). This leads to more efficient updates for large embedding lookup tables (where most of the slices are not accessed in a particular graph execution), but differs from the published algorithm. Usage: \nopt = tf.keras.optimizers.RMSprop(learning_rate=0.1)\nvar1 = tf.Variable(10.0)\nloss = lambda: (var1 ** 2) \/ 2.0    # d(loss) \/ d(var1) = var1\nstep_count = opt.minimize(loss, [var1]).numpy()\nvar1.numpy()\n9.683772\n Reference:  Hinton, 2012","title":"tensorflow.keras.optimizers.rmsprop"},{"text":"pandas.wide_to_long   pandas.wide_to_long(df, stubnames, i, j, sep='', suffix='\\\\d+')[source]\n \nUnpivot a DataFrame from wide to long format. Less flexible but more user-friendly than melt. With stubnames [\u2018A\u2019, \u2018B\u2019], this function expects to find one or more group of columns with format A-suffix1, A-suffix2,\u2026, B-suffix1, B-suffix2,\u2026 You specify what you want to call this suffix in the resulting long format with j (for example j=\u2019year\u2019) Each row of these wide variables are assumed to be uniquely identified by i (can be a single column name or a list of column names) All remaining variables in the data frame are left intact.  Parameters \n \ndf:DataFrame\n\n\nThe wide-format DataFrame.  \nstubnames:str or list-like\n\n\nThe stub name(s). The wide format variables are assumed to start with the stub names.  \ni:str or list-like\n\n\nColumn(s) to use as id variable(s).  \nj:str\n\n\nThe name of the sub-observation variable. What you wish to name your suffix in the long format.  \nsep:str, default \u201c\u201d\n\n\nA character indicating the separation of the variable names in the wide format, to be stripped from the names in the long format. For example, if your column names are A-suffix1, A-suffix2, you can strip the hyphen by specifying sep=\u2019-\u2019.  \nsuffix:str, default \u2018\\d+\u2019\n\n\nA regular expression capturing the wanted suffixes. \u2018\\d+\u2019 captures numeric suffixes. Suffixes with no numbers could be specified with the negated character class \u2018\\D+\u2019. You can also further disambiguate suffixes, for example, if your wide variables are of the form A-one, B-two,.., and you have an unrelated column A-rating, you can ignore the last one by specifying suffix=\u2019(!?one|two)\u2019. When all suffixes are numeric, they are cast to int64\/float64.    Returns \n DataFrame\n\nA DataFrame that contains each stub name as a variable, with new index (i, j).      See also  melt\n\nUnpivot a DataFrame from wide to long format, optionally leaving identifiers set.  pivot\n\nCreate a spreadsheet-style pivot table as a DataFrame.  DataFrame.pivot\n\nPivot without aggregation that can handle non-numeric data.  DataFrame.pivot_table\n\nGeneralization of pivot that can handle duplicate values for one index\/column pair.  DataFrame.unstack\n\nPivot based on the index values instead of a column.    Notes All extra variables are left untouched. This simply uses pandas.melt under the hood, but is hard-coded to \u201cdo the right thing\u201d in a typical case. Examples \n>>> np.random.seed(123)\n>>> df = pd.DataFrame({\"A1970\" : {0 : \"a\", 1 : \"b\", 2 : \"c\"},\n...                    \"A1980\" : {0 : \"d\", 1 : \"e\", 2 : \"f\"},\n...                    \"B1970\" : {0 : 2.5, 1 : 1.2, 2 : .7},\n...                    \"B1980\" : {0 : 3.2, 1 : 1.3, 2 : .1},\n...                    \"X\"     : dict(zip(range(3), np.random.randn(3)))\n...                   })\n>>> df[\"id\"] = df.index\n>>> df\n  A1970 A1980  B1970  B1980         X  id\n0     a     d    2.5    3.2 -1.085631   0\n1     b     e    1.2    1.3  0.997345   1\n2     c     f    0.7    0.1  0.282978   2\n>>> pd.wide_to_long(df, [\"A\", \"B\"], i=\"id\", j=\"year\")\n... \n                X  A    B\nid year\n0  1970 -1.085631  a  2.5\n1  1970  0.997345  b  1.2\n2  1970  0.282978  c  0.7\n0  1980 -1.085631  d  3.2\n1  1980  0.997345  e  1.3\n2  1980  0.282978  f  0.1\n  With multiple id columns \n>>> df = pd.DataFrame({\n...     'famid': [1, 1, 1, 2, 2, 2, 3, 3, 3],\n...     'birth': [1, 2, 3, 1, 2, 3, 1, 2, 3],\n...     'ht1': [2.8, 2.9, 2.2, 2, 1.8, 1.9, 2.2, 2.3, 2.1],\n...     'ht2': [3.4, 3.8, 2.9, 3.2, 2.8, 2.4, 3.3, 3.4, 2.9]\n... })\n>>> df\n   famid  birth  ht1  ht2\n0      1      1  2.8  3.4\n1      1      2  2.9  3.8\n2      1      3  2.2  2.9\n3      2      1  2.0  3.2\n4      2      2  1.8  2.8\n5      2      3  1.9  2.4\n6      3      1  2.2  3.3\n7      3      2  2.3  3.4\n8      3      3  2.1  2.9\n>>> l = pd.wide_to_long(df, stubnames='ht', i=['famid', 'birth'], j='age')\n>>> l\n... \n                  ht\nfamid birth age\n1     1     1    2.8\n            2    3.4\n      2     1    2.9\n            2    3.8\n      3     1    2.2\n            2    2.9\n2     1     1    2.0\n            2    3.2\n      2     1    1.8\n            2    2.8\n      3     1    1.9\n            2    2.4\n3     1     1    2.2\n            2    3.3\n      2     1    2.3\n            2    3.4\n      3     1    2.1\n            2    2.9\n  Going from long back to wide just takes some creative use of unstack \n>>> w = l.unstack()\n>>> w.columns = w.columns.map('{0[0]}{0[1]}'.format)\n>>> w.reset_index()\n   famid  birth  ht1  ht2\n0      1      1  2.8  3.4\n1      1      2  2.9  3.8\n2      1      3  2.2  2.9\n3      2      1  2.0  3.2\n4      2      2  1.8  2.8\n5      2      3  1.9  2.4\n6      3      1  2.2  3.3\n7      3      2  2.3  3.4\n8      3      3  2.1  2.9\n  Less wieldy column names are also handled \n>>> np.random.seed(0)\n>>> df = pd.DataFrame({'A(weekly)-2010': np.random.rand(3),\n...                    'A(weekly)-2011': np.random.rand(3),\n...                    'B(weekly)-2010': np.random.rand(3),\n...                    'B(weekly)-2011': np.random.rand(3),\n...                    'X' : np.random.randint(3, size=3)})\n>>> df['id'] = df.index\n>>> df \n   A(weekly)-2010  A(weekly)-2011  B(weekly)-2010  B(weekly)-2011  X  id\n0        0.548814        0.544883        0.437587        0.383442  0   0\n1        0.715189        0.423655        0.891773        0.791725  1   1\n2        0.602763        0.645894        0.963663        0.528895  1   2\n  \n>>> pd.wide_to_long(df, ['A(weekly)', 'B(weekly)'], i='id',\n...                 j='year', sep='-')\n... \n         X  A(weekly)  B(weekly)\nid year\n0  2010  0   0.548814   0.437587\n1  2010  1   0.715189   0.891773\n2  2010  1   0.602763   0.963663\n0  2011  0   0.544883   0.383442\n1  2011  1   0.423655   0.791725\n2  2011  1   0.645894   0.528895\n  If we have many columns, we could also use a regex to find our stubnames and pass that list on to wide_to_long \n>>> stubnames = sorted(\n...     set([match[0] for match in df.columns.str.findall(\n...         r'[A-B]\\(.*\\)').values if match != []])\n... )\n>>> list(stubnames)\n['A(weekly)', 'B(weekly)']\n  All of the above examples have integers as suffixes. It is possible to have non-integers as suffixes. \n>>> df = pd.DataFrame({\n...     'famid': [1, 1, 1, 2, 2, 2, 3, 3, 3],\n...     'birth': [1, 2, 3, 1, 2, 3, 1, 2, 3],\n...     'ht_one': [2.8, 2.9, 2.2, 2, 1.8, 1.9, 2.2, 2.3, 2.1],\n...     'ht_two': [3.4, 3.8, 2.9, 3.2, 2.8, 2.4, 3.3, 3.4, 2.9]\n... })\n>>> df\n   famid  birth  ht_one  ht_two\n0      1      1     2.8     3.4\n1      1      2     2.9     3.8\n2      1      3     2.2     2.9\n3      2      1     2.0     3.2\n4      2      2     1.8     2.8\n5      2      3     1.9     2.4\n6      3      1     2.2     3.3\n7      3      2     2.3     3.4\n8      3      3     2.1     2.9\n  \n>>> l = pd.wide_to_long(df, stubnames='ht', i=['famid', 'birth'], j='age',\n...                     sep='_', suffix=r'\\w+')\n>>> l\n... \n                  ht\nfamid birth age\n1     1     one  2.8\n            two  3.4\n      2     one  2.9\n            two  3.8\n      3     one  2.2\n            two  2.9\n2     1     one  2.0\n            two  3.2\n      2     one  1.8\n            two  2.8\n      3     one  1.9\n            two  2.4\n3     1     one  2.2\n            two  3.3\n      2     one  2.3\n            two  3.4\n      3     one  2.1\n            two  2.9","title":"pandas.reference.api.pandas.wide_to_long"},{"text":"SMTP.send_message(msg, from_addr=None, to_addrs=None, mail_options=(), rcpt_options=())  \nThis is a convenience method for calling sendmail() with the message represented by an email.message.Message object. The arguments have the same meaning as for sendmail(), except that msg is a Message object. If from_addr is None or to_addrs is None, send_message fills those arguments with addresses extracted from the headers of msg as specified in RFC 5322: from_addr is set to the Sender field if it is present, and otherwise to the From field. to_addrs combines the values (if any) of the To, Cc, and Bcc fields from msg. If exactly one set of Resent-* headers appear in the message, the regular headers are ignored and the Resent-* headers are used instead. If the message contains more than one set of Resent-* headers, a ValueError is raised, since there is no way to unambiguously detect the most recent set of Resent- headers. send_message serializes msg using BytesGenerator with \\r\\n as the linesep, and calls sendmail() to transmit the resulting message. Regardless of the values of from_addr and to_addrs, send_message does not transmit any Bcc or Resent-Bcc headers that may appear in msg. If any of the addresses in from_addr and to_addrs contain non-ASCII characters and the server does not advertise SMTPUTF8 support, an SMTPNotSupported error is raised. Otherwise the Message is serialized with a clone of its policy with the utf8 attribute set to True, and SMTPUTF8 and BODY=8BITMIME are added to mail_options.  New in version 3.2.   New in version 3.5: Support for internationalized addresses (SMTPUTF8).","title":"python.library.smtplib#smtplib.SMTP.send_message"}]}
{"task_id":29881993,"prompt":"def f_29881993(trans):\n\treturn ","suffix":"","canonical_solution":"\"\"\",\"\"\".join(trans['category'])","test_start":"\ndef check(candidate):","test":["\n    trans = {'category':[\"hello\", \"world\",\"test\"], 'dummy_key':[\"dummy_val\"]}\n    assert candidate(trans) == \"hello,world,test\"\n"],"entry_point":"f_29881993","intent":"join together with \",\" elements inside a list indexed with 'category' within a dictionary `trans`","library":[],"docs":[]}
{"task_id":34158494,"prompt":"def f_34158494():\n\treturn ","suffix":"","canonical_solution":"\"\"\"\"\"\".join(['A', 'B', 'C', 'D'])","test_start":"\ndef check(candidate):","test":["\n    assert candidate() == 'ABCD'\n"],"entry_point":"f_34158494","intent":"concatenate array of strings `['A', 'B', 'C', 'D']` into a string","library":[],"docs":[]}
{"task_id":12666897,"prompt":"def f_12666897(sents):\n\treturn ","suffix":"","canonical_solution":"[x for x in sents if not x.startswith('@$\\t') and not x.startswith('#')]","test_start":"\ndef check(candidate):","test":["\n    sents = [\"@$\tabcd\", \"#453923\", \"abcd\", \"hello\", \"1\"]\n    assert candidate(sents) == [\"abcd\", \"hello\", \"1\"]\n","\n    sents = [\"@$\tabcd\", \"@$t453923\", \"abcd\", \"hello\", \"1\"]\n    assert candidate(sents) == [\"@$t453923\", \"abcd\", \"hello\", \"1\"]\n","\n    sents = [\"#tabcd\", \"##453923\", \"abcd\", \"hello\", \"1\"]\n    assert candidate(sents) == [\"abcd\", \"hello\", \"1\"] \n"],"entry_point":"f_12666897","intent":"Remove all strings from a list a strings `sents` where the values starts with `@$\\t` or `#`","library":[],"docs":[]}
{"task_id":5944630,"prompt":"def f_5944630(list):\n\t","suffix":"\n\treturn list","canonical_solution":"list.sort(key=lambda item: (item['points'], item['time']))","test_start":"\ndef check(candidate):","test":["\n    list = [\n        {'name':'JOHN', 'points' : 30, 'time' : '0:02:2'},\n        {'name':'KARL','points':50,'time': '0:03:00'},\n        {'name':'TEST','points':20,'time': '0:03:00'}\n    ]\n    assert candidate(list) == [\n        {'name':'TEST','points':20,'time': '0:03:00'}, \n        {'name':'JOHN', 'points' : 30, 'time' : '0:02:2'},\n        {'name':'KARL','points':50,'time': '0:03:00'}\n    ]\n","\n    list = [\n        {'name':'JOHN', 'points' : 30, 'time' : '0:02:2'},\n        {'name':'KARL','points':30,'time': '0:03:00'},\n        {'name':'TEST','points':30,'time': '0:01:01'}\n    ]\n    assert candidate(list) == [\n        {'name':'TEST','points':30,'time': '0:01:01'},\n        {'name':'JOHN', 'points' : 30, 'time' : '0:02:2'},\n        {'name':'KARL','points':30,'time': '0:03:00'}\n    ]\n"],"entry_point":"f_5944630","intent":"sort a list of dictionary `list` first by key `points` and then by `time`","library":[],"docs":[]}
{"task_id":7852855,"prompt":"def f_7852855():\n\treturn ","suffix":"","canonical_solution":"datetime.datetime(1970, 1, 1).second","test_start":"\nimport time\nimport datetime\n\ndef check(candidate):","test":["\n    assert candidate() == 0\n"],"entry_point":"f_7852855","intent":"convert datetime object `(1970, 1, 1)` to seconds","library":["datetime","time"],"docs":[{"text":"datetime.timestamp()  \nReturn POSIX timestamp corresponding to the datetime instance. The return value is a float similar to that returned by time.time(). Naive datetime instances are assumed to represent local time and this method relies on the platform C mktime() function to perform the conversion. Since datetime supports wider range of values than mktime() on many platforms, this method may raise OverflowError for times far in the past or far in the future. For aware datetime instances, the return value is computed as: (dt - datetime(1970, 1, 1, tzinfo=timezone.utc)).total_seconds()\n  New in version 3.3.   Changed in version 3.6: The timestamp() method uses the fold attribute to disambiguate the times during a repeated interval.   Note There is no method to obtain the POSIX timestamp directly from a naive datetime instance representing UTC time. If your application uses this convention and your system timezone is not set to UTC, you can obtain the POSIX timestamp by supplying tzinfo=timezone.utc: timestamp = dt.replace(tzinfo=timezone.utc).timestamp()\n or by calculating the timestamp directly: timestamp = (dt - datetime(1970, 1, 1)) \/ timedelta(seconds=1)","title":"python.library.datetime#datetime.datetime.timestamp"},{"text":"matplotlib.dates.num2epoch(d)[source]\n \n[Deprecated] Convert days since Matplotlib epoch to UNIX time.  Parameters \n \ndlist of floats\n\n\nTime in days since Matplotlib epoch (see get_epoch()).    Returns \n numpy.array\n\nTime in seconds since 1970-01-01.     Notes  Deprecated since version 3.5.","title":"matplotlib.dates_api#matplotlib.dates.num2epoch"},{"text":"timedelta.total_seconds()  \nReturn the total number of seconds contained in the duration. Equivalent to td \/ timedelta(seconds=1). For interval units other than seconds, use the division form directly (e.g. td \/ timedelta(microseconds=1)). Note that for very large time intervals (greater than 270 years on most platforms) this method will lose microsecond accuracy.  New in version 3.2.","title":"python.library.datetime#datetime.timedelta.total_seconds"},{"text":"pandas.Timedelta.seconds   Timedelta.seconds\n \nNumber of seconds (>= 0 and less than 1 day).","title":"pandas.reference.api.pandas.timedelta.seconds"},{"text":"pandas.Timedelta.total_seconds   Timedelta.total_seconds()\n \nTotal seconds in the duration.","title":"pandas.reference.api.pandas.timedelta.total_seconds"},{"text":"calendar.timegm(tuple)  \nAn unrelated but handy function that takes a time tuple such as returned by the gmtime() function in the time module, and returns the corresponding Unix timestamp value, assuming an epoch of 1970, and the POSIX encoding. In fact, time.gmtime() and timegm() are each others\u2019 inverse.","title":"python.library.calendar#calendar.timegm"},{"text":"pandas.Timestamp.timestamp   Timestamp.timestamp()\n \nReturn POSIX timestamp as float. Examples \n>>> ts = pd.Timestamp('2020-03-14T15:32:52.192548')\n>>> ts.timestamp()\n1584199972.192548","title":"pandas.reference.api.pandas.timestamp.timestamp"},{"text":"time.mktime(t)  \nThis is the inverse function of localtime(). Its argument is the struct_time or full 9-tuple (since the dst flag is needed; use -1 as the dst flag if it is unknown) which expresses the time in local time, not UTC. It returns a floating point number, for compatibility with time(). If the input value cannot be represented as a valid time, either OverflowError or ValueError will be raised (which depends on whether the invalid value is caught by Python or the underlying C libraries). The earliest date for which it can generate a time is platform-dependent.","title":"python.library.time#time.mktime"},{"text":"pandas.Series.dt.seconds   Series.dt.seconds\n \nNumber of seconds (>= 0 and less than 1 day) for each element.","title":"pandas.reference.api.pandas.series.dt.seconds"},{"text":"email.utils.mktime_tz(tuple)  \nTurn a 10-tuple as returned by parsedate_tz() into a UTC timestamp (seconds since the Epoch). If the timezone item in the tuple is None, assume local time.","title":"python.library.email.utils#email.utils.mktime_tz"}]}
{"task_id":2763750,"prompt":"def f_2763750():\n\treturn ","suffix":"","canonical_solution":"re.sub('(\\\\_a)?\\\\.([^\\\\.]*)$', '_suff.\\\\2', 'long.file.name.jpg')","test_start":"\nimport re \n\ndef check(candidate):","test":["\n    assert candidate() == 'long.file.name_suff.jpg'\n"],"entry_point":"f_2763750","intent":"insert `_suff` before the file extension in `long.file.name.jpg` or replace `_a` with `suff` if it precedes the extension.","library":["re"],"docs":[{"text":"suffix_map  \nDictionary mapping suffixes to suffixes. This is used to allow recognition of encoded files for which the encoding and the type are indicated by the same extension. For example, the .tgz extension is mapped to .tar.gz to allow the encoding and type to be recognized separately. This is initially a copy of the global suffix_map defined in the module.","title":"python.library.mimetypes#mimetypes.MimeTypes.suffix_map"},{"text":"mimetypes.suffix_map  \nDictionary mapping suffixes to suffixes. This is used to allow recognition of encoded files for which the encoding and the type are indicated by the same extension. For example, the .tgz extension is mapped to .tar.gz to allow the encoding and type to be recognized separately.","title":"python.library.mimetypes#mimetypes.suffix_map"},{"text":"pandas.DataFrame.add_suffix   DataFrame.add_suffix(suffix)[source]\n \nSuffix labels with string suffix. For Series, the row labels are suffixed. For DataFrame, the column labels are suffixed.  Parameters \n \nsuffix:str\n\n\nThe string to add after each label.    Returns \n Series or DataFrame\n\nNew Series or DataFrame with updated labels.      See also  Series.add_prefix\n\nPrefix row labels with string prefix.  DataFrame.add_prefix\n\nPrefix column labels with string prefix.    Examples \n>>> s = pd.Series([1, 2, 3, 4])\n>>> s\n0    1\n1    2\n2    3\n3    4\ndtype: int64\n  \n>>> s.add_suffix('_item')\n0_item    1\n1_item    2\n2_item    3\n3_item    4\ndtype: int64\n  \n>>> df = pd.DataFrame({'A': [1, 2, 3, 4], 'B': [3, 4, 5, 6]})\n>>> df\n   A  B\n0  1  3\n1  2  4\n2  3  5\n3  4  6\n  \n>>> df.add_suffix('_col')\n     A_col  B_col\n0       1       3\n1       2       4\n2       3       5\n3       4       6","title":"pandas.reference.api.pandas.dataframe.add_suffix"},{"text":"get_alternative_name(file_root, file_ext)  \nReturns an alternative filename based on the file_root and file_ext parameters, an underscore plus a random 7 character alphanumeric string is appended to the filename before the extension.","title":"django.ref.files.storage#django.core.files.storage.Storage.get_alternative_name"},{"text":"stat.UF_APPEND  \nThe file may only be appended to.","title":"python.library.stat#stat.UF_APPEND"},{"text":"PurePath.with_stem(stem)  \nReturn a new path with the stem changed. If the original path doesn\u2019t have a name, ValueError is raised: >>> p = PureWindowsPath('c:\/Downloads\/draft.txt')\n>>> p.with_stem('final')\nPureWindowsPath('c:\/Downloads\/final.txt')\n>>> p = PureWindowsPath('c:\/Downloads\/pathlib.tar.gz')\n>>> p.with_stem('lib')\nPureWindowsPath('c:\/Downloads\/lib.gz')\n>>> p = PureWindowsPath('c:\/')\n>>> p.with_stem('')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"\/home\/antoine\/cpython\/default\/Lib\/pathlib.py\", line 861, in with_stem\n    return self.with_name(stem + self.suffix)\n  File \"\/home\/antoine\/cpython\/default\/Lib\/pathlib.py\", line 851, in with_name\n    raise ValueError(\"%r has an empty name\" % (self,))\nValueError: PureWindowsPath('c:\/') has an empty name\n  New in version 3.9.","title":"python.library.pathlib#pathlib.PurePath.with_stem"},{"text":"pandas.Series.add_suffix   Series.add_suffix(suffix)[source]\n \nSuffix labels with string suffix. For Series, the row labels are suffixed. For DataFrame, the column labels are suffixed.  Parameters \n \nsuffix:str\n\n\nThe string to add after each label.    Returns \n Series or DataFrame\n\nNew Series or DataFrame with updated labels.      See also  Series.add_prefix\n\nPrefix row labels with string prefix.  DataFrame.add_prefix\n\nPrefix column labels with string prefix.    Examples \n>>> s = pd.Series([1, 2, 3, 4])\n>>> s\n0    1\n1    2\n2    3\n3    4\ndtype: int64\n  \n>>> s.add_suffix('_item')\n0_item    1\n1_item    2\n2_item    3\n3_item    4\ndtype: int64\n  \n>>> df = pd.DataFrame({'A': [1, 2, 3, 4], 'B': [3, 4, 5, 6]})\n>>> df\n   A  B\n0  1  3\n1  2  4\n2  3  5\n3  4  6\n  \n>>> df.add_suffix('_col')\n     A_col  B_col\n0       1       3\n1       2       4\n2       3       5\n3       4       6","title":"pandas.reference.api.pandas.series.add_suffix"},{"text":"extensions_map  \nA dictionary mapping suffixes into MIME types, contains custom overrides for the default system mappings. The mapping is used case-insensitively, and so should contain only lower-cased keys.  Changed in version 3.9: This dictionary is no longer filled with the default system mappings, but only contains overrides.","title":"python.library.http.server#http.server.SimpleHTTPRequestHandler.extensions_map"},{"text":"get_alternative_name(file_root, file_ext)","title":"django.howto.custom-file-storage#django.core.files.storage.get_alternative_name"},{"text":"PurePath.with_suffix(suffix)  \nReturn a new path with the suffix changed. If the original path doesn\u2019t have a suffix, the new suffix is appended instead. If the suffix is an empty string, the original suffix is removed: >>> p = PureWindowsPath('c:\/Downloads\/pathlib.tar.gz')\n>>> p.with_suffix('.bz2')\nPureWindowsPath('c:\/Downloads\/pathlib.tar.bz2')\n>>> p = PureWindowsPath('README')\n>>> p.with_suffix('.txt')\nPureWindowsPath('README.txt')\n>>> p = PureWindowsPath('README.txt')\n>>> p.with_suffix('')\nPureWindowsPath('README')","title":"python.library.pathlib#pathlib.PurePath.with_suffix"}]}
{"task_id":6420361,"prompt":"def f_6420361(module):\n\t","suffix":"\n\treturn ","canonical_solution":"imp.reload(module)","test_start":"\nimport imp\nfrom unittest.mock import Mock\n\ndef check(candidate):","test":["\n    imp.reload = Mock()\n    try:\n        candidate('ads')\n        assert True\n    except:\n        assert False\n"],"entry_point":"f_6420361","intent":"reload a module `module`","library":["imp"],"docs":[{"text":"importlib.reload(module)  \nReload a previously imported module. The argument must be a module object, so it must have been successfully imported before. This is useful if you have edited the module source file using an external editor and want to try out the new version without leaving the Python interpreter. The return value is the module object (which can be different if re-importing causes a different object to be placed in sys.modules). When reload() is executed:  Python module\u2019s code is recompiled and the module-level code re-executed, defining a new set of objects which are bound to names in the module\u2019s dictionary by reusing the loader which originally loaded the module. The init function of extension modules is not called a second time. As with all other objects in Python the old objects are only reclaimed after their reference counts drop to zero. The names in the module namespace are updated to point to any new or changed objects. Other references to the old objects (such as names external to the module) are not rebound to refer to the new objects and must be updated in each namespace where they occur if that is desired.  There are a number of other caveats: When a module is reloaded, its dictionary (containing the module\u2019s global variables) is retained. Redefinitions of names will override the old definitions, so this is generally not a problem. If the new version of a module does not define a name that was defined by the old version, the old definition remains. This feature can be used to the module\u2019s advantage if it maintains a global table or cache of objects \u2014 with a try statement it can test for the table\u2019s presence and skip its initialization if desired: try:\n    cache\nexcept NameError:\n    cache = {}\n It is generally not very useful to reload built-in or dynamically loaded modules. Reloading sys, __main__, builtins and other key modules is not recommended. In many cases extension modules are not designed to be initialized more than once, and may fail in arbitrary ways when reloaded. If a module imports objects from another module using from \u2026 import \u2026, calling reload() for the other module does not redefine the objects imported from it \u2014 one way around this is to re-execute the from statement, another is to use import and qualified names (module.name) instead. If a module instantiates instances of a class, reloading the module that defines the class does not affect the method definitions of the instances \u2014 they continue to use the old class definition. The same is true for derived classes.  New in version 3.4.   Changed in version 3.7: ModuleNotFoundError is raised when the module being reloaded lacks a ModuleSpec.","title":"python.library.importlib#importlib.reload"},{"text":"exec_module(module)  \nConcrete implementation of Loader.exec_module().  New in version 3.4.","title":"python.library.importlib#importlib.abc.SourceLoader.exec_module"},{"text":"exec_module(module)  \nImplementation of Loader.exec_module().  New in version 3.4.","title":"python.library.importlib#importlib.abc.InspectLoader.exec_module"},{"text":"exec_module(module)  \nAn abstract method that executes the module in its own namespace when a module is imported or reloaded. The module should already be initialized when exec_module() is called. When this method exists, create_module() must be defined.  New in version 3.4.   Changed in version 3.6: create_module() must also be defined.","title":"python.library.importlib#importlib.abc.Loader.exec_module"},{"text":"test.support.forget(module_name)  \nRemove the module named module_name from sys.modules and delete any byte-compiled files of the module.","title":"python.library.test#test.support.forget"},{"text":"exec_module(module)  \nInitializes the given module object in accordance with PEP 489.  New in version 3.5.","title":"python.library.importlib#importlib.machinery.ExtensionFileLoader.exec_module"},{"text":"load_module(name=None)  \nConcrete implementation of importlib.abc.Loader.load_module() where specifying the name of the module to load is optional.  Deprecated since version 3.6: Use importlib.abc.Loader.exec_module() instead.","title":"python.library.importlib#importlib.machinery.SourceFileLoader.load_module"},{"text":"test.support.unload(name)  \nDelete name from sys.modules.","title":"python.library.test#test.support.unload"},{"text":"torch.jit.freeze(mod, preserved_attrs=None, optimize_numerics=True) [source]\n \nFreezing a ScriptModule will clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph. By default, forward will be preserved, as well as attributes & methods specified in preserved_attrs. Additionally, any attribute that is modified within a preserved method will be preserved. Freezing currently only accepts ScriptModules that are in eval mode.  Parameters \n \nmod (ScriptModule) \u2013 a module to be frozen \npreserved_attrs (Optional[List[str]]) \u2013 a list of attributes to preserve in addition to the forward method. \nmodified in preserved methods will also be preserved. (Attributes) \u2013  \noptimize_numerics (bool) \u2013 If True, a set of optimization passes will be run that does not strictly \nnumerics. Full details of optimization can be found at torch.jit.optimize_frozen_module. (preserve) \u2013    Returns \nFrozen ScriptModule.   Example (Freezing a simple module with a Parameter):     def forward(self, input):\n        output = self.weight.mm(input)\n        output = self.linear(output)\n        return output\n\nscripted_module = torch.jit.script(MyModule(2, 3).eval())\nfrozen_module = torch.jit.freeze(scripted_module)\n# parameters have been removed and inlined into the Graph as constants\nassert len(list(frozen_module.named_parameters())) == 0\n# See the compiled graph as Python code\nprint(frozen_module.code)\n Example (Freezing a module with preserved attributes)     def forward(self, input):\n        self.modified_tensor += 1\n        return input + self.modified_tensor\n\nscripted_module = torch.jit.script(MyModule2().eval())\nfrozen_module = torch.jit.freeze(scripted_module, preserved_attrs=[\"version\"])\n# we've manually preserved `version`, so it still exists on the frozen module and can be modified\nassert frozen_module.version == 1\nfrozen_module.version = 2\n# `modified_tensor` is detected as being mutated in the forward, so freezing preserves\n# it to retain model semantics\nassert frozen_module(torch.tensor(1)) == torch.tensor(12)\n# now that we've run it once, the next result will be incremented by one\nassert frozen_module(torch.tensor(1)) == torch.tensor(13)\n  Note If you\u2019re not sure why an attribute is not being inlined as a constant, you can run dump_alias_db on frozen_module.forward.graph to see if freezing has detected the attribute is being modified.","title":"torch.generated.torch.jit.freeze#torch.jit.freeze"},{"text":"runpy.run_module(mod_name, init_globals=None, run_name=None, alter_sys=False)  \nExecute the code of the specified module and return the resulting module globals dictionary. The module\u2019s code is first located using the standard import mechanism (refer to PEP 302 for details) and then executed in a fresh module namespace. The mod_name argument should be an absolute module name. If the module name refers to a package rather than a normal module, then that package is imported and the __main__ submodule within that package is then executed and the resulting module globals dictionary returned. The optional dictionary argument init_globals may be used to pre-populate the module\u2019s globals dictionary before the code is executed. The supplied dictionary will not be modified. If any of the special global variables below are defined in the supplied dictionary, those definitions are overridden by run_module(). The special global variables __name__, __spec__, __file__, __cached__, __loader__ and __package__ are set in the globals dictionary before the module code is executed (Note that this is a minimal set of variables - other variables may be set implicitly as an interpreter implementation detail). __name__ is set to run_name if this optional argument is not None, to mod_name + '.__main__' if the named module is a package and to the mod_name argument otherwise. __spec__ will be set appropriately for the actually imported module (that is, __spec__.name will always be mod_name or mod_name + '.__main__, never run_name). __file__, __cached__, __loader__ and __package__ are set as normal based on the module spec. If the argument alter_sys is supplied and evaluates to True, then sys.argv[0] is updated with the value of __file__ and sys.modules[__name__] is updated with a temporary module object for the module being executed. Both sys.argv[0] and sys.modules[__name__] are restored to their original values before the function returns. Note that this manipulation of sys is not thread-safe. Other threads may see the partially initialised module, as well as the altered list of arguments. It is recommended that the sys module be left alone when invoking this function from threaded code.  See also The -m option offering equivalent functionality from the command line.   Changed in version 3.1: Added ability to execute packages by looking for a __main__ submodule.   Changed in version 3.2: Added __cached__ global variable (see PEP 3147).   Changed in version 3.4: Updated to take advantage of the module spec feature added by PEP 451. This allows __cached__ to be set correctly for modules run this way, as well as ensuring the real module name is always accessible as __spec__.name.","title":"python.library.runpy#runpy.run_module"}]}
{"task_id":19546911,"prompt":"def f_19546911(number):\n\treturn ","suffix":"","canonical_solution":"struct.unpack('H', struct.pack('h', number))","test_start":"\nimport struct \n\ndef check(candidate):","test":["\n    assert candidate(3) == (3,)\n"],"entry_point":"f_19546911","intent":"Convert integer `number` into an unassigned integer","library":["struct"],"docs":[{"text":"operator.inv(obj)  \noperator.invert(obj)  \noperator.__inv__(obj)  \noperator.__invert__(obj)  \nReturn the bitwise inverse of the number obj. This is equivalent to ~obj.","title":"python.library.operator#operator.__invert__"},{"text":"operator.inv(obj)  \noperator.invert(obj)  \noperator.__inv__(obj)  \noperator.__invert__(obj)  \nReturn the bitwise inverse of the number obj. This is equivalent to ~obj.","title":"python.library.operator#operator.__inv__"},{"text":"operator.inv(obj)  \noperator.invert(obj)  \noperator.__inv__(obj)  \noperator.__invert__(obj)  \nReturn the bitwise inverse of the number obj. This is equivalent to ~obj.","title":"python.library.operator#operator.invert"},{"text":"operator.inv(obj)  \noperator.invert(obj)  \noperator.__inv__(obj)  \noperator.__invert__(obj)  \nReturn the bitwise inverse of the number obj. This is equivalent to ~obj.","title":"python.library.operator#operator.inv"},{"text":"numpy.number.__class_getitem__ method   number.__class_getitem__(item, \/)\n \nReturn a parametrized wrapper around the number type.  New in version 1.22.   Returns \n \naliastypes.GenericAlias\n\n\nA parametrized number type.      See also  PEP 585\n\nType hinting generics in standard collections.    Notes This method is only available for python 3.9 and later. Examples >>> from typing import Any\n>>> import numpy as np\n >>> np.signedinteger[Any]\nnumpy.signedinteger[typing.Any]","title":"numpy.reference.generated.numpy.number.__class_getitem__"},{"text":"class numbers.Number  \nThe root of the numeric hierarchy. If you just want to check if an argument x is a number, without caring what kind, use isinstance(x, Number).","title":"python.library.numbers#numbers.Number"},{"text":"round(number[, ndigits])  \nReturn number rounded to ndigits precision after the decimal point. If ndigits is omitted or is None, it returns the nearest integer to its input. For the built-in types supporting round(), values are rounded to the closest multiple of 10 to the power minus ndigits; if two multiples are equally close, rounding is done toward the even choice (so, for example, both round(0.5) and round(-0.5) are 0, and round(1.5) is 2). Any integer value is valid for ndigits (positive, zero, or negative). The return value is an integer if ndigits is omitted or None. Otherwise the return value has the same type as number. For a general Python object number, round delegates to number.__round__.  Note The behavior of round() for floats can be surprising: for example, round(2.675, 2) gives 2.67 instead of the expected 2.68. This is not a bug: it\u2019s a result of the fact that most decimal fractions can\u2019t be represented exactly as a float. See Floating Point Arithmetic: Issues and Limitations for more information.","title":"python.library.functions#round"},{"text":"repeat(repeat=5, number=1000000)  \nCall timeit() a few times. This is a convenience function that calls the timeit() repeatedly, returning a list of results. The first argument specifies how many times to call timeit(). The second argument specifies the number argument for timeit().  Note It\u2019s tempting to calculate mean and standard deviation from the result vector and report these. However, this is not very useful. In a typical case, the lowest value gives a lower bound for how fast your machine can run the given code snippet; higher values in the result vector are typically not caused by variability in Python\u2019s speed, but by other processes interfering with your timing accuracy. So the min() of the result is probably the only number you should be interested in. After that, you should look at the entire vector and apply common sense rather than statistics.   Changed in version 3.7: Default value of repeat changed from 3 to 5.","title":"python.library.timeit#timeit.Timer.repeat"},{"text":"timeit(number=1000000)  \nTime number executions of the main statement. This executes the setup statement once, and then returns the time it takes to execute the main statement a number of times, measured in seconds as a float. The argument is the number of times through the loop, defaulting to one million. The main statement, the setup statement and the timer function to be used are passed to the constructor.  Note By default, timeit() temporarily turns off garbage collection during the timing. The advantage of this approach is that it makes independent timings more comparable. The disadvantage is that GC may be an important component of the performance of the function being measured. If so, GC can be re-enabled as the first statement in the setup string. For example: timeit.Timer('for i in range(10): oct(i)', 'gc.enable()').timeit()","title":"python.library.timeit#timeit.Timer.timeit"},{"text":"torch.nn.functional.prelu(input, weight) \u2192 Tensor [source]\n \nApplies element-wise the function PReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)  where weight is a learnable parameter. See PReLU for more details.","title":"torch.nn.functional#torch.nn.functional.prelu"}]}
{"task_id":9746522,"prompt":"def f_9746522(numlist):\n\t","suffix":"\n\treturn numlist","canonical_solution":"numlist = [float(x) for x in numlist]","test_start":"\ndef check(candidate):","test":["\n    assert candidate([3, 4]) == [3.0, 4.0]\n"],"entry_point":"f_9746522","intent":"convert int values in list `numlist` to float","library":[],"docs":[]}
{"task_id":20107570,"prompt":"def f_20107570(df, filename):\n\t","suffix":"\n\treturn ","canonical_solution":"df.to_csv(filename, index=False)","test_start":"\nimport pandas as pd\n\ndef check(candidate):","test":["\n    file_name = 'a.csv'\n    df = pd.DataFrame([1, 2, 3], columns = ['Vals'])\n    candidate(df, file_name)\n    with open (file_name, 'r') as f:\n        lines = f.readlines()\n    assert len(lines) == 4\n"],"entry_point":"f_20107570","intent":"write dataframe `df`, excluding index, to a csv file `filename`","library":["pandas"],"docs":[{"text":"pandas.io.stata.StataWriter.write_file   StataWriter.write_file()[source]\n \nExport DataFrame object to Stata dta format.","title":"pandas.reference.api.pandas.io.stata.statawriter.write_file"},{"text":"pandas.errors.DtypeWarning   exceptionpandas.errors.DtypeWarning[source]\n \nWarning raised when reading different dtypes in a column from a file. Raised for a dtype incompatibility. This can happen whenever read_csv or read_table encounter non-uniform dtypes in a column(s) of a given CSV file.  See also  read_csv\n\nRead CSV (comma-separated) file into a DataFrame.  read_table\n\nRead general delimited file into a DataFrame.    Notes This warning is issued when dealing with larger files because the dtype checking happens per chunk read. Despite the warning, the CSV file is read with mixed types in a single column which will be an object type. See the examples below to better understand this issue. Examples This example creates and reads a large CSV file with a column that contains int and str. \n>>> df = pd.DataFrame({'a': (['1'] * 100000 + ['X'] * 100000 +\n...                          ['1'] * 100000),\n...                    'b': ['b'] * 300000})  \n>>> df.to_csv('test.csv', index=False)  \n>>> df2 = pd.read_csv('test.csv')  \n... # DtypeWarning: Columns (0) have mixed types\n  Important to notice that df2 will contain both str and int for the same input, \u20181\u2019. \n>>> df2.iloc[262140, 0]  \n'1'\n>>> type(df2.iloc[262140, 0])  \n<class 'str'>\n>>> df2.iloc[262150, 0]  \n1\n>>> type(df2.iloc[262150, 0])  \n<class 'int'>\n  One way to solve this issue is using the dtype parameter in the read_csv and read_table functions to explicit the conversion: \n>>> df2 = pd.read_csv('test.csv', sep=',', dtype={'a': str})  \n  No warning was issued.","title":"pandas.reference.api.pandas.errors.dtypewarning"},{"text":"class werkzeug.middleware.shared_data.SharedDataMiddleware(app, exports, disallow=None, cache=True, cache_timeout=43200, fallback_mimetype='application\/octet-stream')  \nA WSGI middleware which provides static content for development environments or simple server setups. Its usage is quite simple: import os\nfrom werkzeug.middleware.shared_data import SharedDataMiddleware\n\napp = SharedDataMiddleware(app, {\n    '\/shared': os.path.join(os.path.dirname(__file__), 'shared')\n})\n The contents of the folder .\/shared will now be available on http:\/\/example.com\/shared\/. This is pretty useful during development because a standalone media server is not required. Files can also be mounted on the root folder and still continue to use the application because the shared data middleware forwards all unhandled requests to the application, even if the requests are below one of the shared folders. If pkg_resources is available you can also tell the middleware to serve files from package data: app = SharedDataMiddleware(app, {\n    '\/static': ('myapplication', 'static')\n})\n This will then serve the static folder in the myapplication Python package. The optional disallow parameter can be a list of fnmatch() rules for files that are not accessible from the web. If cache is set to False no caching headers are sent. Currently the middleware does not support non-ASCII filenames. If the encoding on the file system happens to match the encoding of the URI it may work but this could also be by accident. We strongly suggest using ASCII only file names for static files. The middleware will guess the mimetype using the Python mimetype module. If it\u2019s unable to figure out the charset it will fall back to fallback_mimetype.  Parameters \n \napp (WSGIApplication) \u2013 the application to wrap. If you don\u2019t want to wrap an application you can pass it NotFound. \nexports (Union[Dict[str, Union[str, Tuple[str, str]]], Iterable[Tuple[str, Union[str, Tuple[str, str]]]]]) \u2013 a list or dict of exported files and folders. \ndisallow (None) \u2013 a list of fnmatch() rules. \ncache (bool) \u2013 enable or disable caching headers. \ncache_timeout (int) \u2013 the cache timeout in seconds for the headers. \nfallback_mimetype (str) \u2013 The fallback mimetype for unknown files.   Return type \nNone    Changelog Changed in version 1.0: The default fallback_mimetype is application\/octet-stream. If a filename looks like a text mimetype, the utf-8 charset is added to it.   New in version 0.6: Added fallback_mimetype.   Changed in version 0.5: Added cache_timeout.   \nis_allowed(filename)  \nSubclasses can override this method to disallow the access to certain files. However by providing disallow in the constructor this method is overwritten.  Parameters \nfilename (str) \u2013   Return type \nbool","title":"werkzeug.middleware.shared_data.index#werkzeug.middleware.shared_data.SharedDataMiddleware"},{"text":"tf.train.SequenceExample     View source on GitHub    A ProtocolMessage  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.train.SequenceExample \n \n\n\n Attributes\n  context   Features context  \n  feature_lists   FeatureLists feature_lists","title":"tensorflow.train.sequenceexample"},{"text":"pandas.DataFrame.to_csv   DataFrame.to_csv(path_or_buf=None, sep=',', na_rep='', float_format=None, columns=None, header=True, index=True, index_label=None, mode='w', encoding=None, compression='infer', quoting=None, quotechar='\"', line_terminator=None, chunksize=None, date_format=None, doublequote=True, escapechar=None, decimal='.', errors='strict', storage_options=None)[source]\n \nWrite object to a comma-separated values (csv) file.  Parameters \n \npath_or_buf:str, path object, file-like object, or None, default None\n\n\nString, path object (implementing os.PathLike[str]), or file-like object implementing a write() function. If None, the result is returned as a string. If a non-binary file object is passed, it should be opened with newline=\u2019\u2019, disabling universal newlines. If a binary file object is passed, mode might need to contain a \u2018b\u2019.  Changed in version 1.2.0: Support for binary file objects was introduced.   \nsep:str, default \u2018,\u2019\n\n\nString of length 1. Field delimiter for the output file.  \nna_rep:str, default \u2018\u2019\n\n\nMissing data representation.  \nfloat_format:str, default None\n\n\nFormat string for floating point numbers.  \ncolumns:sequence, optional\n\n\nColumns to write.  \nheader:bool or list of str, default True\n\n\nWrite out the column names. If a list of strings is given it is assumed to be aliases for the column names.  \nindex:bool, default True\n\n\nWrite row names (index).  \nindex_label:str or sequence, or False, default None\n\n\nColumn label for index column(s) if desired. If None is given, and header and index are True, then the index names are used. A sequence should be given if the object uses MultiIndex. If False do not print fields for index names. Use index_label=False for easier importing in R.  \nmode:str\n\n\nPython write mode, default \u2018w\u2019.  \nencoding:str, optional\n\n\nA string representing the encoding to use in the output file, defaults to \u2018utf-8\u2019. encoding is not supported if path_or_buf is a non-binary file object.  \ncompression:str or dict, default \u2018infer\u2019\n\n\nFor on-the-fly compression of the output data. If \u2018infer\u2019 and \u2018%s\u2019 path-like, then detect compression from the following extensions: \u2018.gz\u2019, \u2018.bz2\u2019, \u2018.zip\u2019, \u2018.xz\u2019, or \u2018.zst\u2019 (otherwise no compression). Set to None for no compression. Can also be a dict with key 'method' set to one of {'zip', 'gzip', 'bz2', 'zstd'} and other key-value pairs are forwarded to zipfile.ZipFile, gzip.GzipFile, bz2.BZ2File, or zstandard.ZstdDecompressor, respectively. As an example, the following could be passed for faster compression and to create a reproducible gzip archive: compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}.  Changed in version 1.0.0: May now be a dict with key \u2018method\u2019 as compression mode and other entries as additional compression options if compression mode is \u2018zip\u2019.   Changed in version 1.1.0: Passing compression options as keys in dict is supported for compression modes \u2018gzip\u2019, \u2018bz2\u2019, \u2018zstd\u2019, and \u2018zip\u2019.   Changed in version 1.2.0: Compression is supported for binary file objects.   Changed in version 1.2.0: Previous versions forwarded dict entries for \u2018gzip\u2019 to gzip.open instead of gzip.GzipFile which prevented setting mtime.   \nquoting:optional constant from csv module\n\n\nDefaults to csv.QUOTE_MINIMAL. If you have set a float_format then floats are converted to strings and thus csv.QUOTE_NONNUMERIC will treat them as non-numeric.  \nquotechar:str, default \u2018\"\u2019\n\n\nString of length 1. Character used to quote fields.  \nline_terminator:str, optional\n\n\nThe newline character or character sequence to use in the output file. Defaults to os.linesep, which depends on the OS in which this method is called (\u2019\\n\u2019 for linux, \u2018\\r\\n\u2019 for Windows, i.e.).  \nchunksize:int or None\n\n\nRows to write at a time.  \ndate_format:str, default None\n\n\nFormat string for datetime objects.  \ndoublequote:bool, default True\n\n\nControl quoting of quotechar inside a field.  \nescapechar:str, default None\n\n\nString of length 1. Character used to escape sep and quotechar when appropriate.  \ndecimal:str, default \u2018.\u2019\n\n\nCharacter recognized as decimal separator. E.g. use \u2018,\u2019 for European data.  \nerrors:str, default \u2018strict\u2019\n\n\nSpecifies how encoding and decoding errors are to be handled. See the errors argument for open() for a full list of options.  New in version 1.1.0.   \nstorage_options:dict, optional\n\n\nExtra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib as header options. For other URLs (e.g. starting with \u201cs3:\/\/\u201d, and \u201cgcs:\/\/\u201d) the key-value pairs are forwarded to fsspec. Please see fsspec and urllib for more details.  New in version 1.2.0.     Returns \n None or str\n\nIf path_or_buf is None, returns the resulting csv format as a string. Otherwise returns None.      See also  read_csv\n\nLoad a CSV file into a DataFrame.  to_excel\n\nWrite DataFrame to an Excel file.    Examples \n>>> df = pd.DataFrame({'name': ['Raphael', 'Donatello'],\n...                    'mask': ['red', 'purple'],\n...                    'weapon': ['sai', 'bo staff']})\n>>> df.to_csv(index=False)\n'name,mask,weapon\\nRaphael,red,sai\\nDonatello,purple,bo staff\\n'\n  Create \u2018out.zip\u2019 containing \u2018out.csv\u2019 \n>>> compression_opts = dict(method='zip',\n...                         archive_name='out.csv')  \n>>> df.to_csv('out.zip', index=False,\n...           compression=compression_opts)  \n  To write a csv file to a new folder or nested folder you will first need to create it using either Pathlib or os: \n>>> from pathlib import Path  \n>>> filepath = Path('folder\/subfolder\/out.csv')  \n>>> filepath.parent.mkdir(parents=True, exist_ok=True)  \n>>> df.to_csv(filepath)  \n  \n>>> import os  \n>>> os.makedirs('folder\/subfolder', exist_ok=True)  \n>>> df.to_csv('folder\/subfolder\/out.csv')","title":"pandas.reference.api.pandas.dataframe.to_csv"},{"text":"tf.keras.losses.sparse_categorical_crossentropy     View source on GitHub    Computes the sparse categorical crossentropy loss.  View aliases  Main aliases \ntf.keras.metrics.sparse_categorical_crossentropy, tf.losses.sparse_categorical_crossentropy, tf.metrics.sparse_categorical_crossentropy Compat aliases for migration See Migration guide for more details. tf.compat.v1.keras.losses.sparse_categorical_crossentropy, tf.compat.v1.keras.metrics.sparse_categorical_crossentropy  \ntf.keras.losses.sparse_categorical_crossentropy(\n    y_true, y_pred, from_logits=False, axis=-1\n)\n Standalone usage: \ny_true = [1, 2]\ny_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\nloss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\nassert loss.shape == (2,)\nloss.numpy()\narray([0.0513, 2.303], dtype=float32)\n\n \n\n\n Args\n  y_true   Ground truth values.  \n  y_pred   The predicted values.  \n  from_logits   Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution.  \n  axis   (Optional) Defaults to -1. The dimension along which the entropy is computed.   \n \n\n\n Returns   Sparse categorical crossentropy loss value.","title":"tensorflow.keras.losses.sparse_categorical_crossentropy"},{"text":"ModelAdmin.get_prepopulated_fields(request, obj=None)  \nThe get_prepopulated_fields method is given the HttpRequest and the obj being edited (or None on an add form) and is expected to return a dictionary, as described above in the ModelAdmin.prepopulated_fields section.","title":"django.ref.contrib.admin.index#django.contrib.admin.ModelAdmin.get_prepopulated_fields"},{"text":"pandas.DatetimeIndex.is_year_start   propertyDatetimeIndex.is_year_start\n \nIndicate whether the date is the first day of a year.  Returns \n Series or DatetimeIndex\n\nThe same type as the original data with boolean values. Series will have the same name and index. DatetimeIndex will have the same name.      See also  is_year_end\n\nSimilar property indicating the last day of the year.    Examples This method is available on Series with datetime values under the .dt accessor, and directly on DatetimeIndex. \n>>> dates = pd.Series(pd.date_range(\"2017-12-30\", periods=3))\n>>> dates\n0   2017-12-30\n1   2017-12-31\n2   2018-01-01\ndtype: datetime64[ns]\n  \n>>> dates.dt.is_year_start\n0    False\n1    False\n2    True\ndtype: bool\n  \n>>> idx = pd.date_range(\"2017-12-30\", periods=3)\n>>> idx\nDatetimeIndex(['2017-12-30', '2017-12-31', '2018-01-01'],\n              dtype='datetime64[ns]', freq='D')\n  \n>>> idx.is_year_start\narray([False, False,  True])","title":"pandas.reference.api.pandas.datetimeindex.is_year_start"},{"text":"get_json(force=False, silent=False)  \nParse data as JSON. Useful during testing. If the mimetype does not indicate JSON (application\/json, see is_json()), this returns None. Unlike Request.get_json(), the result is not cached.  Parameters \n \nforce (bool) \u2013 Ignore the mimetype and always try to parse JSON. \nsilent (bool) \u2013 Silence parsing errors and return None instead.   Return type \nOptional[Any]","title":"flask.api.index#flask.Response.get_json"},{"text":"tf.raw_ops.ParseSequenceExample Transforms a vector of brain.SequenceExample protos (as strings) into typed tensors.  View aliases  Compat aliases for migration \nSee Migration guide for more details. tf.compat.v1.raw_ops.ParseSequenceExample  \ntf.raw_ops.ParseSequenceExample(\n    serialized, debug_name, context_dense_defaults,\n    feature_list_dense_missing_assumed_empty, context_sparse_keys,\n    context_dense_keys, feature_list_sparse_keys, feature_list_dense_keys,\n    Ncontext_sparse=0, Ncontext_dense=0, Nfeature_list_sparse=0,\n    Nfeature_list_dense=0, context_sparse_types=[], feature_list_dense_types=[],\n    context_dense_shapes=[], feature_list_sparse_types=[],\n    feature_list_dense_shapes=[], name=None\n)\n\n \n\n\n Args\n  serialized   A Tensor of type string. A vector containing binary serialized SequenceExample protos.  \n  debug_name   A Tensor of type string. A vector containing the names of the serialized protos. May contain, for example, table key (descriptive) name for the corresponding serialized proto. This is purely useful for debugging purposes, and the presence of values here has no effect on the output. May also be an empty vector if no name is available.  \n  context_dense_defaults   A list of Tensor objects with types from: float32, int64, string. A list of Ncontext_dense Tensors (some may be empty). context_dense_defaults[j] provides default values when the SequenceExample's context map lacks context_dense_key[j]. If an empty Tensor is provided for context_dense_defaults[j], then the Feature context_dense_keys[j] is required. The input type is inferred from context_dense_defaults[j], even when it's empty. If context_dense_defaults[j] is not empty, its shape must match context_dense_shapes[j].  \n  feature_list_dense_missing_assumed_empty   A list of strings. A vector listing the FeatureList keys which may be missing from the SequenceExamples. If the associated FeatureList is missing, it is treated as empty. By default, any FeatureList not listed in this vector must exist in the SequenceExamples.  \n  context_sparse_keys   A list of strings. A list of Ncontext_sparse string Tensors (scalars). The keys expected in the Examples' features associated with context_sparse values.  \n  context_dense_keys   A list of strings. A list of Ncontext_dense string Tensors (scalars). The keys expected in the SequenceExamples' context features associated with dense values.  \n  feature_list_sparse_keys   A list of strings. A list of Nfeature_list_sparse string Tensors (scalars). The keys expected in the FeatureLists associated with sparse values.  \n  feature_list_dense_keys   A list of strings. A list of Nfeature_list_dense string Tensors (scalars). The keys expected in the SequenceExamples' feature_lists associated with lists of dense values.  \n  Ncontext_sparse   An optional int that is >= 0. Defaults to 0.  \n  Ncontext_dense   An optional int that is >= 0. Defaults to 0.  \n  Nfeature_list_sparse   An optional int that is >= 0. Defaults to 0.  \n  Nfeature_list_dense   An optional int that is >= 0. Defaults to 0.  \n  context_sparse_types   An optional list of tf.DTypes from: tf.float32, tf.int64, tf.string. Defaults to []. A list of Ncontext_sparse types; the data types of data in each context Feature given in context_sparse_keys. Currently the ParseSingleSequenceExample supports DT_FLOAT (FloatList), DT_INT64 (Int64List), and DT_STRING (BytesList).  \n  feature_list_dense_types   An optional list of tf.DTypes from: tf.float32, tf.int64, tf.string. Defaults to [].  \n  context_dense_shapes   An optional list of shapes (each a tf.TensorShape or list of ints). Defaults to []. A list of Ncontext_dense shapes; the shapes of data in each context Feature given in context_dense_keys. The number of elements in the Feature corresponding to context_dense_key[j] must always equal context_dense_shapes[j].NumEntries(). The shape of context_dense_values[j] will match context_dense_shapes[j].  \n  feature_list_sparse_types   An optional list of tf.DTypes from: tf.float32, tf.int64, tf.string. Defaults to []. A list of Nfeature_list_sparse types; the data types of data in each FeatureList given in feature_list_sparse_keys. Currently the ParseSingleSequenceExample supports DT_FLOAT (FloatList), DT_INT64 (Int64List), and DT_STRING (BytesList).  \n  feature_list_dense_shapes   An optional list of shapes (each a tf.TensorShape or list of ints). Defaults to []. A list of Nfeature_list_dense shapes; the shapes of data in each FeatureList given in feature_list_dense_keys. The shape of each Feature in the FeatureList corresponding to feature_list_dense_key[j] must always equal feature_list_dense_shapes[j].NumEntries().  \n  name   A name for the operation (optional).   \n \n\n\n Returns   A tuple of Tensor objects (context_sparse_indices, context_sparse_values, context_sparse_shapes, context_dense_values, feature_list_sparse_indices, feature_list_sparse_values, feature_list_sparse_shapes, feature_list_dense_values, feature_list_dense_lengths).     context_sparse_indices   A list of Ncontext_sparse Tensor objects with type int64.  \n  context_sparse_values   A list of Tensor objects of type context_sparse_types.  \n  context_sparse_shapes   A list of Ncontext_sparse Tensor objects with type int64.  \n  context_dense_values   A list of Tensor objects. Has the same type as context_dense_defaults.  \n  feature_list_sparse_indices   A list of Nfeature_list_sparse Tensor objects with type int64.  \n  feature_list_sparse_values   A list of Tensor objects of type feature_list_sparse_types.  \n  feature_list_sparse_shapes   A list of Nfeature_list_sparse Tensor objects with type int64.  \n  feature_list_dense_values   A list of Tensor objects of type feature_list_dense_types.  \n  feature_list_dense_lengths   A list of Nfeature_list_dense Tensor objects with type int64.","title":"tensorflow.raw_ops.parsesequenceexample"}]}
{"task_id":8740353,"prompt":"def f_8740353(unescaped):\n\t","suffix":"\n\treturn json_data","canonical_solution":"json_data = json.loads(unescaped)","test_start":"\nimport json \n\ndef check(candidate):","test":["\n    x = \"\"\"{\n    \"Name\": \"Jennifer Smith\",\n    \"Contact Number\": 7867567898,\n    \"Email\": \"jen123@gmail.com\",\n    \"Hobbies\":[\"Reading\", \"Sketching\", \"Horse Riding\"]\n    }\"\"\"\n    assert candidate(x) == {'Hobbies': ['Reading', 'Sketching', 'Horse Riding'], 'Name': 'Jennifer Smith', 'Email': 'jen123@gmail.com', 'Contact Number': 7867567898}\n"],"entry_point":"f_8740353","intent":"convert a urllib unquoted string `unescaped` to a json data `json_data`","library":["json"],"docs":[{"text":"decode(s)  \nReturn the Python representation of s (a str instance containing a JSON document). JSONDecodeError will be raised if the given JSON document is not valid.","title":"python.library.json#json.JSONDecoder.decode"},{"text":"urllib.parse.unquote_to_bytes(string)  \nReplace %xx escapes with their single-octet equivalent, and return a bytes object. string may be either a str or a bytes object. If it is a str, unescaped non-ASCII characters in string are encoded into UTF-8 bytes. Example: unquote_to_bytes('a%26%EF') yields b'a&\\xef'.","title":"python.library.urllib.parse#urllib.parse.unquote_to_bytes"},{"text":"urllib.parse.unquote(string, encoding='utf-8', errors='replace')  \nReplace %xx escapes with their single-character equivalent. The optional encoding and errors parameters specify how to decode percent-encoded sequences into Unicode characters, as accepted by the bytes.decode() method. string may be either a str or a bytes object. encoding defaults to 'utf-8'. errors defaults to 'replace', meaning invalid sequences are replaced by a placeholder character. Example: unquote('\/El%20Ni%C3%B1o\/') yields '\/El Ni\u00f1o\/'.  Changed in version 3.9: string parameter supports bytes and str objects (previously only str).","title":"python.library.urllib.parse#urllib.parse.unquote"},{"text":"werkzeug.urls.url_unquote(s, charset='utf-8', errors='replace', unsafe='')  \nURL decode a single string with a given encoding. If the charset is set to None no decoding is performed and raw bytes are returned.  Parameters \n \ns (Union[str, bytes]) \u2013 the string to unquote. \ncharset (str) \u2013 the charset of the query string. If set to None no decoding will take place. \nerrors (str) \u2013 the error handling for the charset decoding. \nunsafe (str) \u2013    Return type \nstr","title":"werkzeug.urls.index#werkzeug.urls.url_unquote"},{"text":"xml.sax.saxutils.unescape(data, entities={})  \nUnescape '&amp;', '&lt;', and '&gt;' in a string of data. You can unescape other strings of data by passing a dictionary as the optional entities parameter. The keys and values must all be strings; each key will be replaced with its corresponding value. '&amp', '&lt;', and '&gt;' are always unescaped, even if entities is provided.","title":"python.library.xml.sax.utils#xml.sax.saxutils.unescape"},{"text":"werkzeug.utils.unescape(s)  \nThe reverse of escape(). This unescapes all the HTML entities, not only those inserted by escape.  Deprecated since version 2.0: Will be removed in Werkzeug 2.1. Use MarkupSafe instead.   Parameters \ns (str) \u2013   Return type \nstr","title":"werkzeug.utils.index#werkzeug.utils.unescape"},{"text":"werkzeug.urls.url_unquote_plus(s, charset='utf-8', errors='replace')  \nURL decode a single string with the given charset and decode \u201c+\u201d to whitespace. Per default encoding errors are ignored. If you want a different behavior you can set errors to 'replace' or 'strict'.  Parameters \n \ns (Union[str, bytes]) \u2013 The string to unquote. \ncharset (str) \u2013 the charset of the query string. If set to None no decoding will take place. \nerrors (str) \u2013 The error handling for the charset decoding.   Return type \nstr","title":"werkzeug.urls.index#werkzeug.urls.url_unquote_plus"},{"text":"html.unescape(s)  \nConvert all named and numeric character references (e.g. &gt;, &#62;, &#x3e;) in the string s to the corresponding Unicode characters. This function uses the rules defined by the HTML 5 standard for both valid and invalid character references, and the list of\nHTML 5 named character references.  New in version 3.4.","title":"python.library.html#html.unescape"},{"text":"email.utils.decode_rfc2231(s)  \nDecode the string s according to RFC 2231.","title":"python.library.email.utils#email.utils.decode_rfc2231"},{"text":"urllib.parse.unquote_plus(string, encoding='utf-8', errors='replace')  \nLike unquote(), but also replace plus signs with spaces, as required for unquoting HTML form values. string must be a str. Example: unquote_plus('\/El+Ni%C3%B1o\/') yields '\/El Ni\u00f1o\/'.","title":"python.library.urllib.parse#urllib.parse.unquote_plus"}]}
{"task_id":5891453,"prompt":"def f_5891453():\n\treturn ","suffix":"","canonical_solution":"[chr(i) for i in range(127)]","test_start":"\ndef check(candidate):","test":["\n    chars = candidate()\n    assert len(chars) == 127\n    assert chars == [chr(i) for i in range(127)]\n"],"entry_point":"f_5891453","intent":"Create a list containing all ascii characters as its elements","library":[],"docs":[]}
{"task_id":18367007,"prompt":"def f_18367007(newFileBytes, newFile):\n\t","suffix":"\n\treturn ","canonical_solution":"newFile.write(struct.pack('5B', *newFileBytes))","test_start":"\nimport struct \n\ndef check(candidate):","test":["\n    newFileBytes = [123, 3, 123, 100, 99]\n    file_name = 'f.txt'\n    newFile = open(file_name, 'wb')\n    candidate(newFileBytes, newFile)\n    newFile.close()\n    with open (file_name, 'rb') as f:\n        lines = f.readlines()\n        assert lines == [b'{\u0003{dc']\n"],"entry_point":"f_18367007","intent":"write `newFileBytes` to a binary file `newFile`","library":["struct"],"docs":[{"text":"Path.write_bytes(data)  \nOpen the file pointed to in bytes mode, write data to it, and close the file: >>> p = Path('my_binary_file')\n>>> p.write_bytes(b'Binary file contents')\n20\n>>> p.read_bytes()\nb'Binary file contents'\n An existing file of the same name is overwritten.  New in version 3.5.","title":"python.library.pathlib#pathlib.Path.write_bytes"},{"text":"Path.read_bytes()  \nReturn the binary contents of the pointed-to file as a bytes object: >>> p = Path('my_binary_file')\n>>> p.write_bytes(b'Binary file contents')\n20\n>>> p.read_bytes()\nb'Binary file contents'\n  New in version 3.5.","title":"python.library.pathlib#pathlib.Path.read_bytes"},{"text":"set_data(path, data)  \nOptional abstract method which writes the specified bytes to a file path. Any intermediate directories which do not exist are to be created automatically. When writing to the path fails because the path is read-only (errno.EACCES\/PermissionError), do not propagate the exception.  Changed in version 3.4: No longer raises NotImplementedError when called.","title":"python.library.importlib#importlib.abc.SourceLoader.set_data"},{"text":"class urllib.parse.SplitResultBytes(scheme, netloc, path, query, fragment)  \nConcrete class for urlsplit() results containing bytes data. The decode() method returns a SplitResult instance.  New in version 3.2.","title":"python.library.urllib.parse#urllib.parse.SplitResultBytes"},{"text":"Path.read_bytes()  \nRead the current file as bytes.","title":"python.library.zipfile#zipfile.Path.read_bytes"},{"text":"update(new_scale=None) [source]\n \nUpdates the scale factor. If any optimizer steps were skipped the scale is multiplied by backoff_factor to reduce it. If growth_interval unskipped iterations occurred consecutively, the scale is multiplied by growth_factor to increase it. Passing new_scale sets the scale directly.  Parameters \nnew_scale (float or torch.cuda.FloatTensor, optional, default=None) \u2013 New scale factor.    Warning update() should only be called at the end of the iteration, after scaler.step(optimizer) has been invoked for all optimizers used this iteration.","title":"torch.amp#torch.cuda.amp.GradScaler.update"},{"text":"models.BaseModelFormSet.new_objects","title":"django.topics.forms.modelforms#django.forms.models.BaseModelFormSet.new_objects"},{"text":"bytes.replace(old, new[, count])  \nbytearray.replace(old, new[, count])  \nReturn a copy of the sequence with all occurrences of subsequence old replaced by new. If the optional argument count is given, only the first count occurrences are replaced. The subsequence to search for and its replacement may be any bytes-like object.  Note The bytearray version of this method does not operate in place - it always produces a new object, even if no changes were made.","title":"python.library.stdtypes#bytes.replace"},{"text":"bytes.replace(old, new[, count])  \nbytearray.replace(old, new[, count])  \nReturn a copy of the sequence with all occurrences of subsequence old replaced by new. If the optional argument count is given, only the first count occurrences are replaced. The subsequence to search for and its replacement may be any bytes-like object.  Note The bytearray version of this method does not operate in place - it always produces a new object, even if no changes were made.","title":"python.library.stdtypes#bytearray.replace"},{"text":"_thread.start_new_thread(function, args[, kwargs])  \nStart a new thread and return its identifier. The thread executes the function function with the argument list args (which must be a tuple). The optional kwargs argument specifies a dictionary of keyword arguments. When the function returns, the thread silently exits. When the function terminates with an unhandled exception, sys.unraisablehook() is called to handle the exception. The object attribute of the hook argument is function. By default, a stack trace is printed and then the thread exits (but other threads continue to run). When the function raises a SystemExit exception, it is silently ignored.  Changed in version 3.8: sys.unraisablehook() is now used to handle unhandled exceptions.","title":"python.library._thread#_thread.start_new_thread"}]}
{"task_id":21805490,"prompt":"def f_21805490(string):\n\treturn ","suffix":"","canonical_solution":"re.sub('^[A-Z0-9]*(?![a-z])', '', string)","test_start":"\nimport re \n\ndef check(candidate):","test":["\n    assert candidate(\"AASKH317298DIUANFProgramming is fun\") == \"Programming is fun\"\n"],"entry_point":"f_21805490","intent":"python regex - check for a capital letter with a following lowercase in string `string`","library":["re"],"docs":[{"text":"str.istitle()  \nReturn True if the string is a titlecased string and there is at least one character, for example uppercase characters may only follow uncased characters and lowercase characters only cased ones. Return False otherwise.","title":"python.library.stdtypes#str.istitle"},{"text":"str.islower()  \nReturn True if all cased characters 4 in the string are lowercase and there is at least one cased character, False otherwise.","title":"python.library.stdtypes#str.islower"},{"text":"str.isupper()  \nReturn True if all cased characters 4 in the string are uppercase and there is at least one cased character, False otherwise. >>> 'BANANA'.isupper()\nTrue\n>>> 'banana'.isupper()\nFalse\n>>> 'baNana'.isupper()\nFalse\n>>> ' '.isupper()\nFalse","title":"python.library.stdtypes#str.isupper"},{"text":"numpy.chararray.isupper method   chararray.isupper()[source]\n \nReturns true for each element if all cased characters in the string are uppercase and there is at least one character, false otherwise.  See also  char.isupper","title":"numpy.reference.generated.numpy.chararray.isupper"},{"text":"bytes.islower()  \nbytearray.islower()  \nReturn True if there is at least one lowercase ASCII character in the sequence and no uppercase ASCII characters, False otherwise. For example: >>> b'hello world'.islower()\nTrue\n>>> b'Hello world'.islower()\nFalse\n Lowercase ASCII characters are those byte values in the sequence b'abcdefghijklmnopqrstuvwxyz'. Uppercase ASCII characters are those byte values in the sequence b'ABCDEFGHIJKLMNOPQRSTUVWXYZ'.","title":"python.library.stdtypes#bytes.islower"},{"text":"bytes.islower()  \nbytearray.islower()  \nReturn True if there is at least one lowercase ASCII character in the sequence and no uppercase ASCII characters, False otherwise. For example: >>> b'hello world'.islower()\nTrue\n>>> b'Hello world'.islower()\nFalse\n Lowercase ASCII characters are those byte values in the sequence b'abcdefghijklmnopqrstuvwxyz'. Uppercase ASCII characters are those byte values in the sequence b'ABCDEFGHIJKLMNOPQRSTUVWXYZ'.","title":"python.library.stdtypes#bytearray.islower"},{"text":"numpy.chararray.islower method   chararray.islower()[source]\n \nReturns true for each element if all cased characters in the string are lowercase and there is at least one cased character, false otherwise.  See also  char.islower","title":"numpy.reference.generated.numpy.chararray.islower"},{"text":"numpy.char.chararray.isupper method   char.chararray.isupper()[source]\n \nReturns true for each element if all cased characters in the string are uppercase and there is at least one character, false otherwise.  See also  char.isupper","title":"numpy.reference.generated.numpy.char.chararray.isupper"},{"text":"bytes.isupper()  \nbytearray.isupper()  \nReturn True if there is at least one uppercase alphabetic ASCII character in the sequence and no lowercase ASCII characters, False otherwise. For example: >>> b'HELLO WORLD'.isupper()\nTrue\n>>> b'Hello world'.isupper()\nFalse\n Lowercase ASCII characters are those byte values in the sequence b'abcdefghijklmnopqrstuvwxyz'. Uppercase ASCII characters are those byte values in the sequence b'ABCDEFGHIJKLMNOPQRSTUVWXYZ'.","title":"python.library.stdtypes#bytes.isupper"},{"text":"bytes.isupper()  \nbytearray.isupper()  \nReturn True if there is at least one uppercase alphabetic ASCII character in the sequence and no lowercase ASCII characters, False otherwise. For example: >>> b'HELLO WORLD'.isupper()\nTrue\n>>> b'Hello world'.isupper()\nFalse\n Lowercase ASCII characters are those byte values in the sequence b'abcdefghijklmnopqrstuvwxyz'. Uppercase ASCII characters are those byte values in the sequence b'ABCDEFGHIJKLMNOPQRSTUVWXYZ'.","title":"python.library.stdtypes#bytearray.isupper"}]}
{"task_id":16125229,"prompt":"def f_16125229(dict):\n\treturn ","suffix":"","canonical_solution":"list(dict.keys())[-1]","test_start":"\ndef check(candidate):","test":["\n    assert candidate({'t': 1, 'r': 2}) == 'r'\n","\n    assert candidate({'c': 1, 'b': 2, 'a': 1}) == 'a'\n"],"entry_point":"f_16125229","intent":"get the last key of dictionary `dict`","library":[],"docs":[]}
{"task_id":6159900,"prompt":"def f_6159900(f):\n\treturn ","suffix":"","canonical_solution":"print('hi there', file=f)","test_start":"\ndef check(candidate):","test":["\n    file_name = 'a.txt'\n    f = open(file_name, 'w')\n    candidate(f)\n    f.close()\n    with open (file_name, 'r') as f:\n        lines = f.readlines()\n        assert lines[0] == 'hi there\\n'\n"],"entry_point":"f_6159900","intent":"write line \"hi there\" to file `f`","library":[],"docs":[]}
{"task_id":6159900,"prompt":"def f_6159900(myfile):\n\t","suffix":"\n\treturn ","canonical_solution":"\n\tf = open(myfile, 'w')\n\tf.write(\"hi there\\n\")\n\tf.close()\n","test_start":"\ndef check(candidate):","test":["\n    file_name = 'myfile'\n    candidate(file_name)\n    with open (file_name, 'r') as f:\n        lines = f.readlines()\n        assert lines[0] == 'hi there\\n'\n"],"entry_point":"f_6159900","intent":"write line \"hi there\" to file `myfile`","library":[],"docs":[]}
{"task_id":6159900,"prompt":"def f_6159900():\n\t","suffix":"\n\treturn ","canonical_solution":"\n\twith open('somefile.txt', 'a') as the_file: \n\t\tthe_file.write('Hello\\n')\n","test_start":"\ndef check(candidate):","test":["\n    file_name = 'somefile.txt'\n    candidate()\n    with open (file_name, 'r') as f:\n        lines = f.readlines()\n        assert lines[0] == 'Hello\\n'\n"],"entry_point":"f_6159900","intent":"write line \"Hello\" to file `somefile.txt`","library":[],"docs":[]}
{"task_id":19527279,"prompt":"def f_19527279(s):\n\treturn ","suffix":"","canonical_solution":"s.encode('iso-8859-15')","test_start":"\ndef check(candidate):","test":["\n    assert candidate('table') == b'table'\n","\n    assert candidate('hello world!') == b'hello world!'\n"],"entry_point":"f_19527279","intent":"convert unicode string `s` to ascii","library":[],"docs":[]}
{"task_id":356483,"prompt":"def f_356483(text):\n\treturn ","suffix":"","canonical_solution":"re.findall('Test([0-9.]*[0-9]+)', text)","test_start":"\nimport re \n\ndef check(candidate):","test":["\n    assert candidate('Test0.9ssd') == ['0.9']\n","\n    assert candidate('Test0.0 ..2ssd') == ['0.0']\n"],"entry_point":"f_356483","intent":"Find all numbers and dots from a string `text` using regex","library":["re"],"docs":[{"text":"re.S  \nre.DOTALL  \nMake the '.' special character match any character at all, including a newline; without this flag, '.' will match anything except a newline. Corresponds to the inline flag (?s).","title":"python.library.re#re.DOTALL"},{"text":"re.S  \nre.DOTALL  \nMake the '.' special character match any character at all, including a newline; without this flag, '.' will match anything except a newline. Corresponds to the inline flag (?s).","title":"python.library.re#re.S"},{"text":"locale.LC_NUMERIC  \nLocale category for formatting numbers. The functions format(), atoi(), atof() and str() of the locale module are affected by that category. All other numeric formatting operations are not affected.","title":"python.library.locale#locale.LC_NUMERIC"},{"text":"Match.span([group])  \nFor a match m, return the 2-tuple (m.start(group), m.end(group)). Note that if group did not contribute to the match, this is (-1, -1). group defaults to zero, the entire match.","title":"python.library.re#re.Match.span"},{"text":"set_fontsize(fontsize)[source]\n \nSet the font size.  Parameters \n \nfontsizefloat or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}\n\n\nIf float, the fontsize in points. The string values denote sizes relative to the default font size.      See also  font_manager.FontProperties.set_size","title":"matplotlib.text_api#matplotlib.text.Text.set_fontsize"},{"text":"DefaultCookiePolicy.DomainStrictNoDots  \nWhen setting cookies, the \u2018host prefix\u2019 must not contain a dot (eg. www.foo.bar.com can\u2019t set a cookie for .bar.com, because www.foo contains a dot).","title":"python.library.http.cookiejar#http.cookiejar.DefaultCookiePolicy.DomainStrictNoDots"},{"text":"Match.groups(default=None)  \nReturn a tuple containing all the subgroups of the match, from 1 up to however many groups are in the pattern. The default argument is used for groups that did not participate in the match; it defaults to None. For example: >>> m = re.match(r\"(\\d+)\\.(\\d+)\", \"24.1632\")\n>>> m.groups()\n('24', '1632')\n If we make the decimal place and everything after it optional, not all groups might participate in the match. These groups will default to None unless the default argument is given: >>> m = re.match(r\"(\\d+)\\.?(\\d+)?\", \"24\")\n>>> m.groups()      # Second group defaults to None.\n('24', None)\n>>> m.groups('0')   # Now, the second group defaults to '0'.\n('24', '0')","title":"python.library.re#re.Match.groups"},{"text":"set_size(size)[source]\n \nSet the text size.","title":"matplotlib.textpath_api#matplotlib.textpath.TextPath.set_size"},{"text":"set_size(fontsize)[source]\n \nAlias for set_fontsize.","title":"matplotlib.text_api#matplotlib.text.Text.set_size"},{"text":"set_fontsize(size)[source]\n \nSet the text fontsize.","title":"matplotlib.table_api#matplotlib.table.Cell.set_fontsize"}]}
{"task_id":38081866,"prompt":"def f_38081866():\n\treturn ","suffix":"","canonical_solution":"os.system('powershell.exe', 'script.ps1')","test_start":"\nimport os\nfrom unittest.mock import Mock\n\ndef check(candidate):","test":["\n    os.system = Mock()\n    try:\n        candidate()\n        assert True\n    except:\n        assert False\n"],"entry_point":"f_38081866","intent":"execute script 'script.ps1' using 'powershell.exe' shell","library":["os"],"docs":[{"text":"psname\n \nAlias for field number 1","title":"matplotlib.dviread#matplotlib.dviread.PsFont.psname"},{"text":"classmatplotlib.dviread.PsFont(texname, psname, effects, encoding, filename)[source]\n \nBases: tuple Create new instance of PsFont(texname, psname, effects, encoding, filename)   effects\n \nAlias for field number 2 \n   encoding\n \nAlias for field number 3 \n   filename\n \nAlias for field number 4 \n   psname\n \nAlias for field number 1 \n   texname\n \nAlias for field number 0","title":"matplotlib.dviread#matplotlib.dviread.PsFont"},{"text":"msvcrt \u2014 Useful routines from the MS VC++ runtime These functions provide access to some useful capabilities on Windows platforms. Some higher-level modules use these functions to build the Windows implementations of their services. For example, the getpass module uses this in the implementation of the getpass() function. Further documentation on these functions can be found in the Platform API documentation. The module implements both the normal and wide char variants of the console I\/O api. The normal API deals only with ASCII characters and is of limited use for internationalized applications. The wide char API should be used where ever possible.  Changed in version 3.3: Operations in this module now raise OSError where IOError was raised.  File Operations  \nmsvcrt.locking(fd, mode, nbytes)  \nLock part of a file based on file descriptor fd from the C runtime. Raises OSError on failure. The locked region of the file extends from the current file position for nbytes bytes, and may continue beyond the end of the file. mode must be one of the LK_* constants listed below. Multiple regions in a file may be locked at the same time, but may not overlap. Adjacent regions are not merged; they must be unlocked individually. Raises an auditing event msvcrt.locking with arguments fd, mode, nbytes. \n  \nmsvcrt.LK_LOCK  \nmsvcrt.LK_RLCK  \nLocks the specified bytes. If the bytes cannot be locked, the program immediately tries again after 1 second. If, after 10 attempts, the bytes cannot be locked, OSError is raised. \n  \nmsvcrt.LK_NBLCK  \nmsvcrt.LK_NBRLCK  \nLocks the specified bytes. If the bytes cannot be locked, OSError is raised. \n  \nmsvcrt.LK_UNLCK  \nUnlocks the specified bytes, which must have been previously locked. \n  \nmsvcrt.setmode(fd, flags)  \nSet the line-end translation mode for the file descriptor fd. To set it to text mode, flags should be os.O_TEXT; for binary, it should be os.O_BINARY. \n  \nmsvcrt.open_osfhandle(handle, flags)  \nCreate a C runtime file descriptor from the file handle handle. The flags parameter should be a bitwise OR of os.O_APPEND, os.O_RDONLY, and os.O_TEXT. The returned file descriptor may be used as a parameter to os.fdopen() to create a file object. Raises an auditing event msvcrt.open_osfhandle with arguments handle, flags. \n  \nmsvcrt.get_osfhandle(fd)  \nReturn the file handle for the file descriptor fd. Raises OSError if fd is not recognized. Raises an auditing event msvcrt.get_osfhandle with argument fd. \n Console I\/O  \nmsvcrt.kbhit()  \nReturn True if a keypress is waiting to be read. \n  \nmsvcrt.getch()  \nRead a keypress and return the resulting character as a byte string. Nothing is echoed to the console. This call will block if a keypress is not already available, but will not wait for Enter to be pressed. If the pressed key was a special function key, this will return '\\000' or '\\xe0'; the next call will return the keycode. The Control-C keypress cannot be read with this function. \n  \nmsvcrt.getwch()  \nWide char variant of getch(), returning a Unicode value. \n  \nmsvcrt.getche()  \nSimilar to getch(), but the keypress will be echoed if it represents a printable character. \n  \nmsvcrt.getwche()  \nWide char variant of getche(), returning a Unicode value. \n  \nmsvcrt.putch(char)  \nPrint the byte string char to the console without buffering. \n  \nmsvcrt.putwch(unicode_char)  \nWide char variant of putch(), accepting a Unicode value. \n  \nmsvcrt.ungetch(char)  \nCause the byte string char to be \u201cpushed back\u201d into the console buffer; it will be the next character read by getch() or getche(). \n  \nmsvcrt.ungetwch(unicode_char)  \nWide char variant of ungetch(), accepting a Unicode value. \n Other Functions  \nmsvcrt.heapmin()  \nForce the malloc() heap to clean itself up and return unused blocks to the operating system. On failure, this raises OSError.","title":"python.library.msvcrt"},{"text":"executescript(sql_script)  \nThis is a nonstandard shortcut that creates a cursor object by calling the cursor() method, calls the cursor\u2019s executescript() method with the given sql_script, and returns the cursor.","title":"python.library.sqlite3#sqlite3.Connection.executescript"},{"text":"winreg \u2014 Windows registry access These functions expose the Windows registry API to Python. Instead of using an integer as the registry handle, a handle object is used to ensure that the handles are closed correctly, even if the programmer neglects to explicitly close them.  Changed in version 3.3: Several functions in this module used to raise a WindowsError, which is now an alias of OSError.  Functions This module offers the following functions:  \nwinreg.CloseKey(hkey)  \nCloses a previously opened registry key. The hkey argument specifies a previously opened key.  Note If hkey is not closed using this method (or via hkey.Close()), it is closed when the hkey object is destroyed by Python.  \n  \nwinreg.ConnectRegistry(computer_name, key)  \nEstablishes a connection to a predefined registry handle on another computer, and returns a handle object. computer_name is the name of the remote computer, of the form r\"\\\\computername\". If None, the local computer is used. key is the predefined handle to connect to. The return value is the handle of the opened key. If the function fails, an OSError exception is raised. Raises an auditing event winreg.ConnectRegistry with arguments computer_name, key.  Changed in version 3.3: See above.  \n  \nwinreg.CreateKey(key, sub_key)  \nCreates or opens the specified key, returning a handle object. key is an already open key, or one of the predefined HKEY_* constants. sub_key is a string that names the key this method opens or creates. If key is one of the predefined keys, sub_key may be None. In that case, the handle returned is the same key handle passed in to the function. If the key already exists, this function opens the existing key. The return value is the handle of the opened key. If the function fails, an OSError exception is raised. Raises an auditing event winreg.CreateKey with arguments key, sub_key, access. Raises an auditing event winreg.OpenKey\/result with argument key.  Changed in version 3.3: See above.  \n  \nwinreg.CreateKeyEx(key, sub_key, reserved=0, access=KEY_WRITE)  \nCreates or opens the specified key, returning a handle object. key is an already open key, or one of the predefined HKEY_* constants. sub_key is a string that names the key this method opens or creates. reserved is a reserved integer, and must be zero. The default is zero. access is an integer that specifies an access mask that describes the desired security access for the key. Default is KEY_WRITE. See Access Rights for other allowed values. If key is one of the predefined keys, sub_key may be None. In that case, the handle returned is the same key handle passed in to the function. If the key already exists, this function opens the existing key. The return value is the handle of the opened key. If the function fails, an OSError exception is raised. Raises an auditing event winreg.CreateKey with arguments key, sub_key, access. Raises an auditing event winreg.OpenKey\/result with argument key.  New in version 3.2.   Changed in version 3.3: See above.  \n  \nwinreg.DeleteKey(key, sub_key)  \nDeletes the specified key. key is an already open key, or one of the predefined HKEY_* constants. sub_key is a string that must be a subkey of the key identified by the key parameter. This value must not be None, and the key may not have subkeys. This method can not delete keys with subkeys. If the method succeeds, the entire key, including all of its values, is removed. If the method fails, an OSError exception is raised. Raises an auditing event winreg.DeleteKey with arguments key, sub_key, access.  Changed in version 3.3: See above.  \n  \nwinreg.DeleteKeyEx(key, sub_key, access=KEY_WOW64_64KEY, reserved=0)  \nDeletes the specified key.  Note The DeleteKeyEx() function is implemented with the RegDeleteKeyEx Windows API function, which is specific to 64-bit versions of Windows. See the RegDeleteKeyEx documentation.  key is an already open key, or one of the predefined HKEY_* constants. sub_key is a string that must be a subkey of the key identified by the key parameter. This value must not be None, and the key may not have subkeys. reserved is a reserved integer, and must be zero. The default is zero. access is an integer that specifies an access mask that describes the desired security access for the key. Default is KEY_WOW64_64KEY. See Access Rights for other allowed values. This method can not delete keys with subkeys. If the method succeeds, the entire key, including all of its values, is removed. If the method fails, an OSError exception is raised. On unsupported Windows versions, NotImplementedError is raised. Raises an auditing event winreg.DeleteKey with arguments key, sub_key, access.  New in version 3.2.   Changed in version 3.3: See above.  \n  \nwinreg.DeleteValue(key, value)  \nRemoves a named value from a registry key. key is an already open key, or one of the predefined HKEY_* constants. value is a string that identifies the value to remove. Raises an auditing event winreg.DeleteValue with arguments key, value. \n  \nwinreg.EnumKey(key, index)  \nEnumerates subkeys of an open registry key, returning a string. key is an already open key, or one of the predefined HKEY_* constants. index is an integer that identifies the index of the key to retrieve. The function retrieves the name of one subkey each time it is called. It is typically called repeatedly until an OSError exception is raised, indicating, no more values are available. Raises an auditing event winreg.EnumKey with arguments key, index.  Changed in version 3.3: See above.  \n  \nwinreg.EnumValue(key, index)  \nEnumerates values of an open registry key, returning a tuple. key is an already open key, or one of the predefined HKEY_* constants. index is an integer that identifies the index of the value to retrieve. The function retrieves the name of one subkey each time it is called. It is typically called repeatedly, until an OSError exception is raised, indicating no more values. The result is a tuple of 3 items:   \nIndex Meaning   \n0 A string that identifies the value name  \n1 An object that holds the value data, and whose type depends on the underlying registry type  \n2 An integer that identifies the type of the value data (see table in docs for SetValueEx())   Raises an auditing event winreg.EnumValue with arguments key, index.  Changed in version 3.3: See above.  \n  \nwinreg.ExpandEnvironmentStrings(str)  \nExpands environment variable placeholders %NAME% in strings like REG_EXPAND_SZ: >>> ExpandEnvironmentStrings('%windir%')\n'C:\\\\Windows'\n Raises an auditing event winreg.ExpandEnvironmentStrings with argument str. \n  \nwinreg.FlushKey(key)  \nWrites all the attributes of a key to the registry. key is an already open key, or one of the predefined HKEY_* constants. It is not necessary to call FlushKey() to change a key. Registry changes are flushed to disk by the registry using its lazy flusher. Registry changes are also flushed to disk at system shutdown. Unlike CloseKey(), the FlushKey() method returns only when all the data has been written to the registry. An application should only call FlushKey() if it requires absolute certainty that registry changes are on disk.  Note If you don\u2019t know whether a FlushKey() call is required, it probably isn\u2019t.  \n  \nwinreg.LoadKey(key, sub_key, file_name)  \nCreates a subkey under the specified key and stores registration information from a specified file into that subkey. key is a handle returned by ConnectRegistry() or one of the constants HKEY_USERS or HKEY_LOCAL_MACHINE. sub_key is a string that identifies the subkey to load. file_name is the name of the file to load registry data from. This file must have been created with the SaveKey() function. Under the file allocation table (FAT) file system, the filename may not have an extension. A call to LoadKey() fails if the calling process does not have the SE_RESTORE_PRIVILEGE privilege. Note that privileges are different from permissions \u2013 see the RegLoadKey documentation for more details. If key is a handle returned by ConnectRegistry(), then the path specified in file_name is relative to the remote computer. Raises an auditing event winreg.LoadKey with arguments key, sub_key, file_name. \n  \nwinreg.OpenKey(key, sub_key, reserved=0, access=KEY_READ)  \nwinreg.OpenKeyEx(key, sub_key, reserved=0, access=KEY_READ)  \nOpens the specified key, returning a handle object. key is an already open key, or one of the predefined HKEY_* constants. sub_key is a string that identifies the sub_key to open. reserved is a reserved integer, and must be zero. The default is zero. access is an integer that specifies an access mask that describes the desired security access for the key. Default is KEY_READ. See Access Rights for other allowed values. The result is a new handle to the specified key. If the function fails, OSError is raised. Raises an auditing event winreg.OpenKey with arguments key, sub_key, access. Raises an auditing event winreg.OpenKey\/result with argument key.  Changed in version 3.2: Allow the use of named arguments.   Changed in version 3.3: See above.  \n  \nwinreg.QueryInfoKey(key)  \nReturns information about a key, as a tuple. key is an already open key, or one of the predefined HKEY_* constants. The result is a tuple of 3 items:   \nIndex Meaning   \n0 An integer giving the number of sub keys this key has.  \n1 An integer giving the number of values this key has.  \n2 An integer giving when the key was last modified (if available) as 100\u2019s of nanoseconds since Jan 1, 1601.   Raises an auditing event winreg.QueryInfoKey with argument key. \n  \nwinreg.QueryValue(key, sub_key)  \nRetrieves the unnamed value for a key, as a string. key is an already open key, or one of the predefined HKEY_* constants. sub_key is a string that holds the name of the subkey with which the value is associated. If this parameter is None or empty, the function retrieves the value set by the SetValue() method for the key identified by key. Values in the registry have name, type, and data components. This method retrieves the data for a key\u2019s first value that has a NULL name. But the underlying API call doesn\u2019t return the type, so always use QueryValueEx() if possible. Raises an auditing event winreg.QueryValue with arguments key, sub_key, value_name. \n  \nwinreg.QueryValueEx(key, value_name)  \nRetrieves the type and data for a specified value name associated with an open registry key. key is an already open key, or one of the predefined HKEY_* constants. value_name is a string indicating the value to query. The result is a tuple of 2 items:   \nIndex Meaning   \n0 The value of the registry item.  \n1 An integer giving the registry type for this value (see table in docs for SetValueEx())   Raises an auditing event winreg.QueryValue with arguments key, sub_key, value_name. \n  \nwinreg.SaveKey(key, file_name)  \nSaves the specified key, and all its subkeys to the specified file. key is an already open key, or one of the predefined HKEY_* constants. file_name is the name of the file to save registry data to. This file cannot already exist. If this filename includes an extension, it cannot be used on file allocation table (FAT) file systems by the LoadKey() method. If key represents a key on a remote computer, the path described by file_name is relative to the remote computer. The caller of this method must possess the SeBackupPrivilege security privilege. Note that privileges are different than permissions \u2013 see the Conflicts Between User Rights and Permissions documentation for more details. This function passes NULL for security_attributes to the API. Raises an auditing event winreg.SaveKey with arguments key, file_name. \n  \nwinreg.SetValue(key, sub_key, type, value)  \nAssociates a value with a specified key. key is an already open key, or one of the predefined HKEY_* constants. sub_key is a string that names the subkey with which the value is associated. type is an integer that specifies the type of the data. Currently this must be REG_SZ, meaning only strings are supported. Use the SetValueEx() function for support for other data types. value is a string that specifies the new value. If the key specified by the sub_key parameter does not exist, the SetValue function creates it. Value lengths are limited by available memory. Long values (more than 2048 bytes) should be stored as files with the filenames stored in the configuration registry. This helps the registry perform efficiently. The key identified by the key parameter must have been opened with KEY_SET_VALUE access. Raises an auditing event winreg.SetValue with arguments key, sub_key, type, value. \n  \nwinreg.SetValueEx(key, value_name, reserved, type, value)  \nStores data in the value field of an open registry key. key is an already open key, or one of the predefined HKEY_* constants. value_name is a string that names the subkey with which the value is associated. reserved can be anything \u2013 zero is always passed to the API. type is an integer that specifies the type of the data. See Value Types for the available types. value is a string that specifies the new value. This method can also set additional value and type information for the specified key. The key identified by the key parameter must have been opened with KEY_SET_VALUE access. To open the key, use the CreateKey() or OpenKey() methods. Value lengths are limited by available memory. Long values (more than 2048 bytes) should be stored as files with the filenames stored in the configuration registry. This helps the registry perform efficiently. Raises an auditing event winreg.SetValue with arguments key, sub_key, type, value. \n  \nwinreg.DisableReflectionKey(key)  \nDisables registry reflection for 32-bit processes running on a 64-bit operating system. key is an already open key, or one of the predefined HKEY_* constants. Will generally raise NotImplementedError if executed on a 32-bit operating system. If the key is not on the reflection list, the function succeeds but has no effect. Disabling reflection for a key does not affect reflection of any subkeys. Raises an auditing event winreg.DisableReflectionKey with argument key. \n  \nwinreg.EnableReflectionKey(key)  \nRestores registry reflection for the specified disabled key. key is an already open key, or one of the predefined HKEY_* constants. Will generally raise NotImplementedError if executed on a 32-bit operating system. Restoring reflection for a key does not affect reflection of any subkeys. Raises an auditing event winreg.EnableReflectionKey with argument key. \n  \nwinreg.QueryReflectionKey(key)  \nDetermines the reflection state for the specified key. key is an already open key, or one of the predefined HKEY_* constants. Returns True if reflection is disabled. Will generally raise NotImplementedError if executed on a 32-bit operating system. Raises an auditing event winreg.QueryReflectionKey with argument key. \n Constants The following constants are defined for use in many _winreg functions. HKEY_* Constants  \nwinreg.HKEY_CLASSES_ROOT  \nRegistry entries subordinate to this key define types (or classes) of documents and the properties associated with those types. Shell and COM applications use the information stored under this key. \n  \nwinreg.HKEY_CURRENT_USER  \nRegistry entries subordinate to this key define the preferences of the current user. These preferences include the settings of environment variables, data about program groups, colors, printers, network connections, and application preferences. \n  \nwinreg.HKEY_LOCAL_MACHINE  \nRegistry entries subordinate to this key define the physical state of the computer, including data about the bus type, system memory, and installed hardware and software. \n  \nwinreg.HKEY_USERS  \nRegistry entries subordinate to this key define the default user configuration for new users on the local computer and the user configuration for the current user. \n  \nwinreg.HKEY_PERFORMANCE_DATA  \nRegistry entries subordinate to this key allow you to access performance data. The data is not actually stored in the registry; the registry functions cause the system to collect the data from its source. \n  \nwinreg.HKEY_CURRENT_CONFIG  \nContains information about the current hardware profile of the local computer system. \n  \nwinreg.HKEY_DYN_DATA  \nThis key is not used in versions of Windows after 98. \n Access Rights For more information, see Registry Key Security and Access.  \nwinreg.KEY_ALL_ACCESS  \nCombines the STANDARD_RIGHTS_REQUIRED, KEY_QUERY_VALUE, KEY_SET_VALUE, KEY_CREATE_SUB_KEY, KEY_ENUMERATE_SUB_KEYS, KEY_NOTIFY, and KEY_CREATE_LINK access rights. \n  \nwinreg.KEY_WRITE  \nCombines the STANDARD_RIGHTS_WRITE, KEY_SET_VALUE, and KEY_CREATE_SUB_KEY access rights. \n  \nwinreg.KEY_READ  \nCombines the STANDARD_RIGHTS_READ, KEY_QUERY_VALUE, KEY_ENUMERATE_SUB_KEYS, and KEY_NOTIFY values. \n  \nwinreg.KEY_EXECUTE  \nEquivalent to KEY_READ. \n  \nwinreg.KEY_QUERY_VALUE  \nRequired to query the values of a registry key. \n  \nwinreg.KEY_SET_VALUE  \nRequired to create, delete, or set a registry value. \n  \nwinreg.KEY_CREATE_SUB_KEY  \nRequired to create a subkey of a registry key. \n  \nwinreg.KEY_ENUMERATE_SUB_KEYS  \nRequired to enumerate the subkeys of a registry key. \n  \nwinreg.KEY_NOTIFY  \nRequired to request change notifications for a registry key or for subkeys of a registry key. \n  \nwinreg.KEY_CREATE_LINK  \nReserved for system use. \n 64-bit Specific For more information, see Accessing an Alternate Registry View.  \nwinreg.KEY_WOW64_64KEY  \nIndicates that an application on 64-bit Windows should operate on the 64-bit registry view. \n  \nwinreg.KEY_WOW64_32KEY  \nIndicates that an application on 64-bit Windows should operate on the 32-bit registry view. \n Value Types For more information, see Registry Value Types.  \nwinreg.REG_BINARY  \nBinary data in any form. \n  \nwinreg.REG_DWORD  \n32-bit number. \n  \nwinreg.REG_DWORD_LITTLE_ENDIAN  \nA 32-bit number in little-endian format. Equivalent to REG_DWORD. \n  \nwinreg.REG_DWORD_BIG_ENDIAN  \nA 32-bit number in big-endian format. \n  \nwinreg.REG_EXPAND_SZ  \nNull-terminated string containing references to environment variables (%PATH%). \n  \nwinreg.REG_LINK  \nA Unicode symbolic link. \n  \nwinreg.REG_MULTI_SZ  \nA sequence of null-terminated strings, terminated by two null characters. (Python handles this termination automatically.) \n  \nwinreg.REG_NONE  \nNo defined value type. \n  \nwinreg.REG_QWORD  \nA 64-bit number.  New in version 3.6.  \n  \nwinreg.REG_QWORD_LITTLE_ENDIAN  \nA 64-bit number in little-endian format. Equivalent to REG_QWORD.  New in version 3.6.  \n  \nwinreg.REG_RESOURCE_LIST  \nA device-driver resource list. \n  \nwinreg.REG_FULL_RESOURCE_DESCRIPTOR  \nA hardware setting. \n  \nwinreg.REG_RESOURCE_REQUIREMENTS_LIST  \nA hardware resource list. \n  \nwinreg.REG_SZ  \nA null-terminated string. \n Registry Handle Objects This object wraps a Windows HKEY object, automatically closing it when the object is destroyed. To guarantee cleanup, you can call either the Close() method on the object, or the CloseKey() function. All registry functions in this module return one of these objects. All registry functions in this module which accept a handle object also accept an integer, however, use of the handle object is encouraged. Handle objects provide semantics for __bool__() \u2013 thus if handle:\n    print(\"Yes\")\n will print Yes if the handle is currently valid (has not been closed or detached). The object also support comparison semantics, so handle objects will compare true if they both reference the same underlying Windows handle value. Handle objects can be converted to an integer (e.g., using the built-in int() function), in which case the underlying Windows handle value is returned. You can also use the Detach() method to return the integer handle, and also disconnect the Windows handle from the handle object.  \nPyHKEY.Close()  \nCloses the underlying Windows handle. If the handle is already closed, no error is raised. \n  \nPyHKEY.Detach()  \nDetaches the Windows handle from the handle object. The result is an integer that holds the value of the handle before it is detached. If the handle is already detached or closed, this will return zero. After calling this function, the handle is effectively invalidated, but the handle is not closed. You would call this function when you need the underlying Win32 handle to exist beyond the lifetime of the handle object. Raises an auditing event winreg.PyHKEY.Detach with argument key. \n  \nPyHKEY.__enter__()  \nPyHKEY.__exit__(*exc_info)  \nThe HKEY object implements __enter__() and __exit__() and thus supports the context protocol for the with statement: with OpenKey(HKEY_LOCAL_MACHINE, \"foo\") as key:\n    ...  # work with key\n will automatically close key when control leaves the with block.","title":"python.library.winreg"},{"text":"dwFlags  \nA bit field that determines whether certain STARTUPINFO attributes are used when the process creates a window. si = subprocess.STARTUPINFO()\nsi.dwFlags = subprocess.STARTF_USESTDHANDLES | subprocess.STARTF_USESHOWWINDOW","title":"python.library.subprocess#subprocess.STARTUPINFO.dwFlags"},{"text":"winreg.KEY_EXECUTE  \nEquivalent to KEY_READ.","title":"python.library.winreg#winreg.KEY_EXECUTE"},{"text":"modulefinder \u2014 Find modules used by a script Source code: Lib\/modulefinder.py This module provides a ModuleFinder class that can be used to determine the set of modules imported by a script. modulefinder.py can also be run as a script, giving the filename of a Python script as its argument, after which a report of the imported modules will be printed.  \nmodulefinder.AddPackagePath(pkg_name, path)  \nRecord that the package named pkg_name can be found in the specified path. \n  \nmodulefinder.ReplacePackage(oldname, newname)  \nAllows specifying that the module named oldname is in fact the package named newname. \n  \nclass modulefinder.ModuleFinder(path=None, debug=0, excludes=[], replace_paths=[])  \nThis class provides run_script() and report() methods to determine the set of modules imported by a script. path can be a list of directories to search for modules; if not specified, sys.path is used. debug sets the debugging level; higher values make the class print debugging messages about what it\u2019s doing. excludes is a list of module names to exclude from the analysis. replace_paths is a list of (oldpath, newpath) tuples that will be replaced in module paths.  \nreport()  \nPrint a report to standard output that lists the modules imported by the script and their paths, as well as modules that are missing or seem to be missing. \n  \nrun_script(pathname)  \nAnalyze the contents of the pathname file, which must contain Python code. \n  \nmodules  \nA dictionary mapping module names to modules. See Example usage of ModuleFinder. \n \n Example usage of ModuleFinder The script that is going to get analyzed later on (bacon.py): import re, itertools\n\ntry:\n    import baconhameggs\nexcept ImportError:\n    pass\n\ntry:\n    import guido.python.ham\nexcept ImportError:\n    pass\n The script that will output the report of bacon.py: from modulefinder import ModuleFinder\n\nfinder = ModuleFinder()\nfinder.run_script('bacon.py')\n\nprint('Loaded modules:')\nfor name, mod in finder.modules.items():\n    print('%s: ' % name, end='')\n    print(','.join(list(mod.globalnames.keys())[:3]))\n\nprint('-'*50)\nprint('Modules not imported:')\nprint('\\n'.join(finder.badmodules.keys()))\n Sample output (may vary depending on the architecture): Loaded modules:\n_types:\ncopyreg:  _inverted_registry,_slotnames,__all__\nsre_compile:  isstring,_sre,_optimize_unicode\n_sre:\nsre_constants:  REPEAT_ONE,makedict,AT_END_LINE\nsys:\nre:  __module__,finditer,_expand\nitertools:\n__main__:  re,itertools,baconhameggs\nsre_parse:  _PATTERNENDERS,SRE_FLAG_UNICODE\narray:\ntypes:  __module__,IntType,TypeType\n---------------------------------------------------\nModules not imported:\nguido.python.ham\nbaconhameggs","title":"python.library.modulefinder"},{"text":"run_script(pathname)  \nAnalyze the contents of the pathname file, which must contain Python code.","title":"python.library.modulefinder#modulefinder.ModuleFinder.run_script"},{"text":"msvcrt.putch(char)  \nPrint the byte string char to the console without buffering.","title":"python.library.msvcrt#msvcrt.putch"}]}
{"task_id":7349646,"prompt":"def f_7349646(b):\n\t","suffix":"\n\treturn b","canonical_solution":"b.sort(key=lambda x: x[2])","test_start":"\ndef check(candidate):","test":["\n    b = [(1,2,3), (4,5,6), (7,8,0)]\n    assert candidate(b) == [(7,8,0), (1,2,3), (4,5,6)]\n","\n    b = [(1,2,'a'), (4,5,'c'), (7,8,'A')]\n    assert candidate(b) == [(7,8,'A'), (1,2,'a'), (4,5,'c')]\n"],"entry_point":"f_7349646","intent":"Sort a list of tuples `b` by third item in the tuple","library":[],"docs":[]}
{"task_id":10607688,"prompt":"def f_10607688():\n\treturn ","suffix":"","canonical_solution":"datetime.datetime.now()","test_start":"\nimport datetime\n\ndef check(candidate):","test":["\n    y = candidate()\n    assert y.year >= 2022\n"],"entry_point":"f_10607688","intent":"create a datetime with the current date & time","library":["datetime"],"docs":[{"text":"classmethod datetime.utcnow()  \nReturn the current UTC date and time, with tzinfo None. This is like now(), but returns the current UTC date and time, as a naive datetime object. An aware current UTC datetime can be obtained by calling datetime.now(timezone.utc). See also now().  Warning Because naive datetime objects are treated by many datetime methods as local times, it is preferred to use aware datetimes to represent times in UTC. As such, the recommended way to create an object representing the current time in UTC is by calling datetime.now(timezone.utc).","title":"python.library.datetime#datetime.datetime.utcnow"},{"text":"classmethod datetime.today()  \nReturn the current local datetime, with tzinfo None. Equivalent to: datetime.fromtimestamp(time.time())\n See also now(), fromtimestamp(). This method is functionally equivalent to now(), but without a tz parameter.","title":"python.library.datetime#datetime.datetime.today"},{"text":"classmethod datetime.now(tz=None)  \nReturn the current local date and time. If optional argument tz is None or not specified, this is like today(), but, if possible, supplies more precision than can be gotten from going through a time.time() timestamp (for example, this may be possible on platforms supplying the C gettimeofday() function). If tz is not None, it must be an instance of a tzinfo subclass, and the current date and time are converted to tz\u2019s time zone. This function is preferred over today() and utcnow().","title":"python.library.datetime#datetime.datetime.now"},{"text":"now()  \nReturns a datetime that represents the current point in time. Exactly what\u2019s returned depends on the value of USE_TZ:  If USE_TZ is False, this will be a naive datetime (i.e. a datetime without an associated timezone) that represents the current time in the system\u2019s local timezone. If USE_TZ is True, this will be an aware datetime representing the current time in UTC. Note that now() will always return times in UTC regardless of the value of TIME_ZONE; you can use localtime() to get the time in the current time zone.","title":"django.ref.utils#django.utils.timezone.now"},{"text":"classmethod date.today()  \nReturn the current local date. This is equivalent to date.fromtimestamp(time.time()).","title":"python.library.datetime#datetime.date.today"},{"text":"class Now","title":"django.ref.models.database-functions#django.db.models.functions.Now"},{"text":"pandas.Timestamp.now   classmethodTimestamp.now(tz=None)\n \nReturn new Timestamp object representing current time local to tz.  Parameters \n \ntz:str or timezone object, default None\n\n\nTimezone to localize to.     Examples \n>>> pd.Timestamp.now()  \nTimestamp('2020-11-16 22:06:16.378782')\n  Analogous for pd.NaT: \n>>> pd.NaT.now()\nNaT","title":"pandas.reference.api.pandas.timestamp.now"},{"text":"pandas.Timestamp.today   classmethodTimestamp.today(cls, tz=None)\n \nReturn the current time in the local timezone. This differs from datetime.today() in that it can be localized to a passed timezone.  Parameters \n \ntz:str or timezone object, default None\n\n\nTimezone to localize to.     Examples \n>>> pd.Timestamp.today()    \nTimestamp('2020-11-16 22:37:39.969883')\n  Analogous for pd.NaT: \n>>> pd.NaT.today()\nNaT","title":"pandas.reference.api.pandas.timestamp.today"},{"text":"datetime.date()  \nReturn date object with same year, month and day.","title":"python.library.datetime#datetime.datetime.date"},{"text":"pandas.Timestamp.utcnow   classmethodTimestamp.utcnow()\n \nReturn a new Timestamp representing UTC day and time. Examples \n>>> pd.Timestamp.utcnow()   \nTimestamp('2020-11-16 22:50:18.092888+0000', tz='UTC')","title":"pandas.reference.api.pandas.timestamp.utcnow"}]}
{"task_id":30843103,"prompt":"def f_30843103(lst):\n\treturn ","suffix":"","canonical_solution":"next(i for i, x in enumerate(lst) if not isinstance(x, bool) and x == 1)","test_start":"\ndef check(candidate):","test":["\n    lst = [True, False, 1, 3]\n    assert candidate(lst) == 2\n"],"entry_point":"f_30843103","intent":"get the index of an integer `1` from a list `lst` if the list also contains boolean items","library":[],"docs":[]}
{"task_id":4918425,"prompt":"def f_4918425(a):\n\t","suffix":"\n\treturn a","canonical_solution":"a[:] = [(x - 13) for x in a]","test_start":"\ndef check(candidate):","test":["\n    a = [14, 15]\n    candidate(a)\n    assert a == [1, 2]\n","\n    a = [float(x) for x in range(13, 20)]\n    candidate(a)\n    assert a == [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0]\n"],"entry_point":"f_4918425","intent":"subtract 13 from every number in a list `a`","library":[],"docs":[]}
{"task_id":17794266,"prompt":"def f_17794266(x):\n\treturn ","suffix":"","canonical_solution":"max(x.min(), x.max(), key=abs)","test_start":"\nimport numpy as np \n\ndef check(candidate):","test":["\n    x = np.matrix([[1, 1], [2, -3]])\n    assert candidate(x) == -3\n"],"entry_point":"f_17794266","intent":"get the highest element in absolute value in a numpy matrix `x`","library":["numpy"],"docs":[{"text":"numpy.matrix.argmax method   matrix.argmax(axis=None, out=None)[source]\n \nIndexes of the maximum values along an axis. Return the indexes of the first occurrences of the maximum values along the specified axis. If axis is None, the index is for the flattened matrix.  Parameters \n See `numpy.argmax` for complete descriptions\n    See also  numpy.argmax\n  Notes This is the same as ndarray.argmax, but returns a matrix object where ndarray.argmax would return an ndarray. Examples >>> x = np.matrix(np.arange(12).reshape((3,4))); x\nmatrix([[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]])\n>>> x.argmax()\n11\n>>> x.argmax(0)\nmatrix([[2, 2, 2, 2]])\n>>> x.argmax(1)\nmatrix([[3],\n        [3],\n        [3]])","title":"numpy.reference.generated.numpy.matrix.argmax"},{"text":"numpy.matrix.max method   matrix.max(axis=None, out=None)[source]\n \nReturn the maximum value along an axis.  Parameters \n See `amax` for complete descriptions\n    See also  \namax, ndarray.max\n\n  Notes This is the same as ndarray.max, but returns a matrix object where ndarray.max would return an ndarray. Examples >>> x = np.matrix(np.arange(12).reshape((3,4))); x\nmatrix([[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]])\n>>> x.max()\n11\n>>> x.max(0)\nmatrix([[ 8,  9, 10, 11]])\n>>> x.max(1)\nmatrix([[ 3],\n        [ 7],\n        [11]])","title":"numpy.reference.generated.numpy.matrix.max"},{"text":"numpy.ndarray.max method   ndarray.max(axis=None, out=None, keepdims=False, initial=<no value>, where=True)\n \nReturn the maximum along a given axis. Refer to numpy.amax for full documentation.  See also  numpy.amax\n\nequivalent function","title":"numpy.reference.generated.numpy.ndarray.max"},{"text":"numpy.record.argmax method   record.argmax()\n \nScalar method identical to the corresponding array attribute. Please see ndarray.argmax.","title":"numpy.reference.generated.numpy.record.argmax"},{"text":"abs(x)  \nReturn the absolute value of a number. The argument may be an integer, a floating point number, or an object implementing __abs__(). If the argument is a complex number, its magnitude is returned.","title":"python.library.functions#abs"},{"text":"numpy.ma.MaskedArray.argmax method   ma.MaskedArray.argmax(axis=None, fill_value=None, out=None, *, keepdims=<no value>)[source]\n \nReturns array of indices of the maximum values along the given axis. Masked values are treated as if they had the value fill_value.  Parameters \n \naxis{None, integer}\n\n\nIf None, the index is into the flattened array, otherwise along the specified axis  \nfill_valuescalar or None, optional\n\n\nValue used to fill in the masked values. If None, the output of maximum_fill_value(self._data) is used instead.  \nout{None, array}, optional\n\n\nArray into which the result can be placed. Its type is preserved and it must be of the right shape to hold the output.    Returns \n \nindex_array{integer_array}\n\n   Examples >>> a = np.arange(6).reshape(2,3)\n>>> a.argmax()\n5\n>>> a.argmax(0)\narray([1, 1, 1])\n>>> a.argmax(1)\narray([2, 2])","title":"numpy.reference.generated.numpy.ma.maskedarray.argmax"},{"text":"numpy.recarray.max method   recarray.max(axis=None, out=None, keepdims=False, initial=<no value>, where=True)\n \nReturn the maximum along a given axis. Refer to numpy.amax for full documentation.  See also  numpy.amax\n\nequivalent function","title":"numpy.reference.generated.numpy.recarray.max"},{"text":"sklearn.utils.arrayfuncs.min_pos()  \nFind the minimum value of an array over positive values Returns a huge value if none of the values are positive","title":"sklearn.modules.generated.sklearn.utils.arrayfuncs.min_pos#sklearn.utils.arrayfuncs.min_pos"},{"text":"numpy.record.max method   record.max()\n \nScalar method identical to the corresponding array attribute. Please see ndarray.max.","title":"numpy.reference.generated.numpy.record.max"},{"text":"numpy.recarray.argmax method   recarray.argmax(axis=None, out=None)\n \nReturn indices of the maximum values along the given axis. Refer to numpy.argmax for full documentation.  See also  numpy.argmax\n\nequivalent function","title":"numpy.reference.generated.numpy.recarray.argmax"}]}
{"task_id":30551576,"prompt":"def f_30551576(s):\n\treturn ","suffix":"","canonical_solution":"re.findall(r'\"(http.*?)\"', s, re.MULTILINE | re.DOTALL)","test_start":"\nimport re\n\ndef check(candidate):","test":["\n    s = (\n      '     [irrelevant javascript code here]'\n      '     sources:[{file:\"http:\/\/url.com\/folder1\/v.html\",label:\"label1\"},'\n      '     {file:\"http:\/\/url.com\/folder2\/v.html\",label:\"label2\"},'\n      '     {file:\"http:\/\/url.com\/folder3\/v.html\",label:\"label3\"}],'\n      '     [irrelevant javascript code here]'\n    )\n    assert candidate(s) == ['http:\/\/url.com\/folder1\/v.html', 'http:\/\/url.com\/folder2\/v.html', 'http:\/\/url.com\/folder3\/v.html']\n","\n    s = (\n      '     [irrelevant javascript code here]'\n      '     [irrelevant python code here]'\n    )\n    assert candidate(s) == []\n"],"entry_point":"f_30551576","intent":"Get all urls within text `s`","library":["re"],"docs":[{"text":"get_urls()[source]\n \nReturn a list of URLs, one for each element of the collection. The list contains None for elements without a URL. See Hyperlinks for an example.","title":"matplotlib.collections_api#matplotlib.collections.LineCollection.get_urls"},{"text":"get_glyphs_tex(prop, s, glyph_map=None, return_new_glyphs_only=False)[source]\n \nConvert the string s to vertices and codes using usetex mode.","title":"matplotlib.textpath_api#matplotlib.textpath.TextToPath.get_glyphs_tex"},{"text":"get_glyphs_with_font(font, s, glyph_map=None, return_new_glyphs_only=False)[source]\n \nConvert string s to vertices and codes using the provided ttf font.","title":"matplotlib.textpath_api#matplotlib.textpath.TextToPath.get_glyphs_with_font"},{"text":"get_urls()[source]\n \nReturn a list of URLs, one for each element of the collection. The list contains None for elements without a URL. See Hyperlinks for an example.","title":"matplotlib.collections_api#matplotlib.collections.PatchCollection.get_urls"},{"text":"get_urls()[source]\n \nReturn a list of URLs, one for each element of the collection. The list contains None for elements without a URL. See Hyperlinks for an example.","title":"matplotlib.collections_api#matplotlib.collections.RegularPolyCollection.get_urls"},{"text":"get_urls()[source]\n \nReturn a list of URLs, one for each element of the collection. The list contains None for elements without a URL. See Hyperlinks for an example.","title":"matplotlib.collections_api#matplotlib.collections.PolyCollection.get_urls"},{"text":"get_urls()[source]\n \nReturn a list of URLs, one for each element of the collection. The list contains None for elements without a URL. See Hyperlinks for an example.","title":"matplotlib.collections_api#matplotlib.collections.StarPolygonCollection.get_urls"},{"text":"werkzeug.urls.url_unquote(s, charset='utf-8', errors='replace', unsafe='')  \nURL decode a single string with a given encoding. If the charset is set to None no decoding is performed and raw bytes are returned.  Parameters \n \ns (Union[str, bytes]) \u2013 the string to unquote. \ncharset (str) \u2013 the charset of the query string. If set to None no decoding will take place. \nerrors (str) \u2013 the error handling for the charset decoding. \nunsafe (str) \u2013    Return type \nstr","title":"werkzeug.urls.index#werkzeug.urls.url_unquote"},{"text":"get_urls()[source]\n \nReturn a list of URLs, one for each element of the collection. The list contains None for elements without a URL. See Hyperlinks for an example.","title":"matplotlib.collections_api#matplotlib.collections.AsteriskPolygonCollection.get_urls"},{"text":"classmethod escape(s)  \nEscape a string. Calls escape() and ensures that for subclasses the correct type is returned.  Parameters \ns (Any) \u2013   Return type \nmarkupsafe.Markup","title":"flask.api.index#flask.Markup.escape"}]}
{"task_id":113534,"prompt":"def f_113534(mystring):\n\treturn ","suffix":"","canonical_solution":"mystring.replace(' ', '! !').split('!')","test_start":"\ndef check(candidate):","test":["\n    assert candidate(\"This is the string I want to split\") ==       ['This',' ','is',' ','the',' ','string',' ','I',' ','want',' ','to',' ','split']\n"],"entry_point":"f_113534","intent":"split a string `mystring` considering the spaces ' '","library":[],"docs":[]}
{"task_id":5838735,"prompt":"def f_5838735(path):\n\treturn ","suffix":"","canonical_solution":"open(path, 'r')","test_start":"\ndef check(candidate):","test":["\n    with open('tmp.txt', 'w') as fw: fw.write('hello world!')\n    f = candidate('tmp.txt')\n    assert f.name == 'tmp.txt'\n    assert f.mode == 'r'\n"],"entry_point":"f_5838735","intent":"open file `path` with mode 'r'","library":[],"docs":[]}
{"task_id":36003967,"prompt":"def f_36003967(data):\n\treturn ","suffix":"","canonical_solution":"[[sum(item) for item in zip(*items)] for items in zip(*data)]","test_start":"\ndef check(candidate):","test":["\n    data = [[[5, 10, 30, 24, 100], [1, 9, 25, 49, 81]],\n            [[15, 10, 10, 16, 70], [10, 1, 25, 11, 19]],\n            [[34, 20, 10, 10, 30], [9, 20, 25, 30, 80]]]\n    assert candidate(data) == [[54, 40, 50, 50, 200], [20, 30, 75, 90, 180]]\n"],"entry_point":"f_36003967","intent":"sum elements at the same index in list `data`","library":[],"docs":[]}
{"task_id":7635237,"prompt":"def f_7635237(a):\n\treturn ","suffix":"","canonical_solution":"a[:, (np.newaxis)]","test_start":"\nimport numpy as np \n\ndef check(candidate):","test":["\n    data = np.array([[[5, 10, 30, 24, 100], [1, 9, 25, 49, 81]],\n            [[15, 10, 10, 16, 70], [10, 1, 25, 11, 19]],\n            [[34, 20, 10, 10, 30], [9, 20, 25, 30, 80]]])\n    assert candidate(data).tolist() == [[[[  5,  10,  30,  24, 100],\n         [  1,   9,  25,  49,  81]]],\n       [[[ 15,  10,  10,  16,  70],\n         [ 10,   1,  25,  11,  19]]],\n       [[[ 34,  20,  10,  10,  30],\n         [  9,  20,  25,  30,  80]]]]\n"],"entry_point":"f_7635237","intent":"add a new axis to array `a`","library":["numpy"],"docs":[{"text":"matplotlib.axes.Axes.add_child_axes   Axes.add_child_axes(ax)[source]\n \nAdd an AxesBase to the Axes' children; return the child Axes. This is the lowlevel version. See axes.Axes.inset_axes.","title":"matplotlib._as_gen.matplotlib.axes.axes.add_child_axes"},{"text":"numpy.newaxis\n \nA convenient alias for None, useful for indexing arrays. Examples >>> newaxis is None\nTrue\n>>> x = np.arange(3)\n>>> x\narray([0, 1, 2])\n>>> x[:, newaxis]\narray([[0],\n[1],\n[2]])\n>>> x[:, newaxis, newaxis]\narray([[[0]],\n[[1]],\n[[2]]])\n>>> x[:, newaxis] * x\narray([[0, 0, 0],\n[0, 1, 2],\n[0, 2, 4]])\n Outer product, same as outer(x, y): >>> y = np.arange(3, 6)\n>>> x[:, newaxis] * y\narray([[ 0,  0,  0],\n[ 3,  4,  5],\n[ 6,  8, 10]])\n x[newaxis, :] is equivalent to x[newaxis] and x[None]: >>> x[newaxis, :].shape\n(1, 3)\n>>> x[newaxis].shape\n(1, 3)\n>>> x[None].shape\n(1, 3)\n>>> x[:, newaxis].shape\n(3, 1)","title":"numpy.reference.constants#numpy.newaxis"},{"text":"register_axis(axis)[source]\n \nRegister an axis. An axis should be registered with its corresponding spine from the Axes instance. This allows the spine to clear any axis properties when needed.","title":"matplotlib.spines_api#matplotlib.spines.Spine.register_axis"},{"text":"set_axis(axis)[source]","title":"matplotlib.ticker_api#matplotlib.ticker.TickHelper.set_axis"},{"text":"append_axes(position, size, pad=None, add_to_figure=<deprecated parameter>, **kwargs)[source]\n \nCreate an axes at the given position with the same height (or width) of the main axes.  position\n\n[\"left\"|\"right\"|\"bottom\"|\"top\"]   size and pad should be axes_grid.axes_size compatible.","title":"matplotlib._as_gen.mpl_toolkits.axes_grid1.axes_divider.axesdivider#mpl_toolkits.axes_grid1.axes_divider.AxesDivider.append_axes"},{"text":"axis=None","title":"matplotlib.ticker_api#matplotlib.ticker.TickHelper.axis"},{"text":"create_dummy_axis(**kwargs)[source]","title":"matplotlib.ticker_api#matplotlib.ticker.TickHelper.create_dummy_axis"},{"text":"sca(a)[source]\n \nSet the current Axes to be a and return a.","title":"matplotlib.figure_api#matplotlib.figure.Figure.sca"},{"text":"new_fixed_axis(loc, nth_coord=None, axis_direction=None, offset=None, axes=None)[source]","title":"matplotlib._as_gen.mpl_toolkits.axisartist.grid_helper_curvelinear.gridhelpercurvelinear#mpl_toolkits.axisartist.grid_helper_curvelinear.GridHelperCurveLinear.new_fixed_axis"},{"text":"pandas.Series.set_axis   Series.set_axis(labels, axis=0, inplace=False)[source]\n \nAssign desired index to given axis. Indexes for row labels can be changed by assigning a list-like or Index.  Parameters \n \nlabels:list-like, Index\n\n\nThe values for the new index.  \naxis:{0 or \u2018index\u2019}, default 0\n\n\nThe axis to update. The value 0 identifies the rows.  \ninplace:bool, default False\n\n\nWhether to return a new Series instance.    Returns \n \nrenamed:Series or None\n\n\nAn object of type Series or None if inplace=True.      See also  Series.rename_axis\n\nAlter the name of the index.    Examples \n>>> s = pd.Series([1, 2, 3])\n>>> s\n0    1\n1    2\n2    3\ndtype: int64\n  \n>>> s.set_axis(['a', 'b', 'c'], axis=0)\na    1\nb    2\nc    3\ndtype: int64","title":"pandas.reference.api.pandas.series.set_axis"}]}
